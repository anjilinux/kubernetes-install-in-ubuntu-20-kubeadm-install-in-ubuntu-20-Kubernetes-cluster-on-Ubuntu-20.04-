 FINAL  may 2023 k8 cluster k8-cluster kubeadm cluster k8 version 1.27.* k8-cluster new version kubeadm cluster k8-new version cluster  KUBERNETES  CLUSTER SUCCESS
Kubernetes Cluster on Ubuntu 20.04
Kubernetes install  on Ubuntu 20.04
Kubernetes install  in  Ubuntu 20.04 
How To Setup Kubernetes Cluster Using Kubeadm
https://devopscube.com/setup-kubernetes-cluster-kubeadm/

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
--------------------------
sudo modprobe overlay
sudo modprobe br_netfilter
-----------------------------
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF
------------------------------------------
sudo sysctl --system
----------------------------------
sudo systemctl stop firewalld
sudo apt install curl
sudo swapoff -a
(crontab -l 2>/dev/null; echo "@reboot /sbin/swapoff -a") | crontab - || true
-----------------------------------
cat <<EOF | sudo tee /etc/modules-load.d/crio.conf
overlay
br_netfilter
EOF 
-----------------------------------
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
-----------------------------
sudo modprobe overlay
sudo modprobe br_netfilter
--------------------------------------
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
------------------------------
 sudo sysctl --system
--------------------
OS="xUbuntu_20.04"

VERSION="1.23"

cat <<EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list
deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /
EOF
cat <<EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list
deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /
EOF
---------------------------------
curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers.gpg add -
curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers.gpg add -
--------------------------------
sudo apt-get update
sudo apt-get install cri-o cri-o-runc cri-tools -y
------------------------------
sudo systemctl daemon-reload
sudo systemctl enable crio --now
----------------------------------------
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
---------------------------------------------
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
------------------------------
sudo apt-get update -y
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
-#-------------------------- OR   OR  OR  OR  
  sudo apt update
  apt-cache madison kubeadm | tac
  sudo apt-get install -y kubelet=1.26.1-00 kubectl=1.26.1-00 kubeadm=1.26.1-00
  sudo apt-mark hold kubelet kubeadm kubectl
========--------------------
sudo apt-get install -y jq
local_ip="$(ip --json a s | jq -r '.[] | if .ifname == "eth1" then .addr_info[] | if .family == "inet" then .local else empty end else empty end')"
cat > /etc/default/kubelet << EOF
KUBELET_EXTRA_ARGS=--node-ip=$local_ip
EOF
-----------------------------  SOME EXTRS USING VARIABLES     {{{}{{SOME TIKES USES }{}}}}
IPADDR="10.0.0.10"
NODENAME=$(hostname -s)
POD_CIDR="192.168.0.0/16"

IPADDR=$(curl ifconfig.me && echo "")
NODENAME=$(hostname -s)
POD_CIDR="192.168.0.0/16"
------------
sudo kubeadm init --apiserver-advertise-address=$IPADDR  --apiserver-cert-extra-sans=$IPADDR  --pod-network-cidr=$POD_CIDR --node-name $NODENAME --ignore-preflight-errors Swap

sudo kubeadm init --control-plane-endpoint=$IPADDR  --apiserver-cert-extra-sans=$IPADDR  --pod-network-cidr=$POD_CIDR --node-name $NODENAME --ignore-preflight-errors Swap
/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\\/\/\/\\/\/\/\/\/\\/\/\/\\\/\/\/\/\/\\/\/\/\/\/\/\/

 mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
   
    export KUBECONFIG=/etc/kubernetes/admin.conf      //////  USE  ROOT -USER 

kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml  

lan@lan:~$ kubectl get node 
NAME   STATUS   ROLES           AGE     VERSION
lan    Ready    control-plane   3m18s   v1.27.1


 
 ##########################################################################################################
 ###################################################################################################
 ##################################################################################3
 How to Install Kubernetes Cluster on Ubuntu 22.04
https://www.linuxtechi.com/install-kubernetes-on-ubuntu-22-04/

root@ssd:~# sudo swapoff -a
root@ssd:~#  sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

root@ssd:~# systemctl stop firewalld 

root@ssd:~# sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF

root@ssd:~# sudo modprobe overlay
root@ssd:~# sudo modprobe br_netfilter

root@dev:~# sudo tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

[]{{{{{{{][\]][[]][][][]][]]][}}{}{{}{{}}}]}}}}}}}   carefully   results 

root@ssd:~#  sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF
overlay
br_netfilter
root@ssd:~# sudo modprobe overlay
root@ssd:~# sudo modprobe br_netfilter
root@ssd:~#  sudo tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
root@ssd:~# 
====-----
root@ssd:~# sudo sysctl --system

root@ssd:~#  sudo apt install -y curl gnupg2 software-properties-common apt-transport-https ca-certificates

root@ssd:~#  sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmour -o /etc/apt/trusted.gpg.d/docker.gpg

root@ssd:~# sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

root@ssd:~#  sudo apt update

root@ssd:~# sudo apt install -y containerd.io

root@ssd:~# containerd config default | sudo tee /etc/containerd/config.toml >/dev/null 2>&1
root@ssd:~#  sudo sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml

root@ssd:~# sudo systemctl restart containerd
root@ssd:~#  sudo systemctl enable containerd 

root@ssd:~# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

root@ssd:~# sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"

root@ssd:~# sudo apt update

root@ssd:~# sudo apt install -y kubelet kubeadm kubectl
root@ssd:~# sudo apt-mark hold kubelet kubeadm kubectl

  
        systemctl stop firewalld 
root@ssd:~#  sudo kubeadm init --pod-network-cidr=10.244.0.0/16   
    


Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.122.103:6443 --token etlbem.un3nsyo072l91yot \
	--discovery-token-ca-cert-hash sha256:0d213c6eef7dc1d7f83c9eb82e2ccd8b24182d0caf436e8d04a046450e823800 
dev@master:~$ 




kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml   

==========+++++++++++++++++++++++++==================================================
ssh client 

curl -1sLf 'https://dl.cloudsmith.io/public/asbru-cm/release/cfg/setup/bash.deb.sh' | sudo -E bash
sudo apt-get install asbru-cm

=================+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Install and Set Up kubectl on Linux - Kubernetes
https://kubernetes.io/docs/tasks/tools/
https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

curl -LO https://dl.k8s.io/release/v1.26.0/bin/linux/amd64/kubectl

echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

root@anji:~# mkdir -p .kube
root@anji:~# cd .kube/
dev@dev:~/.kube$ scp dev@192.168.122.103:/home/dev/.kube/config  .





/\/\/\/\/\/\/\/\/\\\\\/\\/\/\/\\\\/\/=====+++++++++++=====================[[[[[[[[[]]]]]]]]]\\\\[\/////////////////////////\]
https://github.com/justmeandopensource/kubernetes/blob/master/docs/install-cluster-ubuntu-20.md
https://github.com/rritsoft/kubernetes-install-in-ubuntu-20-Install-Kubernetes-Cluster-on-Ubuntu-20.04-

kubernetes install in ubuntu 20 =kubeadm install in ubuntu 20 =Kubernetes cluster on Ubuntu 20.04 , 
=========================-----------------                  ----------------===================================   


Login as root user

sudo su -
--------------------
Perform all the commands as root user unless otherwise specified
Disable Firewall

ufw disable
systemctl stop firewalld 
----------------
Disable swap

swapoff -a; sed -i '/swap/d' /etc/fstab
--------------------------------------
Update sysctl settings for Kubernetes networking

cat >>/etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
----------------------------------------
Install docker engine

{
  apt install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common
  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
  add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
  apt update
  apt install -y docker-ce=5:19.03.10~3-0~ubuntu-focal containerd.io
}
---------------------------------------
Kubernetes Setup
Add Apt repository

{
  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
  echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" > /etc/apt/sources.list.d/kubernetes.list
}
-----------------------------------------------
Install Kubernetes components

apt update && apt install -y kubeadm kubelet kubectl
-----------------------------------------------------------
In case you are using LXC containers for Kubernetes nodes

Hack required to provision K8s v1.15+ in LXC containers

{
  mknod /dev/kmsg c 1 11
  echo '#!/bin/sh -e' >> /etc/rc.local
  echo 'mknod /dev/kmsg c 1 11' >> /etc/rc.local
  chmod +x /etc/rc.local
}
----------------------------------------------------------
On kmaster
Initialize Kubernetes Cluster

Update the below command with the ip address of kmaster

kubeadm init --apiserver-advertise-address=172.16.16.100 --pod-network-cidr=192.168.0.0/16  --ignore-preflight-errors=all
OR OR 
sudo kubeadm init --pod-network-cidr=10.244.0.0/16


  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

  export KUBECONFIG=/etc/kubernetes/admin.conf


kubeadm join 192.168.122.103:6443 --token etlbem.un3nsyo072l91yot \
	--discovery-token-ca-cert-hash sha256:0d213c6eef7dc1d7f83c9eb82e2ccd8b24182d0caf436e8d04a046450e823800 

-----------------------------------------------------
Deploy WEAVE network
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml   
-------------------------------------------
kubectl get nodes
hai@i7laptop:~$ kubectl get nodes 
NAME     STATUS   ROLES           AGE   VERSION
master   Ready    control-plane   15d   v1.25.4
==============================================   bellow steps in worker nodes===================
sudo systemctl stop firewalld 
sudo rm  -rf /etc/containerd/config.toml
sudo  systemctl restart containerd
hai@i7laptop:~$ sudo kubeadm join 192.168.122.103:6443 --token etlbem.un3nsyo072l91yot \
	--discovery-token-ca-cert-hash sha256:0d213c6eef7dc1d7f83c9eb82e2ccd8b24182d0caf436e8d04a046450e823800 
================================================================= completed ========
hai@i7laptop:~$ kubectl get nodes 
NAME     STATUS   ROLES           AGE   VERSION
master   Ready    control-plane   15d   v1.25.4
worker   Ready    <none>          15d   v1.25.4
===============================================================  if any  errors  ====
//////////////////////////////////////////////////////////////////////////////////////////
hai@master:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16
I1218 06:45:37.145708  633010 version.go:256] remote version is much newer: v1.26.0; falling back to: stable-1.25
[init] Using Kubernetes version: v1.25.5
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR Port-6443]: Port 6443 is in use
	[ERROR Port-10259]: Port 10259 is in use
	[ERROR Port-10257]: Port 10257 is in use
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml]: /etc/kubernetes/manifests/kube-apiserver.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml]: /etc/kubernetes/manifests/kube-controller-manager.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml]: /etc/kubernetes/manifests/kube-scheduler.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-etcd.yaml]: /etc/kubernetes/manifests/etcd.yaml already exists
	[ERROR Port-2379]: Port 2379 is in use
	[ERROR Port-2380]: Port 2380 is in use
	[ERROR DirAvailable--var-lib-etcd]: /var/lib/etcd is not empty
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

SOLUTIONS:= 
systemctl stop firewalld 

hai@master:~$ sudo kubeadm reset 

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/kube-apiserver.yaml 

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/kube-controller-manager.yaml 

hai@master:~$ sudo rm  -rf  /etc/kubernetes/manifests/kube-scheduler.yaml

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/etcd.yaml
 hai@master:~$ sudo rm  -rf /etc/containerd/config.toml
hai@master:~$ sudo  systemctl restart containerd

================= THANK YOU ==========================
======================$$$$$$$$$$$$$$$$5%%%%%%%%%%%%%%%%%%%%%%5&&&&&&&&&&&888888888888*****]]]]]][[[[]][]]][][]][][][[[]]]

https://github.com/kubernetesway/kubernetes/wiki/Kubernetes-Bare-metal-cluster-on-Ubuntu-20.04.2
https://www.youtube.com/watch?v=zrb0daoSEEo&t=951s

https://computingforgeeks.com/deploy-kubernetes-cluster-on-ubuntu-with-kubeadm/   2023 document 
https://www.knowledgehut.com/blog/devops/install-kubernetes-on-ubuntu   2023 document 



sudo apt-get install docker.io

sudo systemctl enable docker

sudo systemctl status docker

sudo apt-get install curl

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add

sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"

sudo apt-get install kubeadm kubelet kubectl

sudo apt-mark hold kubeadm kubelet kubectl

sudo kubeadm version

sudo swapoff -a
sudo systemctl stop firewalld

###########Below step apply only on Master node

sudo hostnamectl set-hostname master

######Below step apply onlyon Worker node

sudo hostnamectl set-hostname worker

#** Below step apply only on Master node**

sudo kubeadm init --pod-network-cidr=10.244.0.0/16   







kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml         // "wavenetworks "



kubectl get nodes

sudo wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

kubectl apply -f kube-flannel.yml

or  //  or  // OR   OR  
git clone  https://github.com/rritsoft/kubernetes-10.git

git clone https://github.com/rritsoft/jenkins-cicd-docker-push-image-project.git



kubectl get nodes

kubeadm token create --print-join-command

hai@ubuntu:~$ kubeadm token create --print-join-command
kubeadm join 192.168.68.137:6443 --token 1bkysu.dydhuqmbuxa63546 --discovery-token-ca-cert-hash sha256:1ea0e848089c60445897d2d0ad1d9493b95a7f0dd9f96c57848052bbd73dac8f 

kubeadm join 192.168.122.29:6443 --token e0heub.1ezekb8t0imcm2fc --discovery-token-ca-cert-hash sha256:c38ee3fc3b5267990b27a222508dff7bdc41f1fb13a9322a5c5dcc0bffc8746f
sudo rm /etc/containerd/config.toml
sudo systemctl restart containerd
kkubeadm init

root@ubuntu:~# kubeadm init
[init] Using Kubernetes version: v1.25.4
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
  [ERROR CRI]: container runtime is not running: output: E1125 08:08:38.456633    2764 remote_runtime.go:948] "Status from runtime service failed" err="rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService"
time="2022-11-25T08:08:38-08:00" level=fatal msg="getting status of runtime: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService"
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
root@ubuntu:~# rm /etc/containerd/config.toml
root@ubuntu:~# systemctl restart containerd

--- == = = = = = = = 
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:
 apply the below steps up to step 10 on both master and worker nodes**
https://github.com/kubernetesway/kubernetes/wiki/Kubernetes-Bare-metal-cluster-on-Ubuntu-20.04.2
https://www.youtube.com/watch?v=zrb0daoSEEo&t=951s


sudo apt-get install docker.io

sudo systemctl enable docker

sudo systemctl status docker

sudo apt-get install curl

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add

sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"

sudo apt-get install kubeadm kubelet kubectl

sudo apt-mark hold kubeadm kubelet kubectl

sudo kubeadm version

sudo swapoff -a

###########Below step apply only on Master node

sudo hostnamectl set-hostname master

######Below step apply onlyon Worker node

sudo hostnamectl set-hostname worker

#** Below step apply only on Master node**

sudo kubeadm init --pod-network-cidr=10.244.0.0/16




Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.122.73:6443 --token kwy6zf.k0lfip6tqzt3urzf \
  --discovery-token-ca-cert-hash sha256:66efbe5d56fa82dac91c81eaf260170bf6b2306881c9c6dba11e2f789d83705c 







kubectl get nodes

sudo wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

kubectl apply -f kube-flannel.yml

kubectl get nodes

kubeadm token create --print-join-command

hai@ubuntu:~$ kubeadm token create --print-join-command
kubeadm join 192.168.68.137:6443 --token 1bkysu.dydhuqmbuxa63546 --discovery-token-ca-cert-hash sha256:1ea0e848089c60445897d2d0ad1d9493b95a7f0dd9f96c57848052bbd73dac8f 


#############Join to cluster from Worker nodes

kubectl get nodes

Metallb Loadbalancer deployment

kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/namespace.yaml

kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/metallb.yaml

kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"

kubectl -n metallb-system get all

Create a YAML file for configuring metallb IP pool

vi /tmp/metallb.yaml
**copy the contents of metallb.yaml that is attached below , save and exit **

metallb.yaml

kubectl create -f /tmp/metallb.yaml

kubectl -n metallb-system get all

Deploying a sample Nginx web application

kubectl create deploy nginx --image nginx

kubectl get deployments

kubectl expose deploy nginx --port 80 --type LoadBalancer

kubectl get svc

hai  ALL=(ALL) NOPASSWD:ALL

}}{{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}    ERROR  "ERROR "
hai@master:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16
I1218 06:45:37.145708  633010 version.go:256] remote version is much newer: v1.26.0; falling back to: stable-1.25
[init] Using Kubernetes version: v1.25.5
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR Port-6443]: Port 6443 is in use
	[ERROR Port-10259]: Port 10259 is in use
	[ERROR Port-10257]: Port 10257 is in use
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml]: /etc/kubernetes/manifests/kube-apiserver.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml]: /etc/kubernetes/manifests/kube-controller-manager.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml]: /etc/kubernetes/manifests/kube-scheduler.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-etcd.yaml]: /etc/kubernetes/manifests/etcd.yaml already exists
	[ERROR Port-2379]: Port 2379 is in use
	[ERROR Port-2380]: Port 2380 is in use
	[ERROR DirAvailable--var-lib-etcd]: /var/lib/etcd is not empty
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/kube-apiserver.yaml 

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/kube-controller-manager.yaml 

hai@master:~$ sudo rm  -rf  /etc/kubernetes/manifests/kube-scheduler.yaml

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/etcd.yaml

   only MASTER   =:==  ERROR ERROR  ERROR  
https://stackoverflow.com/questions/72504257/i-encountered-when-executing-kubeadm-init-error-issue
ANSWER 
sudo rm  -rf /etc/containerd/config.toml
sudo  systemctl restart containerd


=====================================================================================================================
####################################################################################################################################
## commands real time 
https://www.youtube.com/channel/UCTIESbYFCPhiDZReih-j9kA
rajesh 
https://www.youtube.com/watch?v=-suYI4pFNQo&list=PL8SR-mMnL9f6KKYDnrG3saVnyLqNDz1GP
------------------------------------
https://stackoverflow.com/questions/53559291/kubernetes-coredns-in-crashloopbackoff
hai@master:~$ sudo systemctl  restart  kubelet
[sudo] password for hai: 
hai@master:~$ sudo systemctl  restart  kubelet 
hai@master:~$ sudo systemctl daemon-reload
hai@master:~$ kubectl get pods --all-namespaces
NAMESPACE        NAME                             READY   STATUS                       RESTARTS        AGE  
default          lg-54b6dc5746-8v27q              1/1     Running                      0               3h25m
default          nginx                            1/1     Running                      0               47h  
default          nginx-76d6c9b8c-h9vns            1/1     Running                      0               14d  
kube-flannel     kube-flannel-ds-7v8ds            1/1     Running                      0               15d
kube-flannel     kube-flannel-ds-hbg8d            1/1     Running                      0               15d
kube-system      coredns-565d847f94-8drrl         0/1     Running                      378 (65s ago)   15d
kube-system      coredns-565d847f94-nnthx         0/1     Running                      378 (69s ago)   15d
kube-system      etcd-master                      1/1     Running                      0               15d
kube-system      kube-apiserver-master            1/1     Running                      0               15d
kube-system      kube-controller-manager-master   1/1     Running                      0               15d
kube-system      kube-proxy-hsm4r                 1/1     Running                      0               15d
kube-system      kube-proxy-r2x99                 1/1     Running                      0               15d
kube-system      kube-scheduler-master            1/1     Running                      0               15d
metallb-system   controller-6fdc8d5477-stg47      1/1     Running                      0               14d
metallb-system   speaker-4x92g                    0/1     CreateContainerConfigError   0               15d
#=================================
hai@master:~$ kubectl get pods -n kube-system
NAME                             READY   STATUS    RESTARTS        AGE
coredns-565d847f94-8drrl         0/1     Running   381 (63s ago)   15d
coredns-565d847f94-nnthx         0/1     Running   381 (67s ago)   15d
etcd-master                      1/1     Running   0               15d
kube-apiserver-master            1/1     Running   0               15d
kube-controller-manager-master   1/1     Running   0               15d
kube-proxy-hsm4r                 1/1     Running   0               15d
kube-proxy-r2x99                 1/1     Running   0               15d
kube-scheduler-master            1/1     Running   0               15d
## or  or  or  or 
hai@master:~$ kubectl -n kube-system get pods 
NAME                             READY   STATUS    RESTARTS        AGE
coredns-565d847f94-8drrl         0/1     Running   382 (12s ago)   15d
coredns-565d847f94-nnthx         0/1     Running   382 (16s ago)   15d
etcd-master                      1/1     Running   0               15d
kube-apiserver-master            1/1     Running   0               15d
kube-controller-manager-master   1/1     Running   0               15d
kube-proxy-hsm4r                 1/1     Running   0               15d
kube-proxy-r2x99                 1/1     Running   0               15d
kube-scheduler-master            1/1     Running   0               15d
##===============================
https://github.com/kubernetes/kubeadm/issues/1162
hai@master:~$ kubectl logs -n kube-system coredns-565d847f94-8drrl 
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:40554->192.168.68.2:53: i/o timeout
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: no route to host
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:35893->192.168.68.2:53: i/o timeout
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:42322->192.168.68.2:53: i/o timeout
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:33926->192.168.68.2:53: i/o timeout
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:43235->192.168.68.2:53: i/o timeout
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: no route to host
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:59094->192.168.68.2:53: i/o timeout
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:44565->192.168.68.2:53: i/o timeout
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:51991->192.168.68.2:53: i/o timeout
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:58672->192.168.68.2:53: i/o timeout
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: no route to host
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:37747->192.168.68.2:53: i/o timeout
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: no route to host
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:44
### =========================================
hai@master:~$ kubectl logs  -n  kube-system coredns-565d847f94-nnthx
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908     
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[ERROR] plugin/errors: 2 4964736449842461070.1693789343325947944. HINFO: read udp 10.244.0.4:46391->192.168.68.2:53: i/o timeout
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: no route to host
[ERROR] plugin/errors: 2 4964736449842461070.1693789343325947944. HINFO: read udp 10.244.0.4:35687->192.168.68.2:53: i/o timeout
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: no route to host
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: no route to host
##======================---------------------------------======
 sudo systemctl daemon-reload
 sudo systemctl restart kubelet
 $ kubectl get pods -n kube-system -oname |grep coredns |xargs kubectl delete -n kube-system
 hai@master:~$ kubectl  get pods --all-namespaces 
NAMESPACE        NAME                             READY   STATUS                       RESTARTS        AGE
default          lg-54b6dc5746-8v27q              1/1     Running                      0               3h47m
default          nginx                            1/1     Running                      0               47h  
default          nginx-76d6c9b8c-h9vns            1/1     Running                      0               14d  
kube-flannel     kube-flannel-ds-7v8ds            1/1     Running                      0               15d  
kube-flannel     kube-flannel-ds-hbg8d            1/1     Running                      0               15d  
kube-system      coredns-565d847f94-8drrl         0/1     Running                      385 (43s ago)   15d  
kube-system      coredns-565d847f94-nnthx         0/1     Running                      385 (42s ago)   15d  
kube-system      etcd-master                      1/1     Running                      0               15d  
kube-system      kube-apiserver-master            1/1     Running                      0               15d  
kube-system      kube-controller-manager-master   1/1     Running                      0               15d  
kube-system      kube-proxy-hsm4r                 1/1     Running                      0               15d  
kube-system      kube-proxy-r2x99                 1/1     Running                      0               15d  
kube-system      kube-scheduler-master            1/1     Running                      0               15d  
metallb-system   controller-6fdc8d5477-stg47      1/1     Running                      0               14d  
metallb-system   speaker-4x92g                    0/1     CreateContainerConfigError   0               15d  
####=================================
kubectl -n kube-system delete pod -l k8s-app=kube-dns

root@mylaptop:~# hostnamectl  set-hostname  i7laptop
root@mylaptop:~# bash
root@i7laptop:~# 

kubectl install in linux
https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/

root@i7laptop:~# curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   138  100   138    0     0    388      0 --:--:-- --:--:-- --:--:--   388
100 45.7M  100 45.7M    0     0  3555k      0  0:00:13  0:00:13 --:--:-- 3798k
root@i7laptop:~# ll
-rw-r--r--  1 root root 48021504 Dec 13 01:06 kubectl

root@i7laptop:~# chmod +x kubectl 
root@i7laptop:~# mv  kubectl  /usr/local/bin/
hai@master:~$ cd /etc/kubernetes/
hai@master:/etc/kubernetes$ ll

-rwxrwxrwx   1 root root  5642 Nov 27 07:35 admin.conf*
-rw-------   1 root root  5674 Nov 27 07:35 controller-manager.conf
-rw-------   1 root root  1962 Nov 27 07:36 kubelet.conf
drwxr-xr-x   2 root root  4096 Nov 27 07:35 manifests/
drwxr-xr-x   3 root root  4096 Nov 27 07:35 pki/
-rw-------   1 root root  5622 Nov 27 07:35 scheduler.conf
##==== ---------
hai@master:/etc/kubernetes$ cd pki 
hai@master:/etc/kubernetes/pki$ ll
total 68
drwxr-xr-x 3 root root 4096 Nov 27 07:35 ./
drwxr-xr-x 4 root root 4096 Nov 27 07:35 ../
-rw-r--r-- 1 root root 1281 Nov 27 07:35 apiserver.crt
-rw-r--r-- 1 root root 1155 Nov 27 07:35 apiserver-etcd-client.crt   
-rw------- 1 root root 1675 Nov 27 07:35 apiserver-etcd-client.key   
-rw------- 1 root root 1679 Nov 27 07:35 apiserver.key
-rw-r--r-- 1 root root 1164 Nov 27 07:35 apiserver-kubelet-client.crt
-rw------- 1 root root 1679 Nov 27 07:35 apiserver-kubelet-client.key
-rw-r--r-- 1 root root 1099 Nov 27 07:35 ca.crt
-rw------- 1 root root 1679 Nov 27 07:35 ca.key
drwxr-xr-x 2 root root 4096 Nov 27 07:35 etcd/
-rw-r--r-- 1 root root 1115 Nov 27 07:35 front-proxy-ca.crt
-rw------- 1 root root 1675 Nov 27 07:35 front-proxy-ca.key
-rw-r--r-- 1 root root 1119 Nov 27 07:35 front-proxy-client.crt      
-rw------- 1 root root 1679 Nov 27 07:35 front-proxy-client.key      
-rw------- 1 root root 1675 Nov 27 07:35 sa.key
-rw------- 1 root root  451 Nov 27 07:35 sa.pub
# == ---  = -----
hai@master:/etc/kubernetes/manifests$ ll
total 24
drwxr-xr-x 2 root root 4096 Nov 27 07:35 ./
drwxr-xr-x 4 root root 4096 Nov 27 07:35 ../
-rw------- 1 root root 2382 Nov 27 07:35 etcd.yaml
-rw------- 1 root root 4019 Nov 27 07:35 kube-apiserver.yaml
-rw------- 1 root root 3520 Nov 27 07:35 kube-controller-manager.yaml
-rw------- 1 root root 1440 Nov 27 07:35 kube-scheduler.yaml
#== -- ==--=-----
nano /etc/ssh/sshd_config

    permitRootLogin  yes
    passwordAuthenticated  yes

root@ubuntu:~# passwd
New password: 
Retype new password: 
passwd: password updated successfully

root@master:/etc/kubernetes# scp admin.conf  root@192.168.68.134:/root
The authenticity of host '192.168.68.134 (192.168.68.134)' can't be established.
ECDSA key fingerprint is SHA256:WERuPb5G60GuhPYHZmaedTWpEp+1SPbNgsWfKVmlXBU.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.68.134' (ECDSA) to the list of known hosts.
root@192.168.68.134's password: 
admin.conf                                                                  100% 5642   422.5KB/s   00:00    
root@master:/etc/kubernetes# 
#=== ---=====================
root@ubuntu:~# mkdir  .kube
root@ubuntu:~# mv  admin.conf  .kube/config
root@ubuntu:~# ll
drwxr-xr-x  2 root root 4096 Dec 13 04:15 .kube/
root@ubuntu:~# cd .kube/
root@ubuntu:~/.kube# ll
-rwxr-xr-x 1 root root 5642 Dec 13 04:10 config*

hai@i7laptop:~$ kubectl get nodes 
NAME     STATUS   ROLES           AGE   VERSION
master   Ready    control-plane   15d   v1.25.4
worker   Ready    <none>          15d   v1.25.4

https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-reset/

preflight           Run reset pre-flight checks
remove-etcd-member  Remove a local etcd member.
cleanup-node        Run cleanup node.

##=====================
kubeadm token list
https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-token/
https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/

hai@master:~$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
8f3c5c8c74a13b2d34856b6775c8aceb46c7d2410517671d9deb80672a1657ab

hai@master:~$ kubeadm token list 
TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                     EXTRA GROUPS
ypfouv.gvxkq8gb7sqqpazh   23h         2022-12-15T05:22:11Z   authentication,signing   <none>                          system:bootstrappers:kubeadm:default-node-token

hai@master:~$ kubeadm token create  --ttl 10m
wnl2to.xqjr8wrl16a5g6o1
hai@master:~$ kubeadm token list 
TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
wnl2to.xqjr8wrl16a5g6o1   9m          2022-12-14T05:53:44Z   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-token
ypfouv.gvxkq8gb7sqqpazh   23h         2022-12-15T05:22:11Z   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-token

---- 
hai@master:~$ kubeadm token create --ttl 0
wnmi2n.3mrwpd0hghlqdqa5
hai@master:~$ kubeadm token list 
TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
wnl2to.xqjr8wrl16a5g6o1   1m          2022-12-14T05:53:44Z   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-token
wnmi2n.3mrwpd0hghlqdqa5   <forever>   <never>   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-token
ypfouv.gvxkq8gb7sqqpazh   23h         2022-12-15T05:22:11Z   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-token
####==----
hai@master:~$ kubeadm token create --print-join-command
kubeadm join 192.168.68.138:6443 --token jnd0hu.5r5sctc0g4cktp4o --discovery-token-ca-cert-hash sha256:8f3c5c8c74a13b2d34856b6775c8aceb46c7d2410517671d9deb80672a1657ab 

###  if  suppose  we  add  thee new  token  first genereate the token and  apply   kubeadm  reset on   worker node , then  again add with new token join --
and  delete  worker node in  master cluster 
hai@worker:~$ kubeadm reset 
hai@master:~$ kubectl delete node worker
hai@worker:~$ sudo rm -rf /etc/kubernetes/pki/ca.crt
hai@worker:~$ sudo rm -rf  /etc/kubernetes/kubelet.conf

sudo kubeadm join 192.168.68.138:6443 --token  wnmi2n.3mrwpd0hghlqdqa5  --discovery-token-ca-cert-hash sha256:8f3c5c8c74a13b2d34856b6775c8aceb46c7d2410517671d9deb80672a1657ab 
#---------================
Token-based discovery without CA pinning 
https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/

CA public key, using   --discovery-token-unsafe-skip-ca-verification  

kubeadm join 192.168.68.138:6443 --token jnd0hu.5r5sctc0g4cktp4o --discovery-token-ca-cert-hash sha256:8f3c5c8c74a13b2d34856b6775c8aceb46c7d2410517671d9deb80672a1657ab 
kubeadm join 192.168.68.138:6443 --token jnd0hu.5r5sctc0g4cktp4o   --discovery-token-unsafe-skip-ca-verification 

##############################################
Turning off auto-approval of node client certificates
https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/

hai@master:~$ kubectl delete clusterrolebinding kubeadm:node-autoapprove-bootstrap
clusterrolebinding.rbac.authorization.k8s.io "kubeadm:node-autoapprove-bootstrap" deleted


hai@master:~/test$ kubectl get csr

hai@worker:~$ sudo sudo kubeadm join 192.168.68.138:6443 --token  wnmi2n.3mrwpd0hghlqdqa5  --discovery-token-ca-cert-hash sha256:8f3c5c8c74a13b2d34856b6775c8aceb46c7d2410517671d9deb80672a1657ab 
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...-------...............................................???

hai@master:~/test$ kubectl get csr
NAME        AGE   SIGNERNAME                                    REQUESTOR                 REQUESTEDDURATION   CONDITION
csr-ksd9v   10m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:sgn48v   <none>              Approved,Issued
csr-wffc8   16m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:sgn48v   <none>              Pending
csr-ztxrb   21m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:sgn48v   <none>              Pending
hai@master:~/test$ kubectl certificate approve csr-ksd9v
certificatesigningrequest.certificates.k8s.io/csr-ksd9v approved

hai@master:~/test$ kubectl certificate approve node-csr-c69HXe7aY

kubectl get csr


------
https://github.com/postfinance/kubelet-csr-approver


#================================================================================
https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/

https://qinlj.github.io/docs/reference/setup-tools/kubeadm/kubeadm-join/

Example kubeadm join commands:

kubeadm join --discovery-file path/to/file.conf (local file)

kubeadm join --discovery-file https://url/file.conf (remote HTTPS URL)

Advantages:
Allows bootstrapping nodes to securely discover a root of trust for the control-plane node even if the network or other worker nodes are compromised.

Disadvantages:
Requires that you have some way to carry the discovery information from the control-plane node to the bootstrapping nodes. If the discovery file contains credentials you must keep it secret and transfer it over a secure channel. This might be possible with your cloud provider or provisioning tool

https://github.com/kubernetes/website/blob/main/content/en/docs/reference/setup-tools/kubeadm/kubeadm-join.md

===========##########################################################################
Working With Kubernetes Objects

https://www.containiq.com/post/kubernetes-objects
https://www.htown-tech.com/blogs/kubernetes-its-8-types-of-objects

Kubernetes & Its 8 Types of Objects

Kubernetes Objects:
The Kubernetes Platform contains control over the resources related to Storage and Compute. These resources are regarded as Objects, and it contains 8 Key objects.

1. Pods:

Being a higher-level abstraction grouping containerized component, it consists of one or more containers that can co-exist on the host system and share resources. With each Pod having a unique IP Address in a particular cluster, it allows the usage of ports without any conflicts.

2. Replica Sets:

At any time, it is needed to maintain a stable set of running replica Pods. This is maintained by Replica sets. Also, its purpose is to manage the availability of the required number of identical Pods.

3. Services:

A Kubernetes service is defined by a set of pods that work together. These sets of pods are defined with a label selector. The service discovery can happen in two different modes, using environmental variables or Kubernetes DNS.

4. Volumes:

By default, ephemeral storage will be provided by the File Systems of Kubernetes. This form of storage will remove all the data stored in such containers when the Pod is restarted. The Kubernetes Volume will provide persistent storage such that the data exists for the whole lifetime of the Pod.

5. Name Spaces:

The large number of resources managed by Kubernetes are separated into multiple non-overlapping sets. These sets are referred to as Namespaces. These are generally used when a large number of users exist in the form of multiple teams or projects.

6. ConfigMaps and Secrets:

Some of the Configuration Data may contain confidential information which makes the storage and maintenance of configuration information a challenge. ConfigMaps and Secrets are two mechanisms provided by Kubernetes that can deal with this problem. Both methods permit changes in the configuration without application-build.

7. Stateful Sets:

Stateless applications are easier to handle in terms of scaling. Because all it needs is to add up the number of Pods. But for Stateful workloads, you need to maintain the states when the Pod is restarted, and the state may need to be redistributed for scaling. The stateful sets provided by Kubernetes are used to run stateful Applications ensuring uniqueness and ordering of the instances of a Pod.

8. Daemon Sets:

Generally, Kubernetes Scheduler is responsible for deciding the location where Pods are run. It is done by the algorithm. This mode of scheduling the Pods is implemented by the feature called Daemon Sets.
 

â€‹For your business, container management is as much needed as container creation. This is best handled by Kubernetes. And all the objects provided by the Kubernetes act as resources to fulfil its purpose.

##
hai@master:~/test$ kubectl api-resources
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND   #  kind models = 54 models 
bindings                                       v1                                     true         Binding        
componentstatuses                 cs           v1                                     false        ComponentStatus
configmaps                        cm           v1                                     true         ConfigMap
endpoints                         ep           v1                                     true         Endpoints
events                            ev           v1                                     true         Event
limitranges                       limits       v1                                     true         LimitRange
namespaces                        ns           v1                                     false        Namespace
nodes                             no           v1                                     false        Node
persistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim
persistentvolumes                 pv           v1                                     false        PersistentVolume     
pods                              po           v1                                     true         Pod
podtemplates                                   v1                                     true         PodTemplate
replicationcontrollers            rc           v1                                     true         ReplicationController
resourcequotas                    quota        v1                                     true         ResourceQuota        
secrets                                        v1                                     true         Secret
serviceaccounts                   sa           v1                                     true         ServiceAccount       
services                          svc          v1                                     true         Service
mutatingwebhookconfigurations                  admissionregistration.k8s.io/v1        false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io/v1                false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io/v1              false        APIService
controllerrevisions                            apps/v1                                true         ControllerRevision
daemonsets                        ds           apps/v1                                true         DaemonSet
deployments                       deploy       apps/v1                                true         Deployment
replicasets                       rs           apps/v1                                true         ReplicaSet
statefulsets                      sts          apps/v1                                true         StatefulSet
tokenreviews                                   authentication.k8s.io/v1               false        TokenReview
localsubjectaccessreviews                      authorization.k8s.io/v1                true         LocalSubjectAccessReview
selfsubjectaccessreviews                       authorization.k8s.io/v1                false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io/v1                false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io/v1                false        SubjectAccessReview
horizontalpodautoscalers          hpa          autoscaling/v2                         true         HorizontalPodAutoscaler
cronjobs                          cj           batch/v1                               true         CronJob
jobs                                           batch/v1                               true         Job
certificatesigningrequests        csr          certificates.k8s.io/v1                 false        CertificateSigningRequest
leases                                         coordination.k8s.io/v1                 true         Lease
endpointslices                                 discovery.k8s.io/v1                    true         EndpointSlice
events                            ev           events.k8s.io/v1                       true         Event
flowschemas                                    flowcontrol.apiserver.k8s.io/v1beta2   false        FlowSchema
prioritylevelconfigurations                    flowcontrol.apiserver.k8s.io/v1beta2   false        PriorityLevelConfiguration
ingressclasses                                 networking.k8s.io/v1                   false        IngressClass
ingresses                         ing          networking.k8s.io/v1                   true         Ingress
networkpolicies                   netpol       networking.k8s.io/v1                   true         NetworkPolicy
runtimeclasses                                 node.k8s.io/v1                         false        RuntimeClass
poddisruptionbudgets              pdb          policy/v1                              true         PodDisruptionBudget
clusterrolebindings                            rbac.authorization.k8s.io/v1           false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io/v1           false        ClusterRole
rolebindings                                   rbac.authorization.k8s.io/v1           true         RoleBinding
roles                                          rbac.authorization.k8s.io/v1           true         Role
priorityclasses                   pc           scheduling.k8s.io/v1                   false        PriorityClass
csidrivers                                     storage.k8s.io/v1                      false        CSIDriver
csinodes                                       storage.k8s.io/v1                      false        CSINode
csistoragecapacities                           storage.k8s.io/v1                      true         CSIStorageCapacity
storageclasses                    sc           storage.k8s.io/v1                      false        StorageClass
volumeattachments                              storage.k8s.io/v1                      false        VolumeAttachment
#==============================================
pod
container
ephermel containers
replication  controllers
replica sets
deployments
statefullset
daemon sets
job
cronjob
horizontal pod auto scaler

----##
https://kubernetes.io/docs/reference/

https://kubernetes.io/docs/reference/kubernetes-api/

Workload Resources
Service Resources
Config and Storage Resources
Authentication Resources
Authorization Resources
Policy Resources
Extend Resources
Cluster Resources
Common Definitions
Other Resources
Common Parameters
########################################
Workload Resources  : -- 
https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/
Pod
Pod is a collection of containers that can run on a host.

PodTemplate
PodTemplate describes a template for creating copies of a predefined pod.

ReplicationController
ReplicationController represents the configuration of a replication controller.

ReplicaSet
ReplicaSet ensures that a specified number of pod replicas are running at any given time.

Deployment
Deployment enables declarative updates for Pods and ReplicaSets.

StatefulSet
StatefulSet represents a set of pods with consistent identities.

ControllerRevision
ControllerRevision implements an immutable snapshot of state data.

DaemonSet
DaemonSet represents the configuration of a daemon set.

Job
Job represents the configuration of a single job.

CronJob
CronJob represents the configuration of a single cron job.

HorizontalPodAutoscaler
configuration of a horizontal pod autoscaler.

HorizontalPodAutoscaler
HorizontalPodAutoscaler is the configuration for a horizontal pod autoscaler, which automatically manages the replica count of any resource implementing the scale subresource based on the metrics specified.

PriorityClass
PriorityClass defines mapping from a priority class name to the priority integer value.

PodScheduling v1alpha1
PodScheduling objects hold information that is needed to schedule a Pod with ResourceClaims that use "WaitForFirstConsumer" allocation mode.

ResourceClaim v1alpha1
ResourceClaim describes which resources are needed by a resource consumer.

ResourceClaimTemplate v1alpha1
ResourceClaimTemplate is used to produce ResourceClaim objects.

ResourceClass v1alpha1
ResourceClass is used by administrators to influence how resources are allocated.
#=----////////////////////////////////////////////////////////////////////////////////
Service Resources

Service
Service is a named abstraction of software service (for example, mysql) consisting of local port (for example 3306) that the proxy listens on, and the selector that determines which pods will answer requests sent through the proxy.

Endpoints
Endpoints is a collection of endpoints that implement the actual service.

EndpointSlice
EndpointSlice represents a subset of the endpoints that implement a service.

Ingress
Ingress is a collection of rules that allow inbound connections to reach the endpoints defined by a backend.

IngressClass
IngressClass represents the class of the Ingress, referenced by the Ingress Spec.
#+++++++================================#//////////
Config and Storage Resources

ConfigMap
ConfigMap holds configuration data for pods to consume.

Secret
Secret holds secret data of a certain type.

Volume
Volume represents a named volume in a pod that may be accessed by any container in the pod.

PersistentVolumeClaim
PersistentVolumeClaim is a user's request for and claim to a persistent volume.

PersistentVolume
PersistentVolume (PV) is a storage resource provisioned by an administrator.

StorageClass
StorageClass describes the parameters for a class of storage for which PersistentVolumes can be dynamically provisioned.

VolumeAttachment
VolumeAttachment captures the intent to attach or detach the specified volume to/from the specified node.

CSIDriver
CSIDriver captures information about a Container Storage Interface (CSI) volume driver deployed on the cluster.

CSINode
CSINode holds information about all CSI drivers installed on a node.

CSIStorageCapacity
CSIStorageCapacity stores the result of one CSI GetCapacity call.
-####################################################################################
Authentication Resources

ServiceAccount
ServiceAccount binds together: * a name, understood by users, and perhaps by peripheral systems, for an identity * a principal that can be authenticated and authorized * a set of secrets.

TokenRequest
TokenRequest requests a token for a given service account.

TokenReview
TokenReview attempts to authenticate a token to a known user.

CertificateSigningRequest
CertificateSigningRequest objects provide a mechanism to obtain x509 certificates by submitting a certificate signing request, and having it asynchronously approved and issued.
-#############################################################################
Authorization Resources

LocalSubjectAccessReview
LocalSubjectAccessReview checks whether or not a user or group can perform an action in a given namespace.

SelfSubjectAccessReview
SelfSubjectAccessReview checks whether or the current user can perform an action.

SelfSubjectRulesReview
SelfSubjectRulesReview enumerates the set of actions the current user can perform within a namespace.

SubjectAccessReview
SubjectAccessReview checks whether or not a user or group can perform an action.

SelfSubjectReview v1alpha1
SelfSubjectReview contains the user information that the kube-apiserver has about the user making this request.

ClusterRole
ClusterRole is a cluster level, logical grouping of PolicyRules that can be referenced as a unit by a RoleBinding or ClusterRoleBinding.

ClusterRoleBinding
ClusterRoleBinding references a ClusterRole, but not contain it.

Role
Role is a namespaced, logical grouping of PolicyRules that can be referenced as a unit by a RoleBinding.

RoleBinding
RoleBinding references a role, but does not contain it.
-###################################################################
Policy Resources

LimitRange
LimitRange sets resource usage limits for each kind of resource in a Namespace.

ResourceQuota
ResourceQuota sets aggregate quota restrictions enforced per namespace.

NetworkPolicy
NetworkPolicy describes what network traffic is allowed for a set of Pods.

PodDisruptionBudget
PodDisruptionBudget is an object to define the max disruption that can be caused to a collection of pods.
-######################################################################
Extend Resources
CustomResourceDefinition
CustomResourceDefinition represents a resource that should be exposed on the API server.

MutatingWebhookConfiguration
MutatingWebhookConfiguration describes the configuration of and admission webhook that accept or reject and may change the object.

ValidatingWebhookConfiguration
ValidatingWebhookConfiguration describes the configuration of and admission webhook that accept or reject and object without changing it.

ValidatingAdmissionPolicy v1alpha1
ValidatingAdmissionPolicy describes the definition of an admission validation policy that accepts or rejects an object without changing it.
-###########################################################
Cluster Resources
Node
Node is a worker node in Kubernetes.

Namespace
Namespace provides a scope for Names.

Event
Event is a report of an event somewhere in the cluster.

APIService
APIService represents a server for a particular GroupVersion.

Lease
Lease defines a lease concept.

RuntimeClass
RuntimeClass defines a class of container runtime supported in the cluster.

FlowSchema v1beta3
FlowSchema defines the schema of a group of flows.

PriorityLevelConfiguration v1beta3
PriorityLevelConfiguration represents the configuration of a priority level.

Binding
Binding ties one object to another; for example, a pod is bound to a node by a scheduler.

ComponentStatus
ComponentStatus (and ComponentStatusList) holds the cluster validation info.

ClusterCIDR v1alpha1
ClusterCIDR represents a single configuration for per-Node Pod CIDR allocations when the MultiCIDRRangeAllocator is enabled (see the config for kube-controller-manager).
-############################################################################
Common Definitions
DeleteOptions
DeleteOptions may be provided when deleting an API object.

LabelSelector
A label selector is a label query over a set of resources.

ListMeta
ListMeta describes metadata that synthetic resources must have, including lists and various status objects.

LocalObjectReference
LocalObjectReference contains enough information to let you locate the referenced object inside the same namespace.

NodeSelectorRequirement
A node selector requirement is a selector that contains values, a key, and an operator that relates the key and values.

ObjectFieldSelector
ObjectFieldSelector selects an APIVersioned field of an object.

ObjectMeta
ObjectMeta is metadata that all persisted resources must have, which includes all objects users must create.

ObjectReference
ObjectReference contains enough information to let you inspect or modify the referred object.

Patch
Patch is provided to give a concrete name and type to the Kubernetes PATCH request body.

Quantity
Quantity is a fixed-point representation of a number.

ResourceFieldSelector
ResourceFieldSelector represents container resources (cpu, memory) and their output format.

Status
Status is a return value for calls that don't return other objects.

TypedLocalObjectReference
TypedLocalObjectReference contains enough information to let you locate the typed referenced object inside the same namespace.
-######################################################################
Other Resources

ValidatingAdmissionPolicyBindingList v1alpha1
-#################################################
Common Parameters
allowWatchBookmarks
allowWatchBookmarks requests watch events with type "BOOKMARK". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored.

continue
The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the "next key".

This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.

dryRun
When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed

fieldManager
fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.

fieldSelector
A selector to restrict the list of returned objects by their fields. Defaults to everything.

fieldValidation
fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields, provided that the ServerSideFieldValidation feature gate is also enabled. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23 and is the default behavior when the ServerSideFieldValidation feature gate is disabled. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default when the ServerSideFieldValidation feature gate is enabled. - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered.

force
Force is going to "force" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests.

gracePeriodSeconds
The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.

labelSelector
A selector to restrict the list of returned objects by their labels. Defaults to everything.

limit
limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.

The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.

namespace
object name and auth scope, such as for teams and projects

pretty
If 'true', then the output is pretty printed.

propagationPolicy
Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.

resourceVersion
resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.

Defaults to unset

resourceVersionMatch
resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.

Defaults to unset

timeoutSeconds
Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.

watch
Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
-#######################################################################
https://kubernetes.io/docs/reference/
One-page API Reference for Kubernetes v1.26
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/

API Groups
The API Groups and their versions are summarized in the following table.

Group	Version
admissionregistration.k8s.io	v1, v1alpha1
apiextensions.k8s.io	v1
apiregistration.k8s.io	v1
apps	v1
authentication.k8s.io	v1, v1alpha1
authorization.k8s.io	v1
autoscaling	v2, v1
batch	v1
certificates.k8s.io	v1
coordination.k8s.io	v1
core	v1
discovery.k8s.io	v1
events.k8s.io	v1
flowcontrol.apiserver.k8s.io	v1beta3, v1beta2
internal.apiserver.k8s.io	v1alpha1
networking.k8s.io	v1, v1alpha1
node.k8s.io	v1
policy	v1
rbac.authorization.k8s.io	v1
resource.k8s.io	v1alpha1
scheduling.k8s.io	v1
storage.k8s.io	v1, v1beta1
#####3###################################
Imperative vs. Declarative â€” a Kubernetes Tutorial

There are two basic ways to deploy to Kubernetes: imperatively, with the many kubectl commands, or declaratively, by writing manifests and using kubectl apply.

---#=========================================
hai@master:~/test$ kubectl config view 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://192.168.68.138:6443     
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
##########################################################
hai@master:~/.kube$ cat config 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdw
    cmRXSmwKY201bGRHVnpNQjRYRFRJeU1URXlOekUxTXpVek1sb1hEVE15TVRFeU5ERTFNelV6TWxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BU
    UVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHdTCjNHM3Y4Z0NXeWZSUFhXZGY4VGR4czFmTW9BWjYvdDZWd3I3OE1RWDRxU0RJajlvQ3lPdVFuRldoQzNQMlZUL2IKWVZGTjl5NmE1ZHQyUUR4Unp
    LZ3NEVktCeExaeFZ1T1V4R2pVcTJJYzNBOEN1a0ZNeUdGNjRmSG5heDlTWHRBOApORFJoQ21IUlVxcEhlc3d0MUV5YU5SeXUwZ3dialVxRXVvZFlDV0w0NU1qWTBhOFhMMWlIUGtqTDNyV3FXNm
    5yCjd3aEZoOVVZV2hnTTNkRWlQY0I3OFBRVHVhcmF1NTBzTVpMTFhQVEhpc2ZxaVVzZEtRajluWkRHakoyajB5ZkkKN1ZzZjNXMWlmdTdvWUlCWW05dWZFMUF4NVE2QmEwSzF0UHVYRGE2Nllza
    0c0TkxSOVhOZVIwT1RKMzUzVFZFNAp3ZFJoeGJLR240eXZMamVnODJzQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZ
    RUZKZkgwcG13b2lLMXhhVVRVdWZ1OWxNczRKRDJNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBRm03RmRyRGpzT1E4aitSTmtwQgp6R1l
    4U3NKL0dkOHl1dkhCYzI4ckFmVlRMTzk2c1NjRC82QVpXaXl6NFdSTlFCTVhpZG1QdENTVFlqQnV5T1kwCjdybXFMVW4xVVVMcTE4RG9LYU96ZVMxUjNnRVlXSmtWWmhobkRUc3RIYXBZVWVrYVd
    qZmQ4WVBQU29YdWo0TmMKeHFaMFloS1lvQ01LQXRic0xrUENCcDVYR09qV0k1bzEwUjZ2YWN0cU01M21sQlpINnZhb1UwNzZZd1c3WklDWAovMjAvb1J6Qi91YS9zL2hBZmJvR2ZDaE5IUFFmOE5l
    V2ZWdTR6WnZsVTNQYTFoeHQzRnlzRnk0TkF5MjhnSEVsCnBzZlN4Ums5N0NWTnY5Y0hyOTNtSWNOTnR3SFBJTHhNaGx1SEJ1WTFRSWpRbGRkU24yOEd5QlNsQlhTSmFFbTIKd21rPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://192.168.68.138:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJZlBPNEhVa21NWWN3RFFZSktvWklodmNOQVFFT
    JRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TWpFeE1qY3hOVE0xTXpKYUZ3MHlNekV4TWpjeE5UTTFNelJhTURReApGekFWQmdOVkJBb1REb
    k41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tD
    QVFFQTVEUlQ1TC9TR01wTGVRaGQKV0ExN2JpSEhFWSsxWFN3eUQ1RTdnclp1b3dHSDlpMnloUDdrSTZZcHpWRHpyZ3hRV20rV0xucjJFK0s1YzNONwo1cCt2ZnEwQkV
    DSDFQcklVOU5OSEFDWXRRZkpaMCtCV2pQT2VBVXB6dXN3d3poT1hwS2t3WThOdFVNbTBMRVVsCnVha2NiOTYvSDBtc0VjV2s5akZnbjlQVytXeVZYdWN2eHVBVTIvR0
    ZKa1kwdFBmaTBnNGF5WkdZWHVncExmVngKMUcxd2djTy81U1lBZ0xscklETnBzSUtjOFlSOWVMT2Ewc1FuSFJld0ExYlZEbWR4Z04vUGRSNXgxOUVxWnUzNgpyNmpPS
    m1lRFN5VVZDR3NDMU84dDJqL0YxUjVQMmluQVQ5cjQvd3VyOEw3M3M3Qk5valdBVWZKUFA3dG1KK29DCnU5ek5vUUlEQVFBQm8xWXdWREFPQmdOVkhROEJBZjhFQkFN
    Q0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFmQmdOVkhTTUVHREFXZ0JTWHg5S1pzS0lpdGNXbEUxTG43dlpUTE9DUQo
    5akFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBa2k5SWVIRVZHeWpma3o4OHY4SzRvMDkyNHpJYjhVTEYybC9VCnZkb3Yzby9NYmJCR3BxbWV0MzlPZTdaT0VqYzlDeH
    Z6MjUxZmo4aW81cVBFdEFBZjl1RzBMeldwaHBhaUJtSVYKemIzWUhwQXB4RVNvcDhMa1FGL2RYSEZmdkFpOFBKQUV4Y2V4cGx1aCt1Q3NPUHlQdm5Xc0dOb2xXNkNOL1
    lXNgovWmFDVFJ4OERzMENLb0E4OVUxRVhkM3BQSncrcHQwc0NMT1p4L0w0U2h5Nnh3SnM2cnRBdGhPVW94dE0wa242CnBZOXc4bjhSMDhOVWNMNGhYQ2JGcXQ5cWQrU
    0tjQlFhZFZYMklWSXdQNCtEUmFsdmF1NEN0UHdxTWQrZlZxclMKVnp0alJKNW9vVWh4amhKQTlIOUNuWFQ2WVpJNnZhelVTUTI3SmpscXd3WWU5c3Nhb2c9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==


    client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcGdJQkFBS0NBUUVBNURSVDVML1NHTXBMZVFoZFdBMTdiaUhIRVkrMVhTd3lENUU3Z3JadW93R
    0g5aTJ5CmhQN2tJNllwelZEenJneFFXbStXTG5yMkUrSzVjM043NXArdmZxMEJFQ0gxUHJJVTlOTkhBQ1l0UWZKWjArQlcKalBPZUFVcHp1c3d3emhPWHBLa3dZOE50VU1tMExFVWx1
    YWtjYjk2L0gwbXNFY1drOWpGZ245UFcrV3lWWHVjdgp4dUFVMi9HRkprWTB0UGZpMGc0YXlaR1lYdWdwTGZWeDFHMXdnY08vNVNZQWdMbHJJRE5wc0lLYzhZUjllTE9hCjBzUW5IUmV3QT
    FiVkRtZHhnTi9QZFI1eDE5RXFadTM2cjZqT0ptZURTeVVWQ0dzQzFPOHQyai9GMVI1UDJpbkEKVDlyNC93dXI4TDczczdCTm9qV0FVZkpQUDd0bUorb0N1OXpOb1FJREFRQUJBb0lCQVFERG
    1nNm1yYng1dnp1OQp2R3UyQTZoZWw5azN0a1F3WHFkeUxId2o2QmNnNVRrU3k4eU9ycVFCNG5WR0pUTVpaamlocTYvQW0yaTlWc0s2CnNLdnMyOGJpLzRzL3RydXRSNXNxMXNXdTNRcEIwbW
    NvY3N2OCtQdmdBMTc3Si9aVlRQbllDNlVNZmYvL0ZVUzQKQlhUMmxIdHBjRFhGcmJJdGNZbVo5R3pyMGZvYncxWVdyTVlhZUNXRnE0Z284RDdpM3psY1JuVGRaODYyT2piaApWUHlrQXBjcnl
    ZVWFmOURmckVKR1lQMWZKVUxsMDRQMXZxb2dxUWMzK0ZqNTlDWGtCbXBYMWxtbTZDQ0ZtSTU3CldnRDl2NkU2QkNLYjA3eUtiaHdnb1EvYVZZUXJ6bFIxbjJ1ZHFSZ3JCUyt1bFc4bm5UTXZwY
    3l1ZzdNVFQ4MUsKRHowYm84Q2hBb0dCQVBvRnN2dHBZLzQrOUtpVzEvbEoxSzl5VmlFenRLYzhrWkxIdGEvbUNPMXJreGl5Q0lINApUSysvNkpFaFVxMjdyNTJUM1k1M282eUhiVThvS20rRzNW
    U3RneC9rRGc0QVFvZUJCZWd0b3dIaWtZKy9wbHVqClJZTXdCcm05MXVLaERXaXQ5YUV3RU5sZndvcUVmRmVVZy8yYkNKNGYrU3B4anczZFdaWEtPVFYxQW9HQkFPbXAKRnNMVUxxUmtCRjlxWU02
    aithb29KZTF5Q2pSRml5QzBtU1RZeEcvcXhqeWg1ZkdyaWJLN2owQnRqNUt6cnVVeApXK0wvVFVBQVBtbVBKTm00aFFabkdLOUFROERHYlJFdk5CT3MrZ1I1MTZBbHBjbVU3bGdRdzVlWXZDRFd4
    c1gxCm9xcUpBMzIvWE5lVDlVV0hFVzRoSmJNblZiWi9JeGRFejN2VlF2WDlBb0dCQUxyNDNCcHJmQjI4RXRhb3g2WmoKY2l3cVF3TGRXZkhldDdhZXB0NStGcHNHWWFDTU14U1BEVG81TjhDZ3ZP
    V3pmK2RGbHVCZDFBYkgyQlRrSXFmNApvVGdiOGYwOVhNMVhvR2taTWNPcHJVZFJtaTEzMHY2Z29QRTBUek5FSnBpZ2ZCaVdUeVJWZjZRdm1wcGY3V1RKCmFkT1R4dVFKWVJvK2hnNkdRK0ppc2xt
    cEFvR0JBTE5wR3NuMEEvQkR2N3ZId1pkSS9yMnhZQlhtdDRFVldOTisKK0F2N3lURXA4cGJCdFA2UU9RV2MwRXluRTFPUVZoMHpmaHRZN21iVENSa2lTU2hIYnhUUWVucXQvSmY3ZytscQo5akNa
    WTh1bUJuTzRGSWtvcXEzQ0NYelFVTHRpVG5QWHZOUDJxbENXYStJM2dGK25hekhGajkzMHVQS013bFB1CmlPWExoVU85QW9HQkFJL1RhQ1c5VTBSTWxCQUJKQjBVVFk4dXdTU2pnNVptTnNjOVFy
    VlIzU0tjRVIrdTQ0NTEKUXBpL2JJbEJWVzlLalMwUWZjaEFQMXY5NHpmdmF5U0hqR2hYZlI0VDRJWTNJSGVDNk1SL0hVT2NNUTFtOS8zNwppamJiSWR4Mk5WZ0lmUzRVZ0M2UnBqU04wck5TTSt6
    eEk2ZGtRdUNFaDIzYkowSHR0TXJEOVQvUwotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo=
#################################################################################################
hai@master:~/.kube$ kubectl explain pod
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an      
     object. Servers should convert recognized schemas to the latest internal  
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status       <Object>
     Most recently observed status of the pod. This data may not be up to date.
     Populated by the system. Read-only. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

#====================
https://www.youtube.com/watch?v=Fg9J8qp36V0&t=398s

What are Kubernetes Pods?
A pod is the smallest execution unit in Kubernetes. A pod encapsulates one or more applications. Pods are ephemeral by nature, if a pod (or the node it executes on) fails, Kubernetes can automatically create a new replica of that pod to continue operations. Pods include one or more containers (such as Docker containers).

https://kubernetes.io/docs/tasks/debug/debug-application/get-shell-running-container/
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#exec
kubectl exec --stdin --tty nginx-76d6c9b8c-4zhgz  -- /bin/bash
kubectl exec -i -t my-pod --container main-app -- /bin/bash

==################
hai@master:~$ kubectl explain deployment
KIND:     Deployment
VERSION:  apps/v1

DESCRIPTION:
     Deployment enables declarative updates for Pods and ReplicaSets.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Specification of the desired behavior of the Deployment.

   status       <Object>
     Most recently observed status of the Deployment.
-#----------------------------------------------

apiVersion: apps/v1
kind: Deployment
metadata:
   name: myredis
   labels: 
     app: myredis
spec: 
  selector:
     matchLabels:
        app: myredis
  replicas: 
  template:
     metadata: 
        labels:
           app:  myredis
     spec: 
        containers:
        -   name: myredis
            image: redis
            ports:
            -   containerPort: 6379
                name: myredis
----------------
hai@master:~/test$ kubectl create -f redis.yaml 

hai@master:~$ kubectl get deployments  -o wide 
NAME      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES   SELECTOR   
myredis   1/1     1            1           2m47s   myredis      redis    app=myredis

hai@master:~$ kubectl get pod -o wide 
NAME                       READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
myredis-5bd679c8ff-2j4xx   1/1     Running   0          3m23s   10.244.3.135   worker   <none>           <none>
#######
apiVersion: apps/v1
kind: Deployment
metadata:
   name: myredis
   labels: 
     app: myredis
spec: 
  selector:
     matchLabels:
        app: myredis
  replicas: 3    # patch work
  template:
     metadata: 
        labels:
           app:  myredis
     spec: 
        containers:
        -   name: myredis
            image: redis
            ports:
            -   containerPort: 6379
                name: myredis
hai@master:~/test$ kubectl create -f redis.yaml 
Error from server (AlreadyExists): error when creating "redis.yaml": deployments.apps "myredis" already exists
hai@master:~/test$ kubectl apply -f redis.yaml 
Warning: resource deployments/myredis is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
deployment.apps/myredis configured
------
hai@master:~/test$ kubectl get deployments  -o wide 
NAME      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES   SELECTOR
myredis   3/3     3            3           10m   myredis      redis    app=myredis

hai@master:~/test$ kubectl get pods -o wide 
NAME                       READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
myredis-5bd679c8ff-2j4xx   1/1     Running   0          9m16s   10.244.3.135   worker   <none>           <none>
myredis-5bd679c8ff-4p95p   1/1     Running   0          2m14s   10.244.3.136   worker   <none>           <none>
myredis-5bd679c8ff-s8wsl   1/1     Running   0          2m14s   10.244.3.137   worker   <none>           <none>
####

apiVersion: apps/v1
kind: Deployment
metadata:
   name: myredis
   labels: 
     app: myredis
spec: 
  selector:
     matchLabels:
        app: myredis
  replicas: 5
  template:
     metadata: 
        labels:
           app:  myredis
     spec: 
        containers:
        -   name: myredis
            image: redis
            ports:
            -   containerPort: 6379
                name: myredis
###
hai@master:~/test$ kubectl get pods  -o wide 
NAME                       READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
myredis-5bd679c8ff-2j4xx   1/1     Running   0          23m   10.244.3.135   worker   <none>           <none>
myredis-5bd679c8ff-4p95p   1/1     Running   0          16m   10.244.3.136   worker   <none>           <none>
myredis-5bd679c8ff-pn8gf   1/1     Running   0          17s   10.244.3.139   worker   <none>           <none>
myredis-5bd679c8ff-s2t59   1/1     Running   0          17s   10.244.3.138   worker   <none>           <none>
myredis-5bd679c8ff-s8wsl   1/1     Running   0          16m   10.244.3.137   worker   <none>           <none>
######
hai@master:~/test$ kubectl get pods -o wide
NAME                       READY   STATUS        RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
myredis-5bd679c8ff-2j4xx   1/1     Terminating   0          26m     10.244.3.135   worker   <none>           <none>
myredis-5bd679c8ff-4p95p   1/1     Terminating   0          19m     10.244.3.136   worker   <none>           <none>
myredis-5bd679c8ff-pn8gf   1/1     Terminating   0          3m23s   10.244.3.139   worker   <none>           <none>
myredis-5bd679c8ff-s2t59   1/1     Terminating   0          3m23s   10.244.3.138   worker   <none>           <none>
myredis-5bd679c8ff-s8wsl   1/1     Running       0          19m     10.244.3.137   worker   <none>           <none>
hai@master:~/test$ kubectl get pods -o wide 
NAME                       READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
myredis-5bd679c8ff-s8wsl   1/1     Running   0          19m   10.244.3.137   worker   <none>           <none>
###########################################################
hai@master:~/test$ kubectl get replicaset  -o wide 
NAME                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR
myredis-5bd679c8ff   3         3         3       32m   myredis      redis    app=myredis,pod-template-hash=5bd679c8ff
#### 
apiVersion: apps/v1
kind: Deployment
metadata:
   name: myredis
   labels: 
     app: myredis
spec: 
  selector:
     matchLabels:
        app: myredis
  replicas: 3
  template:
     metadata: 
        labels:
           app:  myredis
     spec: 
        containers:
        -   name: myredis
            image: redis:alpine
            ports:
            -   containerPort: 6379
                name: myredis
------
            
hai@master:~/test$ kubectl apply -f redis.yaml

hai@master:~/test$ kubectl get replicaset   -o wide 
NAME                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES         SELECTOR
myredis-56f6bbf856   3         3         3       80s   myredis      redis:alpine   app=myredis,pod-template-hash=56f6bbf856
myredis-5bd679c8ff   0         0         0       40m   myredis      redis          app=myredis,pod-template-hash=5bd679c8ff
##
hai@master:~/test$ kubectl get deployment 
NAME      READY   UP-TO-DATE   AVAILABLE   AGE
myredis   3/3     3            3           73m

hai@master:~/test$ kubectl set image deployment/myredis   myredis=redis:latest  
deployment.apps/myredis image updated            # just watch modification 

hai@master:~/test$ kubectl get replicaset  -o  wide
NAME                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES         SELECTOR
myredis-56f6bbf856   0         0         0       38m   myredis      redis:alpine   app=myredis,pod-template-hash=56f6bbf856
myredis-5bd679c8ff   0         0         0       77m   myredis      redis          app=myredis,pod-template-hash=5bd679c8ff
myredis-6d77c6b664   3         3         3       84s   myredis      redis:latest   app=myredis,pod-template-hash=6d77c6b664   ### 3 3 3 is in id "" myredis-6d77c6b664"
hai@master:~/test$

hai@master:~/test$ kubectl rollout undo deployment/myredis  
deployment.apps/myredis rolled back

hai@master:~/test$ kubectl get replicaset -o wide 
NAME                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES         SELECTOR
myredis-56f6bbf856   3         3         3       45m     myredis      redis:alpine   app=myredis,pod-template-hash=56f6bbf856   ### 3 3 3  id  old  " myredis-56f6bbf856"
myredis-5bd679c8ff   0         0         0       84m     myredis      redis          app=myredis,pod-template-hash=5bd679c8ff
myredis-6d77c6b664   0         0         0       7m43s   myredis      redis:latest   app=myredis,pod-template-hash=6d77c6b664
#####################################################
hai@master:~/test$ kubectl get all
NAME                           READY   STATUS    RESTARTS   AGE
pod/myredis-56f6bbf856-lnpk5   1/1     Running   0          7m7s
pod/myredis-56f6bbf856-rnzxl   1/1     Running   0          7m12s
pod/myredis-56f6bbf856-s79qr   1/1     Running   0          7m17s

NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   ClusterIP      10.96.0.1       <none>        443/TCP        17d
service/nginx        LoadBalancer   10.99.132.124   <pending>     80:30429/TCP   17d

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/myredis   3/3     3            3           90m      

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/myredis-56f6bbf856   3         3         3       51m
replicaset.apps/myredis-5bd679c8ff   0         0         0       90m
replicaset.apps/myredis-6d77c6b664   0         0         0       14m
--##---
hai@master:~/test$ kubectl describe deployment.apps/myredis 
Name:                   myredis
Namespace:              default
CreationTimestamp:      Thu, 15 Dec 2022 00:28:57 -0800
Labels:                 app=myredis
Annotations:            deployment.kubernetes.io/revision: 4
Selector:               app=myredis
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=myredis
  Containers:
   myredis:
    Image:        redis:alpine
    Port:         6379/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   myredis-56f6bbf856 (3/3 replicas created)
Events:
  Type    Reason             Age                   From                   Message
  ----    ------             ----                  ----                   -------
  Normal  ScalingReplicaSet  59m (x2 over 84m)     deployment-controller  Scaled up   replica set myredis-5bd679c8ff to 3 from 1
  Normal  ScalingReplicaSet  52m                   deployment-controller  Scaled up   replica set myredis-56f6bbf856 to 1
  Normal  ScalingReplicaSet  52m                   deployment-controller  Scaled down replica set myredis-5bd679c8ff to 2 from 3
  Normal  ScalingReplicaSet  52m                   deployment-controller  Scaled down replica set myredis-5bd679c8ff to 1 from 2
  Normal  ScalingReplicaSet  52m                   deployment-controller  Scaled up   replica set myredis-56f6bbf856 to 3 from 2
  Normal  ScalingReplicaSet  52m                   deployment-controller  Scaled down replica set myredis-5bd679c8ff to 0 from 1
  Normal  ScalingReplicaSet  15m                   deployment-controller  Scaled up   replica set myredis-6d77c6b664 to 1
  Normal  ScalingReplicaSet  15m                   deployment-controller  Scaled down replica set myredis-56f6bbf856 to 2 from 3
  Normal  ScalingReplicaSet  15m                   deployment-controller  Scaled up   replica set myredis-6d77c6b664 to 2 from 1
  Normal  ScalingReplicaSet  15m                   deployment-controller  Scaled down replica set myredis-56f6bbf856 to 1 from 2
  Normal  ScalingReplicaSet  15m                   deployment-controller  Scaled up   replica set myredis-6d77c6b664 to 3 from 2
  Normal  ScalingReplicaSet  15m                   deployment-controller  Scaled down replica set myredis-56f6bbf856 to 0 from 1
  Normal  ScalingReplicaSet  8m21s                 deployment-controller  Scaled up   replica set myredis-56f6bbf856 to 1 from 0
  Normal  ScalingReplicaSet  8m16s (x2 over 52m)   deployment-controller  Scaled up   replica set myredis-56f6bbf856 to 2 from 1
  Normal  ScalingReplicaSet  8m16s                 deployment-controller  Scaled down replica set myredis-6d77c6b664 to 2 from 3
  Normal  ScalingReplicaSet  8m5s (x3 over 8m11s)  deployment-controller  (combined from similar events): Scaled down replica set myredis-6d77c6b664 to 0 from 1

hai@master:~/test$ kubectl rollout status deployment/myredis
deployment "myredis" successfully rolled out
hai@master:~/test$ kubectl rollout history deployment/myredis 
deployment.apps/myredis 
REVISION  CHANGE-CAUSE  
1         <none>        
3         <none>        
4         <none> 
##------
    
hai@master:~$ kubectl get ns  -o  wide      # or kubectl get namespaces    -o   wide
NAME              STATUS   AGE
default           Active   17d
kube-flannel      Active   17d
kube-node-lease   Active   17d
kube-public       Active   17d
kube-system       Active   17d
metallb-system    Active   17d
hai@master:~$  kubectl get pods -n default  -o  wide 
NAME                       READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
myredis-56f6bbf856-lnpk5   1/1     Running   0          4h56m   10.244.3.150   worker   <none>           <none>
myredis-56f6bbf856-rnzxl   1/1     Running   0          4h56m   10.244.3.149   worker   <none>           <none>
myredis-56f6bbf856-s79qr   1/1     Running   0          4h56m   10.244.3.148   worker   <none>           <none>
hai@master:~$   kubectl get pods -n kube-flannel   -o  wide
NAME                    READY   STATUS    RESTARTS      AGE   IP               NODE     NOMINATED NODE   READINESS GATES
kube-flannel-ds-4phsm   1/1     Running   0             30h   192.168.68.145   worker   <none>           <none>
kube-flannel-ds-hbg8d   1/1     Running   2 (34h ago)   17d   192.168.68.138   master   <none>           <none>
hai@master:~$  kubectl get pods -n      kube-public   -o  wide 
No resources found in kube-public namespace.
hai@master:~$  kubectl get pods -n      kube-system  -o  wide 
NAME                             READY   STATUS             RESTARTS          AGE   IP               NODE     NOMINATED NODE   READINESS GATES
coredns-565d847f94-8drrl         0/1     CrashLoopBackOff   634 (3m28s ago)   17d   10.244.0.9       master   <none>           <none>
coredns-565d847f94-nnthx         0/1     CrashLoopBackOff   634 (3m28s ago)   17d   10.244.0.8       master   <none>           <none>
etcd-master                      1/1     Running            2 (34h ago)       17d   192.168.68.138   master   <none>           <none>
kube-apiserver-master            1/1     Running            2 (34h ago)       17d   192.168.68.138   master   <none>           <none>
kube-controller-manager-master   1/1     Running            2 (34h ago)       17d   192.168.68.138   master   <none>           <none>
kube-proxy-2sslb                 1/1     Running            0                 30h   192.168.68.145   worker   <none>           <none>
kube-proxy-r2x99                 1/1     Running            2 (34h ago)       17d   192.168.68.138   master   <none>           <none>
kube-scheduler-master            1/1     Running            2 (34h ago)       17d   192.168.68.138   master   <none>           <none>
hai@master:~$  kubectl get pods -n      kube-node-lease 
No resources found in kube-node-lease namespace.
hai@master:~$  kubectl get pods -n   metallb-system  
NAME                          READY   STATUS                       RESTARTS   AGE
controller-6fdc8d5477-t75nw   1/1     Running                      0          30h
speaker-6wqxk                 0/1     CreateContainerConfigError   0          30h
#------------------------------
kubectl create namespace venkata
hai@master:~$ kubectl get  ns
NAME              STATUS   AGE  
anjireddy         Active   2m48s
default           Active   17d  
kube-flannel      Active   17d  
kube-node-lease   Active   17d  
kube-public       Active   17d  
kube-system       Active   17d  
metallb-system    Active   17d  
velpula           Active   113s 
venkata           Active   2m24s
----
hai@master:~$ kubectl describe ns  anjireddy
Name:         anjireddy
Labels:       kubernetes.io/metadata.name=anjireddy
Annotations:  <none>
Status:       Active
No resource quota.
No LimitRange resource.

hai@master:~$ kubectl describe ns  venkata
Name:         venkata
Labels:       kubernetes.io/metadata.name=venkata
Annotations:  <none>
Status:       Active
No resource quota.
No LimitRange resource.
-#-========
kubectl run appserver  --image=nginx  -n  anjireddy
kubectl run appserver --image=nginx  -n  venkata
kubectl run appserver --image=nginx  -n velpula

hai@master:~$ kubectl get pods  -o  wide  -n  anjireddy
NAME         READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
appserver    1/1     Running   0          3m2s    10.244.3.153   worker   <none>           <none>
dataserver   1/1     Running   0          3m21s   10.244.3.152   worker   <none>           <none>
webserver    1/1     Running   0          4m8s    10.244.3.151   worker   <none>           <none>
hai@master:~$ kubectl get pods  -o  wide  -n  velpula
NAME        READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          2m1s    10.244.3.155   worker   <none>           <none>
dbserver    1/1     Running   0          110s    10.244.3.156   worker   <none>           <none>
webserver   1/1     Running   0          2m11s   10.244.3.154   worker   <none>           <none>
hai@master:~$ kubectl get pods  -o  wide  -n  venkata
NAME        READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          93s    10.244.3.158   worker   <none>           <none>
dbserver    1/1     Running   0          104s   10.244.3.157   worker   <none>           <none>
webserver   1/1     Running   0          71s    10.244.3.159   worker   <none>           <none>
####################################################

apiVersion: v1
kind: Pod
metadata:
   name: appserver
   namespace: anjireddy
spec: 
   containers:
    - name: nginx
      image: nginx
      ports: 
       - name: nginx
         containerPort: 80
         protocol : TCP

hai@master:~/test$ kubectl apply  -f  ns.yaml 
pod/appserver created

hai@master:~$ kubectl get pods -n  anjireddy  -o  wide
NAME        READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          2m6s   10.244.3.160   worker   <none>           <none>

#############################################################
apiVersion: v1
kind: Pod
metadata: 
    name: appserver
    namespace: velpula
spec:
  containers:
     - name: nginx
       image: nginx
       ports: 
         - name: nginx
           containerPort: 80
           protocol: TCP

hai@master:~/test$ kubectl apply -f  nsve.yaml   -n  velpula
pod/appserver created

hai@master:~/test$ kubectl get pods -n  velpula
NAME        READY   STATUS    RESTARTS   AGE
appserver   1/1     Running   0          7s
############################################################
apiVersion: v1
kind: Pod
metadata: 
   name: appserver
   namespace: venkata
spec:
  containers:
    - name: nginx
      image: nginx
      ports: 
      - containerPort: 80
        name: nginx
        protocol: TCP

hai@master:~/test$ kubectl apply -f venka.yaml  
pod/appserver created

hai@master:~/test$ kubectl get pods  -n  venkata  -o wide
NAME        READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          41s   10.244.3.162   worker   <none>           <none>

hai@master:~/test$ kubectl delete -f venka.yaml  -n  venkata 
pod "appserver" deleted

hai@master:~/test$ kubectl get pods  -n  venkata  -o wide
No resources found in venkata namespace.
-############################################
hai@master:~$ kubectl exec -it  nginx   -- /bin/bash
root@nginx:/# ls
bin  boot  dev  docker-entrypoint.d  docker-entrypoint.sh  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
root@nginx:/# 

###############################################################
hai@master:~$ kubectl get ns
NAME              STATUS   AGE
anjireddy         Active   13h
default           Active   18d
kube-flannel      Active   18d
kube-node-lease   Active   18d
kube-public       Active   18d
kube-system       Active   18d
metallb-system    Active   18d
velpula           Active   12h
venkata           Active   12h
-###
hai@master:~$ kubectl  config get-contexts 
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   

hai@master:~$ kubectl  config set-context anjireddy1  --namespace=anjireddy  --user=kubernetes-admin --cluster=kubernetes
Context "anjireddy1" created.

hai@master:~$ kubectl config  get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
          anjireddy1                    kubernetes   kubernetes-admin   anjireddy
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin

hai@master:~$ kubectl  config set-context velpula   --namespace=velpula  --user=kubernetes-admin --cluster=kubernetes
Context "velpula" created.

hai@master:~$ kubectl config set-context  venkata1  --namespace=venkata  --user=kubernetes-admin  --cluster=kubernetes
Context "venkata1" created.

hai@master:~$ kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
          anjireddy1                    kubernetes   kubernetes-admin   anjireddy
***       kubernetes-admin@kubernetes   kubernetes   kubernetes-admin
          velpula                       kubernetes   kubernetes-admin   velpula
          venkata1                      kubernetes   kubernetes-admin   venkata

hai@master:~$ kubectl config use-context anjireddy1
Switched to context "anjireddy1".
hai@master:~$ kubectl config  get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
* **      anjireddy1                    kubernetes   kubernetes-admin   anjireddy
          kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   
          velpula                       kubernetes   kubernetes-admin   velpula  
          venkata1                      kubernetes   kubernetes-admin   venkata  
hai@master:~$ kubectl config use-context anjireddy1
Switched to context "anjireddy1".
hai@master:~$ kubectl config  get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         anjireddy1                    kubernetes   kubernetes-admin   anjireddy
          kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   
          velpula                       kubernetes   kubernetes-admin   velpula  
          venkata1                      kubernetes   kubernetes-admin   venkata  

hai@master:~$ kubectl get pods 
NAME        READY   STATUS    RESTARTS   AGE
appserver   1/1     Running   0          12h

hai@master:~$ kubectl run  redis --image=redis
pod/redis created

hai@master:~$ kubectl get pods  -o  wide 
NAME        READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          12h   10.244.3.160   worker   <none>           <none>
redis       1/1     Running   0          15s   10.244.3.166   worker   <none>           <none>

hai@master:~$ kubectl describe ns  anjireddy 
Name:         anjireddy
Labels:       kubernetes.io/metadata.name=anjireddy
Annotations:  <none>
Status:       Active
No resource quota.
-########################################################
hai@master:~$ kubectl get pods 
NAME        READY   STATUS    RESTARTS   AGE
appserver   1/1     Running   0          14h
redis       1/1     Running   0          83m
hai@master:~$ kubectl describe pod  redis
Name:             redis
Namespace:        anjireddy
Node:             worker/192.168.68.145
IP:               10.244.3.166
Containers:
  redis:
    Container ID:   containerd://89a3b1986891dfd975300b89d03a43458467cfe1003584d0af4b003ad8ff5602
    Image:          redis
##############################################################################
hai@master:~$ kubectl   config  get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
****      anjireddy1                    kubernetes   kubernetes-admin   anjireddy
          default
          kubernetes-admin@kubernetes   kubernetes   kubernetes-admin
          velpula                       kubernetes   kubernetes-admin   velpula  
          venkata1                      kubernetes   kubernetes-admin   venkata  

hai@master:~$ kubectl config use-context   kubernetes-admin@kubernetes
Switched to context "kubernetes-admin@kubernetes".

hai@master:~$ kubectl config get-contexts 
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
          anjireddy1                    kubernetes   kubernetes-admin   anjireddy
          default
****      kubernetes-admin@kubernetes   kubernetes   kubernetes-admin
          velpula                       kubernetes   kubernetes-admin   velpula
          venkata1                      kubernetes   kubernetes-admin   venkata
###################
hai@master:~$ kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
          anjireddy1                    kubernetes   kubernetes-admin   anjireddy
          default
***       kubernetes-admin@kubernetes   kubernetes   kubernetes-admin
          velpula                       kubernetes   kubernetes-admin   velpula
          venkata1                      kubernetes   kubernetes-admin   venkata

hai@master:~$ kubectl config delete-context anjireddy1
deleted context anjireddy1 from /home/hai/.kube/config

hai@master:~$ kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
          default
***       kubernetes-admin@kubernetes   kubernetes   kubernetes-admin
          velpula                       kubernetes   kubernetes-admin   velpula
          venkata1                      kubernetes   kubernetes-admin   venkata

hai@master:~$ kubectl config delete-context  velpula 
deleted context velpula from /home/hai/.kube/config

hai@master:~$ kubectl config delete-context  venkata1
deleted context venkata1 from /home/hai/.kube/config

hai@master:~$ kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
****         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin
#######################################################################################
hai@master:~$ kubectl get pods 
NAME    READY   STATUS    RESTARTS   AGE 
nginx   1/1     Running   0          145m

hai@master:~$ kubectl   exec  -it  nginx  --  /bin/bash
root@nginx:/# ls
bin   dev                  docker-entrypoint.sh  home  lib64  mnt  proc  run   srv  tmp  var
boot  docker-entrypoint.d  etc                   lib   media  opt  root  sbin  sys  usr     

root@nginx:/#
hai@master:~$ kubectl describe pod nginx
Name:             nginx  
Namespace:        default

IP:               10.244.3.165
IPs:
  IP:  10.244.3.165
Containers:
  nginx:
    Container ID:   containerd://b1415f5e20cc0edebe9ef8226d8965189d8eed297fdd28367dc82d2a99e97bfa
    Image:          nginx
 
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
hai@master:~$ kubectl exec -it  <POD NAME>  -c  <container NAME> --  /bin/bash
#########################################################################################################
hai@master:~$ kubectl get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          3h4m   10.244.3.165   worker   <none>           <none>

hai@master:~$ kubectl port-forward  nginx 8089:80  --address  0.0.0.0              ###################### best ############################
Forwarding from 0.0.0.0:8089 -> 80                   ## see look port forword  port 
 Handling connection for 8089
Handling connection for 8089
^Z
[1]+  Stopped                 kubectl port-forward nginx 8089:80 --address 0.0.0.0
hai@master:~$  
========##############=================--------------------------------------
taints  taint 

hai@master:~$ kubectl describe node master
Name:               master
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=master
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"02:c7:ec:a6:aa:31"}    
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 192.168.68.138
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 27 Nov 2022 07:36:46 -0800
Taints:             node-role.kubernetes.io/control-plane:NoSchedule      ## best importent - taint  is off mode
Unschedulable:      false
Lease:
  HolderIdentity:  master
  AcquireTime:     <unset>
  RenewTime:       Thu, 15 Dec 2022 23:47:55 -0800
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Tue, 13 Dec 2022 20:44:10 -0800   Tue, 13 Dec 2022 20:44:10 -0800   FlannelIsUp                  Flannel is running on this node
  MemoryPressure       False   Thu, 15 Dec 2022 23:43:37 -0800   Sun, 27 Nov 2022 07:36:46 -0800   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Thu, 15 Dec 2022 23:43:37 -0800   Sun, 27 Nov 2022 07:36:46 -0800   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Thu, 15 Dec 2022 23:43:37 -0800   Sun, 27 Nov 2022 07:36:46 -0800   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Thu, 15 Dec 2022 23:43:37 -0800   Sun, 27 Nov 2022 07:51:50 -0800   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  192.168.68.138
  Hostname:    master
Capacity:
  cpu:                4
  ephemeral-storage:  71151768Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3983208Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  65573469281
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3880808Ki
  pods:               110
System Info:
  Machine ID:                 65b65a2b19884bb294b23d024674f4e1
  System UUID:                a0764d56-7953-c519-4b13-d8ece9e9158b
  Boot ID:                    a8407919-5a25-47b5-b450-9b0484afe50d
  Kernel Version:             5.15.0-56-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.12
  Kubelet Version:            v1.25.4
  Kube-Proxy Version:         v1.25.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                              ------------  ----------  ---------------  -------------  ---
  kube-flannel                kube-flannel-ds-hbg8d             100m (2%)     100m (2%)   50Mi (1%)        50Mi (1%)      18d
  kube-system                 coredns-565d847f94-8drrl          100m (2%)     0 (0%)      70Mi (1%)        170Mi (4%)     18d
  kube-system                 coredns-565d847f94-nnthx          100m (2%)     0 (0%)      70Mi (1%)        170Mi (4%)     18d
  kube-system                 etcd-master                       100m (2%)     0 (0%)      100Mi (2%)       0 (0%)         18d
  kube-system                 kube-apiserver-master             250m (6%)     0 (0%)      0 (0%)           0 (0%)         18d
  kube-system                 kube-controller-manager-master    200m (5%)     0 (0%)      0 (0%)           0 (0%)         18d
  kube-system                 kube-proxy-r2x99                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         18d
  kube-system                 kube-scheduler-master             100m (2%)     0 (0%)      0 (0%)           0 (0%)         18d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (23%)  100m (2%)
  memory             290Mi (7%)  390Mi (10%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>
=================================================================================
hai@master:~$ kubectl describe node master   | grep -i  taints        ###  best  importent ###
Taints:             node-role.kubernetes.io/control-plane:NoSchedule

hai@master:~$ kubectl get nodes 
NAME     STATUS   ROLES           AGE   VERSION
master   Ready    control-plane   18d   v1.25.4
worker   Ready    <none>          47h   v1.25.4

hai@master:~$ kubectl get pods  -o  wide 
NAME    READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          4h3m   10.244.3.165   worker   <none>           <none>

hai@master:~$ kubectl run redis  --image=redis
pod/redis created

hai@master:~$ kubectl get pods  -o  wide
NAME    READY   STATUS              RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running             0          4h4m   10.244.3.165   worker   <none>           <none>
redis   0/1     ContainerCreating   0          3s     <none>         worker   <none>           <none>

hai@master:~$ kubectl get pods  -o  wide 
NAME    READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          4h4m   10.244.3.165   worker   <none>           <none>
redis   1/1     Running   0          6s     10.244.3.168   worker   <none>           <none>

hai@master:~$ kubectl describe  node master |  grep -i taint              ###  best  importent ###
Taints:             node-role.kubernetes.io/control-plane:NoSchedule

hai@master:~$ kubectl taint node  master node-role.kubernetes.io/control-plane-       ###  best  importent ###
node/master untainted

hai@master:~$ kubectl describe node master | grep -i taint      ###  best  importent ###
Taints:             <none>

hai@master:~$ kubectl run nginx1  --image=nginx
pod/nginx1 created

hai@master:~$ kubectl run redis1  --image=redis
pod/redis1 created

hai@master:~$ kubectl get pods  -o wide 
NAME     READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
nginx    1/1     Running   0          4h13m   10.244.3.165   worker   <none>           <none>
nginx1   1/1     Running   0          34s     10.244.3.169   master   <none>           <none>       ###  best  "importent" ###
redis    1/1     Running   0          9m8s    10.244.3.168   worker   <none>           <none>
redis1   1/1     Running   0          16s     10.244.3.170   master   <none>           <none>
---#===============================================================
hai@master:~$ kubectl describe  node master |  grep -i taint              ###  best  "importent-" ###
Taints:             <none>
hai@master:~$ kubectl  taint node  master node-role.kubernetes.io/control-plane:NoSchedule                ###  'best  importent" ###
node/master tainted
hai@master:~$ kubectl describe  node master |  grep -i taint              ###  best  importent ###
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
======================================================================
hai@master:~$ kubectl delete pods --all
pod "app-server" deleted
pod "nginx" deleted
pod "nginx1" deleted
pod "redis" deleted
pod "redis1" deleted
-------====
hai@master:~$ kubectl run nginx --image=nginx
pod/nginx created

hai@master:~$ kubectl get pods  -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          6s    10.244.3.172   worker   <none>           <none>

hai@master:~$ kubectl describe  node worker | grep -i  taint             ###  'best  importent" ###
Taints:             <none>

hai@master:~$ kubectl run app-server  --image=nginx
pod/app-server created

hai@master:~$ kubectl describe  node worker | grep -i  taint      ###  'best  importent'  
Taints:             <none>

hai@master:~$ kubectl taint node worker key=value:NoSchedule    ###  'best  importent" ###
node/worker tainted

hai@master:~$ kubectl describe  node worker | grep -i  taint     ###  'best  importent
Taints:             key=value:NoSchedule

hai@master:~$ kubectl get pods -o  wide 
NAME    READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          10m   10.244.3.172   worker   <none>           <none>

hai@master:~$ kubectl run webserver  --image=nginx
pod/webserver created

hai@master:~$ kubectl get pods 
NAME        READY   STATUS    RESTARTS   AGE
nginx       1/1     Running   0          14m
webserver   0/1     Pending   0          12s

hai@master:~$ kubectl run appserver  --image=nginx
pod/appserver created

hai@master:~$ kubectl run db server  --image=nginx
pod/db created

hai@master:~$ kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
appserver   0/1     Pending   0          11s     ###  'best  importent" ###
db          0/1     Pending   0          3s      ###  'best  importent" ###
nginx       1/1     Running   0          15m
webserver   0/1     Pending   0          62s
-=#########
hai@master:~$ kubectl describe node worker  |  grep -i taint      ###  'best  importent" ###
Taints:             key=value:NoSchedule

hai@master:~$ kubectl taint  node  worker key=value:NoSchedule-       ###  'best  importent" ###
node/worker untainted

hai@master:~$ kubectl describe node worker  |  grep -i taint
Taints:             <none>                                          ## "best

hai@master:~$ kubectl get pods  -o  wide 
NAME        READY   STATUS             RESTARTS     AGE     IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running            0            8m20s   10.244.3.175   worker   <none>           <none>
db          0/1     CrashLoopBackOff   1 (8s ago)   8m12s   10.244.3.173   worker   <none>           <none>
nginx       1/1     Running            0            23m     10.244.3.172   worker   <none>           <none>
webserver   1/1     Running            0            9m11s   10.244.3.174   worker   <none>           <none>


hai@master:~$ kubectl run app-server  --image=nginx
pod/app-server created

hai@master:~$ kubectl describe  node worker | grep -i  taint    ###  'best  importent" ###
Taints:             <none>

hai@master:~$ kubectl taint node worker key=value:NoSchedule   
node/worker tainted

hai@master:~$ kubectl describe  node worker | grep -i  taint      ###  'best  importent" ###
Taints:             key=value:NoSchedule

hai@master:~$ kubectl get pods -o  wide 
NAME    READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          10m   10.244.3.172   worker   <none>           <none>

hai@master:~$ kubectl run webserver  --image=nginx
pod/webserver created

hai@master:~$ kubectl get pods 
NAME        READY   STATUS    RESTARTS   AGE
nginx       1/1     Running   0          14m
webserver   0/1     Pending   0          12s

hai@master:~$ kubectl describe node worker  |  grep -i taint     ###  'best  importent" ###
Taints:             key=value:NoSchedule



hai@master:~$ kubectl taint  node  worker key=value:NoSchedule-
node/worker untainted


hai@master:~$ kubectl describe node worker  |  grep -i taint
Taints:             <none>


hai@master:~$ kubectl get pods  -o  wide
NAME        READY   STATUS             RESTARTS     AGE     IP             NODE     NOMINATED NODE   READINESS GATES

hai@master:~$ kubectl get pods 
NAME        READY   STATUS             RESTARTS      AGE
appserver   1/1     Running            0             11m

hai@master:~$ kubectl run appserver   --image=nginx
pod/appserver created


hai@master:~$ kubectl get pods -o  wide 
NAME        READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          22s   10.244.3.176   worker   <none>           <none>

hai@master:~$ kubectl  describe node  worker  |  grep -i  taint
Taints:             <none>

hai@master:~$ kubectl  taint node  worker key=value:NoExecute         or   ###  'best  importent" ### #####3==3###
node/worker tainted

hai@master:~$ kubectl  describe node  worker  |  grep -i  taint
Taints:             key=value:NoExecute

hai@master:~$ kubectl get nodes 
NAME     STATUS   ROLES           AGE    VERSION
master   Ready    control-plane   18d    v1.25.4
worker   Ready    <none>          2d1h   v1.25.4

hai@master:~$ kubectl get  pods -o  wide 
No resources found in default namespace.

hai@master:~$ kubectl  run   webserver  --image=nginx 
pod/webserver created
hai@master:~$ kubectl  run   appserver  --image=nginx
pod/appserver created
hai@master:~$ kubectl  run   dbserver  --image=nginx
pod/dbserver created

hai@master:~$ kubectl get  pods -o  wide
NAME        READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
appserver   0/1     Pending   0          16s   <none>   <none>   <none>           <none>
dbserver    0/1     Pending   0          4s    <none>   <none>   <none>           <none>
webserver   0/1     Pending   0          30s   <none>   <none>   <none>           <none>
-#####################################################################################
hai@master:~$ kubectl get pods 
NAME        READY   STATUS    RESTARTS   AGE
appserver   0/1     Pending   0          27m
dbserver    0/1     Pending   0          27m
webserver   0/1     Pending   0          27m

hai@master:~$  kubectl describe  pod appserver
Name:             appserver    
Namespace:        default      
Status:           Pending      
IP:
IPs:              <none>
Containers:
  appserver:
    Image:        nginx
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  4m15s (x6 over 29m)  default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key: value}, 1 node(s) 
  had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.
####
hai@master:~$ kubectl  describe pod dbserver
Name:             dbserver
Namespace:        default
Status:           Pending
IP:
IPs:              <none>
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  2m42s (x7 over 33m)  default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key: value}, 1 node(s) 
  had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.

hai@master:~$ kubectl  describe pod webserver 
Name:             webserver
Namespace:        default
Status:           Pending
IP:
IPs:              <none>
Evets:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  3m41s (x7 over 34m)  default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key: value}, 1 node(s)
   had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.

hai@master:~$ kubectl get pods -o wide 
NAME        READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
appserver   0/1     Pending   0          42m   <none>   <none>   <none>           <none>
dbserver    0/1     Pending   0          41m   <none>   <none>   <none>           <none>
webserver   0/1     Pending   0          42m   <none>   <none>   <none>           <none>

hai@master:~$ kubectl taint   node  worker  key:NoExecute-          ###  'best  importent" ###
node/worker untainted

hai@master:~$ kubectl get pods  -o wide 
NAME        READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          43m   10.244.3.178   worker   <none>           <none>
dbserver    1/1     Running   0          43m   10.244.3.180   worker   <none>           <none>
webserver   1/1     Running   0          43m   10.244.3.177   worker   <none>           <none>

hai@master:~$ kubectl describe node  worker | grep -i  taint 
Taints:             <none>
====================================================================================================================================
LABELS    LABELS   LABELS     
hai@master:~$ kubectl label node worker  node=w1
node/worker labeled

---
hai@master:~$ kubectl describe node  worker
Name:               worker
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=worker
                    kubernetes.io/os=linux
                    node=w1                 ##"  best importent ---

2 00:09:05 -0800   Thu, 15 Dec 2022 19:32:07 -0800   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  192.168.68.145
  Hostname:    worker
Capacity:
  cpu:                4
  ephemeral-storage:  71151768Ki
  memory:             3983208Ki
Sy54
  Kernel Version:             5.15.0-56-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.10
  Kubelet Version:            v1.25.4
  Kube-Proxy Version:         v1.25.4eaker-jhrrx                  100m (2%)     100m (2%)   100Mi (2%)       100Mi (2%)     22h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                300m (7%)   300m (7%)
  memory             250Mi (6%)  250Mi (6%)

##########################-######

}}{{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}    ERROR  "ERROR "
hai@master:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16
I1218 06:45:37.145708  633010 version.go:256] remote version is much newer: v1.26.0; falling back to: stable-1.25
[init] Using Kubernetes version: v1.25.5
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR Port-6443]: Port 6443 is in use
	[ERROR Port-10259]: Port 10259 is in use
	[ERROR Port-10257]: Port 10257 is in use
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml]: /etc/kubernetes/manifests/kube-apiserver.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml]: /etc/kubernetes/manifests/kube-controller-manager.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml]: /etc/kubernetes/manifests/kube-scheduler.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-etcd.yaml]: /etc/kubernetes/manifests/etcd.yaml already exists
	[ERROR Port-2379]: Port 2379 is in use
	[ERROR Port-2380]: Port 2380 is in use
	[ERROR DirAvailable--var-lib-etcd]: /var/lib/etcd is not empty
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/kube-apiserver.yaml 

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/kube-controller-manager.yaml 

hai@master:~$ sudo rm  -rf  /etc/kubernetes/manifests/kube-scheduler.yaml

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/etcd.yaml

   only MASTER   =:==  ERROR ERROR  ERROR  
https://stackoverflow.com/questions/72504257/i-encountered-when-executing-kubeadm-init-error-issue
ANSWER 

sudo rm -rf  /etc/kubernetes/kubelet.conf
sudo rm -rf /etc/kubernetes/pki
sudo systemctl restart containerd

[ERROR Port-6443]: Port 6443 is in use   }{}{{{{{{{[[   "error " ]]}}}}}}}

https://github.com/kubernetes/kubeadm/issues/339
https://github.com/kubernetes/kubeadm/issues/1145
https://discuss.kubernetes.io/t/kubeadm-init-fails/16888


$ netstat -lnp | grep 1025
tcp6       0      0 :::10251                :::*                    LISTEN      4366/kube-scheduler
tcp6       0      0 :::10252                :::*                    LISTEN      4353/kube-controlle
$ kill 4366
$ kill 4353


hai@master:~$ sudo netstat -lnp | grep 6443

sudo kill Process_PID




hai@master:~$ sudo netstat -lnp | grep 6443
hai@master:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16
I1218 07:12:20.139997  637971 version.go:256] remote version is much newer: v1.26.0; falling back to: stable-1.25
[init] Using Kubernetes version: v1.25.5
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master] and IPs [10.96.0.1 192.168.68.138]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost master] and IPs [192.168.68.138 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost master] and IPs [192.168.68.138 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
error execution phase kubeconfig/admin: a kubeconfig file "/etc/kubernetes/admin.conf" exists already but has got the wrong CA cert
To see the stack trace of this error execute with --v=5 or higher
hai@master:~$ kubectl get nodes 
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?
hai@master:~$ rm -rf  /etc/kubernetes/admin.conf
rm: cannot remove '/etc/kubernetes/admin.conf': Permission denied
hai@master:~$ sudo rm -rf  /etc/kubernetes/admin.conf


root@master:~# cd /etc/kubernetes/
root@master:/etc/kubernetes# ll
total 60
drwxr-xr-x   5 root root  4096 Dec 18 07:16 ./
drwxr-xr-x 141 root root 12288 Dec 18 06:53 ../
-rw-------   1 root root  5638 Dec 18 07:16 admin.conf
-rw-------   1 root root  5674 Nov 27 07:35 controller-manager.conf
-rw-------   1 root root  5650 Dec 18 07:16 kubelet.conf
drwxr-xr-x   2 root root  4096 Dec 18 06:47 manifests/
drwxr-xr-x   3 root root  4096 Dec 18 07:13 pki/
-rw-------   1 root root  5622 Nov 27 07:35 scheduler.conf
drwx------   4 root root  4096 Dec 13 09:10 tmp/
root@master:/etc/kubernetes# rm -rf  *
root@master:/etc/kubernetes# ll
total 16
drwxr-xr-x   2 root root  4096 Dec 18 07:16 ./
drwxr-xr-x 141 root root 12288 Dec 18 06:53 ../
root@master:/etc/kubernetes# 




Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.68.150:6443 --token dv9gyw.fo8f5ndssq1jh4xb \
	--discovery-token-ca-cert-hash sha256:9d71830a49fdc3e150b23c68d775586c3d5eeb9bba2ad715b79de2256b674309 
hai@master:~$ 




hai@master:~$ kubeadm token create --print-join-commandkubeadm join 192.168.68.150:6443 --token dv9gyw.fo8f5ndssq1jh4xb \
	--discovery-token-ca-cert-hash sha256:9d71830a49fdc3e150b23c68d775586c3d5eeb9bba2ad715b79de2256b674309 

https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/

 get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap.. 

hai@master:~/test$ kubectl get csr
NAME        AGE   SIGNERNAME                                    REQUESTOR                 REQUESTEDDURATION   CONDITION
csr-ksd9v   10m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:sgn48v   <none>              Approved,Issued
csr-wffc8   16m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:sgn48v   <none>              Pending
csr-ztxrb   21m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:sgn48v   <none>              Pending

hai@master:~/test$ kubectl certificate approve csr-ksd9v
certificatesigningrequest.certificates.k8s.io/csr-ksd9v approved

hai@master:~$ kubectl label node ubuntu   node=worker2
node/ubuntu labeled

hai@master:~$ kubectl describe node ubuntu
Name:               ubuntu
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=ubuntu
                    kubernetes.io/os=linux
                    node=worker2       ##3=="ddf"
-###################
hai@master:~/test$ kubectl get node  -o wide 
NAME     STATUS   ROLES           AGE    VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME  
master   Ready    control-plane   19d    v1.25.4   192.168.68.138   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.12
"ubuntu" Ready    <none>          87m    v1.25.4   192.168.68.144   <none>        Ubuntu 20.04.5 LTS   5.15.0-53-generic   containerd://1.6.13
worker   Ready    <none>          3d3h   v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.10

apiVersion: v1
kind:  Pod
metadata:
  name: redis1
spec:
  containers:
  -   image: redis
      name:  redis
  nodeSelector: 
      node: "worker2"     

hai@master:~/test$ kubectl get po -o  wide 
NAME      READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
freddis   1/1     Running   0          9s    10.244.4.14   "ubuntu"  <none>           <none>       #worker2
fredis    1/1     Running   0          39s   10.244.4.13   "ubuntu"   <none>           <none>
redis     1/1     Running   0          96s   10.244.4.12  "ubuntu"  <none>           <none>
hai@master:~/test$ 

https://kubernetes.io/docs/reference/
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#pod-v1-core
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#objectmeta-v1-meta

APIService [apiregistration/v1]
Binding [core/v1]
CSIDriver [storage/v1]
CSINode [storage/v1]
CSIStorageCapacity [storage/v1]
CSIStorageCapacity [storage/v1beta1]
CertificateSigningRequest [certificates/v1]
ClusterCIDR [networking/v1alpha1]
ClusterRole [rbac/v1]
ClusterRoleBinding [rbac/v1]
ComponentStatus [core/v1]
ConfigMap [core/v1]
ControllerRevision [apps/v1]
CronJob [batch/v1]
CustomResourceDefinition [apiextensions/v1]
DaemonSet [apps/v1]
Deployment [apps/v1]
EndpointSlice [discovery/v1]
Endpoints [core/v1]
Event [core/v1]
Event [events/v1]
Eviction [policy/v1]
FlowSchema [flowcontrol/v1beta3]
FlowSchema [flowcontrol/v1beta2]
HorizontalPodAutoscaler [autoscaling/v2]
HorizontalPodAutoscaler [autoscaling/v1]
Ingress [networking/v1]
IngressClass [networking/v1]
Job [batch/v1]
JobTemplateSpec [batch/v1]
Lease [coordination/v1]
LimitRange [core/v1]
LocalSubjectAccessReview [authorization/v1]
MutatingWebhookConfiguration [admissionregistration/v1]
Namespace [core/v1]
NetworkPolicy [networking/v1]
Node [core/v1]
PersistentVolume [core/v1]
PersistentVolumeClaim [core/v1]
PersistentVolumeClaimTemplate [core/v1]
Pod [core/v1]
PodDisruptionBudget [policy/v1]
PodScheduling [resource/v1alpha1]
PodTemplate [core/v1]
PodTemplateSpec [core/v1]
PriorityClass [scheduling/v1]
PriorityLevelConfiguration [flowcontrol/v1beta3]
PriorityLevelConfiguration [flowcontrol/v1beta2]
ReplicaSet [apps/v1]
ReplicationController [core/v1]
ResourceClaim [resource/v1alpha1]
ResourceClaimTemplate [resource/v1alpha1]
ResourceClaimTemplateSpec [resource/v1alpha1]
ResourceClass [resource/v1alpha1]
ResourceQuota [core/v1]
Role [rbac/v1]
RoleBinding [rbac/v1]
RuntimeClass [node/v1]
Scale [autoscaling/v1]
Secret [core/v1]
SelfSubjectAccessReview [authorization/v1]
SelfSubjectReview [authentication/v1alpha1]
SelfSubjectRulesReview [authorization/v1]
Service [core/v1]
ServiceAccount [core/v1]
StatefulSet [apps/v1]
StorageClass [storage/v1]
StorageVersion [apiserverinternal/v1alpha1]
SubjectAccessReview [authorization/v1]
TokenRequest [authentication/v1]
TokenReview [authentication/v1]
ValidatingAdmissionPolicy [admissionregistration/v1alpha1]
ValidatingAdmissionPolicyBinding [admissionregistration/v1alpha1]
ValidatingWebhookConfiguration [admissionregistration/v1]
VolumeAttachment [storage/v1]

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#objectmeta-v1-meta
labels
object	   Map of string keys and values that can be used to organize and categorize (scope and select) objects.
             May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels
https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
PodSpec v1 core

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#container-v1-core

image
string	  Container image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level 
          config management to default or override container images in workload controllers like Deployments and StatefulSets.

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#container-v1-core

ports
ContainerPort array
patch strategy: merge
patch merge key: containerPort	List of ports to expose from the container. Not specifying a port here DOES NOT prevent that port from being exposed. 
Any port which is listening on the default "0.0.0.0" address inside a container will be accessible from the network. Modifying this array with strategic 
merge patch may corrupt the data. For more information See https://github.com/kubernetes/kubernetes/issues/108255. Cannot be updated.

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#containerport-v1-core

Field                	Description
containerPort
integer	            Number of port to expose on the pod's IP address. This must be a valid port number, 0 < x < 65536.

hostIP
string	           What host IP to bind the external port to.

hostPort
integer              	Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. If HostNetwork is specified, this must match ContainerPort. Most containers do not need this.

name
string	             If specified, this must be an IANA_SVC_NAME and unique within the pod. Each named port in a pod must have a unique name. Name for the port that can be referred to by services.

protocol
string              	Protocol for port. Must be UDP, TCP, or SCTP. Defaults to "TCP".


======
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#pod-v1-core

nodeSelector
object	NodeSelector is a selector which must be true for the pod to fit on a node. Selector which must match a node's labels for the pod to be scheduled on that node. More info: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/

=======================================================
https://kubernetes.io/docs/reference/

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#replicationcontroller-v1-core

Group	   Version 	    Kind
core     	v1         	ReplicationController

apiVersion: v1
kind:  Pod
metadata:
  name: freddis
spec:
  containers:
  -   image: redis
      name:  redis
  nodeSelector: 
      node: "worker2"         ### label  

hai@master:~$ kubectl get ns
NAME              STATUS   AGE
anjireddy         Active   2d13h
default           Active   20d
kube-flannel      Active   20d
kube-node-lease   Active   20d
kube-public       Active   20d
kube-system       Active   20d
metallb-system    Active   20d
velpula           Active   2d13h
venkata           Active   2d13h
hai@master:~$ kubectl delete namespace anjireddy
namespace "anjireddy" deleted
hai@master:~$ kubectl delete namespace anjireddy   velpula  venkata
namespace "velpula" deleted
namespace "venkata" deleted
Error from server (NotFound): namespaces "anjireddy" not found

hai@master:~$ kubectl create namespace project1 
namespace/project1 created

hai@master:~$ kubectl create namespace project2
namespace/project2 created

hai@master:~$ kubectl get namespaces 
NAME              STATUS   AGE
default           Active   20d
kube-flannel      Active   20d
kube-node-lease   Active   20d
kube-public       Active   20d
kube-system       Active   20d
metallb-system    Active   20d
project1          Active   48s
project2          Active   42s

hai@master:~$ kubectl get pods  -n  project1   -o wide 
NAME        READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          2m13s   10.244.3.194   worker   <none>           <none>
dbserver    1/1     Running   0          92s     10.244.3.195   worker   <none>           <none>
webserver   1/1     Running   0          43s     10.244.3.196   worker   <none>           <none>

hai@master:~$ kubectl label node worker-2  node=w2
node/worker-2 labeled
hai@master:~$ kubectl describe node worker-2 
Name:               worker-2
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=worker-2
                    kubernetes.io/os=linux
                    node=w2
hai@master:~$ kubectl get nodes  --show-labels  -o  wide 
NAME       STATUS   ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME     LABELS
master     Ready    control-plane   20d     v1.25.4   192.168.68.138   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.12   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
worker     Ready    <none>          3d23h   v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.10   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker,kubernetes.io/os=linux,node=w1
worker-2   Ready    <none>          33m     v1.25.4   192.168.68.144   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.13   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-2,kubernetes.io/os=linux,node=w2
-#####################################################################################################################################
VIDEO  - 8  
Kubernetes-Day-8
https://www.youtube.com/watch?v=uUIobDBLw6o

root@master:~# cd /etc/kubernetes/                   
root@master:/etc/kubernetes# ll
total 56
drwxr-xr-x   5 root root  4096 Dec 13 09:09 ./
drwxr-xr-x 141 root root 12288 Dec 17 19:18 ../
-rwxrwxrwx   1 root root  5638 Dec 13 09:10 admin.conf*
-rw-------   1 root root  5674 Nov 27 07:35 controller-manager.conf
-rw-------   1 root root  1962 Nov 27 07:36 kubelet.conf
drwxr-xr-x   2 root root  4096 Nov 27 07:35 manifests/
drwxr-xr-x   3 root root  4096 Nov 27 07:35 pki/
-rw-------   1 root root  5622 Nov 27 07:35 scheduler.conf
drwx------   4 root root  4096 Dec 13 09:10 tmp/
root@master:/etc/kubernetes# cd manifests/                           ##  ok  best  importent "
root@master:/etc/kubernetes/manifests# ll
total 24
drwxr-xr-x 2 root root 4096 Nov 27 07:35 ./
drwxr-xr-x 5 root root 4096 Dec 13 09:09 ../
-rw------- 1 root root 2382 Nov 27 07:35 etcd.yaml
-rw------- 1 root root 4019 Nov 27 07:35 kube-apiserver.yaml
-rw------- 1 root root 3520 Nov 27 07:35 kube-controller-manager.yaml
-rw------- 1 root root 1440 Nov 27 07:35 kube-scheduler.yaml
root@master:/etc/kubernetes/manifests# 

BEFORE  BEFORE
root@master:/etc/kubernetes/manifests# cat  kube-apiserver.yaml 
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.68.138:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.68.138
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction                    ##- target changed 
    - --enable-bootstrap-token-auth=true

++==### after  after after +++++++++++====================

root@master:/etc/kubernetes/manifests# cat  kube-apiserver.yaml 
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.68.138:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.68.138
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction,PodNodeSelector       ==##  target changed
        - --enable-bootstrap-token-auth=true
ERRORS  RESULTS-- ::
hai@master:~$ kubectl get nodes -o  wide 
NAME       STATUS     ROLES           AGE    VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
master     Ready      control-plane   20d    v1.25.4   192.168.68.138   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.12
worker     Ready      <none>          4d1h   v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.10
worker-2   NotReady   <none>          172m   v1.25.4   192.168.68.144   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.13

hai@master:~$ kubectl get nodes -o  wide                              ##"
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?
hai@master:~$ kubectl get nodes -o  wide 
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?
hai@master:~$ kubectl get nodes -o  wide 
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?
hai@master:~$ kubectl get nodes -o  wide 
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?
hai@master:~$ kubectl get nodes -o  wide 
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?
hai@master:~$ kubectl get nodes -o  wide 
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?
hai@master:~$ kubectl get nodes -o  wide 
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?
hai@master:~$ kubectl get nodes -o  wide 
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?

hai@master:~$ kubectl get nodes -o  wide    #"- 
NAME       STATUS     ROLES           AGE    VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME  
master     Ready      control-plane   20d    v1.25.4   192.168.68.138   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.12
worker     Ready      <none>          4d1h   v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.10
worker-2   Ready      <none>          174m   v1.25.4   192.168.68.144   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.13
===========================================================================================================
labels  labels   binding  

"##"  before  ---- #####
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: "2022-12-18T04:43:22Z"
  labels:
    kubernetes.io/metadata.name: project1
  name: project1
  resourceVersion: "392419"
  uid: 8f636166-d3fe-4b7d-8d66-81ea37653c33
spec:
  finalizers:
  - kubernetes
status:
  phase: Active

====#"after AFTER ==============
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    scheduler.alpha.kuberntetes.io/node-selector:  "node=w1"        ####  watch objerve 
  creationTimestamp: "2022-12-18T04:43:22Z"
  labels:
    kubernetes.io/metadata.name: project1
  name: project1
  resourceVersion: "392419"
  uid: 8f636166-d3fe-4b7d-8d66-81ea37653c33
spec:
  finalizers:
  - kubernetes
status:
  phase: Active

https://www.guru99.com/the-vi-editor.html
VI Editing commands
i â€“ Insert at cursor (goes into insert mode)
a â€“ Write after cursor (goes into insert mode)
A â€“ Write at the end of line (goes into insert mode)
ESC â€“ Terminate insert mode
u â€“ Undo last change
U â€“ Undo all changes to the entire line
o â€“ Open a new line (goes into insert mode)
dd â€“ Delete line
3dd â€“ Delete 3 lines.
D â€“ Delete contents of line after the cursor
C â€“ Delete contents of a line after the cursor and insert new text. Press ESC key to end insertion.
dw â€“ Delete word
4dw â€“ Delete 4 words
cw â€“ Change word
x â€“ Delete character at the cursor
r â€“ Replace character
R â€“ Overwrite characters from cursor onward
s â€“ Substitute one character under cursor continue to insert
S â€“ Substitute entire line and begin to insert at the beginning of the line
~ â€“ Change case of individual character





Commands and their Description   VI  EDITOR LINUX 

k : Moves the cursor up one line.
j : Moves the cursor down one line.
h : Moves the cursor to the left one character position.
l : Moves the cursor to the right one character position.
0 or | : Positions cursor at beginning of line.
$ : Positions cursor at end of line.
W : Positions cursor to the next word.
B : Positions cursor to previous word.
( : Positions cursor to beginning of current sentence.
) : Positions cursor to beginning of next sentence.
H : Move to top of screen.
nH : Moves to nth line from the top of the screen.
M : Move to middle of screen.
L : Move to bottom of screen.
nL : Moves to nth line from the bottom of the screen.
colon along with x : Colon followed by a number would position the cursor on line number represented by x.

CTRL+d : Move forward 1/2 screen.
CTRL+f : Move forward one full screen.
CTRL+u : Move backward 1/2 screen.
CTRL+b : Move backward one full screen.
CTRL+e : Moves screen up one line.
CTRL+y : Moves screen down one line.
CTRL+u : Moves screen up 1/2 page.
CTRL+d : Moves screen down 1/2 page.
CTRL+b : Moves screen up one page.
CTRL+f : Moves screen down one page.
CTRL+I : Redraws screen.

Editing and inserting in Files(Entering and Replacing Text): To edit the file, we need to be in the insert mode. There are many ways to enter insert mode from the command mode

i : Inserts text before current cursor location.
I : Inserts text at beginning of current line.
a : Inserts text after current cursor location.
A : Inserts text at end of current line.
o : Creates a new line for text entry below cursor location.
O : Creates a new line for text entry above cursor location.
r : Replace single character under the cursor with the next character typed.
R : Replaces text from the cursor to right.
s : Replaces single character under the cursor with any number of characters.
S :Replaces entire line.

Deleting Characters: Here is the list of important commands which can be used to delete characters and lines in an opened file.

X Uppercase: Deletes the character before the cursor location.
x Lowercase : Deletes the character at the cursor location.
Dw : Deletes from the current cursor location to the next word.
d^ : Deletes from current cursor position to the beginning of the line.
d$ : Deletes from current cursor position to the end of the line.
Dd : Deletes the line the cursor is on.


Copy and Past Commands: Copy lines or words from one place and paste them on another place by using the following commands.

Yy : Copies the current line.
9yy : Yank current line and 9 lines below.
p : Puts the copied text after the cursor.
P : Puts the yanked text before the cursor.


Save and Exit Commands of the ex Mode : Need to press [Esc] key followed by the colon (:) before typing the following commands:

q : Quit
q! : Quit without saving changes i.e. discard changes.
r fileName : Read data from file called fileName.
wq : Write and quit (save and exit).
w fileName : Write to file called fileName (save as).
w! fileName : Overwrite to file called fileName (save as forcefully).
!cmd : Runs shell commands and returns to Command mode.

Searching and Replacing in (ex Mode): vi also has powerful search and replace capabilities. The formal syntax for searching is:

:s/string 
For example, suppose we want to search some text for the string â€œgeeksforgeeksâ€ Type the following and press ENTER:

:s/geeksforgeeks

===================================================================================
,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker2,kubernetes.io/os=linux,node=w2

####-==========  BEFORE   BEGORE ####################"##########

root@master:/etc/kubernetes/manifests# ll
total 24
drwxrwxrwx 2 root root 4096 Dec 18 09:12 ./
drwxr-xr-x 4 root root 4096 Dec 18 09:12 ../
-rw------- 1 root root 2382 Dec 18 09:12 etcd.yaml
-rw------- 1 root root 4019 Dec 18 09:12 kube-apiserver.yaml
-rw------- 1 root root 3520 Dec 18 09:12 kube-controller-manager.yaml
-rw------- 1 root root 1440 Dec 18 09:12 kube-scheduler.yaml
root@master:/etc/kubernetes/manifests# cat kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.68.150:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.68.150
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction       ##  "best  ok   modifications 
    - --enable-bootstrap-token-auth=true


AFTER  ##########################======== AFTER
root@master:/etc/kubernetes/manifests# cat kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.68.150:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.68.150
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction,PodNodeSelector   "modify "

######### before - ######################""
kubectl edit namespace  project1

apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: "2022-12-18T04:43:28Z"       " modify   "
  labels:
    kubernetes.io/metadata.name: project2
  name: project2
  resourceVersion: "392432"
  uid: bc6851bd-3ef2-42ba-bb63-8455cf5d566e
spec:
  finalizers:
  - kubernetes
status:
  phase: Active
~                
@@@@@@   AFTER -  AFTER  #######
apiVersion: v1
kind: Namespace
metadata:
   annotations:
      scheduler.alpha.kuberntes.io/node-selector:  "node=w2"       " modify  ""
  creationTimestamp: "2022-12-18T04:43:28Z"
  labels:
    kubernetes.io/metadata.name: project2
  name: project2
  resourceVersion: "392432"
  uid: bc6851bd-3ef2-42ba-bb63-8455cf5d566e
spec:
  finalizers:
  - kubernetes
status:
  phase: Active
~                
===""#########################"#########################################################"

hai@master:~$ kubectl  get nodes --show-labels
NAME      STATUS   ROLES           AGE     VERSION   LABELS
master    Ready    control-plane   17h     v1.25.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
worker1   Ready    <none>          3h4m    v1.25.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker1,kubernetes.io/os=linux,node=w1
worker2   Ready    <none>          4h10m   v1.25.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker2,kubernetes.io/os=linux,node=w2
hai@master:~$

hai@master:~$ kubectl get pods -n  project1  -o  wide
NAME        READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          2m48s   10.44.0.1   worker1   <none>           <none>
dbserver    1/1     Running   0          2m29s   10.44.0.2   worker1   <none>           <none>
webserver   1/1     Running   0          2m38s   10.36.0.1   worker1   <none>           <none>
hai@master:~$ kubectl get pods -n  project2  -o  wide
NAME   READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
dev    1/1     Running   0          2m10s   10.44.0.3   worker2   <none>           <none>
prod   1/1     Running   0          2m18s   10.36.0.2   worker2  <none>           <none>
qa     1/1     Running   0          107s    10.36.0.3   worker2   <none>           <none>
hai@master:~$

###############################="############--============= remove  steps 

root@master:/etc/kubernetes/manifests# cat  kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.68.150:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.68.150
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction     == "modify"

---------------------------------

hai@master:~$ kubectl edit namespace project1
namespace/project1 edited
hai@master:~$ kubectl edit namespace project1
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
kind: Namespace
metadata:                                       "modify "
  creationTimestamp: "2022-12-19T07:47:36Z"
  labels:
    kubernetes.io/metadata.name: project1
  name: project1
  resourceVersion: "35515"
  uid: 68d5d913-282a-48ea-a0e1-e32c10325d6d
spec:
  finalizers:
  - kubernetes
status:
  phase: Active
~

------------
hai@master:~$ kubectl edit namespace project2
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
kind: Namespace
metadata:
  annotations:                                           "delete tw0 lines
    scheduler.alpha.kuberntes.io/node-selector: node=w2
  creationTimestamp: "2022-12-19T07:47:41Z"
  labels:
    kubernetes.io/metadata.name: project2
  name: project2
  resourceVersion: "22268"
  uid: 9de248ba-35c0-43d2-8f3d-bd9aec56ebf4
spec:
  finalizers:
  - kubernetes
status:
  phase: Active
~
============================================================================================ "delete lable

hai@master:~$ kubectl label node worker1 node-
node/worker1 unlabeled
hai@master:~$ kubectl label node worker2  node-
node/worker2 unlabeled
hai@master:~$

###############################################################################################################################################
https://www.youtube.com/watch?v=9JMcGr6rbO8
Kubernetes-Day-9

Resource Quotas
When several users or teams share a cluster with a fixed number of nodes, there is a concern that one team could use more than its fair share of resources.

Resource quotas are a tool for administrators to address this concern.

A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per namespace. It can limit the quantity of objects that can be created in a namespace by type, as well as the total amount of compute resources that may be consumed by resources in that namespace.

Resource quotas work like this:

Different teams work in different namespaces. This can be enforced with RBAC.

The administrator creates one ResourceQuota for each namespace.

Users create resources (pods, services, etc.) in the namespace, and the quota system tracks usage to ensure it does not exceed hard resource limits defined in a ResourceQuota.

If creating or updating a resource violates a quota constraint, the request will fail with HTTP status code 403 FORBIDDEN with a message explaining the constraint that would have been violated.

If quota is enabled in a namespace for compute resources like cpu and memory, users must specify requests or limits for those values; otherwise, the quota system may reject pod creation. Hint: Use the LimitRanger admission controller to force defaults for pods that make no compute resource requirements.


Object Count Quota
You can set quota for the total number of certain resources of all standard, namespaced resource types using the following syntax:

count/persistentvolumeclaims
count/services
count/secrets
count/configmaps
count/replicationcontrollers
count/deployments.apps
count/replicasets.apps
count/statefulsets.apps
count/jobs.batch
count/cronjobs.batch
=====================
hai@master:~$ kubectl get ns
NAME              STATUS   AGE
default           Active   20h
kube-node-lease   Active   20h
kube-public       Active   20h
kube-system       Active   20h
project1          Active   5h45m
project2          Active   5h45m
hai@master:~$ kubectl describe namespace project1
Name:         project1
Labels:       kubernetes.io/metadata.name=project1
Annotations:  <none>
Status:       Active
No resource quota.      ##  ## see look  no   resource quota 
No LimitRange resource.

hai@master:~$ kubectl describe namespace project2
Name:         project2
Labels:       kubernetes.io/metadata.name=project2
Annotations:  <none>
Status:       Active
No resource quota.      ## see look  no   resource quota 
No LimitRange resource.

====================================
https://kubernetes.io/docs/reference/
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#resourcequota-v1-core

ResourceQuota v1 core
Group  	Version	   Kind
core	   v1	       ResourceQuota

Field	Description
apiVersion
string	          APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

kind
string	          Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

metadata
ObjectMeta	      Standard object's metadata. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

spec
ResourceQuotaSpec	  Spec defines the desired quota. https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

status
ResourceQuotaStatus   	Status defines the actual enforced quota and its current usage. https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#objectmeta-v1-meta

name
string	    Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request 
             the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated.
            More info: http://kubernetes.io/docs/user-guide/identifiers#names


namespace
string	  Namespace defines the space within which each name must be unique. An empty namespace is equivalent to the "default" namespace, but "default" is the canonical representation. 
          Not all objects are required to be scoped to a namespace - the value of this field for those objects will be empty. Must be a DNS_LABEL. Cannot be updated. 
           More info: http://kubernetes.io/docs/user-guide/namespaces

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#resourcequotaspec-v1-core

ResourceQuotaSpec v1 core
ResourceQuota [core/v1]

hard
object	     hard is the set of desired hard limits for each named resource. More info: https://kubernetes.io/docs/concepts/policy/resource-quotas/

scopeSelector
ScopeSelector    	scopeSelector is also a collection of filters like scopes that must match each object tracked by a quota but expressed using ScopeSelectorOperator in 
                  combination with possible values. For a resource to match, both scopes AND scopeSelector (if specified in spec), must be matched.

scopes
string             array	A collection of filters that must match each object tracked by a quota. If not specified, the quota matches all objects.

https://unofficial-kubernetes.readthedocs.io/en/latest/concepts/policy/resource-quotas/

apiVersion: v1
kind: ResourceQuota
metadata:
   name: cpuramhdd
   namespace: project1
spec:
  hard:
    pods:  "2"
    configmaps: "10"
    secrets: "9"
    services: "12"
    persistentvolumeclaims: "5"
====== " ""

hai@master:~/test$ kubectl apply -f requata.yaml
resourcequota/cpuramhdd created

hai@master:~/test$ kubectl get quota  -n project1
NAME        AGE   REQUEST                                                                                  LIMIT
cpuramhdd   42s   configmaps: 1/10, persistentvolumeclaims: 0/5, pods: 3/2, secrets: 0/9, services: 0/12  

hai@master:~/test$ kubectl describe  quota -n  project1
Name:                   cpuramhdd
Namespace:              project1
Resource                Used  Hard
--------                ----  ----
configmaps              1     10
persistentvolumeclaims  0     5
pods                    0     2     ## look see
secrets                 0     9
services                0     12
========="e

hai@master:~/test$ kubectl run sony   --image=nginx  -n  project1
pod/sony created
hai@master:~/test$ kubectl run dell   --image=nginx  -n  project1
pod/dell created
hai@master:~/test$ kubectl describe quota -n  project1
Name:                   cpuramhdd
Namespace:              project1
Resource                Used  Hard
--------                ----  ----
configmaps              1     10
persistentvolumeclaims  0     5
pods                    2     2     # look  see 
secrets                 0     9
services                0     12
--------

hai@master:~/test$ kubectl run hp    --image=nginx  -n  project1

Error from server (Forbidden): pods "hp" is forbidden: exceeded quota: cpuramhdd, requested: pods=1, used: pods=2, limited: pods=2
============================="""
apiVersion: v1
kind: ResourceQuota
metadata:
   name: cpuramhdd
   namespace: project1
spec:
  hard:
    pods:  "2"
    configmaps: "10"
    secrets: "9"
    services: "12"
    persistentvolumeclaims: "5"
    limits.memory:  "800Mi"
    limits.cpu:  "5"

hai@master:~/test$ kubectl apply  -f requata.yaml
resourcequota/cpuramhdd configured

hai@master:~/test$ kubectl describe quota  -n project1
Name:                   cpuramhdd
Namespace:              project1
Resource                Used  Hard
--------                ----  ----
configmaps              1     10
limits.cpu              0     5
limits.memory           0     800Mi
persistentvolumeclaims  0     5
pods                    2     2
secrets                 0     9
services                0     12
hai@master:~/test$
###"""#########################################################################

Kubernetes-Day-10
https://www.youtube.com/watch?v=mGQ29xYeCrM

hai@master:~/test$ kubectl run devserver  --image=nginx  -n   project1
Error from server (Forbidden): pods "devserver" is forbidden: failed quota: cpuramhdd: must specify limits.cpu for: devserver; limits.memory for: devserver
hai@master:~/test$

apiVersion: v1
kind: Pod
metadata:
   name: quotapod
   namespace: project1
spec:
  containers:
    -  image: nginx
       name: quotapod
       resources:
          limits:  
             memory: "120Mi"
             cpu: "0.2"       
https://kubernetes.io/docs/reference/

hai@master:~$ kubectl explain pod
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status       <Object>
     Most recently observed status of the pod. This data may not be up to date.
     Populated by the system. Read-only. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

hai@master:~$ kubectl explain pod --recursive | less

KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion   <string>
   kind <string>
   metadata     <Object>
      annotations       <map[string]string>
      creationTimestamp <string>
      deletionGracePeriodSeconds        <integer>
      deletionTimestamp <string>
      finalizers        <[]string>
      generateName      <string>
      generation        <integer>
      labels    <map[string]string>
      managedFields     <[]Object>
         apiVersion     <string>
         fieldsType     <string>
         fieldsV1       <map[string]>

===================



hai@master:~$ kubectl explain pod --recursive | grep -i  container
     Pod is a collection of containers that can run on a host. This resource is
      containers        <[]Object>
                  containerName <string>
            containerPort       <integer>
      ephemeralContainers       <[]Object>
                  containerName <string>
            containerPort       <integer>
         targetContainerName    <string>
      initContainers    <[]Object>
                  containerName <string>
            containerPort       <integer>
                  containerName <string>
                        containerName   <string>
      containerStatuses <[]Object>
         containerID    <string>
               containerID      <string>
               containerID      <string>
      ephemeralContainerStatuses        <[]Object>
         containerID    <string>
               containerID      <string>
               containerID      <string>
      initContainerStatuses     <[]Object>
         containerID    <string>
               containerID      <string>
               containerID      <string>
#########################"""
hai@master:~$ kubectl get quota -n  project1
NAME        AGE    REQUEST                                                                                  LIMIT
cpuramhdd   142m   configmaps: 1/10, persistentvolumeclaims: 0/5, pods: 2/2, secrets: 0/9, services: 0/12   limits.cpu: 0 /5, limits.memory: 0/800Misources.

hai@master:~$ kubectl describe quota -n project1
Name:                   cpuramhdd
Namespace:              project1
Resource                Used  Hard
--------                ----  ----
configmaps              1     10
limits.cpu              0     5
limits.memory           0     800Mi
persistentvolumeclaims  0     5
pods                    2     2
secrets                 0     9
services                0     12

hai@master:~$ kubectl apply -f test/podcpu.yaml   -n project1
pod/quotapod created
--------
apiVersion: v1
kind: Pod
metadata:
   name: quotapod
   namespace: project1
spec:
  containers:
    -  image: nginx
       name: quotapod
       resources:
          limits:  
             memory: "120Mi"
             cpu: "05"       
----"
hai@master:~$ kubectl describe quota -n project1
Name:                   cpuramhdd
Namespace:              project1
Resource                Used   Hard
--------                ----   ----
configmaps              1      10
limits.cpu              200m   5
limits.memory           120Mi  800Mi
persistentvolumeclaims  0      5
pods                    1      2
secrets                 0      9
services                0      12
======================================================== """
apiVersion: v1
kind: Pod
metadata:
   name: quotapod1
   namespace: project1
spec:
  containers:
    -  image: nginx
       name: quotapod1
       resources:
          limits:
             memory: "700Mi"
             cpu: "0.2"
====="
hai@master:~/test$ kubectl apply -f podcpu.yaml  -n  project1
Error from server (Forbidden): error when creating "podcpu.yaml": pods "quotapod1" is forbidden: exceeded quota: cpuramhdd, requested: limits.memory=700Mi, used: limits.memory=120Mi, limited: limits.memory=800Mi
-----------------++++++++++++++++++++++++++++====================="""
What is the meaning of CPU and core in Kubernetes?
https://stackoverflow.com/questions/53255956/what-is-the-meaning-of-cpu-and-core-in-kubernetes

If I want to set the CPU request/limit for pod, should the maximum resource that I have be 1000m or 4000m?

1000m (milicores) = 1 core = 1 vCPU = 1 AWS vCPU = 1 GCP Core.
100m (milicores) = 0.1 core = 0.1 vCPU = 0.1 AWS vCPU = 0.1 GCP Core.

8000m = 8 cores = 8 vCPUs
1 CPU = 1000 millicores/millicpu

https://franciscomelojr.ca/2021/11/24/milli-cores-and-cpu-metrics/
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"

https://www.alibabacloud.com/blog/kubernetes-assign-cpu-resource-defaults-and-limits-to-containers_594832

1, 2, 2.5 ... defines 1, 2 and 2.5 CPUs
1000m, 2500m, 150m ... defines 1 CPU, 2.5 CPUs and 0.150 CPUs
The second syntax uses Millicores. 1000m equals one CPU on all computers.
( One Millicores is 1/1000 of a CPU, therefore 1000m equals 1 CPU )
A four core server has a CPU capacity of 4000m.

apiVersion: v1
kind: Pod
metadata:
  name: mybench-pod
spec:
  containers:
  - name: mybench-container
    image: mytutorials/centos:bench
    imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'echo The CPU Bench Pod is Running ; sleep 3600']
        resources:
      limits:
        cpu: "1"
      requests:
        cpu: 500m
        
  restartPolicy: Never
-------
apiVersion: v1
kind: LimitRange
metadata:
  name: mycpu-limit-range
spec:
  limits:
  - default:
      cpu: 0.75
    defaultRequest:
      cpu: 0.25
    type: Container

--------------------
apiVersion: v1
kind: LimitRange
metadata:
  name: mycpu-limit-range
spec:
  limits:
  - default:
      cpu: 0.75
    defaultRequest:
      cpu: 0.25

    max:
      cpu: "2000m"
    min:
      cpu: "200m"

    type: Container
--------------------
apiVersion: v1
kind: Pod
metadata:
  name: mybench-pod
spec:
  containers:
  - name: mybench-container
    image: mytutorials/centos:bench
    imagePullPolicy: IfNotPresent
    
    command: ['sh', '-c', 'echo The CPU Bench Pod is Running ; sleep 3600']
    
    resources:
      limits:
        cpu: "1000m"
      requests:
        cpu: 300m
        
  restartPolicy: Never    
=================================================================== """ #################################
LimitRange v1 core
Group    	Version	  Kind
core	    v1	      LimitRange

https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/


1.   https://kubernetes.io/examples/admin/resource/limit-mem-cpu-container.yaml          ## yaml file downloded

2.   https://kubernetes.io/examples/admin/resource/limit-range-pod-1.yaml               ## yaml file downloded

3.  https://kubernetes.io/examples/admin/resource/limit-mem-cpu-pod.yaml               ## yaml file downloded


4.  https://kubernetes.io/examples/admin/resource/limit-range-pod-2.yaml                 ## yaml file downloded

--------'"""

hai@master:~$ cat limit-mem-cpu-container.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-mem-cpu-per-container
spec:
  limits:
  - max:
      cpu: "800m"
      memory: "1Gi"
    min:
      cpu: "100m"
      memory: "99Mi"
    default:
      cpu: "700m"
      memory: "900Mi"
    defaultRequest:
      cpu: "110m"
      memory: "111Mi"
    type: Container

---------------
https://kubernetes.io/docs/reference/

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#limitrange-v1-core

spec
LimitRangeSpec  	Spec defines the limits enforced. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#limitrangespec-v1-core
LimitRangeList v1 core
Field	Description
apiVersion
string	APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

items
LimitRange array	Items is a list of LimitRange objects. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/

kind
string	Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

metadata
ListMeta	Standard list metadata. More info: https://git.k8s.i
---
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#limitrangespec-v1-core

Field	Description
limits
LimitRangeItem array	  Limits is the list of LimitRangeItem objects that are enforced.

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#limitrangeitem-v1-core
Field	Description
default
object  	Default resource requirement limit value by resource name if resource limit is omitted.

defaultRequest
object	  DefaultRequest is the default resource requirement request value by resource name if resource request is omitted.

max
object	  Max usage constraints on this kind by resource name.

maxLimitRequestRatio
object	   MaxLimitRequestRatio if specified, the named resource must have a request and limit that are both non-zero where limit divided by request is less than or equal to the enumerated value; this represents the max burst for the named resource.

min
object	  Min usage constraints on this kind by resource name.

type
string    Type of resource that this limit applies to.

==========="""
hai@master:~$ kubectl apply -f limit-
limit-mem-cpu-container.yaml  limit-mem-cpu-pod.yaml        limit-range-pod-1.yaml        limit-range-pod-2.yaml

hai@master:~$ kubectl apply -f limit-mem-cpu-container.yaml
limitrange/limit-mem-cpu-per-container created

hai@master:~$ kubectl get limitrange
NAME                          CREATED AT
limit-mem-cpu-per-container   2022-12-20T06:40:37Z
------
hai@master:~$ kubectl describe limitrange
Name:       limit-mem-cpu-per-container
Namespace:  default
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Requ           est Ratio
----        --------  ---   ---   ---------------  -------------  --------------           ---------
Container   cpu       100m  800m  110m             700m           -
Container   memory    99Mi  1Gi   111Mi            900Mi          -
hai@master:~$
=============================================================================

hai@master:~$ cat limit-range-pod-1.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox1
spec:
  containers:
  - name: busybox-cnt01
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt01; sleep 10;done"]
    resources:
      requests:
        memory: "100Mi"
        cpu: "100m"
      limits:
        memory: "200Mi"
        cpu: "500m"
  - name: busybox-cnt02
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt02; sleep 10;done"]
    resources:
      requests:
        memory: "100Mi"
        cpu: "100m"
  - name: busybox-cnt03
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt03; sleep 10;done"]
    resources:
      limits:
        memory: "200Mi"
        cpu: "500m"
  - name: busybox-cnt04
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt04; sleep 10;done"]
#########  abobe file is  edit ----  
apiVersion: v1
kind: Pod
metadata:
  name: busybox1
spec:
  containers:
  - name: busybox-cnt01
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt01; sleep 10;done"]
    resources:
      requests:
        memory: "100Mi"
        cpu: "100m"
      limits:
        memory: "200Mi"
        cpu: "500m"
---
hai@master:~$ kubectl describe limitrange
Name:       limit-mem-cpu-per-container
Namespace:  default
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---   ---   ---------------  -------------  -----------------------
Container   memory    99Mi  1Gi   111Mi            900Mi          -
Container   cpu       100m  800m  110m             700m          

hai@master:~$ kubectl apply -f modifylimitrange.yaml
pod/busybox1 created

hai@master:~$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
busybox1   1/1     Running   0          3m41s   10.44.0.1   worker2   <none>           <none>

hai@master:~$ kubectl describe pod busybox1
Name:             busybox1
Namespace:        default
Containers:
  busybox-cnt01:
    Container ID:  containerd://b240b323de07c2a598f6a482b2a46888f730f66b42beae38fc5a04fd161bdd16
    Image:         busybox:1.28
    Image ID:      docker.io/library/busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47
    Command:
      /bin/sh        ##  look  
    Args:                      ## look
      -c
      while true; do echo hello from cnt01; sleep 10;done        ## look  
    State:          Running
      Started:      Mon, 19 Dec 2022 23:25:55 -0800
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  200Mi
    Requests:
      cpu:        100m
      memory:     100Mi
    Environment:  <none>

Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m12s  default-scheduler  Successfully assigned default/busybox1 to worker2
  Normal  Pulling    5m11s  kubelet            Pulling image "busybox:1.28"
  Normal  Pulled     5m4s   kubelet            Successfully pulled image "busybox:1.28" in 6.824356546s
  Normal  Created    5m4s   kubelet            Created container busybox-cnt01
  Normal  Started    5m4s   kubelet            Started container busybox-cnt01

=================================="""
hai@master:~$ kubectl describe limitrange
Name:       limit-mem-cpu-per-container
Namespace:  default
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---   ---   ---------------  -------------  -----------------------
Container   cpu       100m  800m  110m             700m           -
Container   memory    99Mi  1Gi   111Mi            900Mi          -

apiVersion: v1
kind: Pod
metadata:
  name: busybox1
spec:
  containers:
  - name: busybox-cnt01
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt01; sleep 10;done"]
    resources:
      requests:
        memory: "100Mi"
        cpu: "100m"
      limits:
        memory: "200Mi"
        cpu: "900m"          ##  look  see  modify 

hai@master:~$ nano modifylimitrange.yaml
hai@master:~$ kubectl apply -f modifylimitrange.yaml
Error from server (Forbidden): error when creating "modifylimitrange.yaml": pods "busybox1" is forbidden: maximum cpu usage per Container is 800m, but limit is 900m

===================================================================="
hai@master:~$ kubectl apply -f limit-
limit-mem-cpu-container.yaml  limit-mem-cpu-pod.yaml        limit-range-pod-1.yaml        limit-range-pod-2.yaml

hai@master:~$ kubectl apply -f limit-mem-cpu-pod.yaml
limitrange/limit-mem-cpu-per-pod created

hai@master:~$ kubectl describe limitrange
Name:       limit-mem-cpu-per-container
Namespace:  default
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---   ---   ---------------  -------------  -----------------------
Container   cpu       100m  800m  110m             700m           -
Container   memory    99Mi  1Gi   111Mi            900Mi          -

Name:       limit-mem-cpu-per-pod
Namespace:  default
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Pod         memory    -    2Gi  -                -              -
Pod         cpu       -    2    -                -              -
------==================================================================="""
Name:       limit-mem-cpu-per-pod
Namespace:  default
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Pod         cpu       -    2    -                -              -
Pod         memory    -    2Gi  -                -              -

apiVersion: v1
kind: Pod
metadata:
  name: busybox2
spec:
  containers:
  - name: busybox-cnt01
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt01; sleep 10;done"]
    resources:
      requests:
        memory: "100Mi"
        cpu: "100m"
      limits:
        memory: "3Gi"
        cpu: "500m"

hai@master:~$ kubectl apply -f 3gbpod.yaml
Error from server (Forbidden): error when creating "3gbpod.yaml": pods "busybox2" is forbidden: maximum memory usage per Container is 1Gi, but limit is 3Gi
======================================================================================="""
Name:       limit-mem-cpu-per-pod
Namespace:  default
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Pod         cpu       -    2    -                -              -
Pod         memory    -    2Gi  -                -              -


apiVersion: v1
kind: LimitRange
metadata:
  name: limit-mem-cpu-per-pod
spec:
  limits:
  - max:
      cpu: "2"
      memory: "2Gi"
    min:
      cpu:  "1"
      memory:  "1.3Gi"
    type: Pod
"""
hai@master:~$ nano lmtr2-1.yaml
hai@master:~$ kubectl  apply -f lmtr2-1.yaml
limitrange/limit-mem-cpu-per-pod configured
hai@master:~$ kubectl describe limitrange
Name:       limit-mem-cpu-per-container
Namespace:  default
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---   ---   ---------------  -------------  -----------------------
Container   cpu       100m  800m  110m             700m           -
Container   memory    99Mi  1Gi   111Mi            900Mi          -



hai@master:~$ kubectl describe limitrange
Name:       limit-mem-cpu-per-container
Namespace:  default
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---   ---   ---------------  -------------  -----------------------
Container   cpu       100m  800m  110m             700m           -
Container   memory    99Mi  1Gi   111Mi            900Mi          -


Name:       limit-mem-cpu-per-pod
Namespace:  default
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Pod         cpu       -    2    -                -              -
Pod         memory    -    2Gi  -                -              -

----
apiVersion: v1
kind: Pod
metadata:
  name: busybox1
spec:
  containers:
  - name: busybox-cnt01
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt01; sleep 10;done"]
    resources:
      requests:                    ### MINIMUM 
        memory: "1000Mi"
        cpu: "1000m"
      limits:
        memory: "1200Mi"                   ## MAXIMUM 
        cpu: "1000m"

hai@master:~$ kubectl apply -f  modifylimitrange.yaml
Error from server (Forbidden): error when creating "modifylimitrange.yaml": pods "busybox1" is forbidden: minimum memory usage per Pod is 1395864371200m, but request is 1048576k
=============================#####################################"""
hai@master:~$ kubectl describe limitrange
Name:       limit-mem-cpu-per-container
Namespace:  default
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---   ---   ---------------  -------------  -----------------------
Container   cpu       100m  800m  110m             700m           -
Container   memory    99Mi  1Gi   111Mi            900Mi          -

Name:       limit-mem-cpu-per-pod
Namespace:  default
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Pod         cpu       -    2    -                -              -
Pod         memory    -    2Gi  -                -              -

hai@master:~$ cat  modifylimitrange.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox1
spec:
  containers:
  - name: busybox-cnt01
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt01; sleep 10;done"]
    resources:
      requests:                               ## MINIMUM LOOK  SEE 
        memory: "100Mi"
        cpu: "100m"
      limits:
        memory: "120Mi"                           ####  MAXIMUM 
        cpu: "100m"

========== --------
hai@master:~$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES
busybox1   1/1     Running   0          9m2s   10.44.0.1   worker2   <none>           <none>

hai@master:~$ kubectl describe pod busybox1
Name:             busybox1
Namespace:        default
Node:             worker2/192.168.68.151

Containers:
  busybox-cnt01:
    Container ID:  containerd://c93faf1cc3e0ad2b0f48b9d1e3586530e16c8d0bc14caa6ca818b08227f9a2f2
    Image:         busybox:1.28
    Image ID:      docker.io/library/busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
    Args:
      -c
      while true; do echo hello from cnt01; sleep 10;done
    State:          Running
      Started:      Tue, 20 Dec 2022 03:41:11 -0800
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  120Mi
    Requests:
      cpu:        100m
      memory:     100Mi
  ========================================================================"""
  default       default    default              look   see
hai@master:~$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
busybox1   1/1     Running   0          12m   10.44.0.1   worker2   <none>           <none>
ngidnx     1/1     Running   0          12s   10.36.0.2   worker1   <none>           <none>
nginx      1/1     Running   0          21s   10.36.0.1   worker1   <none>           <none>
nginxd     1/1     Running   0          15s   10.44.0.2   worker2   <none>           <none>

hai@master:~$ kubectl describe pod  nginx
Name:             nginx
Namespace:        default
Priority:         0
Containers:
  nginx:                                "watch look  see
    Limits:
      cpu:     700m                 ## default limits is taken
      memory:  900Mi
    Requests:
      cpu:        110m
      memory:     111Mi
   
hai@master:~$ kubectl describe limitrange
Name:       limit-mem-cpu-per-container
Namespace:  default
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---   ---   ---------------  -------------  -----------------------
Container   cpu       100m  800m  110m             700m           -                           ## default limits is taken
Container   memory    99Mi  1Gi   111Mi            900Mi          -

Name:       limit-mem-cpu-per-pod
Namespace:  default
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Pod         cpu       -    2    -                -              -
Pod         memory    -    2Gi  -                -              -
-----------  2nd  pod  ###########################"""

hai@master:~$ kubectl describe pod nginxd
Name:             nginxd
Namespace:        default
Containers:
  nginxd:
    Limits:
      cpu:     700m
      memory:  900Mi
    Requests:
      cpu:        110m
      memory:     111Mi
#############################################################################################################################################

hai@master:~$ cat limit-range-pod-2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox2
spec:
  containers:
  - name: busybox-cnt01
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt01; sleep 10;done"]
    resources:
      requests:
        memory: "100Mi"                        ## look see 
        cpu: "100m"
      limits:
        memory: "200Mi"                         ## look see
        cpu: "500m"
  - name: busybox-cnt02
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt02; sleep 10;done"]
    resources:
      requests:                 ## look  
        memory: "100Mi"
        cpu: "100m"            %%%  no  limits are  mection  
  - name: busybox-cnt03
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt03; sleep 10;done"]
    resources:
      limits:
        memory: "200Mi"       ##  look  see 
        cpu: "500m"
  - name: busybox-cnt04       %% no  menction resource requests 
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt04; sleep 10;done"]

ERRORS= : 
hai@master:~$ kubectl apply -f 4cont.yaml
Error from server (Forbidden): error when creating "4cont.yaml": pods "busybox2" is forbidden: [maximum cpu usage per Pod is 2, but limit is 2400m, maximum memory usage per Pod is 2Gi, but limit is 2306867200]
#########################################"""
MODIFY THE ABOVE  CODE  ADD THE MAXIMUM 
apiVersion: v1
kind: Pod
metadata:
  name: busybox2
spec:
  containers:
  - name: busybox-cnt01
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt01; sleep 10;done"]
    resources:
      requests:                 ## minimum 
        memory: "100Mi"
        cpu: "100m"
      limits:
        memory: "200Mi"            ## maximum 
        cpu: "500m"
  - name: busybox-cnt02
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt02; sleep 10;done"]
    resources:
      limits:
        memory: "100Mi"        ##  look   only  maximum
        cpu: "100m"
hai@master:~$ kubectl apply -f 4cont.yaml
pod/busybox2 created

hai@master:~$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
busybox2   2/2     Running   0          3m57s   10.44.0.1   worker2   <none>           <none>
           ^^  """ see  look   one pod in tow 2 containers in a single pod 
hai@master:~$ kubectl describe pod  busybox2
Name:             busybox2
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker2/192.168.68.151
Status:           Running
IP:               10.44.0.1
IPs:
  IP:  10.44.0.1
Containers:
  busybox-cnt01:              ## container 1
    Container ID:  containerd://5727cf1e21c36b915fa921e042ec5abfb841926cbdc01d8690274604b2627138
    Image:         busybox:1.28
    Image ID:      docker.io/library/busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47
    Limits:
      cpu:     500m
      memory:  200Mi
    Requests:
      cpu:        100m
      memory:     100Mi
   
  busybox-cnt02:                ## container - 2
    Container ID:  containerd://682fedac9ed1322b98ce15697ac20432b0877e05f7c4749ee933a62a9c863950
    Image:         busybox:1.28
    Image ID:      docker.io/library/busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47
    Limits:
      cpu:     100m
      memory:  100Mi
    Requests:
      cpu:        100m
      memory:     100Mi

#############################################################################"""
https://www.mirantis.com/blog/kubernetes-replication-controller-replica-set-and-deployments-understanding-replication-options/
https://ryaneschinger.com/blog/rolling-updates-kubernetes-replication-controllers-vs-deployments/


                     Replication  or Pod  controller
                kubernetes Replication  type  { OR }  POD Controller type

replication-controller            Replicaset                 Deployments 


1111 .replication controller : - 

replication controller only supports equality-based selectors

The Replication Controller is the original form of replication in Kubernetes

The Replication Controller uses equality-based selectors to manage the pods.

The rolling-update command works with Replication Controllers
Replica Controller is deprecated and replaced by ReplicaSets.

A replication controller continuously monitors the list of running pods by running a reconciliation loop and ensures that the specified number of replicas are always running.
   It maintains the replicas in two ways:

By creating new replicas if the actual replicas are less than the desired replicas, and

By removing extra replicas if the actual replicas are more than the desired replicas. This can be possible if,
A pod is created of the same type manually.

A label of an existing podâ€™s changes to a value which is same as the replication controller label-selector.
Someone decreases the desired number of pods.

Ensures that a specified number of pod replicas are running at any one time.

o"""#################################################################


222.. Replica Set  :-

ReplicaSet is the next-generation ReplicationController that supports the new set-based label selector

ReplicaSets are a higher-level API that gives the ability to easily run multiple instances of a given pod

 rolling-update cannot support 

ReplicaSets Controller uses set-based selectors to manage the pods.

The rolling-update command wonâ€™t work with ReplicaSets.

Deployments are recommended over ReplicaSets.

ReplicaSet is the next-generation ReplicationController that supports the new set-based label selector

Itâ€™s mainly used by Deployment as a mechanism to orchestrate pod creation, deletion and updates.



o"""##############################################################

333.  Deployments -: 

Deployment is a higher-level API object that updates its underlying Replica Sets and their Pods in a similar fashion as kubectl rolling-update.

 rolling-update can support 

â€¢ Deployments can be used for describing the state of a particular application component as a Pod template.

A Deployment provides declarative updates for Pods and Replicaset.

Deployments wrap up Pods and ReplicaSets into a nice package that is capable of deploying your applications.

A deployment is an object in Kubernetes that lets you manage a set of identical pods.

Without a deployment, youâ€™d need to create, update, and delete a bunch of pods manually.

With a deployment, you declare a single object in a YAML file. This object is responsible for creating the pods, making sure they stay up to date 
and ensuring there are enough of them running You can also easily autoscale your applications using a Kubernetes deployment.

we also update the image in the pod and after updating the pod if the image is failing then we able to roll out it to the old image, as called as â€Rolling Back a Deployment â€
using deployment we can undo deployment, pause deployment, resume deployment
----------===
Kubernetes-Day-11    see  LOOK 
https://www.youtube.com/watch?v=veiVV4FeB1U

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#replicationcontroller-v1-core
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#replicationcontrollerstatus-v1-core

ReplicationControllerSpec v1 core
ReplicationController [core/v1]
Field	Description
minReadySeconds
integer	Minimum number of seconds for which a newly created pod should be ready without any of its container crashing, for it to be considered available. Defaults to 0 (pod will be considered available as soon as it is ready)

replicas
integer	Replicas is the number of desired replicas. This is a pointer to distinguish between explicit zero and unspecified. Defaults to 1. More info: https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller#what-is-a-replicationcontroller

selector
object	Selector is a label query over pods that should match the Replicas count. If Selector is empty, it is defaulted to the labels present on the Pod template. Label keys and values that must match in order to be controlled by this replication controller, if empty defaulted to labels on Pod template. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors

template
PodTemplateSpec	Template is the object that describes the pod that will be created if insufficient replicas are detected. This takes precedence over a TemplateRef. More info: https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller#pod-template

ReplicationControllerStatus v1 core
 Appears In:
ReplicationController [core/v1]
Field	Description
availableReplicas
integer	The number of available replicas (ready for at least minReadySeconds) for this replication controller.

conditions
ReplicationControllerCondition array
patch strategy: merge
patch merge key: type	Represents the latest available observations of a replication controller's current state.

fullyLabeledReplicas
integer	The number of pods that have labels matching the labels of the pod template of the replication controller.

observedGeneration
integer	ObservedGeneration reflects the generation of the most recently observed replication controller.

readyReplicas
integer	The number of ready replicas for this replication controller.

replicas
nteger	Replicas is the most recently observed number of replicas. More info: https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller#what-is-a-replicationcontroller

apiVersion: v1
kind: ReplicationController
metadata:
   name: rc
spec:
   replicas: 3
   selector: 
      team: dev   
   template: 
      metadata:
        name: rc
        labels:
           team: dev    ###  look  see 
      spec:
        containers:
           - name: rc
             image: nginx
             ports:
                - containerPort: 80

hai@master:~/test$ kubectl get po  -o wide
NAME       READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE               READINESS GATES
rc-5qm5v   1/1     Running   0          2m57s   10.36.0.1   worker1   <none>                       <none>
rc-9s2bg   1/1     Running   0          2m57s   10.44.0.1   worker2   <none>                       <none>
rc-mhqfk   1/1     Running   0          2m57s   10.44.0.2   worker2   <none>                       <none>
---"""
hai@master:~/test$ kubectl describe pod  rc-5qm5v
Name:             rc-5qm5v
Namespace:        default
Labels:           team=dev
IPs:
  IP:           10.36.0.1
Controlled By:  ReplicationController/rc     ###  see look 
Containers:
  rc:
    Image:          nginx
    Port:           80/TCP

hai@master:~$ kubectl explain rc             or   kubectl explain  rc  --recursive | less
KIND:     ReplicationController
VERSION:  v1

DESCRIPTION:
     ReplicationController represents the configuration of a replication
     controller.

FIELDS:
   apiVersion	<string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind	<string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata	<Object>
     If the Labels of a ReplicationController are empty, they are defaulted to
     be the same as the Pod(s) that the replication controller manages. Standard
     object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec	<Object>
     Spec defines the specification of the desired behavior of the replication
     controller. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status	<Object>
     Status is the most recently observed status of the replication controller.
     This data may be out of date by some window of time. Populated by the
     system. Read-only. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

------"""
hai@master:~$ kubectl get rc
NAME   DESIRED   CURRENT   READY   AGE
rc     3         3         3       112m

hai@master:~$ kubectl get rc  --show-labels
NAME   DESIRED   CURRENT   READY   AGE    LABELS
rc     3         3         3       112m   team=dev

hai@master:~$ kubectl get rc,pods  --show-labels
NAME                       DESIRED   CURRENT   READY   AGE    LABELS
replicationcontroller/rc   3         3         3       113m   team=dev

NAME           READY   STATUS    RESTARTS   AGE    LABELS
pod/rc-5qm5v   1/1     Running   0          113m   team=dev
pod/rc-9s2bg   1/1     Running   0          113m   team=dev
pod/rc-mhqfk   1/1     Running   0          113m   team=dev
hai@master:~$

hai@master:~/test$ kubectl get all
NAME           READY   STATUS    RESTARTS   AGE
pod/rc-hkzwb   1/1     Running   0          71s
pod/rc-r7hr6   1/1     Running   0          71s
pod/rc-vfxx4   1/1     Running   0          71s

NAME                       DESIRED   CURRENT   READY   AGE
replicationcontroller/rc   3         3         3       121m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   2d17h
---------------------"""
hai@master:~/test$ kubectl get all  --all-namespaces
NAMESPACE     NAME                                 READY   STATUS             RESTARTS         A                                                                                GE
default       pod/rc-hkzwb                         1/1     Running            0                5                                                                                m12s
default       pod/rc-r7hr6                         1/1     Running            0                5                                                                                m12s
default       pod/rc-vfxx4                         1/1     Running            0                5                                                                                m12s
kube-system   pod/coredns-565d847f94-q5zw9         0/1     Running            292 (5m4s ago)   2                                                                                d
kube-system   pod/coredns-565d847f94-rg789         0/1     CrashLoopBackOff   289 (14s ago)    2                                                                                d
kube-system   pod/etcd-master                      1/1     Running            6 (42h ago)      2                                                                                d17h
kube-system   pod/kube-apiserver-master            1/1     Running            2 (42h ago)      4                                                                                7h
kube-system   pod/kube-controller-manager-master   1/1     Running            8 (42h ago)      2                                                                                d17h
kube-system   pod/kube-proxy-6t6qp                 1/1     Running            6 (42h ago)      2                                                                                d17h
kube-system   pod/kube-proxy-ffdgc                 1/1     Running            4 (42h ago)      2                                                                                d4h
kube-system   pod/kube-proxy-gq4v9                 1/1     Running            3 (42h ago)      2                                                                                d3h
kube-system   pod/kube-scheduler-master            1/1     Running            8 (42h ago)      2                                                                                d17h
kube-system   pod/weave-net-6bpzq                  2/2     Running            15 (42h ago)     2                                                                                d17h
kube-system   pod/weave-net-8gkx4                  2/2     Running            9 (42h ago)      2                                                                                d4h
kube-system   pod/weave-net-f27sl                  2/2     Running            6 (42h ago)      2                                                                                d3h

NAMESPACE   NAME                       DESIRED   CURRENT   READY   AGE
default     replicationcontroller/rc   3         3         3       125m

NAMESPACE     NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                                                                                                  AGE
default       service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP                                                                                                  2d17h
kube-system   service/kube-dns     ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP                                                                                   2d17h

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   N                                                                                ODE SELECTOR            AGE
kube-system   daemonset.apps/kube-proxy   3         3         3       3            3           k                                                                                ubernetes.io/os=linux   2d17h
kube-system   daemonset.apps/weave-net    3         3         3       3            3           <                                                                                none>                   2d17h

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   deployment.apps/coredns   0/2     2            0           2d17h

NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE
kube-system   replicaset.apps/coredns-565d847f94   2         2         0       2d17h
======================================#################################"""
Every 2.0s: kubectl get pods --show-labels -o wide                                             master: Wed Dec 21 03:28:08 2022

NAME        READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
appserver   1/1     Running   0          3m55s   10.44.0.1   worker2   <none>           <none>            run=appserver   ## look   see
dbserver    1/1     Running   0          3m45s   10.36.0.1   worker1   <none>           <none>            run=dbserver

""" AFTER  AFTER AFTER 
hai@master:~/test$ kubectl label  pod  appserver  run-
pod/appserver unlabeled
hai@master:~/test$ kubectl label pod dbserver  run-
pod/dbserver unlabeled
-----"""
Every 2.0s: kubectl get pods --show-labels -o wide                                             master: Wed Dec 21 03:34:55 2022

NAME        READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS  
appserver   1/1     Running   0          10m   10.44.0.1   worker2   <none>           <none>            <none>    ## watch look see 
dbserver    1/1     Running   0          10m   10.36.0.1   worker1   <none>           <none>            <none>
=============================================#######################################################"""

hai@master:~$ kubectl run   hero1 --image=nginx
pod/hero1 created
hai@master:~$ kubectl run   hero --image=nginx
pod/hero created

hai@master:~/test$ kubectl get pods  --show-labels  -o wide
NAME    READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
hero    1/1     Running   0          9m19s   10.36.0.1   worker1   <none>           <none>            run=hero
hero1   1/1     Running   0          9m28s   10.44.0.1   worker2   <none>           <none>            run=hero1

hai@master:~$ kubectl label pod hero run-
pod/hero unlabeled
hai@master:~$ kubectl label pod hero1 run-
pod/hero1 unlabeled

Every 2.0s: kubectl get pods --show-labels -o wide                                             master: Wed Dec 21 04:00:36 2022
NAME    READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
hero    1/1     Running   0          11m   10.36.0.1   worker1   <none>           <none>            <none>
hero1   1/1     Running   0          11m   10.44.0.1   worker2   <none>           <none>            <none>
--"
hai@master:~$ kubectl label pod  hero   godfather=megastar      # see look 
pod/hero labeled
hai@master:~$ kubectl label pod  hero1   puspa=alluarjun
pod/hero1 labeled

Every 2.0s: kubectl get pods --show-labels -o wide                                             master: Wed Dec 21 04:28:20 2022

NAME    READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
hero    1/1     Running   0          38m   10.36.0.1   worker1   <none>           <none>            godfather=megastar   ## see  look 
hero1   1/1     Running   0          38m   10.44.0.1   worker2   <none>           <none>            puspa=alluarjun


###################################################################################################################"

hai@master:~$ kubectl  run  prod  --image=nginx
pod/prod created

hai@master:~$ kubectl get pods -o wide
NAME   READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   REA                            DINESS GATES
prod   1/1     Running   0          23s   10.44.0.1   worker2   <none>           <no                            ne>

hai@master:~$ kubectl get pods  --show-labels  -o wide
NAME   READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GAT                ES   LABELS
prod   1/1     Running   0          3m26s   10.44.0.1   worker2   <none>           <none>                            run=prod  # watch

hai@master:~$ kubectl label pod  prod run-
pod/prod unlabeled

hai@master:~$ kubectl get pods  --show-labels  -o wide
NAME   READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
prod   1/1     Running   0          7m48s   10.44.0.1   worker2   <none>           <none>            <none>    ##  watch 

--"""
####   BEFORE  AFTER  

hai@master:~$ kubectl label pod  prod  godfather=megastar
pod/prod labeled

hai@master:~$ kubectl get pod --show-labels  -o wide
NAME   READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
prod   1/1     Running   0          13m   10.44.0.1   worker2   <none>           <none>            godfather=megastar  ## LOOK SEE 
                   SAME LABEL USING ONE POD IS  "RUNNING"

apiVersion: v1
kind: ReplicationController
metadata:
   name: rc
spec:
   replicas: 3    # # given 3  but  deploy only 2 
   selector:
        godfather: megastar    ### see look label  
   template:
      metadata:
        name: rc
        labels:
           godfather: megastar  ## see look label 
      spec:
        containers:
           - name: rc
             image: nginx
             ports:
                - containerPort: 80
--"
hai@master:~/test$ kubectl get rc,pods  --show-labels   -o wide
NAME                       DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR             LABELS
replicationcontroller/rc   3         3         3       20m   rc           nginx    godfather=megastar   godfather=megastar

NAME           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/prod       1/1     Running   0          44m   10.44.0.1   worker2   <none>           <none>            godfather=megastar
pod/rc-f4nnf   1/1     Running   0          20m   10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc-jz5vp   1/1     Running   0          20m   10.44.0.2   worker2   <none>           <none>            godfather=megastar
==="
hai@master:~/test$ kubectl apply -f rc.yaml
replicationcontroller/rc created

hai@master:~/test$ kubectl get pods  --show-labels  -o  wide
NAME       READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
prod       1/1     Running   0          24m   10.44.0.1   worker2   <none>           <none>            godfather=megastar  # old manuval deploy 

rc-f4nnf   1/1     Running   0          33s   10.36.0.1   worker1   <none>           <none>            godfather=megastar  # through  replicas 
rc-jz5vp   1/1     Running   0          33s   10.44.0.2   worker2   <none>           <none>            godfather=megastar  #  through replicas

--"

hai@master:~/test$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
prod       1/1     Running   0          26m     10.44.0.1   worker2   <none>           <none>    # manuval deploy 

rc-f4nnf   1/1     Running   0          2m16s   10.36.0.1   worker1   <none>           <none>   # through replica
rc-jz5vp   1/1     Running   0          2m16s   10.44.0.2   worker2   <none>           <none>   # through  replica

hai@master:~/test$ kubectl describe pod prod
Name:             prod
Namespace:        default
Labels:           godfather=megastar          \\\\\\\\\\\\
Controlled By:  ReplicationController/rc      ===============
Containers:
  prod:
    Container ID:   containerd://213aa8b302937e24956e1056acb1118a96dae0aadbb79694cff5692b0417147c
    Image:          nginx
--"
hai@master:~/test$ kubectl describe pod rc-f4nnf
Name:             rc-f4nnf
Namespace:        default
Labels:           godfather=megastar        ////////////////
Controlled By:  ReplicationController/rc    ==================
Containers:
  rc:
    Container ID:   containerd://09fd1264c34de5b38daaea0a1257bace078751ccf897b0d5c5315f0b7baf4837
    Image:          nginx
--"
hai@master:~/test$ kubectl describe pod rc-jz5vp
Name:             rc-jz5vp
Namespace:        default
Labels:           godfather=megastar        /////////////////
Controlled By:  ReplicationController/rc    =========================
Containers:
  rc:
    Container ID:   containerd://6bc069d2cd33fc6bad00e5f7f3934aeaa466d127ee6ea839f5a05124651ab552
    Image:          nginx

\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
IMPORTENT  
hai@master:~/test$ kubectl get pods  --show-labels
NAME       READY   STATUS    RESTARTS   AGE   LABELS
prod       1/1     Running   0          50m   godfather=megastar
rc-f4nnf   1/1     Running   0          26m   godfather=megastar
rc-jz5vp   1/1     Running   0          26m   godfather=megastar
---"
hai@master:~/test$ kubectl label pod  prod  godfather-
pod/prod unlabeled

hai@master:~/test$ kubectl get pods  --show-labels
NAME       READY   STATUS              RESTARTS   AGE   LABELS
prod       1/1     Running             0          52m   <none>
rc-2h669   0/1     ContainerCreating   0          2s    godfather=megastar
rc-f4nnf   1/1     Running             0          28m   godfather=megastar
rc-jz5vp   1/1     Running             0          28m   godfather=megastar

hai@master:~/test$ kubectl get pods  --show-labels
NAME       READY   STATUS    RESTARTS   AGE   LABELS
prod       1/1     Running   0          52m   <none>
rc-2h669   1/1     Running   0          8s    godfather=megastar
rc-f4nnf   1/1     Running   0          28m   godfather=megastar
rc-jz5vp   1/1     Running   0          28m   godfather=megastar

hai@master:~/test$ kubectl get rc,pod  --show-labels  -o wide
NAME                       DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR             LABELS
replicationcontroller/rc   3         3         3       32m   rc           nginx    godfather=megastar   godfather=megastar

NAME           READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/prod       1/1     Running   0          56m     10.44.0.1   worker2   <none>           <none>            <none>
pod/rc-2h669   1/1     Running   0          3m57s   10.36.0.2   worker1   <none>           <none>            godfather=megastar
pod/rc-f4nnf   1/1     Running   0          32m     10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc-jz5vp   1/1     Running   0          32m     10.44.0.2   worker2   <none>           <none>            godfather=megastar
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"
hai@master:~/test$ kubectl get rc,pod  --show-labels  -o wide
NAME                       DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR             LABELS
replicationcontroller/rc   3         3         3       32m   rc           nginx    godfather=megastar   godfather=megastar

NAME           READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/prod       1/1     Running   0          56m     10.44.0.1   worker2   <none>           <none>            <none>
pod/rc-2h669   1/1     Running   0          3m57s   10.36.0.2   worker1   <none>           <none>            godfather=megastar
pod/rc-f4nnf   1/1     Running   0          32m     10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc-jz5vp   1/1     Running   0          32m     10.44.0.2   worker2   <none>           <none>            godfather=megastar

hai@master:~/test$ kubectl label pod prod  godfather=megastar
pod/prod labeled "


hai@master:~$ kubectl get rc,pod  --show-labels  -o wide
NAME                       DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR             LABELS
replicationcontroller/rc   3         3         3       40m   rc           nginx    godfather=megastar   godfather=megastar

NAME           READY   STATUS        RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/prod       1/1     Running       0          64m   10.44.0.1   worker2   <none>           <none>            godfather=megastar
"pod/rc-2h669   1/1     Terminating   0          12m   10.36.0.2   worker1   <none>           <none>            godfather=megastar"
pod/rc-f4nnf   1/1     Running       0          40m   10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc-jz5vp   1/1     Running       0          40m   10.44.0.2   worker2   <none>           <none>            godfather=megastar

hai@master:~$ kubectl get rc,pod  --show-labels  -o wide
NAME                       DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR             LABELS
replicationcontroller/rc   3         3         3       40m   rc           nginx    godfather=megastar   godfather=megastar

NAME           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/prod       1/1     Running   0          64m   10.44.0.1   worker2   <none>           <none>            godfather=megastar
pod/rc-f4nnf   1/1     Running   0          40m   10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc-jz5vp   1/1     Running   0          40m   10.44.0.2   worker2   <none>           <none>            godfather=megastar
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
delete replicacontroller "
hai@master:~$ kubectl get rc,pod  --show-labels  -o wide
NAME                       DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR             LABELS
replicationcontroller/rc   3         3         3       40m   rc           nginx    godfather=megastar   godfather=megastar

NAME           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/prod       1/1     Running   0          64m   10.44.0.1   worker2   <none>           <none>            godfather=megastar
pod/rc-f4nnf   1/1     Running   0          40m   10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc-jz5vp   1/1     Running   0          40m   10.44.0.2   worker2   <none>           <none>            godfather=megastar
--"
hai@master:~$ kubectl delete rc  --cascade=false  rc
warning: --cascade=false is deprecated (boolean value) and can be replaced with --cascade=orphan.    ## importent 
replicationcontroller "rc" deleted
-"
hai@master:~/test$ kubectl get rc,pod  --show-labels  -o wide
NAME           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/prod       1/1     Running   0          76m   10.44.0.1   worker2   <none>           <none>            godfather=megastar
pod/rc-f4nnf   1/1     Running   0          52m   10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc-jz5vp   1/1     Running   0          52m   10.44.0.2   worker2   <none>           <none>            godfather=megastar
hai@master:~/test$

=====================================================================================================================================


hai@master:~$ kubectl describe rc  rc1
Name:         rc1
Namespace:    default
Selector:     godfather=megastar
Labels:       godfather=megastar
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  godfather=megastar
  Containers:
   rc1:
    Image:        nginx:1.16
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:           <none>
hai@master:~$


hai@master:~/test$ cat rc.yaml
apiVersion: v1
kind: ReplicationController
metadata:
   name: rc1
spec:
   replicas: 3
   selector:
      godfather: megastar
   template:
      metadata:
        name: rc
        labels:
           godfather: megastar
      spec:
        containers:
           - name: rc
             image: nginx:1.16
             ports:
                - containerPort: 80
"
hai@master:~/test$ kubectl apply   -f rc.yaml
replicationcontroller/rc1 created

hai@master:~$ kubectl get rc,pod  --show-labels  -o wide
NAME                        DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR             LABELS
replicationcontroller/rc1   3         3         0       9s    rc1          nginx:1.16   godfather=megastar   godfather=megastar

NAME            READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/rc1-dlvpn   0/1     ContainerCreating   0          9s    <none>   worker2   <none>           <none>            godfather=megastar
pod/rc1-f2tvp   0/1     ContainerCreating   0          9s    <none>   worker1   <none>           <none>            godfather=megastar
pod/rc1-hn4cm   0/1     ContainerCreating   0          9s    <none>   worker2   <none>           <none>            godfather=megastar
===-"
hai@master:~$ kubectl get rc,pod  --show-labels  -o wide
NAME                        DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES         SELECTOR             LABELS
replicationcontroller/rc1   3         3         3       17s   rc1          "nginx:1.16 "    godfather=megastar   godfather=megastar

NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/rc1-dlvpn   1/1     Running   0          17s   10.44.0.1   worker2   <none>           <none>            godfather=megastar
pod/rc1-f2tvp   1/1     Running   0          17s   10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc1-hn4cm   1/1     Running   0          17s   10.44.0.2   worker2   <none>           <none>            godfather=megastar
------
hai@master:~/test$ kubectl describe pod rc1-dlvpn
Name:             rc1-dlvpn
Namespace:        default
Controlled By:  ReplicationController/rc1
Containers:
  rc1:
    Image:          "nginx:1.16"    "## look see "

==============================
rolling  update 


apiVersion: v1
kind: ReplicationController
metadata:
   name: rc1
spec:
   replicas: 3
   selector:
      godfather: megastar
   template:
      metadata:
        name: rc1
        labels:
           godfather: megastar
      spec:
        containers:
           - name: rc1
             image: nginx:1.18
             ports:
                - containerPort: 80
"
hai@master:~/test$ kubectl apply -f rc1.18up.yaml
replicationcontroller/rc1 configured


NAME                        DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR             LABELS
replicationcontroller/rc1   3         3         3       91m   rc1          "nginx:1.16"   godfather=megastar   godfather=megastar

NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/rc1-dlvpn   1/1     Running   0          91m   10.44.0.1   worker2   <none>           <none>            godfather=megastar
pod/rc1-f2tvp   1/1     Running   0          91m   10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc1-hn4cm   1/1     Running   0          91m   10.44.0.2   worker2   <none>           <none>            godfather=megastar
"
hai@master:~$ kubectl get rc,pod --show-labels  -o wide
NAME                        DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR             LABELS
replicationcontroller/rc1   3         3         3       91m   rc1          "nginx:1.18"   godfather=megastar   godfather=megastar

NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/rc1-dlvpn   1/1     Running   0          91m   10.44.0.1   worker2   <none>           <none>            godfather=megastar
pod/rc1-f2tvp   1/1     Running   0          91m   10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc1-hn4cm   1/1     Running   0          91m   10.44.0.2   worker2   <none>           <none>            godfather=megastar



hai@master:~$ kubectl describe rc  rc1
Name:         rc1
Namespace:    default
Selector:     godfather=megastar
Labels:       godfather=megastar
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  godfather=megastar
  Containers:
   rc1:
    Image:        nginx:1.16
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:           <none>
hai@master:~$

 kubectl rolling-update rc1   --update-period=10s -f rc.yaml     , this feather not avilable this market 
 ==============================####################################################################################
             "   ReplicaSet   ""
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#replicaset-v1-apps
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#replicasetstatus-v1-apps
hai@master:~/test$ kubectl explain rs 

apiVersion: apps/v1
kind: ReplicaSet
metadata:  
   name: rs1
spec:
  replicas: 3
  selector: 
    matchExpressions:
       - key: team
         operator: In
         values:
           -  dev
           -  prod
           -  test
  template: 
    metadata:
      name: rs1
      labels:
        team: dev
    spec: 
      containers:
        - name: nginx
          image: nginx:1.16
          ports:
            - containerPort: 80
"
hai@master:~/test$  kubectl get rs,pod  --show-labels  -o  wide
NAME                  DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR                  LABELS
replicaset.apps/rs1   3         3         3       80s   nginx        nginx:1.16   team in (dev,prod,test)   <none>

NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/rs1-7c7cg   1/1     Running   0          80s   10.44.0.2   worker2   <none>           <none>            team=dev
pod/rs1-dkjgp   1/1     Running   0          80s   10.44.0.1   worker2   <none>           <none>            team=dev
pod/rs1-lv2tw   1/1     Running   0          80s   10.36.0.1   worker1   <none>           <none>            team=dev
"------
hai@master:~/test$ kubectl run a  --image=nginx
pod/a created
hai@master:~/test$ kubectl run b  --image=nginx
pod/b created
hai@master:~/test$ kubectl run c  --image=nginx
pod/c created

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a   1/1     Running   0          32s   10.44.0.1   worker2   <none>           <none>            "run=a"
pod/b   1/1     Running   0          25s   10.36.0.1   worker1   <none>           <none>            run=b    # look see 
pod/c   1/1     Running   0          20s   10.44.0.2   worker2   <none>           <none>            run=c
"========
hai@master:~/test$ kubectl label pod a  run-
pod/a unlabeled
hai@master:~/test$ kubectl label pod b  run-
pod/b unlabeled
hai@master:~/test$ kubectl label pod c  run-
pod/c unlabeled
hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME    READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a   1/1     Running   0          3m10s   10.44.0.1   worker2   <none>           <none>            <none>   # look see 
pod/b   1/1     Running   0          3m3s    10.36.0.1   worker1   <none>           <none>           " <none>"
pod/c   1/1     Running   0          2m58s   10.44.0.2   worker2   <none>           <none>            <none>
"=========
hai@master:~/test$ kubectl label  pod  a  team=dev
pod/a labeled
hai@master:~/test$ kubectl label  pod  b  team=dev
pod/b labeled
hai@master:~/test$ kubectl label  pod  c  team=dev
pod/c labeled

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME    READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a   1/1     Running   0          6m6s    10.44.0.1   worker2   <none>           <none>            team=dev"
pod/b   1/1     Running   0          5m59s   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c   1/1     Running   0          5m54s   10.44.0.2   worker2   <none>           <none>            team=dev

"  ======
hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME                  DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES       SELECTOR                                    LABELS
replicaset.apps/rs1   3         3         3       2m47s   nginx        nginx:1.16   team in (dev,prod,test),team notin (test)   <none>

NAME    READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a   1/1     Running   0          14m   10.44.0.1   worker2   <none>           <none>            team=dev
pod/b   1/1     Running   0          14m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c   1/1     Running   0          14m   10.44.0.2   worker2   <none>           <none>            team=dev
"==============
hai@master:~/test$ kubectl delete rs  --cascade=false  rs1      ##  importnet ok 
warning: --cascade=false is deprecated (boolean value) and can be replaced with --cascade=orphan.
replicaset.apps "rs1" deleted

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a   1/1     Running   0          14m   10.44.0.1   worker2   <none>           <none>            team=dev
pod/b   1/1     Running   0          14m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c   1/1     Running   0          14m   10.44.0.2   worker2   <none>           <none>            team=dev
"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a   1/1     Running   0          24m   10.44.0.1   worker2   <none>           <none>            team=prod
pod/b   1/1     Running   0          24m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c   1/1     Running   0          24m   10.44.0.2   worker2   <none>           <none>            team=test"

apiVersion: apps/v1
kind: ReplicaSet
metadata:
   name: rs1
spec:
  replicas: 3
  selector:
    matchExpressions:
       - key: team                          ## see  look  2 keys   2 operators 
         operator: In
         values:
           -  dev
           -  prod
           -  test
       - key: team                 ## see  look   2 keys 2 operators     
         operator: NotIn
         values:
           -  test
  template:
    metadata:
      name: rs1
      labels:
        team: dev
    spec:
      containers:
        - name: nginx
          image: nginx:1.16
          ports:
            - containerPort: 80



hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME                  DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR                                        LABELS
replicaset.apps/rs1   3         3         2       0s    nginx        nginx:1.16   team in (dev,prod,test),team notin (test)        <none>

NAME            READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a           1/1     Running             0          25m   10.44.0.1   worker2   <none>           <none>            team=prod
pod/b           1/1     Running             0          24m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c           1/1     Running             0          24m   10.44.0.2   worker2   <none>           <none>            team=test                                                  
pod/rs1-sr64m   0/1     ContainerCreating   0          0s    <none>      worker1   <none>           <none>            team=dev"

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME                  DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR                                       LABELS
replicaset.apps/rs1   3         3         2       1s    nginx        nginx:1.16   team in (dev,prod,test),team notin (test)       <none>

NAME            READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a           1/1     Running             0          25m   10.44.0.1   worker2   <none>           <none>            team=prod
pod/b           1/1     Running             0          24m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c           1/1     Running             0          24m   10.44.0.2   worker2   <none>           <none>            team=test
pod/rs1-sr64m   0/1     ContainerCreating   0          2s    <none>      worker1   <none>           <none>            team=dev"

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME                  DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR                                        LABELS
replicaset.apps/rs1   3         3         2       3s    nginx        nginx:1.16   team in (dev,prod,test),team notin (test)        <none>

NAME            READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a           1/1     Running             0          25m   10.44.0.1   worker2   <none>           <none>            team=prod
pod/b           1/1     Running             0          24m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c           1/1     Running             0          24m   10.44.0.2   worker2   <none>           <none>            team=test
pod/rs1-sr64m   0/1     ContainerCreating   0          3s    <none>      worker1   <none>           <none>            team=dev"

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME                  DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR                                    LABELS
replicaset.apps/rs1   3         3         3       21s   nginx        nginx:1.16   team in (dev,prod,test),team notin (test)   <none>

NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a           1/1     Running   0          25m   10.44.0.1   worker2   <none>           <none>            team=prod
pod/b           1/1     Running   0          25m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c           1/1     Running   0          25m   10.44.0.2   worker2   <none>           <none>            team=test
pod/rs1-sr64m   1/1     Running   0          21s   10.36.0.2   worker1   <none>           <none>            team=dev

hai@master:~/test$ kubectl  describe pod a
Name:             a
Namespace:        default
Labels:           team=prod
Controlled By:  "ReplicaSet/rs1"    #  see look rs controlled 

hai@master:~/test$ kubectl  describe pod b
Name:             b
Name:             a
Namespace:        default
Labels:            team=dev
Controlled By:  "ReplicaSet/rs1"    #  see look rs controlled 


hai@master:~/test$ kubectl  describe pod rs1-sr64m
Name:             rs1-sr64m
Namespace:        default
Labels:           team=dev
Controlled By:  "ReplicaSet/rs1

hai@master:~/test$ kubectl  describe pod c
Name:             c
Namespace:        default
Labels:           team=test     #  " SEE LOOK  NO  CONTROLLED "
NO NO NO  " no CONTROLLED" 
========================="
hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide

NAME                  DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR                                    LABELS
replicaset.apps/rs1   3         3         3       45m   nginx        nginx:1.16   team in (dev,prod,test),team notin (test)   <none>

NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a           1/1     Running   0          70m   10.44.0.1   worker2   <none>           <none>            team=prod
pod/b           1/1     Running   0          70m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c           1/1     Running   0          70m   10.44.0.2   worker2   <none>           <none>            "team=test"
pod/rs1-sr64m   1/1     Running   0          45m   10.36.0.2   worker1   <none>           <none>            team=dev
"--
hai@master:~/test$ kubectl delete rs  rs1
replicaset.apps "rs1" deleted

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME                  DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR                                    LABELS
replicaset.apps/rs1   3         3         3       45m   nginx        nginx:1.16   team in (dev,prod,test),team notin (test)   <none>

NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a           1/1     Running   0          70m   10.44.0.1   worker2   <none>           <none>            team=prod
pod/b           1/1     Running   0          70m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c           1/1     Running   0          70m   10.44.0.2   worker2   <none>           <none>            team=test
pod/rs1-sr64m   1/1     Running   0          45m   10.36.0.2   worker1   <none>           <none>            team=dev

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME            READY   STATUS        RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a           1/1     "Terminating "  0          72m   10.44.0.1   worker2   <none>           <none>            team=prod
pod/b           1/1     "Terminating"   0          72m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c           1/1     Running       0          72m   10.44.0.2   worker2   <none>           <none>            team=test
pod/rs1-sr64m   1/1     "Terminating"   0          47m   10.36.0.2   worker1   <none>           <none>            team=dev"

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/c   1/1     Running   0          72m   10.44.0.2   worker2   <none>           <none>            team=test
################################################################################################################################
---     DEPLOYMENT ==   

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#deployment-v1-apps

Deployment v1 apps
Group     	Version	         Kind
apps	      v1	             Deployment

DeploymentSpec v1 apps
 Appears In:
Deployment [apps/v1]
Field	Description
minReadySeconds
integer	Minimum number of seconds for which a newly created pod should be ready without any of its container crashing, for it to be considered available. Defaults to 0 (pod will be considered available as soon as it is ready)

paused
boolean	Indicates that the deployment is paused.

progressDeadlineSeconds
integer	The maximum time in seconds for a deployment to make progress before it is considered to be failed. The deployment controller will continue to process failed deployments and a condition with a ProgressDeadlineExceeded reason will be surfaced in the deployment status. Note that progress will not be estimated during the time a deployment is paused. Defaults to 600s.

replicas
integer	Number of desired pods. This is a pointer to distinguish between explicit zero and not specified. Defaults to 1.

revisionHistoryLimit
integer	The number of old ReplicaSets to retain to allow rollback. This is a pointer to distinguish between explicit zero and not specified. Defaults to 10.
selector
LabelSelector	Label selector for pods. Existing ReplicaSets whose pods are selected by this will be the ones affected by this deployment. It must match the pod template's labels.

strategy
DeploymentStrategy
patch strategy: retainKeys	The deployment strategy to use to replace existing pods with new ones.

template
PodTemplateSpec	Template describes the pods that will be created.

kubectl explain deployment --recursive | less

  spec <Object>
      minReadySeconds   <integer>
      paused    <boolean>
      progressDeadlineSeconds   <integer>
      replicas  <integer>
      revisionHistoryLimit      <integer>
      selector  <Object>
         matchExpressions       <[]Object>
            key <string>
            operator    <string>
            values      <[]string>
         matchLabels    <map[string]string>
      strategy  <Object>
         rollingUpdate  <Object>
            maxSurge    <string>
            maxUnavailable      <string>
         type   <string>
      template  <Object>
=======
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: web-app
spec:
  replicas: 3
  selector: 
     matchLabels:
        app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec: 
      containers:
      -  name: nginx
         image: nginx:1.7.1
         ports:
           - containerPort: 80


hai@master:~$ kubectl get deployment -o wide
No resources found in default namespace.

hai@master:~$ kubectl get deployment -o wide
NAME      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR
web-app   0/3     0            0           5s    nginx        nginx:1.7.1   app=nginx"


hai@master:~$ kubectl get all -o wide
NAME                           READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES
pod/web-app-788b5bb875-77k9f   0/1     ContainerCreating   0          39s   <none>   worker1   <none>           <none>
pod/web-app-788b5bb875-fxtck   0/1     ContainerCreating   0          39s   <none>   worker2   <none>           <none>
pod/web-app-788b5bb875-ltzhf   0/1     ContainerCreating   0          39s   <none>   worker2   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d16h   <none>
NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR
deployment.apps/web-app   0/3     3            0           43s   nginx        nginx:1.7.1   app=nginx
NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR
replicaset.apps/web-app-788b5bb875   3         3         0       42s   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875"


hai@master:~$ kubectl get rs,pod  --show-labels  -o  wide
NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   3         3         3       46m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875

NAME                           READY   STATUS    RESTARTS        AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-77k9f   1/1     Running   1 (2m28s ago)   46m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-fxtck   1/1     Running   1 (3m46s ago)   46m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-ltzhf   1/1     Running   1 (3m45s ago)   46m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

=================================
hai@master:~/test$ kubectl describe deployment
Name:                   web-app
Namespace:              default
CreationTimestamp:      Thu, 22 Dec 2022 01:55:39 -0800
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.7.1     "### see look "
    Port:         80/TCP
 Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>                               "###  see look" 
NewReplicaSet:   web-app-788b5bb875 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  71m   deployment-controller  Scaled up replica set web-app-788b5bb875 to 3"

hai@master:~$ kubectl rollout history deployment.apps/web-app
deployment.apps/web-app
REVISION  CHANGE-CAUSE        "## see look "
1         <none>

hai@master:~$ kubectl get all --show-labels  -o  wide
NAME                           READY   STATUS    RESTARTS      AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-77k9f   1/1     Running   1 (48m ago)   91m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-fxtck   1/1     Running   1 (49m ago)   91m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-ltzhf   1/1     Running   1 (49m ago)   91m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d18h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     3            3           91m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   3         3         3       91m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875"


hai@master:~/test$ kubectl delete -f  dep.yaml
deployment.apps "web-app" deleted"

hai@master:~$ kubectl get all --show-labels  -o  wide
NAME                           READY   STATUS        RESTARTS      AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-77k9f   1/1     "Terminating"   1 (49m ago)   93m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-"788b5bb875"-fxtck   1/1     "Terminating"   1 (50m ago)   93m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-ltzhf   1/1     "Terminating"   1 (50m ago)   93m   10.44.0.2   worker2   <none>           <none>            "app=nginx,pod-template-hash=788b5bb875"

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d18h   <none>     component=apiserver,provider=kubernetes


hai@master:~$ kubectl get all --show-labels  -o  wide
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d18h   <none>     component=apiserver,provider=kubernetes

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
hai@master:~/test$ kubectl delete -f  dep.yaml
deployment.apps "web-app" deleted"

hai@master:~$ kubectl get all --show-labels  -o  wide
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d18h   <none>     component=apiserver,provider=kubernetes"

hai@master:~/test$ kubectl apply -f  dep.yaml  --record=true         " ## importent see best ok "
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/web-app created

------
hai@master:~$ kubectl get all --show-labels  -o  wide
NAME                           READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-dtbbt   0/1     "ContainerCreating"   0          1s    <none>   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-lvrmz   0/1     "ContainerCreating "  0          1s    <none>   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-rq7p4   0/1     "ContainerCreating "  0          1s    <none>   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d18h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   0/3     3            0           1s    nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   3         3         0       1s    nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875"
-------
hai@master:~$ kubectl get all --show-labels  -o  wide
NAME                           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-"788b5bb875-dtbbt"   1/1     Running   0          22s   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-"788b5bb875-lvrmz"   1/1     Running   0          22s   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-"788b5bb875-rq7p4"   1/1     Running   0          22s   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d18h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     3            3           22s   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS   ##"old id safe  look see "
replicaset.apps/web-app-788b5bb875   3         3         3       22s   nginx        nginx:1.7.1   "app=nginx,pod-template-hash=788b5bb875 "  "app=nginx,pod-template-hash=788b5bb875"
----------"

hai@master:~$ kubectl describe  deployment
Name:                   web-app
Namespace:              default
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.7.1
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   web-app-788b5bb875 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  13m   deployment-controller  Scaled up replica set web-app-"788b5bb875" to 3"
-----
hai@master:~$ kubectl rollout history deployment.apps/web-app
deployment.apps/web-app
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=dep.yaml --record=true     "###  see look modifying "


apiVersion: apps/v1
kind: Deployment
metadata: 
   name: web-app
spec:
  replicas: 3
  selector: 
     matchLabels:
        app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec: 
      containers:
      -  name: nginx
         image: nginx:1.9.1    "## see  look  update "
         ports:
           - containerPort: 80









hai@master:~$ kubectl get all --show-labels -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-dtbbt   1/1     Running   0          37m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-lvrmz   1/1     Running   0          37m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-rq7p4   1/1     Running   0          37m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     3            3           37m   nginx        "nginx:1.7.1 "  app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   3         3         3       37m   nginx       " nginx:1.7.1"   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
#"/////////////////////////////////////////////

hai@master:~$ kubectl get all --show-labels -o wide
NAME                           READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-dtbbt   1/1     Running             0          38m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-lvrmz   1/1     Running             0          38m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-rq7p4   1/1     Running             0          38m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-"f4488db8f-s89g9    0/1     ContainerCreating"   0          0s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     1            3           38m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   3         3         3       38m   nginx        "nginx:1.7.1"   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    1         1         0       0s    nginx        "nginx:1.9.1  " app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
"#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\//////////////////\\\\\\\\\/////?///\\/\/\/\/\\/\/

hai@master:~$ kubectl get all --show-labels -o wide
NAME                           READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-dtbbt   1/1     Running             0          38m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-lvrmz   1/1     Running             0          38m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
"pod/web-app-f4488db8f-p9kvn    0/1     ContainerCreating   "0          9s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/web-app-f4488db8f-s89g9    1/1     Running             0          39s   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     2            3           38m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   2         2         2       38m   nginx        "nginx:1.7.1"   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    2         2         1       39s   nginx        "nginx:1.9.1"   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
"#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\//////////////////\\\\\\\\\/////?///\\/\/\/\/\\/\/

hai@master:~$ kubectl get all --show-labels -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-f4488db8f-jmv9w   1/1     Running   0          82s     10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/web-app-f4488db8f-p9kvn   1/1     Running   0          110s    10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/web-app-f4488db8f-s89g9   1/1     Running   0          2m20s   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     3            3           40m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   0         0         0       40m     nginx       " nginx:1.7.1 "  app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    3         3         3       2m20s   nginx        "nginx:1.9.1  " app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
hai@master:~$
---////\\\\\\\///////////'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"

hai@master:~$ kubectl rollout history deployment.apps/web-app
deployment.apps/web-app
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=dep.yaml --record=true
2         <none>"
=\\//\\\\\\\\\\\\\\\\\\\\\\

hai@master:~$ kubectl describe deployment web-app
Name:                   web-app
Namespace:              default
Labels:                 <none>
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.9.1    " # see look "
    Port:         80/TCP
OldReplicaSets:  <none>
NewReplicaSet:   web-app-f4488db8f (3/3 replicas created)"
######################################################################################################################
 kubectl rollout undo deployment.apps/web-app --to-revision=1   "# importent  "" 

hai@master:~/test$  kubectl rollout undo deployment.apps/web-app --to-revision=1
deployment.apps/web-app rolled back
////"
hai@master:~/test$  kubectl rollout history deployment.apps/web-app
deployment.apps/web-app
REVISION  CHANGE-CAUSE
2         <none>
"3         kubectl apply --filename=dep.yaml --record=true "   
+++++++++"
hai@master:~$ kubectl describe deployment web-app
Name:                   web-app
Annotations:            deployment.kubernetes.io/revision: 3
                        kubernetes.io/change-cause: kubectl apply --filename=dep.yaml --record=true
Selector:               app=nginx
Replicas:               3 desired | 1 updated | 4 total | 3 available | 1 unavailable
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.7.1
"OldReplicaSets:  web-app-f4488db8f (3/3 replicas created)
NewReplicaSet:   web-app-788b5bb875 (1/1 replicas created)"
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  49m   deployment-controller  Scaled up replica set web-app-f4488db8f to 1
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 2 from 3
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled up replica set web-app-f4488db8f to 2 from 1
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 1 from 2
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled up replica set web-app-f4488db8f to 3 from 2 
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 0 from 1
  "Normal  ScalingReplicaSet  1s    deployment-controller  Scaled up replica set web-app-788b5bb875 to 1 from 0""
-\\\\\\//////////////\\\\\\/\/\/\/\/\////

hai@master:~$ kubectl describe deployment web-app
Name:                   web-app
Namespace:              default
Annotations:            deployment.kubernetes.io/revision: 3
                        kubernetes.io/change-cause: kubectl apply --filename=dep.yaml --record=true
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 4 total | 3 available | 1 unavailable
  Labels:  app=nginx                            "see look"
  Containers:
   nginx:
    Image:        nginx:1.7.1
    Port:         80/TCP
OldReplicaSets:  web-app-f4488db8f (1/1 replicas created)     "## look see "
NewReplicaSet:   web-app-788b5bb875 (3/3 replicas created)   " ##  look see "
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  49m   deployment-controller  Scaled up replica set web-app-f4488db8f to 1
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 2 from 3
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled up replica set web-app-f4488db8f to 2 from 1
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 1 from 2
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled up replica set web-app-f4488db8f to 3 from 2
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 0 from 1
  Normal  ScalingReplicaSet  12s   deployment-controller  Scaled up replica set web-app-788b5bb875 to 1 from 0
  Normal  ScalingReplicaSet  8s    deployment-controller  Scaled down replica set web-app-f4488db8f to 2 from 3
  Normal  ScalingReplicaSet  8s    deployment-controller  Scaled up replica set web-app-788b5bb875 to 2 from 1
  Normal  ScalingReplicaSet  3s    deployment-controller  Scaled down replica set web-app-f4488db8f to 1 from 2
"  Normal  ScalingReplicaSet  3s    deployment-controller  Scaled up replica set web-app-788b5bb875 to 3 from 2""
=////////////////////  \\\\\\\\\\\\\\\//
hai@master:~$ kubectl describe deployment web-app
Name:                   web-app
Namespace:              default
Annotations:            deployment.kubernetes.io/revision: 3
                        kubernetes.io/change-cause: kubectl apply --filename=dep.yaml --record=true
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate           "see look "
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.7.1
    Port:         80/TCP
OldReplicaSets:  <none>               "##see  look "
NewReplicaSet:   web-app-788b5bb875 (3/3 replicas created)  "# see look"
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  49m   deployment-controller  Scaled up replica set web-app-f4488db8f to 1
  Normal  ScalingReplicaSet  49m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 2 from 3
  Normal  ScalingReplicaSet  49m   deployment-controller  Scaled up replica set web-app-f4488db8f to 2 from 1
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 1 from 2
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled up replica set web-app-f4488db8f to 3 from 2
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 0 from 1
  Normal  ScalingReplicaSet  14s   deployment-controller  Scaled up replica set web-app-788b5bb875 to 1 from 0
  Normal  ScalingReplicaSet  10s   deployment-controller  Scaled down replica set web-app-f4488db8f to 2 from 3
  Normal  ScalingReplicaSet  10s   deployment-controller  Scaled up replica set web-app-788b5bb875 to 2 from 1
  Normal  ScalingReplicaSet  5s    deployment-controller  Scaled down replica set web-app-f4488db8f to 1 from 2
  Normal  ScalingReplicaSet  5s    deployment-controller  Scaled up replica set web-app-788b5bb875 to 3 from 2
  Normal  ScalingReplicaSet  1s    deployment-controller  Scaled down replica set web-app-f4488db8f to 0 from 1"
==/++++++++++++++++++++++++\\\\\\//////////\\\\=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
hai@master:~$ kubectl get all --show-labels -o wide
NAME                           READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-8ffnd   0/1     ContainerCreating   0          2s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-dnjgt   1/1     Running             0          6s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-f4488db8f-jmv9w    1/1     Running             0          48m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
"pod/web-app-f4488db8f-p9kvn    1/1     Terminating "        0          48m   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/web-app-f4488db8f-s89g9    1/1     Running             0          49m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     2            3           87m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   2         2         1       87m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    2         2         2       49m   nginx        "nginx:1.9.1 "  app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
=======/////////////////\\\\\\\\\\\\\\\\\\\\\\\\\\\

hai@master:~$ kubectl get all --show-labels -o wide
NAME                           READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
"pod/web-app-788b5bb875-8ffnd   0/1     ContainerCreating"   0          3s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-dnjgt   1/1     Running             0          7s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-f4488db8f-jmv9w    1/1     Running             0          48m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/web-app-f4488db8f-p9kvn    0/1     Terminating         0          48m   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/web-app-f4488db8f-s89g9    1/1     Running             0          49m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     2            3           87m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   2         2         1       87m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    2         2         2       49m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
---==##################################"
hai@master:~$ kubectl get all --show-labels -o wide
NAME                           READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-8ffnd   0/1     ContainerCreating   0          4s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-dnjgt   1/1     Running             0          8s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-f4488db8f-jmv9w    1/1     Running             0          48m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/web-app-f4488db8f-s89g9    1/1     Running             0          49m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     2            3           87m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875 "  2         2         1       87m   nginx        nginx:1.7.1"   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    2         2         2       49m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
##############################\\\\\//////////////////////////////////"

hai@master:~$ kubectl get all --show-labels -o wide
NAME                           READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-25t9m   0/1     ContainerCreating   0          0s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
"pod/web-app-788b5bb875-8ffnd   1/1     Running"             0          5s    10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-dnjgt   1/1     Running             0          9s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-f4488db8f-jmv9w    1/1     Terminating         0          48m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/web-app-f4488db8f-s89g9    1/1     Running             0          49m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     3            3           87m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   "3         3         2       87m   nginx        nginx:1.7.1 "  app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    1         1         1       49m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
========================/////////////////////////"
hai@master:~$ kubectl get all --show-labels -o wide
NAME                           READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-25t9m   1/1     Running   0          5m6s    10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-8ffnd   1/1     Running   0          5m11s   10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-dnjgt   1/1     Running   0          5m15s   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     3            3           92m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   3         3         3       92m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    0         0         0       54m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
"/////////////////////////+++++++++++++=======================##########################################

hai@master:~$ kubectl rollout pause  deployment.apps/web-app
deployment.apps/web-app paused "

apiVersion: apps/v1
kind: Deployment
metadata:
   name: web-app
spec:
  replicas: 3
  selector:
     matchLabels:
        app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      -  name: nginx
         image: nginx:1.7.1
         ports:
           - containerPort: 80"

hai@master:~/test$ kubectl apply -f depup1.18.yaml --record=true
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/web-app configured"

hai@master:~$ kubectl rollout resume  deployment.apps/web-app
deployment.apps/web-app resumed "

hai@master:~/test$ kubectl scale  deployment.apps/web-app  --replicas=10
deployment.apps/web-app scaled

hai@master:~$ kubectl get all  --show-labels  -o wide
NAME                           READY   STATUS              RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-25t9m   1/1     Running             0          4h2m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-54zb9   0/1     ContainerCreating   0          27s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-8ffnd   1/1     Running             0          4h2m   10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-dnjgt   1/1     Running             0          4h2m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-f5vvt   0/1     ContainerCreating   0          32s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-fnv65   0/1     ContainerCreating   0          27s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-gw8jm   0/1     ContainerCreating   0          27s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-nrz2l   0/1     Pending             0          27s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-pqj9s   0/1     ContainerCreating   0          30s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-zr7p7   0/1     ContainerCreating   0          29s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d23h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/10    10           3           5h30m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   10        10        3       5h30m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb
replicaset.apps/web-app-f4488db8f    0         0         0       4h52m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db

###########################################################==========================================
rollout  method   2   nd   3 rd    

"2 and method"    kubectl set image  deployment.apps/web-app  nginx=nginx:1.9.1   --record=true 


hai@master:~$  kubectl set image  deployment.apps/web-app  nginx=nginx:1.9.1   --record=true
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/web-app image updated
hai@master:~$ kubectl get all -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
pod/web-app-f4488db8f-bs9st   1/1     Running   0          4m5s    10.36.0.2   worker1   <none>           <non  e>
pod/web-app-f4488db8f-lzjh6   1/1     Running   0          4m10s   10.36.0.6   worker1   <none>           <non  e>
pod/web-app-f4488db8f-wm965   1/1     Running   0          4m8s    10.36.0.8   worker1   <none>           <non  e>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d12h   <none>

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR
deployment.apps/web-app   3/3     3            3           18h   nginx        nginx:1.9.1   app=nginx

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR
replicaset.apps/web-app-788b5bb875   0         0         0       18h   nginx        nginx:1.7.1   app=nginx,po                                            d-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    3         3         3       17h   nginx        "nginx:1.9.1 "  app=nginx,po                                            d-template-hash=f4488db8f

hai@master:~$ kubectl get all -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/web-app-788b5bb875-7d5qs   1/1     Running   0          4s    10.44.0.2   worker2   <none>           <none>
pod/web-app-788b5bb875-g2l86   1/1     Running   0          6s    10.36.0.1   worker1   <none>           <none>
pod/web-app-788b5bb875-knpsb   1/1     Running   0          7s    10.44.0.1   worker2   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d12h   <none>

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR
deployment.apps/web-app   3/3     3            3           18h   nginx        nginx:1.7.1   app=nginx

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR
replicaset.apps/web-app-788b5bb875   3         3         3       18h   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    0         0         0       17h   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f



"3rd  method"     kubectl set image  deployment.apps/web-app     

hai@master:~$ kubectl get all -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
pod/web-app-f4488db8f-bs9st   1/1     Running   0          4m5s    10.36.0.2   worker1   <none>           <non  e>
pod/web-app-f4488db8f-lzjh6   1/1     Running   0          4m10s   10.36.0.6   worker1   <none>           <non  e>
pod/web-app-f4488db8f-wm965   1/1     Running   0          4m8s    10.36.0.8   worker1   <none>           <non  e>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d12h   <none>

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR
deployment.apps/web-app   3/3     3            3           18h   nginx        nginx:1.9.1   app=nginx

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR
replicaset.apps/web-app-788b5bb875   0         0         0       18h   nginx        nginx:1.7.1   app=nginx,po                                            d-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    3         3         3       17h   nginx        "nginx:1.9.1 "  app=nginx,po                                            d-template-hash=f4488db8f

hai@master:~$ kubectl get all -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/web-app-788b5bb875-7d5qs   1/1     Running   0          4s    10.44.0.2   worker2   <none>           <none>
pod/web-app-788b5bb875-g2l86   1/1     Running   0          6s    10.36.0.1   worker1   <none>           <none>
pod/web-app-788b5bb875-knpsb   1/1     Running   0          7s    10.44.0.1   worker2   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d12h   <none>

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR
deployment.apps/web-app   3/3     3            3           18h   nginx        nginx:1.7.1   app=nginx

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR
replicaset.apps/web-app-788b5bb875   3         3         3       18h   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    0         0         0       17h   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f

#############################################################################################################################################################################
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#alpha
                    deployment 

apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginxsurge
spec:
  replicas: 5
  strategy:
    rollingUpdate:
       maxSurge: 1                    ## see look 1
       maxUnavailable: 0
    type: RollingUpdate     
  selector:
    matchLabels: 
       app: nginx
  template:
    metadata:
      labels: 
        app: nginx
    spec:
      containers:
        -  name: nginx
           image: nginx:1.7.1
           ports:
             - containerPort: 80

/////////////////////////////////////////////////////////\\\\
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4gsn4   0/1     "ContainerCreating "  0          2s    <none>   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-55h47   0/1     ContainerCreating   0          2s    <none>   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-bjk8p   0/1     ContainerCreating   0          2s    <none>   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-fvltd   0/1     ContainerCreating   0          2s    <none>   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-pj5kd   0/1     "ContainerCreating "  0          2s    <none>   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d14h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   0/5     5            0           2s    nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5        " 0  "     2s    nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875"
======================================================================""

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4gsn4   0/1     ContainerCreating   0          3s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-55h47   0/1     ContainerCreating   0          3s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-bjk8p   1/1    " Running "            0          3s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-fvltd   0/1     ContainerCreating   0          3s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-pj5kd   0/1     ContainerCreating   0          3s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d14h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   1/5     5            1           3s    nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5        " 1 "      3s    nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875"

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4gsn4   1/1     Running   0          4s    10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-55h47   1/1     Running   0          4s    10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-bjk8p   1/1     Running   0          4s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-fvltd   1/1     Running   0          4s    10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-pj5kd   1/1     Running   0          4s    10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d14h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     5            5           4s    nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5         "5  "     4s    nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
hai@master:~$ kubectl get all --show-labels    -o wide
==+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginxsurge
spec:
  replicas: 5
  strategy:
    rollingUpdate:
       maxSurge: 2                ## see look  2 
       maxUnavailable: 0
    type: RollingUpdate     
  selector:
    matchLabels: 
       app: nginx
  template:
    metadata:
      labels: 
        app: nginx
    spec:
      containers:
        -  name: nginx
           image: nginx:1.9.1    #  see look 
           ports:
             - containerPort: 80
---"             
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4gsn4   1/1     Running             0          32m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-55h47   1/1     Running             0          32m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-bjk8p   1/1     Running             0          32m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-fvltd   1/1     Running             0          32m   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-pj5kd   1/1     Running             0          32m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
"pod/nginxsurge-f4488db8f-k7d7v    0/1     ContainerCreating"   0          1s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
"pod/nginxsurge-f4488db8f-xczlt    0/1     ContainerCreating"   0          1s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d14h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5    " 2  "          5           32m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5         5       32m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    2 "        2         0       1s    nginx        nginx:1.9.1  " app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
----==== "
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4gsn4   1/1     Running             0          32m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-55h47   1/1     Running             0          32m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-bjk8p   1/1     Running             0          32m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
"pod/nginxsurge-788b5bb875-fvltd   1/1     Terminating  "       0          32m   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
"pod/nginxsurge-788b5bb875-pj5kd   1/1     Terminating "        0          32m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-k7d7v    1/1     Running             0          2s    10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-w45lj    0/1     Con"tainerCrea"ting   0          0s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-xczlt    1/1     Running             0          2s    10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-xxmtg    0/1     Con"tainerCrea"ting   0          0s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d14h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5    " 4 "           5           32m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   3         3         3       32m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f   " 4         4         2       2s    nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f  "  app=nginx,pod-template-hash=f4488db8f
=====------"

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4gsn4   1/1     Running             0          32m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-55h47   1/1     Running             0          32m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-bjk8p   1/1     Running             0          32m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-pj5kd   1/1     "Terminating "        0          32m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-k7d7v    1/1     Running             0          4s    10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-w45lj    0/1     ContainerCreating   0          2s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-xczlt    1/1     Running             0          4s    10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-xxmtg    0/1     ContainerCreating   0          2s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d14h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     4            5           32m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   3         3         3       32m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    4         4         2       4s    nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
======================================================="
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4gsn4   1/1     Running             0          32m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-55h47   1/1     "Terminating"         0          32m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-bjk8p   1/1     "Terminating "        0          32m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-k7d7v    1/1     Running             0          4s    10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-r969w    0/1     ContainerCreating   0          0s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-w45lj    1/1     Running             0          2s    10.36.0.4   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-xczlt    1/1     Running             0          4s    10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-xxmtg    1/1     Running             0          2s    10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d14h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     5            5           32m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   1         1         1       32m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    5         5         4       4s    nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f

================================================== "
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4gsn4   1/1     Running   0          32m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-k7d7v    1/1     Running   0          6s    10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-r969w    1/1     Running   0          2s    10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-w45lj    1/1     Running   0          4s    10.36.0.4   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-xczlt    1/1     Running   0          6s    10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-xxmtg    1/1     Running   0          4s    10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d14h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   6/5     5            6           32m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         1         1       32m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    5         5         5       6s    nginx        "nginx:1.9.1 "  app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f

##################################################################### "
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginxsurge
spec:
  replicas: 5
  strategy:
    rollingUpdate:
       maxSurge: 0            ## see look       
       maxUnavailable: 5     ## see   look  
    type: RollingUpdate     
  selector:
    matchLabels: 
       app: nginx
  template:
    metadata:
      labels: 
        app: nginx
    spec:
      containers:
        -  name: nginx
           image: nginx:1.7.1   # see look 
           ports:
             - containerPort: 80
"
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-qxgvj   1/1    " Terminating "        0          2m28s   10.44.0.5   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-r8qsj   1/1     "Terminating"         0          2m28s   10.36.0.5   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-tgb9t   1/1     "Terminating  "       0          2m28s   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-tv7d8   1/1    " Terminating   "      0          2m28s   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-xvsf9   1/1     "Terminating "        0          2m28s   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-96l9k    0/1     ContainerCreating   0          1s      <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-bxcsf    0/1     ContainerCreating   0          1s      <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-grcpq    0/1     ContainerCreating   0          1s      <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-jcn62    0/1     ContainerCreating   0          1s      <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-qctxm    0/1     ContainerCreating   0          1s      <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d16h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   0/5     5            0           123m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         0         0       123m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    "5         5         0   "    91m    nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
=/////////////////////\\\\\\\\\\\\\\\\\\\\\\\\\\\\///////\\\\\\\\\\\\\\\\\\\\\""

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-r8qsj   1/1     Terminating         0          2m29s   10.36.0.5   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-tgb9t   1/1     Terminating         0          2m29s   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-xvsf9   1/1     Terminating         0          2m29s   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-96l9k    0/1     "ContainerCreating "  0          2s      <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-bxcsf    0/1     "ContainerCreating "  0          2s      <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-grcpq    0/1     "ContainerCreating "  0          2s      <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-jcn62    0/1    " ContainerCreating  " 0          2s      <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-qctxm    0/1     "ContainerCreating "  0          2s      <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d16h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   0/5     5            0           123m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         0         0       123m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f  "  5         5         0  "     91m    nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
////////////////////\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\///////////////////////\\\\\\\\\\\\\\\\\\"""

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-f4488db8f-96l9k   1/1     Running   0          5s    10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-bxcsf   1/1     Running   0          5s    10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-grcpq   1/1     Running   0          5s    10.36.0.4   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-jcn62   1/1     Running   0          5s    10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-qctxm   1/1     Running   0          5s    10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d16h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     5            5           123m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         0         0       123m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f   " 5         5         5  "     91m    nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++////////////////\\\\\\\\\""
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginxsurge
spec:
  replicas: 5
  strategy:
    rollingUpdate:
       maxSurge: 62%           # see look 
       maxUnavailable:        # see look 
    type: RollingUpdate     
  selector:
    matchLabels: 
       app: nginx
  template:
    metadata:
      labels: 
        app: nginx
    spec:
      containers:
        -  name: nginx
           image: nginx:1.7.1
           ports:
             - containerPort: 80
"
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-f4488db8f-96l9k   1/1     Running   0          16m   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-bxcsf   1/1     Running   0          16m   10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-grcpq   1/1     Running   0          16m   10.36.0.4   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-jcn62   1/1     Running   0          16m   10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-qctxm   1/1     Running   0          16m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d16h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     5            5           140m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         0         0       140m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    5         5         5       107m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
///////////////////////\\\\\\\\\\\\\/////////\\\\\\\/\\\\\\\\/\\\\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\\/\/\/\\/
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-2n65k   0/1     "ContainerCreating"   0          1s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-4w6jp   0/1     "ContainerCreating"   0          1s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-9kph7   0/1     "ContainerCreating "  0          1s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-dz598   0/1    " ContainerCreating"   0          1s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-kfn6n   0/1    " ContainerCreating"   0          1s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-96l9k    1/1     Running             0          16m   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-bxcsf    1/1     Running             0          16m   10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-grcpq    1/1     Running             0          16m   10.36.0.4   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-jcn62    1/1     Running             0          16m   10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-qctxm    1/1     "Terminating  "       0          16m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d16h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   4/5     5            4           140m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5         0       140m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    4         4         4       108m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
######\\\\\\\\\\\\\\\\\\\\\\\\\\\/////////////////////////////////////////////

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-2n65k   1/1     Running             0          4s    10.36.0.5   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-4w6jp   0/1     ContainerCreating   0          4s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-9kph7   1/1     Running             0          4s    10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-dz598   1/1     Running             0          4s    10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-kfn6n   0/1     ContainerCreating   0          4s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-96l9k    1/1     Running             0          16m   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-bxcsf    1/1     Running             0          16m   10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/"nginxsurge-f4488db8f-grcpq    1/1     Terminating   "      0          16m   10.36.0.4   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-jcn62    1/1     Running             0          16m   10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d16h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   6/5     5            6           140m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5         3       140m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    1         3         3       108m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
=///////////////////////////\\\\\\\\\\\\\\\\\\\\\\\\\\\/////////\\/\/\/\/\\\/\\/\/\\/\/\\/\/

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS        RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-2n65k   1/1     Running       0          5s    10.36.0.5   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-4w6jp   1/1     Running       0          5s    10.44.0.5   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-9kph7   1/1     Running       0          5s    10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-dz598   1/1     Running       0          5s    10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-kfn6n   1/1     Running       0          5s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-96l9k    1/1     Terminating   0          16m   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-bxcsf    1/1     Terminating   0          16m   10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-grcpq    1/1     Terminating   0          16m   10.36.0.4   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-jcn62    1/1     Terminating   0          16m   10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d16h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     5            5           140m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5         5       140m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    0         0         0       108m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
=======================================================================================
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS        RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-2n65k   1/1     Running       0          6s    10.36.0.5   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-4w6jp   1/1     Running       0          6s    10.44.0.5   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-9kph7   1/1     Running       0          6s    10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-dz598   1/1     Running       0          6s    10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-kfn6n   1/1     Running       0          6s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-jcn62    1/1     Terminating   0          16m   10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d16h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     5            5           140m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5         5       140m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    0         0         0       108m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
####################################################################################################################################
                                     RECREATE DEPLOYMENT 
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginxsurge
spec:
  replicas: 5
  strategy:
    
    type:  Recreate       ## look see  importent 
  selector:
    matchLabels: 
       app: nginx
  template:
    metadata:
      labels: 
        app: nginx
    spec:
      containers:
        -  name: nginx
           image: nginx:1.7.1
           ports:
             - containerPort: 80

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-f4488db8f-6tntc   1/1     Running   0          64s   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-8mnz4   1/1     Running   0          64s   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-mjhxl   1/1     Running   0          64s   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-qw9p8   1/1     Running   0          64s   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-wqxt7   1/1     Running   0          64s   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d17h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     5            5           11m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         0         0       11m     nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    5         5         5       7m15s   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
==\\\\\\\\\\\\\\\\\\\\\\\\\\\/////////////////////////\\\\\\\\\\\\\\\\\\\

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                             READY   STATUS        RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-f4488db8f-6tntc   1/1     "erminating"'   0          92s   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-8mnz4   1/1     "Terminating"   0          92s   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-mjhxl   1/1     "Terminating "  0          92s   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-qw9p8   1/1     Terminating   0          92s   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-wqxt7   1/1     "Terminating "  0          92s   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d17h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   0/5     0            0           11m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         0         0       11m     nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    0         0         0       7m43s   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
\\\\\\\\\\\\\|||\/\/\/\/\/\//\\////////////\\\\\\\\\\\\\\/\//\/\/\/\\\\\\\/////"

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                             READY   STATUS        RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-f4488db8f-mjhxl   0/1     "Terminating "  0          92s   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-qw9p8   1/1     "Terminating "  0          92s   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-wqxt7   0/1     "Terminating "  0          92s   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d17h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   0/5     0            0           11m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         0         0       11m     nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    0         0         0       7m43s   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
==#####\\\\\\\\\\\\\\\\\\\\\///////////\\\\\\\\\\\/\\\\\\\\\\////////////\\\\

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                             READY   STATUS        RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
"pod/nginxsurge-f4488db8f-qw9p8   1/1     Terminating "  0          93s   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d17h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   0/5     0            0           11m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         0         0       11m     nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    0         0         0       7m44s   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
=============\\\\\\\\\\\\\\\\\\\\\\////////////////////////\\\\\\\\\\////////
 
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4jqzq   0/1     ContainerCreating   0          2s    <none>   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-ck9tr   0/1     ContainerCreating   0          2s    <none>   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-fxflw   0/1     ContainerCreating   0          2s    <none>   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-qvkxl   0/1     ContainerCreating   0          2s    <none>   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-sdlgd   0/1     ContainerCreating   0          2s    <none>   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d17h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   0/5     5            0           11m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5         0       11m     nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    0         0         0       7m46s   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
\\\\\\\\/\//////////\\\\\\\\\\\\\/\\\\\\\\//////////////\/////\\\\\\\\\\\/\
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4jqzq   1/1     Running   0          7s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-ck9tr   1/1     Running   0          7s    10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-fxflw   1/1     Running   0          7s    10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-qvkxl   1/1     Running   0          7s    10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-sdlgd   1/1     Running   0          7s    10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d17h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     5            5           12m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5         5       12m     nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    0         0         0       7m51s   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
#############################################################################################################################################"
                                                  SERVICES 
1. CLUSTER IP                            2. NODEPORT                     3. LOADBALANCER                         4. EXTRENALNAME

apiVersion: apps/v1
kind: Deployment
metadata:
   name: webserver
   labels: 
      app: webserver
spec:
   replicas: 1
   strategy:
      type: Recreate    # see  look  
   selector: 
      matchLabels:
        app: webserver
   template:
      metadata:
         labels:
            app: webserver
      spec:
        containers:
          - name: nginx
            image: nginx:1.7.1
            ports:
              - containerPort: 80  
----"                                
hai@master:~$ kubectl get all --show-labels -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/"webserver-744d6b7964-vzg2t   1/1     Running "  0          32s   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d20h   <none>     component=apiserver,provider=kubernetes

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/webserver   1/1     1            1           32s   nginx        nginx:1.7.1   app=webserver   app=webserver

NAME                                   DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/webserver-744d6b7964   1         1         1       32s   nginx        "nginx:1.7.1"   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964"

apiVersion: apps/v1
kind: Deployment
metadata:
   name: dbserver
   labels: 
      app: dbserver
spec:
   replicas: 1
   strategy:
      type: Recreate
   selector: 
      matchLabels:
        app: dbserver
   template:
      metadata:
         labels:
            app: dbserver
      spec:
        containers:
          - name: nginx
            image: nginx:1.7.1
            ports:
              - containerPort: 80                    
--"
hai@master:~$ kubectl get all --show-labels -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
"pod/dbserver-5b844594d8-h67hh "   1/1     Running   0          7s    10.36.0.1   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
"pod/webserver-744d6b7964-vzg2t "  1/1     Running   0          11m   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d20h   <none>     component=apiserver,provider=kubernetes

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/dbserver    1/1     1            1           7s    nginx        nginx:1.7.1   app=dbserver    app=dbserver
deployment.apps/webserver   1/1     1            1           11m   nginx        nginx:1.7.1   app=webserver   app=webserver

NAME                                   DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/dbserver-5b844594d8    1         1         1       7s    nginx        nginx:1.7.1   app=dbserver,pod-template-hash=5b844594d8    app=dbserver,pod-template-hash=5b844594d8
replicaset.apps/webserver-744d6b7964   1         1         1       11m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964"
===================/////////////////////////////
hai@master:~$ kubectl get all  --show-labels  -o wide
NAME                             READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/dbserver-5b844594d8-h67hh    1/1     Running   0          101m   10.36.0.1   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/webserver-744d6b7964-vzg2t   1/1     Running   0          112m   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

"NAME      "           TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d22h   <none>     component=apiserver,provider=kubernetes

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/dbserver    1/1     1            1           101m   nginx        nginx:1.7.1   app=dbserver    app=dbserver
deployment.apps/webserver   1/1     1            1           112m   nginx        nginx:1.7.1   app=webserver   app=webserver

NAME                                   DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/dbserver-5b844594d8    1         1         1       101m   nginx        nginx:1.7.1   app=dbserver,pod-template-hash=5b844594d8    app=dbserver,pod-template-hash=5b844594d8
replicaset.apps/webserver-744d6b7964   1         1         1       112m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964
#+++++++++++++++++++++++++++++++++++++++++++++++
https://cloud.google.com/kubernetes-engine/docs/how-to/exposing-apps

kubectl expose deployment dbserver --type=ClusterIP  --type=ClusterIP  --port  80  --target-port  8085

kubectl expose deployment my-deployment --name my-cip-service --type ClusterIP --protocol TCP --port 80 --target-port 8080 "




hai@master:~$ kubectl get all  --show-labels  -o wide
NAME                             READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/dbserver-5b844594d8-h67hh    1/1     Running   0          133m   10.36.0.1   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/webserver-744d6b7964-vzg2t   1/1     Running   0          144m   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE     SELECTOR       LABELS
"service/dbserver     ClusterIP   10.98.65.253 "  <none>        80/TCP    2s      app=dbserver   app=dbserver
service/kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP   4d22h   <none>         component=apiserver,provider=kubernetes

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/dbserver    1/1     1            1           133m   nginx        nginx:1.7.1   app=dbserver    app=dbserver
deployment.apps/webserver   1/1     1            1           144m   nginx        nginx:1.7.1   app=webserver   app=webserver

NAME                                   DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/dbserver-5b844594d8    1         1         1       133m   nginx        nginx:1.7.1   app=dbserver,pod-template-hash=5b844594d8    app=dbserver,pod-template-hash=5b844594d8
replicaset.apps/webserver-744d6b7964   1         1         1       144m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964"
///////////////////
kubectl expose deployment dbserver --type=ClusterIP  --type=ClusterIP  --port  80  --target-port  8085


hai@master:~/test$ kubectl describe service dbserver
Name:              dbserver
Namespace:         default
Labels:            app=dbserver
Annotations:       <none>
Selector:          app=dbserver
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:               " 10.98.65.253 "   #  look see 
IPs:               10.98.65.253
Port:              <unset>  80/TCP
TargetPort:        8085/TCP
Endpoints:         10.36.0.1:8085
Session Affinity:  None
Events:            <none>"
--===========
apiVersion: apps/v1
kind: Deployment
metadata:
   name: dbserver
   labels: 
      app: dbserver
spec:
   replicas: 4    "  # SEE LOOK "
   strategy:
      type: Recreate
   selector: 
      matchLabels:
        app: dbserver
   template:
      metadata:
         labels:
            app: dbserver
      spec:
        containers:
          - name: nginx
            image: nginx:1.7.1
            ports:
              - containerPort: 80                    
++++++++++++++++++++++++++++++++++++++++++++++++++:"
hai@master:~$ kubectl get all  --show-labels  -o wide
NAME                             READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/dbserver-5b844594d8-h67hh    1/1     Running   0          3h      10.36.0.1   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-kts25    1/1     Running   0          9s      10.36.0.2   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-qxvz2    1/1     Running   0          9s      10.44.0.2   worker2   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-zhvfc    1/1     Running   0          9s      10.44.0.3   worker2   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/webserver-744d6b7964-vzg2t   1/1     Running   0          3h12m   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE     SELECTOR       LABELS
service/dbserver     ClusterIP   10.98.65.253   <none>        80/TCP    47m     app=dbserver   app=dbserver
service/kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP   4d23h   <none>         component=apiserver,provider=kubernetes

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/dbserver    4/4     4            4           3h      nginx        nginx:1.7.1   app=dbserver    app=dbserver
deployment.apps/webserver   1/1     1            1           3h12m   nginx        nginx:1.7.1   app=webserver   app=webserver

NAME                                   DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/dbserver-5b844594d8    4         4         4       3h      nginx        nginx:1.7.1   app=dbserver,pod-template-hash=5b844594d8    app=dbserver,pod-template-hash=5b844594d8
replicaset.apps/webserver-744d6b7964   1         1         1       3h12m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964"
=======================
hai@master:~$ kubectl get svc -o wide
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE     SELECTOR
"dbserver "    ClusterIP   "10.98.65.253 "  <none>        80/TCP    55m     app=dbserver
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP   4d23h   <none>"

hai@master:~$ kubectl describe  service  dbserver
Name:              dbserver
Namespace:         default
Labels:            app=dbserver
Annotations:       <none>
Selector:          app=dbserver
Type:             " ClusterIP"
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.98.65.253    " # see look "
IPs:               10.98.65.253
Port:              <unset>  80/TCP
TargetPort:        8085/TCP
Endpoints:        " 10.36.0.1:8085,10.36.0.2:8085,10.44.0.2:8085 + 1 more..."
Session Affinity:  None
Events:            <none>
============================================================================="

hai@master:~/test$ kubectl  expose deployment  webserver  --type=ClusterIP
service/webserver exposed

hai@master:~$ kubectl get all  --show-labels  -o wide
NAME                             READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/dbserver-5b844594d8-h67hh    1/1     Running   0          3h20m   10.36.0.1   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-kts25    1/1     Running   0          20m     10.36.0.2   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-qxvz2    1/1     Running   0          20m     10.44.0.2   worker2   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-zhvfc    1/1     Running   0          20m     10.44.0.3   worker2   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/webserver-744d6b7964-vzg2t   1/1     Running   0          3h32m   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE     SELECTOR        LABELS
service/dbserver     ClusterIP   10.98.65.253     <none>        80/TCP    67m     app=dbserver    app=dbserver
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   4d23h   <none>          component=apiserver,provider=kubernetes
service/webserver    ClusterIP  " 10.111.157.160  " <none>       " 80/TCP "   112s    app=webserver   app=webserver

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/dbserver    4/4     4            4           3h20m   nginx        nginx:1.7.1   app=dbserver    app=dbserver
deployment.apps/webserver   1/1     1            1           3h32m   nginx        nginx:1.7.1   app=webserver   app=webserver

NAME                                   DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/dbserver-5b844594d8    4         4         4       3h20m   nginx        nginx:1.7.1   app=dbserver,pod-template-hash=5b844594d8    app=dbserver,pod-template-hash=5b844594d8
replicaset.apps/webserver-744d6b7964   1         1         1       3h32m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964"

hai@master:~/test$ kubectl describe service webserver
Name:              webserver
Namespace:         default
Labels:            app=webserver
Annotations:       <none>
Selector:          app=webserver
Type:             " ClusterIP"
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                "10.111.157.160"
IPs:               10.111.157.160
Port:              <unset>  80/TCP
TargetPort:        80/TCP
Endpoints:         10.44.0.1:80
Session Affinity:  None
Events:            <none>
+++++++++++++++++++++++++++++++++
https://kubernetes.io/docs/concepts/services-networking/service/

apiVersion: apps/v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: webserver
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
++++++++++++++++++++"
hai@master:~$ kubectl get all  --show-labels  -o wide
NAME                             READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/dbserver-5b844594d8-h67hh    1/1     Running   0          3h41m   10.36.0.1   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-kts25    1/1     Running   0          41m     10.36.0.2   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-qxvz2    1/1     Running   0          41m     10.44.0.2   worker2   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-zhvfc    1/1     Running   0          41m     10.44.0.3   worker2   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/"webserver-744d6b7964-vzg2t   1/1     Running   0          3h53m   10.44.0.1 "  worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE   SELECTOR        LABELS
service/dbserver     ClusterIP   10.98.65.253     <none>        80/TCP    88m   app=dbserver    app=dbserver
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   5d    <none>          component=apiserver,provider=kubernetes
service/"my-service   ClusterIP   10.96.213.135"    <none>        80/TCP    4s    app=webserver   <none>
service/webserver    ClusterIP   10.111.157.160   <none>        80/TCP    22m   app=webserver   app=webserver

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/dbserver    4/4     4            4           3h41m   nginx        nginx:1.7.1   app=dbserver    app=dbserver
deployment.apps/webserver   1/1     1            1           3h53m   nginx        nginx:1.7.1   app=webserver   app=webserver

NAME                                   DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/dbserver-5b844594d8    4         4         4       3h41m   nginx        nginx:1.7.1   app=dbserver,pod-template-hash=5b844594d8    app=dbserver,pod-template-hash=5b844594d8
replicaset.apps/webserver-744d6b7964   1         1         1       3h53m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964"

hai@master:~$ kubectl describe service my-service
Name:              my-service
Namespace:         default
Labels:            <none>
Annotations:       <none>
Selector:          app=webserver
Type:             " ClusterIP"
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:               " 10.96.213.135"
IPs:               10.96.213.135
Port:              <unset>  80/TCP
TargetPort:        9376/TCP
Endpoints:         "10.44.0.1:9376"
Session Affinity:  None
Events:            <none>
#####################################################################################"

hai@master:~$ kubectl get all  --show-labels  -o wide
NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE     SELECTOR        LABELS
service/"kubernetes"   ClusterIP   10.96.0.1        <none>        443/TCP   5d      <none>          "component=apiserver,provider=kubernetes""


hai@master:~$ kubectl get namespaces  --show-labels  -o  wide
NAME              STATUS   AGE    LABELS
default           Active   5d     kubernetes.io/metadata.name=default
kube-node-lease   Active   5d     kubernetes.io/metadata.name=kube-node-lease
kube-public       Active   5d     kubernetes.io/metadata.name=kube-public
"kube-system       Active   5d     kubernetes.io/metadata.name=kube-system"
project1          Active   4d9h   kubernetes.io/metadata.name=project1
project2          Active   4d9h   kubernetes.io/metadata.name=project2
----------------"
hai@master:~$  kubectl get pod -n kube-system  --show-labels   -o wide
NAME                             READY   STATUS    RESTARTS          AGE     IP               NODE      NOMINATED NODE   READINESS GATES   LABELS
coredns-565d847f94-q5zw9         1/1     Running   465 (5h10m ago)   4d7h    10.32.0.3        master    <none>           <none>            k8s-app=kube-dns,pod-template-hash=565d847f94
coredns-565d847f94-rg789         1/1     Running   454 (5h10m ago)   4d7h    10.32.0.2        master    <none>           <none>            k8s-app=kube-dns,pod-template-hash=565d847f94
etcd-master                      1/1     Running   10 (5h10m ago)    5d      192.168.68.150   master    <none>           <none>            component=etcd,tier=control-plane
kube-apiserver-master            1/1     Running   6 (5h10m ago)     4d6h    192.168.68.150   master    <none>           <none>            component=kube-apiserver,tier=control-plane
kube-controller-manager-master   1/1     Running   16 (5h10m ago)    5d      192.168.68.150   master    <none>           <none>            component=kube-controller-manager,tier=control-plane
kube-proxy-6t6qp                 1/1     Running   10 (5h10m ago)    5d      192.168.68.150   master    <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-proxy-ffdgc                 1/1     Running   8 (5h11m ago)     4d11h   192.168.68.151   worker2   <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-proxy-gq4v9                 1/1     Running   7 (5h11m ago)     4d10h   192.168.68.145   worker1   <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-scheduler-master            1/1     Running   15 (5h10m ago)    5d      192.168.68.150   master    <none>           <none>            component=kube-scheduler,tier=control-plane
weave-net-6bpzq                  2/2     Running   23 (5h10m ago)    5d      192.168.68.150   master    <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
weave-net-8gkx4                  2/2     Running   17 (5h11m ago)    4d11h   192.168.68.151   worker2   <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
weave-net-f27sl                  2/2     Running   15 (5h11m ago)    4d10h   192.168.68.145   worker1   <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1"
===================================
hai@master:~$ kubectl get all  --show-labels  -o wide
NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE     SELECTOR        LABELS
service/"kubernetes"   ClusterIP   "10.96.0.1 "       <none>        443/TCP   5d      <none>          "component=apiserver,provider=kubernetes""


hai@master:~$ kubectl describe service kubernetes
Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              "ClusterIP"
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                "10.96.0.1"
IPs:               10.96.0.1
Port:              https  "443/"TCP
TargetPort:       " 6443/TC"P
Endpoints:        " 192.168.68.150:6443"
Session Affinity:  None
Events:            <none> 

######################################################################################################################"
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: webapp
spec:
  replicas: 1
  strategy:
     type: Recreate
  selector: 
     matchLabels:
        app: webserver
  template:
    metadata:
      labels: 
         app: webserver
    spec: 
      containers:
        - name: nginx
          image: nginx:1.7.1          
          ports:
            - containerPort: 80
========="               
hai@master:~$ kubectl get all --show-labels  -o  wide
NAME                          READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/webapp-744d6b7964-7v5jj   1/1     Running   0          5m36s   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964
pod/webapp-744d6b7964-gmdmg   1/1     Running   0          3s      10.36.0.1   worker1   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   40m   <none>     component=apiserver,provider=kubernetes

NAME                     READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/webapp   2/2     2            2           5m36s   nginx        nginx:1.7.1   app=webserver   <none>

NAME                                DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/webapp-744d6b7964   2         2         2       5m36s   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964"
==============///////////////\\\\\\\\\\\\\\\////////
hai@master:~$ kubectl expose deployment  webserver   --type=ClusterIP  --port  80 --target-port 8089
service/webserver exposed
==============
hai@master:~$ kubectl get svc  --show-labels  -o wide
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE    SELECTOR        LABELS
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   63m    <none>          component=apiserver,provider=kubernetes
"webserver "   "ClusterIP "  10.105.28.230   <none>        80/TCP    113s   app=webserver   <none>"

hai@master:~$ kubectl expose deployment webserver   "--type=NodePort"  --port  80  --target-port 8087
Error from server "(AlreadyExists)": services "webserver" already exists"

hai@master:~$ kubectl delete service webserver
service "webserver" deleted"

hai@master:~$ kubectl expose deployment  webserver --type=NodePort  --port  80  --target-port 8086
service/webserver exposed"

hai@master:~$ kubectl get svc  --show-labels  -o wide
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE   SELECTOR        LABELS
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        73m   <none>          component=apiserver,provider=kubernetes
"webserver "   "NodePort"    10.109.110.80   <none>        80:30117/TCP   93s   app=webserver   <none>"

hai@master:~$ kubectl get all --show-labels  -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/webserver-744d6b7964-pqnws   1/1     Running   0          28m   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964
pod/webserver-744d6b7964-x2kpw   1/1     Running   0          28m   10.36.0.1   worker1   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE       " CLUSTER-IP   "   EXTERNAL-IP   PORT(S)        AGE   SELECTOR        LABELS
service/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        83m   <none>          component=apiserver,provider=kubernetes
service/webserver    "NodePort" 10.109.110.80   <none>        "80:30117/TCP "  11m   app=webserver   <none>

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/webserver   2/2     2            2           28m   nginx        nginx:1.7.1   app=webserver   <none>

NAME                                   DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/webserver-744d6b7964   2         2         2       28m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964"

hai@master:~$ kubectl describe service webserver
Name:                     webserver
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=webserver
Type:                    " NodePort"
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                      " 10.109.110.80"
IPs:                      10.109.110.80
Port:                     <unset>  80/TCP
TargetPort:               8086/TCP
NodePort:                 <unset>  /"30117TCP"
Endpoints:                10.36.0.1:"8086",10.44.0.1:8086
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>:"

*Nodeport range: 30,000 TO 32767

apiVersion: v1
kind: Service
metadata:
  name: webserver 
  labels:
    name: webserver 
spec:
  type: NodePort
  ports:
    - port: 80
      nodePort: 30080
      name: http
    - port: 443
      nodePort: 30443
      name: https
  selector:
    name: webserver 
"
hai@master:~$ kubectl get all --show-labels  -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/webserver-744d6b7964-pqnws   1/1     Running   0          61m   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964
pod/webserver-744d6b7964-x2kpw   1/1     Running   0          61m   10.36.0.1   worker1   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE    SELECTOR         LABELS
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP                      116m   <none>           component=apiserver,provider=kubernetes
service/webserver    "NodePort "   10.109.169.104   <none>        "80:30080"/TCP,443:30443/TCP   15s    name=webserver   name=webserver

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/webserver   2/2     2            2           61m   nginx        nginx:1.7.1   app=webserver   <none>

NAME                                   DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/webserver-744d6b7964   2         2         2       61m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964
====================================

hai@master:~$ kubectl port-forward    webserver-744d6b7964-pqnws    8089:80  --address 0.0.0.0
Forwarding from 0.0.0.0:8089 -> 80
Handling connection for 8089
^Z
[2]+  Stopped                 kubectl port-forward webserver-744d6b7964-pqnws 8089:80 --address 0.0.0.0 "
============================
hai@master:~/test$ kubectl expose deployment webserver  --type=LoadBalancer
Error from server (AlreadyExists): services "webserver" already exists"

hai@master:~/test$ kubectl delete services webserver
service "webserver" deleted"

hai@master:~/test$ kubectl expose deployment webserver  --type=LoadBalancer
service/webserver exposed

hai@master:~$ kubectl get all --show-labels  -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/webserver-744d6b7964-pqnws   1/1     Running   0          99m   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964
pod/webserver-744d6b7964-x2kpw   1/1     Running   0          99m   10.36.0.1   worker1   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE    SELECTOR        LABELS
service/kubernetes   ClusterIP      10.96.0.1       <none>        443/TCP        155m   <none>          component=apiserver,provider=kubernetes
service/webserver   " LoadBalancer"   10.105.244.86  " <pending> "    "80:30677/TCP"   21s    app=webserver   <none>

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/webserver   2/2     2            2           99m   nginx        nginx:1.7.1   app=webserver   <none>

NAME                                   DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/webserver-744d6b7964   2         2         2       99m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=
########################################################################################################"
          Init Containers    
https://kubernetes.io/docs/concepts/workloads/pods/init-containers/

A Pod can have multiple containers running apps within it, but it can also have one or more init containers, which are run before the app containers are started.
Init containers are exactly like regular containers, except:
Init containers always run to completion.
Each init container must complete successfully before the next one starts.

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app.kubernetes.io/name: MyApp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"]
----------------------"
hai@master:~$ kubectl get all --show-labels  -o wide
NAME            READY   STATUS     RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/myapp-pod   0/1     Init:0/2   0          10s   10.44.0.1   worker2   <none>           <none>            app.kubernetes.io/name=MyApp

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   62s   <none>     component=apiserver,provider=kubernetes"

hai@master:~$ kubectl get pod --show-labels  -o wide
NAME        READY   STATUS     RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
myapp-pod   0/1     Init:0/2   0          60s   10.44.0.1   worker2   <none>           <none>            app.kubernetes.io/name=MyApp"
======----------
hai@master:~/test$ kubectl  describe  pod myapp-pod
Name:             myapp-pod
Namespace:        "default"
Priority:         0
Labels:           app.kubernetes.io/name=MyApp
Status:           "Pending"
Init Containers:
  "init-myservice:"
    Container ID:  containerd://a6eee39408a02e575e0aa71bd0f0c625cb23f579e9fc3ec5d148b134b077012c
    "Image:         busybox:1.28"
    Command:
      sh
      -c
      until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done
    "State:          Running"
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lsrbh (ro)
  "init-mydb:"
    Container ID:
    "Image:         busybox:1.28"
    Image ID:
     Command:
      sh
      -c
      until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done
   " State:          Waiting"
    "Reason:       PodInitializing"
    Ready:          False
     Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lsrbh (ro)
      
"Containers:"
  myapp-conta"iner:"
    Container ID:
   " Image:         busybox:1.28"
    Image ID:
     Command:
      sh
      -c
      echo The app is running! && sleep 3600
   " State:          Waiting"
      "Reason:       PodInitializing"
  "  Ready:          False"
       Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lsrbh (ro)
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  4m47s  default-scheduler  Successfully assigned default/myapp-pod to worker2
  Normal  Pulled     4m46s  kubelet            Container image "busybox:1.28" already present on machine
  Normal  Created    4m46s  kubelet            Created container init-myservice
  Normal  Started    4m46s  kubelet            Started container init-myservice
==================++++++++++++++++++++"
hai@master:~/test$ kubectl logs "myapp-pod" -c "init-myservice"
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'myservice.default.svc.cluster.local'
waiting for myservice
nslookup: can't resolve 'myservice.default.svc.cluster.local'
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

waiting for myservice
nslookup: can't resolve 'myservice.default.svc.cluster.local'
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

waiting for myservice
nslookup: can't resolve 'myservice.default.svc.cluster.local'
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local
========================+++++++++"
hai@master:~/test$ kubectl logs myapp-pod -c init-mydb
Error from server (BadRequest): container "init-mydb" in pod "myapp-pod" is waiting to start: PodInitializing

https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#understanding-init-containers
------------------
hai@master:~$ kubectl get pod --show-labels  -o wide
NAME        READY   STATUS     RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
myapp-pod   0/1     Init:0/2   0          91m   10.44.0.1   worker2   <none>           <none>            app.kubernetes.io/name=MyApp
--------------------"
---
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
---
apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9377

====+++==="
hai@master:~$ kubectl get all --show-labels  -o wide
NAME            READY   STATUS     RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/myapp-pod   0/1     "Init:0/2   0 "         10s   10.44.0.1   worker2   <none>           <none>            app.kubernetes.io/name=MyApp
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   62s   <none>     component=apiserver,provider=kubernetes"

hai@master:~$ kubectl get all --show-labels  -o wide
NAME            READY   STATUS     RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/myapp-pod   0/1    " Init:0/2 "  0          95m   10.44.0.1   worker2   <none>           <none>            app.kubernetes.io/name=MyApp
NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE   SELECTOR   LABELS
service/kubernetes "  ClusterIP "  10.96.0.1       <none>        443/TCP   96m   <none>     component=apiserver,provider=kubernetes
service/mydb         C"lusterIP "  10.98.174.26    <none>        80/TCP    1s    <none>     <none>
service/myservice    "ClusterIP "  10.107.72.183   <none>        80/TCP    1s    <none>     <none>"

hai@master:~$ kubectl get all --show-labels  -o wide
NAME            READY   STATUS            RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/myapp-pod   0/1    " PodInitializing "  0          95m   10.44.0.1   worker2   <none>           <none>            app.kubernetes.io/name=MyApp

NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE   SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   96m   <none>     component=apiserver,provider=kubernetes
service/mydb         ClusterIP   10.98.174.26    <none>        80/TCP    3s    <none>     <none>
service/myservice    ClusterIP   10.107.72.183   <none>        80/TCP    3s    <none>     <none>"

hai@master:~$ kubectl get all --show-labels  -o wide
NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/myapp-pod   1/1     "Running"   0          95m   10.44.0.1   worker2   <none>           <none>            app.kubernetes.io/name=MyApp
NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE   SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   96m   <none>     component=apiserver,provider=kubernetes
service/mydb         ClusterIP   10.98.174.26    <none>        80/TCP    10s   <none>     <none>
service/myservice    ClusterIP   10.107.72.183   <none>        80/TCP    10s   <none>     <none>
===----

hai@master:~$ kubectl describe  pod myapp-pod
Name:             myapp-pod
Namespace:        default

"Status:           Running"
Init Containers:
 " init-myservice:"
    Container ID:  containerd://a6eee39408a02e575e0aa71bd0f0c625cb23f579e9fc3ec5d148b134b077012c
    Image:         busybox:1.28
        Command:
      sh
      -c
      until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done
    State:          "Terminated"
      Reason:       Completed
     Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lsrbh (ro)
 " init-mydb:"
    Container ID:  containerd://f0c35385036543cdb7a060a048e6c13b22151c9d6adcc326481ca30f33ed41aa
    Image:         busybox:1.28
       sh
      -c
      until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done
    State:          "Terminated"
      Reason:       Completed
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lsrbh (ro)
"Containers:"
 " myapp-container:"
    Container ID:  containerd://94f7f17085c5209c3a3026ec053a19167e93855c006131d345c29ceb685a96bf
    Image:         busybox:1.28
    Command:
      sh
      -c
      echo The app is running! && sleep 3600
    State:        "  Running"
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lsrbh (ro)
############################################################################

init cobtainer and MULTICONTAINER POD

https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container

apiVersion: v1
kind: Pod
metadata:
  name: init-demo
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - name: workdir
      mountPath: /usr/share/nginx/html
  # These containers are run during pod initialization
  initContainers:
  - name: install
    image: busybox:1.28
    command:
    - wget
    - "-O"
    - "/work-dir/index.html"
    - http://info.cern.ch
    volumeMounts:
    - name: workdir
      mountPath: "/work-dir"
  dnsPolicy: Default
  volumes:
  - name: workdir
    emptyDir: {}
------========"
hai@master:~$ kubectl get all --show-labels  -o wide
NAME            READY   STATUS     RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/init-demo   0/1     "Init:0/1 "  0          2s    <none>   worker2   <none>           <none>            <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3h39m   <none>     component=apiserver,provider=kubernetes"

hai@master:~$ kubectl get all --show-labels  -o wide
NAME            READY   STATUS            RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/init-demo   0/1     "PodInitializing "  0          4s    10.44.0.1   worker2   <none>           <none>            <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3h39m   <none>     component=apiserver,provider=kubernetes"

hai@master:~$ kubectl get all --show-labels  -o wide
NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/init-demo   1/1     "Running  " 0          7s    10.44.0.1   worker2   <none>           <none>            <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3h39m   <none>     component=apiserver,provider=kubernetes
===========================++++++++++++++++++++"
hai@master:~$ kubectl describe pod init-demo
Name:             init-demo
Namespace:        default
Status:           Running
"Init Containers:"
  install:
    Image:         busybox:1.28
    Command:
      wget
      -O
      /work-dir/index.html
      http://info.cern.ch
    State:          Terminated
      Reason:       Completed
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-99pcd (ro)
      /work-dir from workdir (rw)
Containers:
  nginx:
    Image:          nginx
    State:          Running
    Mounts:
      /usr/share/nginx/html from workdir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-99pcd (ro)
Volumes:
  workdir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)

#######################################################################################

apiVersion: v1
kind: Pod
metadata: 
   name: onein2
spec:
  containers: 
     - name: cont1
       image: nginx

     -  name:  cont2
        image: nginx

hai@master:~$ kubectl get po -o wide
NAME     READY   STATUS             RESTARTS       AGE     IP          NODE      NOMINATED NODE   READINESS GATES
onein2   1/2     CrashLoopBackOff   6 (2m6s ago)   8m32s   10.44.0.1   worker2   <none>           <none>

hai@master:~/test$ kubectl describe pod onein2
Name:             onein2
Namespace:        default
Annotations:      <none>
Status:           Running

Containers:
  cont1:
    Image:          nginx
    State:          Running
    Ready:          True
    Restart Count:  0
  
  cont2:
       Image:          nginx
    State:          Terminated
      Reason:       Error
      Exit Code:    1
    Last State:     Terminated
     Reason:       Error
    Ready:          False
    Restart Count:  3
 ==========----------  "small  modifications"" 

apiVersion: v1
kind: Pod
metadata: 
   name: onein2
spec:
  containers: 
     - name: cont1
       image: nginx
       args: ["sleep","3600"]      "## see look   "
     - name:  cont2
       image: nginx

hai@master:~/test$ kubectl get po  -o  wide
NAME     READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
onein2   "2/2     Running  " 0          40s   10.44.0.1   worker2   <none>           <none>"
=====
hai@master:~/test$ kubectl describe pod onein2
Name:             onein2
Status:           Running
Containers:
 " cont1:"
    Image:         nginx
    Args:
      sleep                      "## look see " 
      3600                     "" ## look see 
    State:          Running    
  "cont2:"
    Image:          nginx
    State:          Running
    Ready:          True
    Restart Count:  0
======---------"
hai@master:~$ kubectl  exec  onein2 -it   --  /bin/bash
Defaulted container "cont1" out of: cont1, cont2
root@onein2:/# ls
bin   dev                  docker-entrypoint.sh  home  lib64  mnt  proc  run   srv  tmp  var
boot  docker-entrypoint.d  etc                   lib   media  opt  root  sbin  sys  usr

"apt update -y 
root@onein2:/# apt install net-tools
root@onein2:/# apt install netcat
root@onein2:/# netcat -l -p 3306  
root@onein2:/# apt  install telnet


root@onein2:/# netstat -tupln
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      -
tcp6       0      0 :::80                   :::*                    LISTEN      -"


root@onein2:/# netcat -l -p 3306          # "second container "
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      "0 0.0.0.0:3306  "          0.0.0.0:*               LISTEN      -
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      -
tcp6       0      0 :::80                   :::*                    LISTEN      -

root@onein2:/# telnet localhost 3306
Trying ::1...
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
------"


root@onein2:/# telnet localhost 3306
Trying ::1...
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
hello  i am from 1st  pod
 ok
how  are you
 iam  good  u
i am also  good"
--
root@onein2:/# netcat -l -p 3306
hello  i am from 1st  pod
 ok
how  are you
 iam  good  u
i am also  good

https://www.mirantis.com/blog/multi-container-pods-and-container-communication-in-kubernetes/      "

apiVersion: v1
kind: Pod
metadata:
  name: mc1
spec:
  volumes:
  - name: html
    emptyDir: {}
  containers:
  - name: 1st
    image: nginx
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
  - name: 2nd
    image: debian
    volumeMounts:
    - name: html
      mountPath: /html
    command: ["/bin/sh", "-c"]
    args:
      - while true; do
          date >> /html/index.html;
          sleep 1;
        done
"----
hai@master:~$ kubectl get pods -o wide --show-labels
NAME   READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES   LABELS
mc1    "0/2     ContainerCreating  " 0          28s   <none>   worker2   <none>           <none>            <none>

hai@master:~$ kubectl get pods -o wide --show-labels
NAME   READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
mc1   " 2/2     Running  " 0          37s   10.44.0.1   worker2   <none>           <none>            <none>
==================+++++++++++++++++++++++++++++++++++++++++++"
hai@master:~$ kubectl describe pod mc1
Name:             mc1
Status:           Running
Containers:
 " 1st:"
     Image:          nginx
    State:          Running
    Mounts:
      /usr/share/nginx/html from html (rw)      ##  check data  see  look 
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fqw7f (ro)
 " 2nd:"
    Image:         debian
    Command:
      /bin/sh
      -c
    Args:
      while true; do date >> /html/index.html; sleep 1; done    " ## see look path data"
     Mounts:
      /html from html (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fqw7f (ro)
Volumes:
  html:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
---"
hai@master:~$ kubectl exec    mc1 -c 2nd -it -- /bin/bash        "## see look  2nd container "
root@mc1:/# ls
bin  boot  dev  etc  home  html  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
root@mc1:/# ls /html
index.html  "test"

root@mc1:/# cd html/
root@mc1:/html# cat index.html
Sun Dec 25 07:47:09 UTC 2022
Sun Dec 25 07:47:10 UTC 2022
Sun Dec 25 07:47:11 UTC 2022
Sun Dec 25 07:47:12 UTC 2022
Sun Dec 25 07:47:13 UTC 2022
Sun Dec 25 07:47:14 UTC 2022
Sun Dec 25 07:47:15 UTC 2022"


hai@master:~$ kubectl exec  mc1 -it -- /bin/bash              " ### see look 1st container"
Defaulted container "1st" out of: 1st, 2nd
root@mc1:/# ls
bin  boot  dev  docker-entrypoint.d  docker-entrypoint.sh  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var

root@mc1:/# cd /usr/share/
root@mc1:/usr/share# ls
"X11         base-passwd      ca-certificates  debianutils  doc-base    fonts  info      libc-bin  man         misc   pam-confi
adduser     bash-completion  common-licenses  dict         dpkg        gcc    java      lintian   maven-repo  nginx  perl5
base-files  bug              debconf          doc          fontconfig  gdb    keyrings  locale    menu        pam    pixmaps"
root@mc1:/usr/share# cd nginx/html/
root@mc1:/usr/share/nginx/html# ls
index.html  "test"
root@mc1:/usr/share/nginx/html# cat index.html
Sun Dec 25 07:47:09 UTC 2022
Sun Dec 25 07:47:10 UTC 2022
Sun Dec 25 07:47:11 UTC 2022
Sun Dec 25 07:47:12 UTC 2022
Sun Dec 25 07:47:13 UTC 2022
Sun Dec 25 07:47:14 UTC 2022
Sun Dec 25 07:47:15 UTC 2022
Sun Dec 25 07:47:16 UTC 2022

root@mc1:/usr/share/nginx/html# tail -f index.html
Sun Dec 25 09:00:40 UTC 2022
Sun Dec 25 09:00:41 UTC 2022
Sun Dec 25 09:00:42 UTC 2022
Sun Dec 25 09:00:43 UTC 2022
############################################################################################

https://www.youtube.com/watch?v=OMzVbeTKN-o

Kubernetes-Day-15
Explanation on DaemonSet, Affinity & AntiAffinity, Scheduling Topologies,Static pods"

apiVersion: apps/v1
kind: DaemonSet
metadata: 
  name: dmset
  labels:
    app: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  template: 
    metadata:
       labels: 
          app: nginx
    spec: 
      containers:
         - name: nginx
           image: nginx:1.7.1
           ports:
             - containerPort: 80   "

hai@master:~$ kubectl get pods --show-labels  -o wide
NAME          READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
dmset-5gp84   1/1     Running   0          21s   10.36.0.1   "worker1  " <none>           <none>            app=nginx,controller-revision-hash=788b5bb875,pod-template-generation=1
dmset-pb8pw   1/1     Running   0          21s   10.44.0.1   "worker2"   <none>           <none>            app=nginx,controller-revision-hash=788b5bb875,pod-template-generation=1"

hai@master:~$ kubectl get all -o wide --show-labels
NAME              READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/dmset-5gp84   1/1     Running   0          2m4s   10.36.0.1   worker1   <none>           <none>            app=nginx,controller-revision-hash=788b5bb875,pod-template-generation=1
pod/dmset-pb8pw   1/1     Running   0          2m4s   10.44.0.1   worker2   <none>           <none>            app=nginx,controller-revision-hash=788b5bb875,pod-template-generation=1

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   26h   <none>     component=apiserver,provider=kubernetes

NAME                   DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
daemonset.apps/"dmset   2         2         2       2            2  "         <none>          2m4s   nginx        nginx:1.7.1   app=nginx   app=nginx "

hai@master:~$ kubectl get pods -n  kube-system -o wide --show-labels
NAME                              READY   STATUS    RESTARTS          AGE     IP               NODE      NOMINATED NODE   READINESS GATES   LABELS
coredns-565d847f94-q5zw9         1/1     Running   467 (6h44m ago)   6d1h    10.32.0.3        master    <none>           <none>            k8s-app=kube-dns,pod-template-hash=565d847f94
coredns-565d847f94-rg789         1/1     Running   456 (6h44m ago)   6d1h    10.32.0.2        master    <none>           <none>            k8s-app=kube-dns,pod-template-hash=565d847f94
etcd-master                      1/1     Running   12 (6h44m ago)    6d18h   192.168.68.150   master    <none>           <none>            component=etcd,tier=control-plane
kube-apiserver-master            1/1     Running   8 (6h44m ago)     6d      192.168.68.150   master    <none>           <none>            component=kube-apiserver,tier=control-plane
kube-controller-manager-master   1/1     Running   18 (6h44m ago)    6d18h   192.168.68.150   master    <none>           <none>            component=kube-controller-manager,tier=control-plane
kube-proxy-6t6qp                 1/1     Running   12 (6h44m ago)    6d18h   192.168.68.150   master    <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-proxy-ffdgc                 1/1     Running   10 (6h44m ago)    6d5h    192.168.68.151   worker2   <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-proxy-gq4v9                 1/1     Running   9 (6h44m ago)     6d4h    192.168.68.145   worker1   <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-scheduler-master            1/1     Running   17 (6h44m ago)    6d18h   192.168.68.150   master    <none>           <none>            component=kube-scheduler,tier=control-plane
weave-net-6bpzq                  2/2     Running   28 (6h44m ago)    6d18h   192.168.68.150   master    <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
weave-net-8gkx4                  2/2     Running   21 (6h44m ago)    6d5h    192.168.68.151   worker2   <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
weave-net-f27sl                  2/2     Running   19 (6h44m ago)    6d4h    192.168.68.145   worker1   <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1 "

hai@master:~$ hai@master:~$ kubectl get all -n  kube-system -o wide --show-labels
hai@master:~$: command not found
hai@master:~$  kubectl get all -n  kube-system -o wide --show-labels
NAME                                 READY   STATUS    RESTARTS         AGE     IP               NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/coredns-565d847f94-q5zw9         1/1     Running   467 (7h5m ago)   6d1h    10.32.0.3        master    <none>           <none>            k8s-app=kube-dns,pod-template-hash=565d847f94
pod/coredns-565d847f94-rg789         1/1     Running   456 (7h5m ago)   6d1h    10.32.0.2        master    <none>           <none>            k8s-app=kube-dns,pod-template-hash=565d847f94
pod/etcd-master                      1/1     Running   12 (7h5m ago)    6d19h   192.168.68.150   master    <none>           <none>            component=etcd,tier=control-plane
pod/kube-apiserver-master            1/1     Running   8 (7h5m ago)     6d1h    192.168.68.150   master    <none>           <none>            component=kube-apiserver,tier=control-plane
pod/kube-controller-manager-master   1/1     Running   18 (7h5m ago)    6d19h   192.168.68.150   master    <none>           <none>            component=kube-controller-manager,tier=control-plane
pod/kube-proxy-6t6qp                 1/1     Running   12 (7h5m ago)    6d18h   192.168.68.150   master    <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
pod/kube-proxy-ffdgc                 1/1     Running   10 (7h5m ago)    6d5h    192.168.68.151   worker2   <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
pod/kube-proxy-gq4v9                 1/1     Running   9 (7h5m ago)     6d4h    192.168.68.145   worker1   <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
pod/kube-scheduler-master            1/1     Running   17 (7h5m ago)    6d19h   192.168.68.150   master    <none>           <none>            component=kube-scheduler,tier=control-plane
pod/weave-net-6bpzq                  2/2     Running   28 (7h5m ago)    6d18h   192.168.68.150   master    <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
pod/weave-net-8gkx4                  2/2     Running   21 (7h5m ago)    6d5h    192.168.68.151   worker2   <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
pod/weave-net-f27sl                  2/2     Running   19 (7h5m ago)    6d4h    192.168.68.145   worker1   <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1

NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE     SELECTOR           LABELS
service/kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   6d19h   k8s-app=kube-dns   k8s-app=kube-dns,kubernetes.io/cluster-service=true,kubernetes.io/name=CoreDNS

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE     CONTAINERS        IMAGES                                                     SELECTOR             LABELS
"daemonset.apps/kube-proxy   3         3         3       3            3  "         kubernetes.io/os=linux   6d19h   kube-proxy        registry.k8s.io/kube-proxy:v1.25.5                         k8s-app=kube-proxy   k8s-app=kube-proxy
"daemonset.apps/weave-net    3         3         3       3            3  "         <none>                   6d18h   weave,weave-npc   weaveworks/weave-kube:latest,weaveworks/weave-npc:latest   name=weave-net       name=weave-net

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES                                   SELECTOR           LABELS
deployment.apps/coredns   2/2     2            2           6d19h   coredns      registry.k8s.io/coredns/coredns:v1.9.3   k8s-app=kube-dns   k8s-app=kube-dns

NAME                                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES                                   SELECTOR                                        LABELS
replicaset.apps/coredns-565d847f94   2         2         2       6d18h   coredns      registry.k8s.io/coredns/coredns:v1.9.3   k8s-app=kube-dns,pod-template-hash=565d847f94   k8s-app=kube-dns,pod-template-hash=565d847f94"

https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/
DaemonSet
A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. 
As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.
Some typical uses of a DaemonSet are:

running a cluster "storage" daemon on every node
running a "logs" collection daemon on every node
running a "node monitoring" daemon on every node

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      # these tolerations are to have the daemonset runnable on control plane nodes
      # remove them if your control plane nodes should not run pods
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log          "
##################################################################################################
scheduling Topologies 

"1. nodeName:
2. nodeSelector:
3. cordon and uncordon "

apiVersion: apps/v1
kind: Deployment
metadata:
  name: topology
  labels: 
     app: stopology
spec: 
  replicas: 4
  selector: 
     matchLabels: 
        app: stopology
  template:
    metadata:
       labels: 
         app: stopology
    spec: 
      nodeName: worker1           "# see look "
      containers:
         - name: nginx
           image: nginx
           ports:
             - containerPort: 80   "
hai@master:~$ kubectl get nodes -o wide --show-labels
NAME      STATUS   ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME     LABELS
master    Ready    control-plane   6d19h   v1.25.4   192.168.68.150   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
worker1   Ready    <none>          6d5h    v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker1,kubernetes.io/os=linux
"worker2"   Ready    <none>          6d6h    v1.25.4   192.168.68.151   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker2,kubernetes.io/os=linux

hai@master:~$ kubectl get pods --show-labels -o  wide
NAME                        READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
topology-85649bdbc9-966rq   1/1     Running   0          18s   10.36.0.2   "worker1 "  <none>           <none>            app=stopology,pod-template-hash=85649bdbc9
topology-85649bdbc9-9jd68   1/1     Running   0          19s   10.36.0.4   '"worker1" '  <none>           <none>            app=stopology,pod-template-hash=85649bdbc9
topology-85649bdbc9-bhpj8   1/1     Running   0          19s   10.36.0.3  " worker1 "  <none>           <none>            app=stopology,pod-template-hash=85649bdbc9
topology-85649bdbc9-f8krb   1/1     Running   0          19s   10.36.0.1   "worker1  " <none>           <none>            app=stopology,pod-template-hash=85649bdbc9
+++++++++++++++++++++++++++++++++++++++++++++++++
hai@master:~/test$ kubectl label node worker1  cpu=intel
node/worker1 labeled "

hai@master:~/test$ kubectl label node worker2 "cpu=amd"
node/worker2 labeled

apiVersion: apps/v1
kind: Deployment
metadata: 
  name: ndselector
  labels:
    app: ndselector
spec:
   replicas: 4
   selector:
     matchLabels:
       app: ndselector
   template:    
     metadata: 
       labels:
         app: ndselector
     spec:
       nodeSelector:        " # see look  "
          cpu: amd            # see look 
       containers:      
        - name: nginx
          image: nginx
          ports: 
           - containerPort: 80             
++++++++++++++++++++"
hai@master:~$ kubectl get pods --show-labels -o  wide
NAME                          READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
ndselector-5fc7d6d75d-bdcd9   1/1     Running   0          83s   10.44.0.3   "worker2"   <none>           <none>            app=ndselector,pod-template-hash=5fc7d6d75d
ndselector-5fc7d6d75d-gwplp   1/1     Running   0          83s   10.44.0.4   "worker2 "  <none>           <none>            app=ndselector,pod-template-hash=5fc7d6d75d
ndselector-5fc7d6d75d-r4hdm   1/1     Running   0          83s   10.44.0.1   "worker2"   <none>           <none>            app=ndselector,pod-template-hash=5fc7d6d75d
ndselector-5fc7d6d75d-tpzpb   1/1     Running   0          83s   10.44.0.2   "worker2 "  <none>           <none>            app=ndselector,pod-template-hash=5fc7d6d75d
++++++++++++++======================###########"

3. cordon and uncordon

hai@master:~$ kubectl get nodes -o wide --show-labels
NAME      STATUS   ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME     LABELS
master    Ready    control-plane   6d21h   v1.25.4   192.168.68.150   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hos
worker1   Ready    <none>          6d6h    v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cpu=intel,kubernetes.io/arch=amd64,kuberne
worker2   Ready    <none>          6d8h    v1.25.4   192.168.68.151   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cpu=amd,kubernetes.io/arch=amd64,kubernete"

hai@master:~$ kubectl cordon worker1
node/worker1 cordoned   "

hai@master:~$ kubectl get nodes -o wide
NAME      STATUS                     ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
master    Ready                      control-plane   6d21h   v1.25.4   192.168.68.150   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
"worker1   Ready,SchedulingDisabled"   <none>          6d6h    v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
worker2   Ready                      <none>          6d8h    v1.25.4   192.168.68.151   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14 "

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ndselector
  labels:
    app: ndselector
spec:
   replicas: 4
   selector:
     matchLabels:
       app: ndselector
   template:
     metadata:
       labels:
         app: ndselector
     spec:
       containers:
        - name: nginx
          image: nginx
          ports:
           - containerPort: 80
                                  "
hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
ndselector-74ff8bbfb5-58czk   1/1     Running   0          24s   10.44.0.4   "worker2  " <none>           <none>
ndselector-74ff8bbfb5-8rfkl   1/1     Running   0          24s   10.44.0.2   "worker2 "  <none>           <none>
ndselector-74ff8bbfb5-bzdn5   1/1     Running   0          24s   10.44.0.3   "worker2 "  <none>           <none>
ndselector-74ff8bbfb5-hm9t7   1/1     Running   0          24s   10.44.0.1   "worker2 "  <none>           <none>

========================+++++++++++++"

hai@master:~$ kubectl get nodes -o wide
NAME      STATUS                     ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
master    Ready                      control-plane   6d21h   v1.25.4   192.168.68.150   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
"worker1   Ready,SchedulingDisabled "  <none>          6d7h    v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
worker2   Ready                      <none>          6d8h    v1.25.4   192.168.68.151   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14

apiVersion: apps/v1
kind: Deployment
metadata:
  name: topology
  labels: 
     app: stopology
spec: 
  replicas: 4
  selector: 
     matchLabels: 
        app: stopology
  template:
    metadata:
       labels: 
         app: stopology
    spec: 
      nodeName: worker1           " ### importent ok best  "
      containers:                  "## see look "
         - name: nginx
           image: nginx
           ports:
             - containerPort: 80 
======"
hai@master:~$ kubectl get pods -o wide
No resources found in default namespace.

hai@master:~$ kubectl get pods -o wide
NAME                        READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES
topology-85649bdbc9-krvqb   0/1     ContainerCreating   0          3s    <none>   worker1   <none>           <none>
topology-85649bdbc9-njv97   0/1     ContainerCreating   0          3s    <none>   worker1   <none>           <none>
topology-85649bdbc9-scbnx   0/1     ContainerCreating   0          3s    <none>   worker1   <none>           <none>
topology-85649bdbc9-vf6w9   0/1     ContainerCreating   0          3s    <none>   worker1   <none>           <none>

hai@master:~$ kubectl get pods -o wide
NAME                        READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
topology-85649bdbc9-krvqb   1/1     Running   0          88s   10.36.0.4   worker1   <none>           <none>
topology-85649bdbc9-njv97   1/1     Running   0          88s   10.36.0.2   worker1   <none>           <none>
topology-85649bdbc9-scbnx   1/1     Running   0          88s   10.36.0.3   worker1   <none>           <none>
topology-85649bdbc9-vf6w9   1/1     Running   0          88s   10.36.0.1   worker1   <none>           <none>

====================================##############################"

hai@master:~$ kubectl get nodes -o wide
NAME      STATUS                     ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
master    Ready                      control-plane   6d21h   v1.25.4   192.168.68.150   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
"worker1   Ready,SchedulingDisabled "  <none>          6d7h    v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
worker2   Ready                      <none>          6d8h    v1.25.4   192.168.68.151   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14 "

apiVersion: apps/v1
kind: Deployment
metadata: 
  name: ndselector
  labels:
    app: ndselector
spec:
   replicas: 4
   selector:
     matchLabels:
       app: ndselector
   template:    
     metadata: 
       labels:
         app: ndselector
     spec:
       nodeSelector:      ## see look 
          cpu: intel      ## importent  see look  ok best 
       containers:      
        - name: nginx
          image: nginx
          ports: 
           - containerPort: 80           "    

No resources found in default namespace.
hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
ndselector-6cffc794bd-44ffx   0/1     "Pending  " 0          1s    <none>   <none>   <none>           <none>
ndselector-6cffc794bd-46ggk   0/1     "Pending "  0          1s    <none>   <none>   <none>           <none>
ndselector-6cffc794bd-ngn2z   0/1   "  Pending  " 0          1s    <none>   <none>   <none>           <none>
ndselector-6cffc794bd-xfdsd   0/1     "Pending"   0          1s    <none>   <none>   <none>           <none>"

hai@master:~$ kubectl get nodes -o wide
NAME      STATUS                     ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
master    Ready                      control-plane   6d21h   v1.25.4   192.168.68.150   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
"worker1   Ready,SchedulingDisabled"   <none>          6d7h    v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
worker2   Ready                      <none>          6d8h    v1.25.4   192.168.68.151   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14 "

hai@master:~/test$ kubectl uncordon worker1
node/worker1 uncordoned

hai@master:~$ kubectl get nodes -o wide
NAME      STATUS   ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
master    Ready    control-plane   6d21h   v1.25.4   192.168.68.150   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
"worker1   Ready "   <none>          6d7h    v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
worker2   Ready    <none>          6d8h    v1.25.4   192.168.68.151   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14

hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
ndselector-6cffc794bd-44ffx   0/1     Pending   0          10m   <none>   <none>   <none>           <none>
ndselector-6cffc794bd-46ggk   0/1     Pending   0          10m   <none>   <none>   <none>           <none>
ndselector-6cffc794bd-ngn2z   0/1     Pending   0          10m   <none>   <none>   <none>           <none>
ndselector-6cffc794bd-xfdsd   0/1     Pending   0          10m   <none>   <none>   <none>           <none>"
hai@master:~$
hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES
ndselector-6cffc794bd-44ffx   0/1     "ContainerCreating "  0          11m   <none>   worker1   <none>           <none>
ndselector-6cffc794bd-46ggk   0/1     ContainerCreating   0          11m   <none>   worker1   <none>           <none>
ndselector-6cffc794bd-ngn2z   0/1     ContainerCreating   0          11m   <none>   worker1   <none>           <none>
ndselector-6cffc794bd-xfdsd   0/1    " ContainerCreating "  0          11m   <none>   worker1   <none>           <none>"

hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
ndselector-6cffc794bd-44ffx   0/1     ContainerCreating   0          11m   <none>      worker1   <none>           <none>
ndselector-6cffc794bd-46ggk   0/1     ContainerCreating   0          11m   <none>      worker1   <none>           <none>
ndselector-6cffc794bd-ngn2z   1/1     "Running "            0          11m   10.36.0.1   worker1   <none>           <none>
ndselector-6cffc794bd-xfdsd   0/1     ContainerCreating   0          11m   <none>      worker1   <none>           <none>"

hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
ndselector-6cffc794bd-44ffx   1/1     Running   0          11m   10.36.0.4   worker1   <none>           <none>
ndselector-6cffc794bd-46ggk   1/1     Running   0          11m   10.36.0.3   worker1   <none>           <none>
ndselector-6cffc794bd-ngn2z   1/1     Running   0          11m   10.36.0.1   worker1   <none>           <none>
ndselector-6cffc794bd-xfdsd   1/1     Running   0          11m   10.36.0.2   worker1   <none>           <none>

############################################################ "

affinity and anti affinity kubernetes 

https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
https://www.densify.com/kubernetes-autoscaling/kubernetes-affinity
https://blog.kubecost.com/blog/kubernetes-node-affinity/
https://www.howtogeek.com/devops/what-is-pod-affinity-and-anti-affinity-in-kubernetes/


apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-cache
spec:
  selector:
    matchLabels:
      app: store
  replicas: 3
  template:
    metadata:
      labels:
        app: store
    spec:
      affinity:                      ##  importent  see look  
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: redis-server
        image: redis:3.2-alpine
--"
hai@master:~$ kubectl get nodes -o wide
NAME      STATUS   ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
master    Ready    control-plane   6d23h   v1.25.4   192.168.68.150   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
"worker1  " Ready    <none>          6d9h    v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
"worker2 "  Ready    <none>          6d10h   v1.25.4   192.168.68.151   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14"
---
hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES
redis-cache-7c5dd87dd-8kdk5   0/1     Pending             0          0s    <none>   <none>    <none>           <none>
redis-cache-7c5dd87dd-kmbdn   0/1     "ContainerCreating   0          0s    <none>   worker1"   <none>           <none>
redis-cache-7c5dd87dd-pxw7f   0/1     "ContainerCreating   0          0s    <none>   worker2"   <non"

hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES
redis-cache-7c5dd87dd-8kdk5   0/1    " Pending   0          5m3s   <none>   "   <none>    <none>           <none>
redis-cache-7c5dd87dd-kmbdn   1/1     Running   0          5m3s   10.36.0.1   worker1   <none>           <none>
redis-cache-7c5dd87dd-pxw7f   1/1     Running   0          5m3s   10.44.0.1   worker2   <none>           <none>"

hai@master:~$ kubectl get all -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/redis-cache-7c5dd87dd-8kdk5   "0/1     Pending   0          11m   <none>  "    <none>    <none>           <none>
pod/redis-cache-7c5dd87dd-kmbdn   1/1     Running   0          11m   10.36.0.1   worker1   <none>           <none>
pod/redis-cache-7c5dd87dd-pxw7f   1/1     Running   0          11m   10.44.0.1   worker2   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   32h   <none>

NAME                          READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS     IMAGES             SELECTOR
deployment.apps/redis-cache  " 2/3     3            2      "     11m   redis-server   redis:3.2-alpine   app=store

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS     IMAGES             SELECTOR
replicaset.apps/redis-cache-7c5dd87dd   "3         3         2  "     11m   redis-server   redis:3.2-alpine   app=store,pod-template-hash=7c5dd87dd
========+++++++++++++++++++++++++++++
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-server
spec:
  selector:
    matchLabels:
      app: web-store
  replicas: 3
  template:
    metadata:
      labels:
        app: web-store
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web-store
            topologyKey: "kubernetes.io/hostname"
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: web-app
        image: nginx:1.16-alpine

hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
redis-cache-7c5dd87dd-bvcmf   1/1     Running   0          2m31s   10.36.0.1   worker1   <none>           <none>
redis-cache-7c5dd87dd-lmmft   0/1     Pending   0          2m31s   <none>      <none>    <none>           <none>
redis-cache-7c5dd87dd-m55cv   1/1     Running   0          2m31s   10.44.0.1   worker2   <none>           <none>
web-server-54d4dcb578-4rv4z   1/1     Running   0          2m      10.44.0.2   worker2   <none>           <none>
web-server-54d4dcb578-4tctl   0/1     Pending   0          2m      <none>      <none>    <none>           <none>
web-server-54d4dcb578-h6nzp   1/1     Running   0          2m      10.36.0.2   worker1   <none>           <none>
############################################################################ "
       "  #%%#$%#$^______________  staticpod _______+============#%#$"
https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/   

hai@master:~$ kubectl get pods -n kube-system  ## "static pods "
NAME                             READY   STATUS    RESTARTS        AGE
coredns-565d847f94-q5zw9         1/1     Running   471 (23m ago)   6d20h
coredns-565d847f94-rg789         1/1     Running   460 (23m ago)   6d20h
etcd-master                      1/1     Running   16 (23m ago)    7d13h
kube-apiserver-master            1/1     Running   12 (23m ago)    6d19h
kube-controller-manager-master   1/1     Running   22 (23m ago)    7d13h
kube-proxy-6t6qp                 1/1     Running   16 (23m ago)    7d13h
kube-proxy-ffdgc                 1/1     Running   14 (24m ago)    7d
kube-proxy-gq4v9                 1/1     Running   13 (24m ago)    6d23h
kube-scheduler-master            1/1     Running   21 (23m ago)    7d13h
weave-net-6bpzq                  2/2     Running   36 (23m ago)    7d13h
weave-net-8gkx4                  2/2     Running   32 (22m ago)    7d
weave-net-f27sl                  2/2     Running   27 (24m ago)    6d23h "

hai@master:~$ kubectl get ns     ## " static pods "
NAME              STATUS   AGE
default           Active   7d13h
kube-node-lease   Active   7d13h
kube-public       Active   7d13h
kube-system       Active   7d13h
project1          Active   6d22h
project2          Active   6d22h

static pods  are always managed by the " KUBELETS "
static pods are never managed by the "MASTER NODE "

scheduler  is not  aware of any of the static pods 

static pods  are  always created and  managed by kubelet  agent running in  each and every node 


hai@ubuntu:~/test$ kubectl get pods  -o wide --show-labels
No resources found in default namespace "

root@ubuntu:/etc/kubernetes/manifests# ll
total 24
drwxr-xr-x 2 root root 4096 Dec 25 23:49 ./
drwxr-xr-x 4 root root 4096 Dec 25 23:49 ../
-rw------- 1 root root 2382 Dec 25 23:49 etcd.yaml
-rw------- 1 root root 4019 Dec 25 23:49 kube-apiserver.yaml
-rw------- 1 root root 3520 Dec 25 23:49 kube-controller-manager.yaml
-rw------- 1 root root 1440 Dec 25 23:49 kube-scheduler.yaml "

root@ubuntu:~# cd /var/lib/kubelet/
root@ubuntu:/var/lib/kubelet# ll
total 48
drwx------  8 root root 4096 Dec 25 23:49 ./
drwxr-xr-x 74 root root 4096 Dec 26 00:00 ../
-rw-r--r--  1 root root 1015 Dec 25 23:49 config.yaml
-rw-------  1 root root   62 Dec 25 23:49 cpu_manager_state
drwxr-xr-x  2 root root 4096 Dec 25 23:50 device-plugins/
-rw-r--r--  1 root root  176 Dec 25 23:49 kubeadm-flags.env
-rw-------  1 root root   61 Dec 25 23:49 memory_manager_state
drwxr-xr-x  2 root root 4096 Dec 25 23:49 pki/
drwxr-x---  2 root root 4096 Dec 25 23:49 plugins/
drwxr-x---  2 root root 4096 Dec 25 23:49 plugins_registry/
drwxr-x---  2 root root 4096 Dec 25 23:50 pod-resources/
drwxr-x--- 10 root root 4096 Dec 26 00:00 pods/ "

root@ubuntu:/var/lib/kubelet# cat config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
"staticPodPath: /etc/kubernetes/manifests"    ## importent  see look 
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s "

hai@ubuntu:~/test$ cd /etc/kubernetes/manifests/
hai@ubuntu:/etc/kubernetes/manifests$ ll
total 24
drwxr-xr-x 2 root root 4096 Dec 25 23:49 ./
drwxr-xr-x 4 root root 4096 Dec 25 23:49 ../
-rw------- 1 root root 2382 Dec 25 23:49 etcd.yaml
-rw------- 1 root root 4019 Dec 25 23:49 kube-apiserver.yaml
-rw------- 1 root root 3520 Dec 25 23:49 kube-controller-manager.yaml
-rw------- 1 root root 1440 Dec 25 23:49 kube-scheduler.yaml "

hai@ubuntu:/etc/kubernetes/manifests$ kubectl get pods -n
error: flag needs an argument: 'n' in -n
See 'kubectl get --help' for usage.
hai@ubuntu:/etc/kubernetes/manifests$ kubectl get  namespace
NAME              STATUS   AGE
default           Active   72m
kube-node-lease   Active   72m
kube-public       Active   72m
kube-system       Active   72m  "

hai@ubuntu:/etc/kubernetes/manifests$ kubectl get pods  -n kube-system  
NAME                             READY   STATUS    RESTARTS      AGE
coredns-565d847f94-qpcn7         1/1     Running   0             73m
coredns-565d847f94-x2kzb         1/1     Running   0             73m
etcd-ubuntu                      1/1     Running   0             73m
kube-apiserver-ubuntu            1/1     Running   0             73m
kube-controller-manager-ubuntu   1/1     Running   0             73m
kube-proxy-c9jk5                 1/1     Running   0             30m
kube-proxy-g69ms                 1/1     Running   0             73m
kube-proxy-hd7k2                 1/1     Running   0             41m
kube-scheduler-ubuntu            1/1     Running   0             73m
weave-net-m9pzz                  2/2     Running   1 (63m ago)   63m
weave-net-mh2f8                  2/2     Running   0             41m
weave-net-t459v                  2/2     Running   0             30m "

root@ubuntu:/etc/kubernetes/manifests# ll
total 28
drwxr-xr-x 2 root root 4096 Dec 26 01:26 ./
drwxr-xr-x 4 root root 4096 Dec 25 23:49 ../
-rw------- 1 root root 2382 Dec 25 23:49 etcd.yaml
-rw------- 1 root root 4019 Dec 25 23:49 kube-apiserver.yaml
-rw------- 1 root root 3520 Dec 25 23:49 kube-controller-manager.yaml
-rw------- 1 root root 1440 Dec 25 23:49 kube-scheduler.yaml
-rw-r--r-- 1 root root  123 Dec 26 01:25 "withoutcommand.yaml" "  ## importent look see "

"withoutcommand.yaml"
apiVersion: v1      
kind: Pod
metadata:
  name:" kubelet-automated-test"
spec:
  containers:
    - name: web
      image: nginx  "

hai@ubuntu:~$ kubectl get pods -o wide
No resources found in default namespace.

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS              RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATE                                                   S
kubelet-automated-test-ubuntu   0/1     ContainerCreating   0          3s    <none>  " ubuntu "  <none>           <none>

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE    IP          NODE     NOMINATED NODE   READINESS GATES
"kubelet-automated-test-ubuntu "  1/1     Running   0          112s   10.32.0.4  " ubuntu "  <none>           <none>"

root@ubuntu:/etc/kubernetes/manifests# mv withoutcommand.yaml /

root@ubuntu:/etc/kubernetes/manifests# ll
total 24
drwxr-xr-x 2 root root 4096 Dec 26 01:42 ./
drwxr-xr-x 4 root root 4096 Dec 25 23:49 ../
-rw------- 1 root root 2382 Dec 25 23:49 etcd.yaml
-rw------- 1 root root 4019 Dec 25 23:49 kube-apiserver.yaml
-rw------- 1 root root 3520 Dec 25 23:49 kube-controller-manager.yaml
-rw------- 1 root root 1440 Dec 25 23:49 kube-scheduler.yaml

hai@ubuntu:~$ kubectl get pods -o wide
No resources found in default namespace."

root@ubuntu:/etc/kubernetes/manifests# mv /withoutcommand.yaml .
root@ubuntu:/etc/kubernetes/manifests# ll
total 28
drwxr-xr-x 2 root root 4096 Dec 26 01:45 ./
drwxr-xr-x 4 root root 4096 Dec 25 23:49 ../
-rw------- 1 root root 2382 Dec 25 23:49 etcd.yaml
-rw------- 1 root root 4019 Dec 25 23:49 kube-apiserver.yaml
-rw------- 1 root root 3520 Dec 25 23:49 kube-controller-manager.yaml
-rw------- 1 root root 1440 Dec 25 23:49 kube-scheduler.yaml
-rw-r--r-- 1 root root  123 Dec 26 01:25 withoutcommand.yaml"

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
kubelet-automated-test-ubuntu   0/1     Pending   0          2s    <none>   "ubuntu  " <none>           <none>
hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
"kubelet-automated-test-ubuntu"   1/1     Running   0          4s    10.32.0.4   '"ubuntu  "' <none>           <none>"

hai@ubuntu:~/test$ kubectl delete pod  kubelet-automated-test-ubuntu
pod "kubelet-automated-test-ubuntu" deleted

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
kubelet-automated-test-ubuntu   0/1     Pending   0          6s    <none>   ubuntu   <none>           <none>"

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
kubelet-automated-test-ubuntu   1/1     Running   0          8s    10.32.0.4   ubuntu   <none>           <none>

###############################################################"

hai@ubuntu:~/test$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
"kubelet-automated-test-ubuntu  " 1/1     Running   0          77m   10.32.0.4   ubuntu   <none>           <none>

root@worker1:/etc/kubernetes# ll
total 24
drwxr-xr-x   3 root root  4096 Dec 26 00:22 ./
drwxr-xr-x 138 root root 12288 Dec 26 00:21 ../
-rw-------   1 root root  1955 Dec 26 00:22 kubelet.conf
drwxr-xr-x   2 root root  4096 Dec 26 00:22 pki/ "

root@worker1:/etc/kubernetes# ll
total 28
drwxr-xr-x   3 root root  4096 Dec 26 03:18 ./
drwxr-xr-x 138 root root 12288 Dec 26 00:21 ../
-rw-------   1 root root  1955 Dec 26 00:22 kubelet.conf
drwxr-xr-x   2 root root  4096 Dec 26 00:22 pki/ "

root@worker1:/etc/kubernetes# mkdir manifests
root@worker1:/etc/kubernetes# ll
total 32
drwxr-xr-x   4 root root  4096 Dec 26 03:31 ./
drwxr-xr-x 138 root root 12288 Dec 26 00:21 ../
-rw-------   1 root root  1955 Dec 26 00:22 kubelet.conf
drwxr-xr-x   2 root root  4096 Dec 26 03:32 manifests/
drwxr-xr-x   2 root root  4096 Dec 26 00:22 pki/
-rw-r--r--   1 root root   116 Dec 26 03:28 pod.yaml
root@worker1:/etc/kubernetes# cd manifests/
root@worker1:/etc/kubernetes/manifests# nano pod.yaml  "

apiVersion: v1
kind: Pod
metadata:
  name: worker1pod
spec:
  containers:
    - name: worker1pod
      image: nginx      "
hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
kubelet-automated-test-ubuntu   1/1     Running   0          94m   10.32.0.4   ubuntu   <none>           <none> "

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
kubelet-automated-test-ubuntu   1/1     Running   0          98m   10.32.0.4   ubuntu    <none>           <none>
"worker1pod-worker1 "             0/1     Pending   0          6s    <none>      worker1   <none>           <none>"

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
kubelet-automated-test-ubuntu   1/1     Running             0          98m   10.32.0.4   ubuntu    <none>           <none>
"worker1pod-worker1"              0/1     ContainerCreating   0          7s    <none>      "worker1 "  <none>           <none>

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES
kubelet-automated-test-ubuntu   1/1     Running   0          109m   10.32.0.4   ubuntu    <none>           <none>
"worker1pod-worker1 "             1/1     Running   0          11m    10.44.0.1   "worker1  " <none>           <none>

====+++++++++++"

root@worker2:~# cd /etc/kubernetes/
root@worker2:/etc/kubernetes# mkdir manifests
root@worker2:/etc/kubernetes# ll
total 28
drwxr-xr-x   4 root root  4096 Dec 26 03:48 ./
drwxr-xr-x 137 root root 12288 Dec 26 00:30 ../
-rw-------   1 root root  1955 Dec 26 00:32 kubelet.conf
drwxr-xr-x   2 root root  4096 Dec 26 03:48 manifests/
drwxr-xr-x   2 root root  4096 Dec 26 00:32 pki/
root@worker2:/etc/kubernetes# cd manifests/
root@worker2:/etc/kubernetes/manifests# ll
total 8
drwxr-xr-x 2 root root 4096 Dec 26 03:48 ./
drwxr-xr-x 4 root root 4096 Dec 26 03:48 ../
root@worker2:/etc/kubernetes/manifests# nano worker2-to-master.yaml
root@worker2:/etc/kubernetes/manifests# ll
total 12
drwxr-xr-x 2 root root 4096 Dec 26 03:50 ./
drwxr-xr-x 4 root root 4096 Dec 26 03:48 ../
-rw-r--r-- 1 root root  122 Dec 26 03:50 worker2-to-master.yaml  "

apiVersion: v1
kind: Pod
metadata:
  name: from-worker2pod
spec:
  containers:
    - name: worker2pod
      image: nginx   "

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES
"from-worker2pod-worker2  "       1/1     Running   0          5s     10.47.0.1   "worker2 "  <none>           <none>
kubelet-automated-test-ubuntu   1/1     Running   0          115m   10.32.0.4   ubuntu    <none>           <none>
worker1pod-worker1              1/1     Running   0          17m    10.44.0.1   worker1   <none>           <none>

#################################################"

Persistent Volumes 

 PersistentVolume (PV) := is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes.

PersistentVolumeClaim (PVC) =:  is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources.

Types of Persistent Volumes  : = 
PersistentVolume types are implemented as plugins. Kubernetes currently supports the following plugins:

cephfs - CephFS volume
csi - Container Storage Interface (CSI)
fc - Fibre Channel (FC) storage
hostPath - HostPath volume (for single node testing only; WILL NOT WORK in a multi-node cluster; consider using local volume instead)
iscsi - iSCSI (SCSI over IP) storage
local - local storage devices mounted on nodes.
nfs - Network File System (NFS) storage
rbd - Rados Block Device (RBD) volume "

The following types of PersistentVolume are deprecated. This means that support is still available but will be removed in a future Kubernetes release.
awsElasticBlockStore - AWS Elastic Block Store (EBS) (deprecated in v1.17)
azureDisk - Azure Disk (deprecated in v1.19)
azureFile - Azure File (deprecated in v1.21)
cinder - Cinder (OpenStack block storage) (deprecated in v1.18)
flexVolume - FlexVolume (deprecated in v1.23)
gcePersistentDisk - GCE Persistent Disk (deprecated in v1.17)
portworxVolume - Portworx volume (deprecated in v1.25)
vsphereVolume - vSphere VMDK volume (deprecated in v1.19) "

Older versions of Kubernetes also supported the following in-tree PersistentVolume types:
photonPersistentDisk - Photon controller persistent disk. (not available starting v1.15)
scaleIO - ScaleIO volume (not available starting v1.21)
flocker - Flocker storage (not available starting v1.25)
quobyte - Quobyte volume (not available starting v1.25)
storageos - StorageOS volume (not available starting v1.25)
Persistent Volumes

RWO - ReadWriteOnce
ROX - ReadOnlyMany
RWX - ReadWriteMany
RWOP - ReadWriteOncePod

awsElasticBlockStore
azureDisk
azureFile
cephfs
cinder (deprecated in v1.18)
gcePersistentDisk
iscsi
nfs
rbd
vsphereVolume

======++++++++++++++++++++++++++++++++++++++++++//////\\\\\\\\
How To Set Up an NFS Mount on Ubuntu 20.04

https://www.tecmint.com/install-nfs-server-on-ubuntu/       best

sudo apt install nfs-kernel-server
 sudo mkdir -p /mnt/nfs_share

root@ubuntu:/mnt# ll
total 12
drwxr-xr-x  3 root root 4096 Dec 26 21:34 ./
drwxr-xr-x 20 root root 4096 Nov 24 21:53 ../
drwxr-xr-x  2 root root 4096 Dec 26 21:34 nfs_share/

root@ubuntu:~#  sudo chown -R nobody:nogroup /mnt/nfs_share/
root@ubuntu:/mnt# ll
total 12
drwxr-xr-x  3 root   root    4096 Dec 26 21:34 ./
drwxr-xr-x 20 root   root    4096 Nov 24 21:53 ../
drwxr-xr-x  2 "nobody nogroup" 4096 Dec 26 21:34 nfs_share/

root@ubuntu:/mnt# sudo chmod 777 /mnt/nfs_share/
root@ubuntu:/mnt# ll
total 12
drwxr-xr-x  3 root   root    4096 Dec 26 21:34 ./
drwxr-xr-x 20 root   root    4096 Nov 24 21:53 ../
drwxrwxrwx  2 nobody nogroup 4096 Dec 26 21:34 nfs_share/

sudo nano  /etc/exports
/mnt/nfs_share  192.168.68.133/24(rw,sync,no_subtree_check)
   #// OR  OR  OR 
/mnt/nfs_share *(rw,sync,no_root_squash,no_subtree_check)
  

root@ubuntu:~# cat /etc/exports
# /etc/exports: the access control list for filesystems which may be exported
#               to NFS clients.  See exports(5).
#
# Example for NFSv2 and NFSv3:
# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)
#(rw,sync,no_root_squash,no_subtree_check)
#  
# Example for NFSv4:
# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)
# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)
#
/mnt/nfs_share  192.168.68.133/24(rw,sync,no_subtree_check)
   # OR  OR  
   /mnt/nfs_share *(rw,sync,no_root_squash,no_subtree_check)


     rw: Stands for Read/Write.
sync: Requires changes to be written to the disk before they are applied.
No_subtree_check: Eliminates subtree checking.
-----
sudo systemctl restart nfs-kernel-server

root@nfs:~# sudo systemctl restart nfs-kernel-server
root@nfs:~# sudo systemctl enable  nfs-kernel-server
Synchronizing state of nfs-kernel-server.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable nfs-kernel-server

root@ubuntu:~#  sudo exportfs -a

root@ubuntu:~#  sudo systemctl restart nfs-kernel-server

root@nfs:~# exportfs -v
/mnt/nfs_share	<world>(rw,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash)

root@ubuntu:~# sudo ufw allow from 192.168.68.133/24  to any port nfs
WARN: Rule changed after normalization
Rules updated
---
root@ubuntu:~# sudo ufw status
Status: active
To                         Action      From
--                         ------      ----
2049                       ALLOW       192.168.68.0/24
--
sudo apt update -y
sudo apt install nfs-common

root@ubuntu:/mnt# sudo mkdir -p /mnt/nfs_clientshare
root@ubuntu:/mnt# ll
total 16
drwxr-xr-x  4 root   root    4096 Dec 26 22:03 ./
drwxr-xr-x 20 root   root    4096 Nov 24 21:53 ../
drwxr-xr-x  2 root   root    4096 Dec 26 22:03 nfs_clientshare/
drwxrwxrwx  2 nobody nogroup 4096 Dec 26 21:34 nfs_share/

root@ubuntu:/mnt# ifconfig
ens33: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.68.153  netmask 255.255.255.0  broadcast 192.168.68.255

sudo mount 192.168.68.153:/mnt/nfs_share  /mnt/nfs_clientshare

root@ubuntu:/mnt/nfs_share# touch fromnfsshare  a  b  c d 
root@ubuntu:/mnt/nfs_share# ll
total 8
drwxrwxrwx 2 nobody nogroup 4096 Dec 26 22:16 ./
drwxr-xr-x 4 root   root    4096 Dec 26 22:03 ../
-rw-r--r-- 1 root   root       0 Dec 26 22:08 a
-rw-r--r-- 1 root   root       0 Dec 26 22:08 b
-rw-r--r-- 1 root   root       0 Dec 26 22:08 c
-rw-r--r-- 1 root   root       0 Dec 26 22:08 d
-rw-r--r-- 1 root   root       0 Dec 26 22:16 fromnfsshare

root@ubuntu:/mnt/nfs_clientshare# ll
total 8
drwxrwxrwx 2 nobody nogroup 4096 Dec 26 22:16 ./
drwxr-xr-x 4 root   root    4096 Dec 26 22:03 ../
-rw-r--r-- 1 root   root       0 Dec 26 22:08 a
-rw-r--r-- 1 root   root       0 Dec 26 22:08 b
-rw-r--r-- 1 root   root       0 Dec 26 22:08 c
-rw-r--r-- 1 root   root       0 Dec 26 22:08 d
-rw-r--r-- 1 root   root       0 Dec 26 22:16 fromnfsshare

############################################################

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#persistentvolume-v1-core

PersistentVolume v1 core
Group    	Version   	Kind
core     	v1	          PersistentVolume

PersistentVolumeSpec v1 core
 Appears In:
PersistentVolume [core/v1]
VolumeAttachmentSource [storage/v1]
Field	Description
accessModes
string array	accessModes contains all ways the volume can be mounted. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes

awsElasticBlockStore
AWSElasticBlockStoreVolumeSource	awsElasticBlockStore represents an AWS Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore

azureDisk
AzureDiskVolumeSource	azureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.

azureFile
AzureFilePersistentVolumeSource	azureFile represents an Azure File Service mount on the host and bind mount to the pod.

capacity
object	capacity is the description of the persistent volume's resources and capacity. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#capacity

cephfs
CephFSPersistentVolumeSource	cephFS represents a Ceph FS mount on the host that shares a pod's lifetime

cinder
CinderPersistentVolumeSource	cinder represents a cinder volume attached and mounted on kubelets host machine. More info: https://examples.k8s.io/mysql-cinder-pd/README.md

claimRef
ObjectReference	claimRef is part of a bi-directional binding between PersistentVolume and PersistentVolumeClaim. Expected to be non-nil when bound. claim.VolumeName is the authoritative bind between PV and PVC. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#binding

csi
CSIPersistentVolumeSource	csi represents storage that is handled by an external CSI driver (Beta feature).

fc
FCVolumeSource	fc represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to the pod.

flexVolume
FlexPersistentVolumeSource	flexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin.

flocker
FlockerVolumeSource	flocker represents a Flocker volume attached to a kubelet's host machine and exposed to the pod for its usage. This depends on the Flocker control service being running

gcePersistentDisk
GCEPersistentDiskVolumeSource	gcePersistentDisk represents a GCE Disk resource that is attached to a kubelet's host machine and then exposed to the pod. Provisioned by an admin. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk

glusterfs
GlusterfsPersistentVolumeSource	glusterfs represents a Glusterfs volume that is attached to a host and exposed to the pod. Provisioned by an admin. More info: https://examples.k8s.io/volumes/glusterfs/README.md

hostPath
HostPathVolumeSource	hostPath represents a directory on the host. Provisioned by a developer or tester. This is useful for single-node development and testing only! On-host storage is not supported in any way and WILL NOT WORK in a multi-node cluster. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath

iscsi
ISCSIPersistentVolumeSource	iscsi represents an ISCSI Disk resource that is attached to a kubelet's host machine and then exposed to the pod. Provisioned by an admin.

local
LocalVolumeSource	local represents directly-attached storage with node affinity

mountOptions
string array	mountOptions is the list of mount options, e.g. ["ro", "soft"]. Not validated - mount will simply fail if one is invalid. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#mount-options

nfs
NFSVolumeSource	nfs represents an NFS mount on the host. Provisioned by an admin. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs

nodeAffinity
VolumeNodeAffinity	nodeAffinity defines constraints that limit what nodes this volume can be accessed from. This field influences the scheduling of pods that use this volume.

persistentVolumeReclaimPolicy
string	persistentVolumeReclaimPolicy defines what happens to a persistent volume when released from its claim. Valid options are Retain (default for manually created PersistentVolumes), Delete (default for dynamically provisioned PersistentVolumes), and Recycle (deprecated). Recycle must be supported by the volume plugin underlying this PersistentVolume. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#reclaiming

photonPersistentDisk
PhotonPersistentDiskVolumeSource	photonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine

portworxVolume
PortworxVolumeSource	portworxVolume represents a portworx volume attached and mounted on kubelets host machine

quobyte
QuobyteVolumeSource	quobyte represents a Quobyte mount on the host that shares a pod's lifetime

rbd
RBDPersistentVolumeSource	rbd represents a Rados Block Device mount on the host that shares a pod's lifetime. More info: https://examples.k8s.io/volumes/rbd/README.md

scaleIO
ScaleIOPersistentVolumeSource	scaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes.

storageClassName
string	storageClassName is the name of StorageClass to which this persistent volume belongs. Empty value means that this volume does not belong to any StorageClass.

storageos
StorageOSPersistentVolumeSource	storageOS represents a StorageOS volume that is attached to the kubelet's host machine and mounted into the pod More info: https://examples.k8s.io/volumes/storageos/README.md

volumeMode
string	volumeMode defines if a volume is intended to be used with a formatted filesystem or to remain in raw block state. Value of Filesystem is implied when not included in spec.
-----------+++"
Reclaim Policy

Retain -- manual reclamation

Recycle -- basic scrub (rm -rf /thevolume/*)

Delete -- associated storage asset such as AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder volume is deleted

Currently, only NFS and HostPath support recycling. AWS EBS, GCE PD, Azure Disk, and Cinder volumes support deletion. "
-+++++++++++++++++++///\\/\\/\/\/\/\\/\/###############################\\/\/ "
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mypv
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /mnt/nfs_share
    server: 192.168.68.153   
---"
hai@ubuntu:~$ kubectl get pv --show-labels -o wide
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE   VOLUMEMODE   LABELS
mypv   5Gi        RWO            Recycle          Available           slow                    47s   Filesystem   <none> "

hai@ubuntu:~$ kubectl describe pv
Name:            mypv
Labels:          <none>
Annotations:     <none>
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    slow
Status:          Available
Claim:
"Reclaim Policy:  Recycle"
Access Modes:   " RWO"
VolumeMode:      Filesystem
Capacity:        5Gi
Node Affinity:   <none>
Message:
Source:
    Type:      NFS (an NFS mount that lasts the lifetime of a pod)
    Server:    192.168.68.153
    Path:      "/mnt/nfs_share"
    ReadOnly:  "false"
Events:        <none>
===---=== "++++++++++++++++++++++++++++"
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
   name: mypvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
     requests:
        storage: 1Gi
  storageClassName: slow
 

hai@ubuntu:~/test$ kubectl apply -f pvc.yaml 
error: "when path, url, or stdin is provided as input, you may not specify resource arguments as well"
ERROR  ERRORS  PV ERROR  VOLUME ERROR  {{}}}}/\\\/\/\/\/\/\/\/\/\/{{{{{{{{{{{{{{[[][[[[[[[]]]]]]]]}}}}}}}}}}}}}}

hai@nfs:~$ cat /etc/exports 
# /etc/exports: the access control list for filesystems which may be exported
#		to NFS clients.  See exports(5).           "  ##  SEE LOOK "
#
# Example for NFSv2 and NFSv3:
# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)
#
# Example for NFSv4:
# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)
# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)
#

/mnt/nfs_share * (rw,sync,no_root_squash)   "### see look "

## in  MASTER  MASTER 
root@ubuntu:~# systemctl start  firewalld

root@ubuntu:~# ufw enable 
Firewall is active and enabled on system startup

root@ubuntu:~# ufw status 
Status: active  "


hai@ubuntu:~$ kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   "CLAIM "          STORAGECLASS   REASON   AGE
mypv   5Gi        RWO            Recycle          Bound    "default/mypvc "  slow                    4h9m

hai@ubuntu:~$ kubectl get pvc -o wide --show-labels
NAME    STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE    VOLUMEMODE   LABELS
mypvc  " Bound"    mypv     5Gi        RWO           " slow  "         107m   Filesystem   <none>

hai@ubuntu:~$ kubectl describe pvc
Name:          mypvc
Namespace:     default
StorageClass:  slow
Status:        "Bound"
Volume:        "mypv"
Labels:        <none>
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      5Gi
Access Modes:  RWO
VolumeMode:    Filesystem
Used By:       <none>
Events:        <none>

############################################################ "

https://kubernetes.io/docs/concepts/storage/persistent-volumes/
https://kubernetes.io/search/?q=wordpress
https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/  

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: mysql
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: mysql
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: anji
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mypvc
--=========="
hai@ubuntu:~$ kubectl get pods -o wide --show-labels
NAME                               READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES   LABELS
wordpress-mysql-7664b65dc8-k5b72   0/1     "ContainerCreating  " 0          42s   <none>   worker1   <none>           <none>            app=wordpress,pod-template-hash=7664b65dc8,tier=mysql


hai@ubuntu:~$ kubectl get pods -o wide --show-labels
NAME                               READY   STATUS              RESTARTS   AGE     IP       NODE      NOMINATED NODE   READINESS GATES   LABELS
wordpress-mysql-7664b65dc8-k5b72   0/1     ContainerCreating   0          2m18s   <none>   worker1   <none>           <none>            app=wordpress,pod-template-hash=7664b65dc8,tier=mysql"

apiVersion: v1
kind: PersistentVolume
metadata:
  name: mypv
  labels: 
    type: local
spec:   
  storageClassName: slow
  capacity:
    storage: 10Gi
  accessModes:
     - ReadWriteOnce
  hostPath:
     path: /mnt/data     "
 
anji@master:~$ kubectl get pv -o wide --show-labels
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE   VOLUMEMODE   LABELS
mypv   10Gi       RWO            Retain           Available           slow                    67s   Filesystem   type=local

anji@master:~/test$ kubectl describe pv
Name:            mypv
Labels:          type=local
Annotations:     pv.kubernetes.io/bound-by-controller: yes
Finalizers:      [kubernetes.io/pv-protection]
S"torageClass:    slow"
"Status:          Bound"
"Claim:           default/mpvc"
Reclaim Policy:  Retain
"Access Modes:    RWO"
VolumeMode:      Filesystem
Capacity:        10Gi
Node Affinity:   <none>
Message:         
Source:
    Type:          HostPath (bare host directory volume)
   " Path:          /mnt/data"
    HostPathType:  
Events:            <none> "
--- ""
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: mpvc
spec: 
  storageClassName: slow
  accessModes:
     - ReadWriteOnce
  resources:
     requests:
        storage: 3Gi  
==//
anji@master:~/test$ kubectl describe pvc 
Name:          mpvc
Namespace:     default
StorageClass:  slow
Status:        Bound
Volume:        mypv
Labels:        <none>
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      10Gi
Access Modes:  RWO
VolumeMode:    Filesystem
Used By:       <none>
Events:        <none>
====================================###################
=## error dailing tcp 10250 ,
 {}{{}{{}{{{{{{{{{}}}}}}}}}}}}}}}}}}}[][[[[\/\/{{}{}{}{}{}}{}}
 https://stackoverflow.com/questions/47140813/error-from-server-error-dialing-backend-dial-tcp-10-9-84-14910250-getsockopt
https://bobcares.com/blog/kubernetes-error-dialing-backend/



anji@master:~$ kubectl exec nginx1 -it  -- /bin/bash
Error from server: error dialing backend: dial tcp 192.168.122.131:10250: connect: no route to host
anji@master:~$ systemctl restart kube-apiserver
Failed to restart kube-apiserver.service: Unit kube-apiserver.service not found.
anji@master:~$ kubectl get pods --namespace kube-system
NAME                             READY   STATUS    RESTARTS       AGE
coredns-565d847f94-7khmz         1/1     Running   0              171m
coredns-565d847f94-rkt8t         1/1     Running   0              171m
etcd-master                      1/1     Running   0              171m
kube-apiserver-master            1/1     Running   0              171m
kube-controller-manager-master   1/1     Running   0              171m
kube-proxy-4hx6h                 1/1     Running   0              171m
kube-proxy-psvxz                 1/1     Running   0              160m
kube-scheduler-master            1/1     Running   0              171m
weave-net-jc99v                  2/2     Running   1 (168m ago)   168m
weave-net-kpjtt                  2/2     Running   0              160m
anji@master:~$ systemctl disable firewalld 
Synchronizing state of firewalld.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install disable firewalld
update-rc.d: error: Permission denied
anji@master:~$ kubectl exec nginx1 -it  -- /bin/bash
Error from server: error dialing backend: dial tcp 192.168.122.131:10250: i/o timeout
anji@master:~$ ss -tnpl  | grep 10250
LISTEN  0       4096                    *:10250                *:*              

root@master:~# systemctl disable firewalld 
Synchronizing state of firewalld.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install disable firewalld
Removed /etc/systemd/system/multi-user.target.wants/firewalld.service.
Removed /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.

root@master:~# lsof -i :10250
COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
kubelet 3045 root   11u  IPv6  48249      0t0  TCP *:10250 (LISTEN)

https://www.codegrepper.com/tpc/Error+from+server%3A+error+dialing+backend%3A+dial+tcp+%3A10250%3A+connect%3A+no+route+to+host+kubernetes

systemctl stop kubelet
systemctl stop docker
iptables --flush
iptables -tnat --flush
systemctl start kubelet
systemctl start docker


telnet hostname(worker1) 10250

systemctl enable firewalld
systemctl start firewalld
firewall-cmd --permanent --add-port=6443/tcp

firewall-cmd --permanent --add-port=2379-2380/tcp

firewall-cmd --permanent --add-port=10250-10255/tcp

 sudo firewall-cmd --permanent --add-port={6443,2379-2381,10250-10252,80,8080,8001}/tcp

  sudo firewall-cmd --permanent --add-port={6443,2379-2381,10250-10255}/tcp
  
oot@worker1:/mnt/data# systemctl start firewalld
root@worker1:/mnt/data# firewall-cmd --permanent --add-port=6443/tcp
Warning: ALREADY_ENABLED: 6443:tcp
success
root@worker1:/mnt/data# firewall-cmd --permanent --add-port=2379-2380/tcp
success
root@worker1:/mnt/data# firewall-cmd --permanent --add-port=10250-10255/tcp
Warning: ALREADY_ENABLED: 10250-10255:tcp
success
root@worker1:/mnt/data#  sudo firewall-cmd --permanent --add-port={6443,2379-2381,10250-10252,80,8080,8001}/tcp

Warning: ALREADY_ENABLED: 6443:tcp
Warning: ALREADY_ENABLED: 2379-2381:tcp
success
root@worker1:/mnt/data#  
root@worker1:/mnt/data# sudo firewall-cmd --permanent --add-port={6443,2379-2381,10250-10255}/tcp
Warning: ALREADY_ENABLED: 6443:tcp
Warning: ALREADY_ENABLED: 2379-2381:tcp
Warning: ALREADY_ENABLED: 10250-10255:tcp
success
root@worker1:/mnt/data# 

oot@master:~# iptables -tnat --flush
root@master:~# systemctl enable firewalld
Synchronizing state of firewalld.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable firewalld
Created symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service â†’ /lib/systemd/system/firewalld.service.
Created symlink /etc/systemd/system/multi-user.target.wants/firewalld.service â†’ /lib/systemd/system/firewalld.service.

root@master:~# systemctl start firewalld
root@master:~# firewall-cmd --permanent --add-port=6443/tcp
success
root@master:~# firewall-cmd --permanent --add-port=2379-2380/tcp
success
root@master:~# firewall-cmd --permanent --add-port=10250-10255/tcp
success
root@master:~#  sudo firewall-cmd --permanent --add-port={6443,2379-2381,10250-10252,80,8080,8001}/tcp
Warning: ALREADY_ENABLED: 6443:tcp
success

root@master:~#  
root@master:~# sudo firewall-cmd --permanent --add-port={6443,2379-2381,10250-10255}/tcp
Warning: ALREADY_ENABLED: 6443:tcp
Warning: ALREADY_ENABLED: 2379-2381:tcp
Warning: ALREADY_ENABLED: 10250-10255:tcp
success

anji@master:~$ kubectl exec nginx1  -it  -- /bin/bash
root@nginx1:/# ls
bin   dev		   docker-entrypoint.sh  home  lib64  mnt  proc  run   srv  tmp  var
boot  docker-entrypoint.d  etc			 lib   media  opt  root  sbin  sys  usr
root@nginx1:/# 

########################################################################
https://www.containiq.com/post/kubernetes-persistent-volumes

apiVersion: v1
kind: PersistentVolume
metadata:
  name: nginx-pv-volume
spec:
  storageClassName: standard
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/nginx"

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pv-claim
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

---

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pv-pod
spec:
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: nginx-storage
  volumes:
    - name: nginx-storage
      persistentVolumeClaim:
        claimName: nginx-pv-claim  
"
anji@master:~$ kubectl get pv
NAME              CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE
nginx-pv-volume   5Gi        RWO            Retain           Bound    default/nginx-pv-claim   standard                7s
anji@master:~$ kubectl get pvc
NAME             STATUS   VOLUME            CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nginx-pv-claim   Bound    nginx-pv-volume   5Gi        RWO            standard       8s
anji@master:~$ kubectl get  po
NAME           READY   STATUS    RESTARTS   AGE
nginx-pv-pod   1/1     Running   0          12s  "

apiVersion: v1
kind: PersistentVolume
metadata:
  name: nginx-pv-volume
spec:
  storageClassName: sony
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/nginx"

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc
spec:
  storageClassName: sony
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
         - mountPath: "/usr/share/nginx/html"
           name: nginx
      volumes:
       - name: nginx
         persistentVolumeClaim:
           claimName: mypvc
######################################################################"
" error " master notready  not ready {}{{{{}{][{}{{}{{{{}{{}{{}[[}}}{{}}{{{{}}{}{}}{}}]]}}}}}}]}}}}
# error "   
anji@master:~$ kubectl get nodes --show-labels -o wide 
NAME      STATUS     ROLES           AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME    LABELS
master    "NotReady  " control-plane   28h   v1.25.5   192.168.122.34    <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.5.9   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
worker1   Ready      <none>          28h   v1.25.5   192.168.122.131   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.5.9   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker1,kubernetes.io/os=linux
====++++"
anji@master:~$ kubectl get all --all-namespaces  --show-labels
NAMESPACE     NAME                                 READY   STATUS        RESTARTS      AGE   LABELS
kube-system   pod/coredns-565d847f94-4cj95         1/1     Running       0             25h   k8s-app=kube-dns,pod-template-hash=565d847f94
kube-system   pod/coredns-565d847f94-5zgp7         1/1     Running       0             25h   k8s-app=kube-dns,pod-template-hash=565d847f94
"kube-system   pod/coredns-565d847f94-7khmz         1/1     Terminating "  0             28h   k8s-app=kube-dns,pod-template-hash=565d847f94
"kube-system   pod/coredns-565d847f94-rkt8t         1/1     Terminating  " 0             28h   k8s-app=kube-dns,pod-template-hash=565d847f94
kube-system   pod/etcd-master                      1/1     Running       0             28h   component=etcd,tier=control-plane
kube-system   pod/kube-apiserver-master            1/1     Running       0             28h   component=kube-apiserver,tier=control-plane
kube-system   pod/kube-controller-manager-master   1/1     Running       0             28h   component=kube-controller-manager,tier=control-plane
kube-system   pod/kube-proxy-4hx6h                 1/1     Running       0             28h   controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-system   pod/kube-proxy-psvxz                 1/1     Running       0             28h   controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-system   pod/kube-scheduler-master            1/1     Running       0             28h   component=kube-scheduler,tier=control-plane
kube-system   pod/weave-net-jc99v                  2/2     Running       1 (28h ago)   28h   controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
kube-system   pod/weave-net-kpjtt                  2/2     Running       0             28h   controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1

NAMESPACE     NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE   LABELS
default       service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP                  28h   component=apiserver,provider=kubernetes
kube-system   service/kube-dns     ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   28h   k8s-app=kube-dns,kubernetes.io/cluster-service=true,kubernetes.io/name=CoreDNS

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE   LABELS
kube-system   daemonset.apps/kube-proxy   2         2         1       2            1           kubernetes.io/os=linux   28h   k8s-app=kube-proxy
kube-system   daemonset.apps/weave-net    2         2         1       2            1           <none>                   28h   name=weave-net

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   LABELS
kube-system   deployment.apps/coredns   2/2     2            2           28h   k8s-app=kube-dns

NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE   LABELS
kube-system   replicaset.apps/coredns-565d847f94   2         2         2       28h   k8s-app=kube-dns,pod-template-hash=565d847f94"

{{{{  rebbot system }{}{{[[[[]]]]}}}}}}
========================================================================
https://www.youtube.com/watch?v=A4svk80wPbA&list=PL8SR-mMnL9f6KKYDnrG3saVnyLqNDz1GP&index=17
day - 16  Kubernetes volume (PVC,PV) and explanation on RBAC

anji@master:~$ ls -a 
.              .bashrc  Documents  .local    .profile                   Templates
..             .cache   Downloads  .mozilla  Public                     test
.bash_history  .config  .gnupg     Music     .ssh                       Videos
.bash_logout   Desktop  .kube      Pictures  .sudo_as_admin_successful
anji@master:~$ cd .kube/
anji@master:~/.kube$ ll
total 20
drwxrwxr-x  3 anji anji 4096 Dec 28 12:12 ./
drwxr-xr-x 18 anji anji 4096 Dec 28 12:51 ../
drwxr-x---  4 anji anji 4096 Dec 28 12:12 cache/
-rw-------  1 anji anji 5642 Dec 28 12:11 config "

ter:~/.kube$ cat configbak 
apiVersion: v1
clusters:
- cluster:
    "certificate-authority-data:" LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBRE

    server: https://192.168.122.34:6443
  name: kubernetes
contexts:
- context:
    "cluster: kubernetes"
    user: "kubernetes-admin"
  name: "kubernetes-admin@kubernetes"
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
   " client-certificate-data: "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJZGxKc0FLY20yZTR3RFFZSktvWklodmNOQVFF

client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcFFJQkFBS0NBUUVBcVlKQkVrTUpnaWdLNjUrdHpNM0t5R1FhYzZIbjFXVmoxcjhGTWF
===========++++++++++"
root@master:~# adduser wipro
Adding user `wipro' ...
Adding new group `wipro' (1001) ...

root@master:~# su - wipro 

wipro@master:~$ kubectl get nodes 
The connection to the server localhost:8080 was refused - did you specify the right host or port?
----=="
anji@master:~$ mkdir wipro
anji@master:~$ cd wipro/
anji@master:~/wipro$ cp /etc/kubernetes/pki/ca.
ca.crt  ca.key  
anji@master:~/wipro$ cp /etc/kubernetes/pki/
apiserver.crt                 apiserver-kubelet-client.key  front-proxy-ca.key
apiserver-etcd-client.crt     ca.crt                        front-proxy-client.crt
apiserver-etcd-client.key     ca.key                        front-proxy-client.key
apiserver.key                 etcd/                         sa.key
apiserver-kubelet-client.crt  front-proxy-ca.crt            sa.pub
anji@master:~/wipro$ cp /etc/kubernetes/pki/ca.crt  .

anji@master:~/wipro$ sudo cp /etc/kubernetes/pki/ca.key  . 
[sudo] password for anji: 
anji@master:~/wipro$ ll
total 16
drwxrwxr-x  2 anji anji 4096 Dec 30 11:47 ./
drwxr-xr-x 19 anji anji 4096 Dec 30 11:43 ../
-rw-r--r--  1 anji anji 1099 Dec 30 11:45 ca.crt
-rw-------  1 root root 1679 Dec 30 11:47 ca.key "

anji@master:~/wipro$ sudo openssl genrsa -out wipro.key 2048
[sudo] password for anji: 
Generating RSA private key, 2048 bit long modulus (2 primes)
.............................+++++
..........+++++
e is 65537 (0x010001)
anji@master:~/wipro$ ll
total 20
drwxrwxr-x  2 anji anji 4096 Dec 30 12:02 ./
drwxr-xr-x 19 anji anji 4096 Dec 30 11:43 ../
-rw-r--r--  1 anji anji 1099 Dec 30 11:45 ca.crt
-rw-------  1 root root 1679 Dec 30 11:47 ca.key
-rw-------  1 root root 1679 Dec 30 12:02 wipro.key "

anji@master:~/wipro$ sudo openssl req -new  -key wipro.key -out  wipro.csr
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [AU]:
State or Province Name (full name) [Some-State]:
Locality Name (eg, city) []:
Organization Name (eg, company) [Internet Widgits Pty Ltd]:
Organizational Unit Name (eg, section) []:
Com"mon Name (e.g. server FQDN or YOUR name) []:wipro"
Email Address []:

Please enter the following 'extra' attributes
to be sent with your certificate request
A challenge password []:
An optional company name []:
anji@master:~/wipro$ ll
total 24
drwxrwxr-x  2 anji anji 4096 Dec 30 12:13 ./
drwxr-xr-x 19 anji anji 4096 Dec 30 11:43 ../
-rw-r--r--  1 anji anji 1099 Dec 30 11:45 ca.crt
-rw-------  1 root root 1679 Dec 30 11:47 ca.key
-rw-r--r--  1 root root  980 Dec 30 12:13 "wipro.csr"
-rw-------  1 root root 1679 Dec 30 12:02 wipro.key

anji@master:~/wipro$ sudo openssl x509 -req -in wipro.csr -CA ca.crt  -CAkey ca.key -CAcreateserial -out  wipro.crt -days  365
[sudo] password for anji: 
Signature ok
subject=C = AU, ST = Some-State, O = Internet Widgits Pty Ltd, CN = wipro
Getting CA Private Key
anji@master:~/wipro$ ll
total 32
drwxrwxr-x  2 anji anji 4096 Dec 30 12:27 ./
drwxr-xr-x 19 anji anji 4096 Dec 30 11:43 ../
-rw-r--r--  1 anji anji 1099 Dec 30 11:45 ca.crt
-rw-------  1 root root 1679 Dec 30 11:47 ca.key
-rw-r--r--  1 root root   41 Dec 30 12:27 ca.srl
-rw-r--r--  1 root root 1082 Dec 30 12:27 "wipro.crt"
-rw-r--r--  1 root root  980 Dec 30 12:13 wipro.csr
-rw-------  1 root root 1679 Dec 30 12:02 wipro.key "

anji@master:~/wipro$ file wipro.crt 
wipro.crt: PEM certificate

anji@master:~/wipro$ cat wipro.crt
-----BEGIN CERTIFICATE-----
MIIC8TCCAdkCFHUFE4Hae4I0d0cNDWn82jgk8RH+MA0GCSqGSIb3DQEBCwUAMBUx
E==
-----END CERTIFICATE----- "

anji@master:~/wipro$ cp /home/anji/.kube/config  .
anji@master:~/wipro$ ll
total 24
drwxrwxr-x  2 anji anji 4096 Dec 30 12:38 ./
drwxr-xr-x 19 anji anji 4096 Dec 30 11:43 ../
-rw-------  1 anji anji 5642 Dec 30 12:38 config
-rw-r--r--  1 root root 1082 Dec 30 12:27 wipro.crt
-rw-------  1 root root 1679 Dec 30 12:02 wipro.key

====  begore  " ==/////### BEFORE ==  

er:~/.kube$ cat config 
apiVersion: v1
clusters:
- cluster:
    "certificate-authority-data:" LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBRE

    server: https://192.168.122.34:6443
  name: kubernetes
contexts:
- context:
    "cluster: kubernetes"
    user: "kubernetes-admin"
  name: "kubernetes-admin@kubernetes"
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
   " client-certificate-data:

###\\\/////////////  after == " AFTER  

anji@master:~/wipro$ cat config 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: 
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: wipro                    # see look  
  name: wipro-context                  # see look  
current-context: wipro-context       # see look  
kind: Config
preferences: {}
users:
- name: wipro                          # see look  
  user:
    client-certificate-data:
    client-key-data: 

#######\\\\////////////=""//////////////////\\\\\\\\\\\\\   after 

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: 
    server: https://192.168.122.34:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: wipro
  name: wipro-context
current-context: wipro-context
kind: Config
preferences: {}
users:
- name: wipro
  user:
    client-certificate-data:
    client-key-data: 
====++++++++++++++++=///\\\
anji@master:~/wipro$ cat wipro.crt | base64 -w0
LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4VENDQWRrQ0ZIVUZFNEhhZTRJMGQwY05EV244MmpnazhSSCtNQTBHQ1NxR1NJYjNEUUVCQ3dVQU1CVXgKRXpBUkJnTlZCQU1UQ210MVltVnlibVYwWlhNd0hoY05Nakl4TWpNd01EWTFOekUzV2hjTk1qTXhNak13TURZMQpOekUzV2pCVk1Rc3dDUVlEVlFRR0V3SkJWVEVUTUJFR0ExVUVDQXdLVTI5dFpTMVRkR0YwWlRFaE1COEdBMVVFCkNnd1lTVzUwWlhKdVpYUWdWMmxrWjJsMGN5QlFkSGtnVEhSa01RNHdEQVlEVlFRRERBVjNhWEJ5YnpDQ0FTSXcKRFFZSktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQU81Wll6NEU2Y3Q2V1VPclF6NHFENW9SeW41Twp6eHlCQVluMlorVmdWdVd4WlZMM3J4OFZ5MDh0VmxBVWRPbkNkWFplZzJrQm9GNW9nd0J0ZDZkWmJ4NXkyYWpvCnlWc3JqcWw0aTNMRVoxOGZIZ3UrNVo5UXNBdDlXQVhkV213RVJZaStKZWZ6VDNvdU5zZUM2YTV2dE90ODBUTVoKcVgwL1k3OVBRZFRsTU9iVExDRXc0WmxyeG1LUWRkQXJjZHhicmpRVS9QZXJaWEVNQklxMUI3OUxJVEVOOE1sYwpqV2NpdUVuY012alVtemJVUldnR1o1QS9MY2hPUlh4eUlFZS8zM09ESFFqQzkvd1lOU3BxcjdpRjFpVmNwK3RoCnQzcE43NzMxOUdhRmdxWUpPY0o4Mm5TNS9xRlB3MUx2VHBoNlJOZWxWNGYvZGFCQlAxUThybnJwREVNQ0F3RUEKQVRBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWYzR1ZMcHkyZVBlSUE4QzA0UVVxUjNEVzhaNzdCT2NFWHNIagpib0ZacHlBTklUWDV1L2VEc2VxSGJYS0UwdjRkWVdKT1VjT0kzdDV5WTRLWmFzWUJURlppcUpjZ3hJaUNUVFJMCi8wTnFwZXVrTDBLSWordldNNTAyYklpV201Rmw2M2FRdnZSK2paV2dxRlRlTnFiaUUrYmE3K0U0aHNKZHBHN3kKUWpIVFFYZzJZY3VxU1pMUTlBM2dkZ3hxTzl3THY5SXZ3VUFzSFRoYXArbzZNdi9tUS8vT0lFcGt1OXBvdG5ZcApFTUREbnZkWVk4VmJBZ0dRS2h4elZvS1dYQ1ZLa0kvUUJibW01b1ZEUVpkZkwvSWZqWFh5MkZDSnlkTHVaMjlFCnFKVlViSFZ0QmVRSkp2V25ydlNTYlovRWRsTC8yZ28wU0xQdVlWZTBNOHpldnluNkVnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=anji@master:~/wipro$ 

anji@master:~/wipro$ vi config
anji@master:~/wipro$ cat config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1USXlPREEyTXprMU1sb1hEVE15TVRJeU5UQTJNemsxTWxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHA2ClA2ZlhNMTVFR0hhaGFnQTNtbnp0NUkyYXFUWmlBbFhFSEtKazdoRzRxaHYvSEdseC95cy8wUEZTVFdIZWMwSUkKeUdWT055V1BZV2xLMUZmWEM1OVVmVUEwN1dtdzNKcUdkYTcrNmI0ZXNjTituajBHV3lRQUZhMXBoVVFtRzIzNQpvSnFuTm5nekdIR1JDeDhpakJLQnBNQ0Z5NWpEYVA3WEJ3ejV0bWVvNDU5NVM2eHVYcTNvZGRkUGk3OVNnQmlMCjE3di9UNnRTNjFvbE8xQS9wNUUzOVFvSUxyMEpEQ0VlY2h0dXU3RXludlo5QlBSdm9aanF0REI5dTdWOTRuOTMKbnlNQjdWcDRMbnpJMEUxUjNWaCtFbXZVWlluQWo2NUgrMmxMWThKYWRZMTgxS2xNVHhQSG9SSGk3R0I2U0tpbApxOHdkZHowdmJGckppTE54amI4Q0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZDZndxRGZpeXVReCtFeHA5VTdQQTZ4M0JVb0FNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBQ0I4bjNyUUZDOGpBTkRyYkVGVApMaDloK3hrRHRaMm5kcXc3OWdiVk0xYVpZYlpKUmIvUzdVMGZ0U29MM1hKOWdxOEFpbXRMYTdlTmdEZENCbGZjCnVhaXV1dUFieHl1enByMGxiK01ZclpyV0NUNGcvZ3ZkaTd3cmZBUUZycjlNSWUwb0plaHdYdE5lUzdJZXlORHkKNTJHSnloNFJERHpwMnlIa3lTU0RoM1NvYjhJZTZMSUpqL0pzZ2pJL3hmRWt2cXczdmVDRnkrZ1poek0zdG1FYQppSndmcnBnZldoMVRrcG5sNmFiZ21EWS9iTXd5UUt0YnY0dWptRnpoZ3ptNjg1WWovTStETU5RU2JaZ2ZzZDRlClMxc3BweWdibVkwRllwaWxjVlFZUWtodE9KSm9vZWJBaE00Y0VWRXR2UE9iVnZzdVpzWk1QM3RYRWFscWkwRUcKTUk4PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://192.168.122.34:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: wipro
  name: wipro-context
current-context: wipro-context
kind: Config
preferences: {}
users:
- name: wipro
  user:
    client-certificate-data:  LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4VENDQWRrQ0ZIVUZFNEhhZTRJMGQwY05EV244MmpnazhSSCtNQTBHQ1NxR1NJYjNEUUVCQ3dVQU1CVXgKRXpBUkJnTlZCQU1UQ210MVltVnlibVYwWlhNd0hoY05Nakl4TWpNd01EWTFOekUzV2hjTk1qTXhNak13TURZMQpOekUzV2pCVk1Rc3dDUVlEVlFRR0V3SkJWVEVUTUJFR0ExVUVDQXdLVTI5dFpTMVRkR0YwWlRFaE1COEdBMVVFCkNnd1lTVzUwWlhKdVpYUWdWMmxrWjJsMGN5QlFkSGtnVEhSa01RNHdEQVlEVlFRRERBVjNhWEJ5YnpDQ0FTSXcKRFFZSktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQU81Wll6NEU2Y3Q2V1VPclF6NHFENW9SeW41Twp6eHlCQVluMlorVmdWdVd4WlZMM3J4OFZ5MDh0VmxBVWRPbkNkWFplZzJrQm9GNW9nd0J0ZDZkWmJ4NXkyYWpvCnlWc3JqcWw0aTNMRVoxOGZIZ3UrNVo5UXNBdDlXQVhkV213RVJZaStKZWZ6VDNvdU5zZUM2YTV2dE90ODBUTVoKcVgwL1k3OVBRZFRsTU9iVExDRXc0WmxyeG1LUWRkQXJjZHhicmpRVS9QZXJaWEVNQklxMUI3OUxJVEVOOE1sYwpqV2NpdUVuY012alVtemJVUldnR1o1QS9MY2hPUlh4eUlFZS8zM09ESFFqQzkvd1lOU3BxcjdpRjFpVmNwK3RoCnQzcE43NzMxOUdhRmdxWUpPY0o4Mm5TNS9xRlB3MUx2VHBoNlJOZWxWNGYvZGFCQlAxUThybnJwREVNQ0F3RUEKQVRBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWYzR1ZMcHkyZVBlSUE4QzA0UVVxUjNEVzhaNzdCT2NFWHNIagpib0ZacHlBTklUWDV1L2VEc2VxSGJYS0UwdjRkWVdKT1VjT0kzdDV5WTRLWmFzWUJURlppcUpjZ3hJaUNUVFJMCi8wTnFwZXVrTDBLSWordldNNTAyYklpV201Rmw2M2FRdnZSK2paV2dxRlRlTnFiaUUrYmE3K0U0aHNKZHBHN3kKUWpIVFFYZzJZY3VxU1pMUTlBM2dkZ3hxTzl3THY5SXZ3VUFzSFRoYXArbzZNdi9tUS8vT0lFcGt1OXBvdG5ZcApFTUREbnZkWVk4VmJBZ0dRS2h4elZvS1dYQ1ZLa0kvUUJibW01b1ZEUVpkZkwvSWZqWFh5MkZDSnlkTHVaMjlFCnFKVlViSFZ0QmVRSkp2V25ydlNTYlovRWRsTC8yZ28wU0xQdVlWZTBNOHpldnluNkVnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    client-key-data: 
###3===-----
anji@master:~/wipro$ sudo cat wipro.key  | base64  -w0
[sudo] password for anji: 
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@master:~/wipro$ vi config
anji@master:~/wipro$ cat config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1USXlPREEyTXprMU1sb1hEVE15TVRJeU5UQTJNemsxTWxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHA2ClA2ZlhNMTVFR0hhaGFnQTNtbnp0NUkyYXFUWmlBbFhFSEtKazdoRzRxaHYvSEdseC95cy8wUEZTVFdIZWMwSUkKeUdWT055V1BZV2xLMUZmWEM1OVVmVUEwN1dtdzNKcUdkYTcrNmI0ZXNjTituajBHV3lRQUZhMXBoVVFtRzIzNQpvSnFuTm5nekdIR1JDeDhpakJLQnBNQ0Z5NWpEYVA3WEJ3ejV0bWVvNDU5NVM2eHVYcTNvZGRkUGk3OVNnQmlMCjE3di9UNnRTNjFvbE8xQS9wNUUzOVFvSUxyMEpEQ0VlY2h0dXU3RXludlo5QlBSdm9aanF0REI5dTdWOTRuOTMKbnlNQjdWcDRMbnpJMEUxUjNWaCtFbXZVWlluQWo2NUgrMmxMWThKYWRZMTgxS2xNVHhQSG9SSGk3R0I2U0tpbApxOHdkZHowdmJGckppTE54amI4Q0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZDZndxRGZpeXVReCtFeHA5VTdQQTZ4M0JVb0FNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBQ0I4bjNyUUZDOGpBTkRyYkVGVApMaDloK3hrRHRaMm5kcXc3OWdiVk0xYVpZYlpKUmIvUzdVMGZ0U29MM1hKOWdxOEFpbXRMYTdlTmdEZENCbGZjCnVhaXV1dUFieHl1enByMGxiK01ZclpyV0NUNGcvZ3ZkaTd3cmZBUUZycjlNSWUwb0plaHdYdE5lUzdJZXlORHkKNTJHSnloNFJERHpwMnlIa3lTU0RoM1NvYjhJZTZMSUpqL0pzZ2pJL3hmRWt2cXczdmVDRnkrZ1poek0zdG1FYQppSndmcnBnZldoMVRrcG5sNmFiZ21EWS9iTXd5UUt0YnY0dWptRnpoZ3ptNjg1WWovTStETU5RU2JaZ2ZzZDRlClMxc3BweWdibVkwRllwaWxjVlFZUWtodE9KSm9vZWJBaE00Y0VWRXR2UE9iVnZzdVpzWk1QM3RYRWFscWkwRUcKTUk4PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://192.168.122.34:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: wipro
  name: wipro-context
current-context: wipro-context
kind: Config
preferences: {}
users:
- name: wipro
  user:
    client-certificate-data:  LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4VENDQWRrQ0ZIVUZFNEhhZTRJMGQwY05EV244MmpnazhSSCtNQTBHQ1NxR1NJYjNEUUVCQ3dVQU1CVXgKRXpBUkJnTlZCQU1UQ210MVltVnlibVYwWlhNd0hoY05Nakl4TWpNd01EWTFOekUzV2hjTk1qTXhNak13TURZMQpOekUzV2pCVk1Rc3dDUVlEVlFRR0V3SkJWVEVUTUJFR0ExVUVDQXdLVTI5dFpTMVRkR0YwWlRFaE1COEdBMVVFCkNnd1lTVzUwWlhKdVpYUWdWMmxrWjJsMGN5QlFkSGtnVEhSa01RNHdEQVlEVlFRRERBVjNhWEJ5YnpDQ0FTSXcKRFFZSktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQU81Wll6NEU2Y3Q2V1VPclF6NHFENW9SeW41Twp6eHlCQVluMlorVmdWdVd4WlZMM3J4OFZ5MDh0VmxBVWRPbkNkWFplZzJrQm9GNW9nd0J0ZDZkWmJ4NXkyYWpvCnlWc3JqcWw0aTNMRVoxOGZIZ3UrNVo5UXNBdDlXQVhkV213RVJZaStKZWZ6VDNvdU5zZUM2YTV2dE90ODBUTVoKcVgwL1k3OVBRZFRsTU9iVExDRXc0WmxyeG1LUWRkQXJjZHhicmpRVS9QZXJaWEVNQklxMUI3OUxJVEVOOE1sYwpqV2NpdUVuY012alVtemJVUldnR1o1QS9MY2hPUlh4eUlFZS8zM09ESFFqQzkvd1lOU3BxcjdpRjFpVmNwK3RoCnQzcE43NzMxOUdhRmdxWUpPY0o4Mm5TNS9xRlB3MUx2VHBoNlJOZWxWNGYvZGFCQlAxUThybnJwREVNQ0F3RUEKQVRBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWYzR1ZMcHkyZVBlSUE4QzA0UVVxUjNEVzhaNzdCT2NFWHNIagpib0ZacHlBTklUWDV1L2VEc2VxSGJYS0UwdjRkWVdKT1VjT0kzdDV5WTRLWmFzWUJURlppcUpjZ3hJaUNUVFJMCi8wTnFwZXVrTDBLSWordldNNTAyYklpV201Rmw2M2FRdnZSK2paV2dxRlRlTnFiaUUrYmE3K0U0aHNKZHBHN3kKUWpIVFFYZzJZY3VxU1pMUTlBM2dkZ3hxTzl3THY5SXZ3VUFzSFRoYXArbzZNdi9tUS8vT0lFcGt1OXBvdG5ZcApFTUREbnZkWVk4VmJBZ0dRS2h4elZvS1dYQ1ZLa0kvUUJibW01b1ZEUVpkZkwvSWZqWFh5MkZDSnlkTHVaMjlFCnFKVlViSFZ0QmVRSkp2V25ydlNTYlovRWRsTC8yZ28wU0xQdVlWZTBNOHpldnluNkVnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    client-key-data:   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
-----"

root@master:/home/anji/wipro# cp config   /home/wipro/
wipro@master:~$ ll
total 28
drwxr-xr-x 2 wipro wipro 4096 Dec 30 13:44 ./
drwxr-xr-x 4 root  root  4096 Dec 30 11:38 ../
-rw-r--r-- 1 wipro wipro  220 Dec 30 11:38 .bash_logout
-rw-r--r-- 1 wipro wipro 3771 Dec 30 11:38 .bashrc
-rw------- 1 "root  root  5507 Dec 30 13:48 config"
-rw-r--r-- 1 wipro wipro  807 Dec 30 11:38 .profile

root@master:/home/anji/wipro# chown wipro:wipro  /home/wipro/config 
wipro@master:~$ ll
total 28
drwxr-xr-x 2 wipro wipro 4096 Dec 30 13:44 ./
drwxr-xr-x 4 root  root  4096 Dec 30 11:38 ../
-rw-r--r-- 1 wipro wipro  220 Dec 30 11:38 .bash_logout
-rw-r--r-- 1 wipro wipro 3771 Dec 30 11:38 .bashrc
-rw------- 1 "wipro wipro "5507 Dec 30 13:48 config
-rw-r--r-- 1 wipro wipro  807 Dec 30 11:38 .profile

wipro@master:~$ ls
config
wipro@master:~$ mkdir .kube
wipro@master:~$ ll
total 32
drwxr-xr-x 3 wipro wipro 4096 Dec 30 13:55 ./
drwxr-xr-x 4 root  root  4096 Dec 30 11:38 ../
-rw-r--r-- 1 wipro wipro  220 Dec 30 11:38 .bash_logout
-rw-r--r-- 1 wipro wipro 3771 Dec 30 11:38 .bashrc
-rw------- 1 wipro wipro 5507 Dec 30 13:48 config
drwxrwxr-x 2 wipro wipro 4096 Dec 30 13:55 .kube/
-rw-r--r-- 1 wipro wipro  807 Dec 30 11:38 .profile
wipro@master:~$ mv config .kube/
wipro@master:~$ ls -l
total 0
wipro@master:~$ cd .kube/
wipro@master:~/.kube$ ls
config

root@master:~# adduser cpu
anji@master:~/ssd$ sudo openssl genrsa  -out ssd.key 2048
anji@master:~/ssd$ sudo openssl req -new  -key ssd.key -out ssd.csr
anji@master:~/ssd$ sudo openssl x509 -req -in ssd.csr -CA  ca.crt -CAkey ca.key  -CAcreateserial -out ssd.crt  -days 365
anji@master:~/ssd$ sudo cp /home/anji/.kube/config  .
anji@master:~/ssd$ sudo nano config 
anji@master:~/ssd$ cat ssd.crt  | base64 -w0
anji@master:~/ssd$ sudo vi config 
 sudo cat ssd.key  | base64 -w0

anji@master:~/ssd$ sudo vi config 
anji@master:~/ssd$ sudo cp config  /home/ssd/
anji@master:~/ssd$ sudo mkdir  /home/ssd/.kube 
anji@master:~/ssd$ sudo mv  /home/ssd/config   /home/ssd/.kube/
anji@master:~/ssd$ sudo chown ssd:ssd /home/ssd/.kube/config 
anji@master:~$ kubectl create clusterrolebinding --help | less 

   kubectl create clusterrolebinding cluster-admin --clusterrole=cluster-admin --user=user1 
  kubectl create clusterrolebinding  ssd1 --clusterrole=cluster-admin  --user=ssd  "

{{}{{{}{{{{{}}}}}}}}{}====  AUTHENTICATION IS COMPLETED  "NEXT AUTHORIZATION"
+++++++++++///\/\/\/=\\/\\/\/\/\\//
wipro@master:~$ kubectl get nodes 
Error from server (Forbidden): nodes is forbidden: User "wipro" cannot list resource "nodes" in API group "" at the cluster scope

AUTHORIZATION IS MAINTAINED BY "RBAC"
  
 1.CLUSTER-ROLE
   A) cluster-admin if we attach this role to someone , that person will have root access on the cluster
   B) self-provisioner - user with this  role can create namespace and delete them
 2. PROJECT/LOCAL-ROLE = project level
    a) view
    b) edit
    c) admin - full- access
---""
THERE ARE 2 TYPES OF ROLE BINDINGS
 1).CLUSTERROLE == attached== user == clusterrole
 2.).PROJECT/Local role  ==== attached == user == role binding

 kubectl get rolebinding

============ "
https://kubernetes.io/docs/reference/access-authn-authz/rbac/
kubectl create rolebinding --help | less 

kubectl create rolebinding bob-admin-binding --clusterrole=admin --user=bob --namespace=acme

kubectl create rolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp --namespace=acme

kubectl create rolebinding myappnamespace-myapp-view-binding --clusterrole=view --serviceaccount=myappnamespace:myapp --namespace=acme

kubectl create clusterrolebinding root-cluster-admin-binding --clusterrole=cluster-admin --user=root

kubectl create clusterrolebinding kube-proxy-binding --clusterrole=system:node-proxier --user=system:kube-proxy

kubectl create clusterrolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp

=====
anji@master:~/wipro$ kubectl create rolebinding --help | less 
Create a role binding for a particular role or cluster role.
Examples:
  # Create a role binding for user1, user2, and group1 using the admin cluster role
  kubectl create rolebinding admin --clusterrole=admin --user=user1 --user=user2 --group=group1

=++++++++++
kubectl create rolebinding wipro1 --clusterrole=admin --user=user1 --user=user2 --group=group1

kubectl create clusterrolebinding wipro1  --clusterrole=cluster-admin --user=wipro  "

anji@master:~$ kubectl create clusterrolebinding wipro1  --clusterrole=cluster-admin --user=wipro 
clusterrolebinding.rbac.authorization.k8s.io/wipro1 created

"wipro"@master:~$ kubectl get nodes  -o wide --show-labels
NAME      STATUS   ROLES           AGE    VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME    LABELS
master    Ready    control-plane   2d2h   v1.25.5   192.168.122.34    <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.5.9   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
worker1   Ready    <none>          2d2h   v1.25.5   192.168.122.131   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.5.9   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker1,kubernetes.io/os=linux "
---"
anji@master:~$ kubectl delete  clusterrolebinding wipro1  
clusterrolebinding.rbac.authorization.k8s.io "wipro1" deleted

wipro@master:~$ kubectl get nodes 
Error from server (Forbidden): nodes is forbidden: User "wipro" cannot list resource "nodes" in API group "" at the cluster scope
=="
anji@master:~$ kubectl create clusterrolebinding wipro1  --clusterrole=cluster-admin --user=wipro 
clusterrolebinding.rbac.authorization.k8s.io/wipro1 created

anji@master:~$ kubectl get clusterrolebinding
NAME                                                   ROLE                                                                               AGE
cluster-admin                                          "ClusterRole/cluster-admin "                                                         2d3h
"kubeadm:get-nodes "                                     ClusterRole/kubeadm:get-nodes                                                      2d3h
kubeadm:kubelet-bootstrap                              ClusterRole/system:node-bootstrapper                                               2d3h
kubeadm:node-autoapprove-bootstrap                     ClusterRole/system:certificates.k8s.io:certificatesigningrequests:nodeclient       2d3h
kubeadm:node-autoapprove-certificate-rotation          ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   2d3h
kubeadm:node-proxier                                   ClusterRole/system:node-proxier                                                    2d3h
system:basic-user                                      ClusterRole/system:basic-user                                                      2d3h
system:controller:attachdetach-controller              ClusterRole/system:controller:attachdetach-controller                              2d3h
system:controller:certificate-controller               ClusterRole/system:controller:certificate-controller                               2d3h
system:controller:clusterrole-aggregation-controller   ClusterRole/system:controller:clusterrole-aggregation-controller                   2d3h
system:controller:cronjob-controller                   ClusterRole/system:controller:cronjob-controller                                   2d3h
system:controller:daemon-set-controller                ClusterRole/system:controller:daemon-set-controller                                2d3h
s"ystem:controller:deployment-controller                ClusterRole/system:controller:deployment-controller  "                              2d3h
system:controller:disruption-controller                ClusterRole/system:controller:disruption-controller                                2d3h
system:controller:endpoint-controller                  ClusterRole/system:controller:endpoint-controller                                  2d3h
system:controller:endpointslice-controller             ClusterRole/system:controller:endpointslice-controller                             2d3h
system:controller:endpointslicemirroring-controller    ClusterRole/system:controller:endpointslicemirroring-controller                    2d3h
system:controller:ephemeral-volume-controller          ClusterRole/system:controller:ephemeral-volume-controller                          2d3h
system:controller:expand-controller                    ClusterRole/system:controller:expand-controller                                    2d3h
system:controller:generic-garbage-collector            ClusterRole/system:controller:generic-garbage-collector                            2d3h
system:controller:horizontal-pod-autoscaler            ClusterRole/system:controller:horizontal-pod-autoscaler                            2d3h
system:controller:job-controller                       ClusterRole/system:controller:job-controller                                       2d3h
system:controller:namespace-controller                 ClusterRole/system:controller:namespace-controller                                 2d3h
system:controller:node-controller                      ClusterRole/system:controller:node-controller                                      2d3h
s"ystem:controller:persistent-volume-binder             ClusterRole/system:controller:persistent-volume-binder  "                           2d3h
system:controller:pod-garbage-collector                ClusterRole/system:controller:pod-garbage-collector                                2d3h
system:controller:pv-protection-controller             ClusterRole/system:controller:pv-protection-controller                             2d3h
system:controller:pvc-protection-controller            ClusterRole/system:controller:pvc-protection-controller                            2d3h
system:controller:replicaset-controller                ClusterRole/system:controller:replicaset-controller                                2d3h
system:controller:replication-controller               ClusterRole/system:controller:replication-controller                               2d3h
system:controller:resourcequota-controller             ClusterRole/system:controller:resourcequota-controller                             2d3h
system:controller:root-ca-cert-publisher               ClusterRole/system:controller:root-ca-cert-publisher                               2d3h
system:controller:route-controller                     ClusterRole/system:controller:route-controller                                     2d3h
system:controller:service-account-controller           ClusterRole/system:controller:service-account-controller                           2d3h
system:controller:service-controller                   ClusterRole/system:controller:service-controller                                   2d3h
system:controller:statefulset-controller               ClusterRole/system:controller:statefulset-controller                               2d3h
system:controller:ttl-after-finished-controller        ClusterRole/system:controller:ttl-after-finished-controller                        2d3h
system:controller:ttl-controller                       ClusterRole/system:controller:ttl-controller                                       2d3h
system:coredns                                         ClusterRole/system:coredns                                                         2d3h
system:discovery                                       ClusterRole/system:discovery                                                       2d3h
system:kube-controller-manager                         ClusterRole/system:kube-controller-manager                                         2d3h
system:kube-dns                                        ClusterRole/system:kube-dns                                                        2d3h
system:kube-scheduler                                  ClusterRole/system:kube-scheduler                                                  2d3h
system:monitoring                                      ClusterRole/system:monitoring                                                      2d3h
system:node                                            ClusterRole/system:node                                                            2d3h
system:node-proxier                                    ClusterRole/system:node-proxier                                                    2d3h
system:public-info-viewer                              ClusterRole/system:public-info-viewer                                              2d3h
system:service-account-issuer-discovery                ClusterRole/system:service-account-issuer-discovery                                2d3h
system:volume-scheduler                                ClusterRole/system:volume-scheduler                                                2d3h
"weave-net                                              ClusterRole/weave-net    "                                                          2d3h
"wipro1 "                                                ClusterRole/cluster-admin             "                                             2m8s
======+++++++++++""
anji@master:~$ kubectl delete  clusterrolebinding wipro1  
clusterrolebinding.rbac.authorization.k8s.io "wipro1" deleted
----
anji@master:~$ kubectl get clusterrole
NAME                                                                   CREATED AT
admin                                                                  2022-12-28T06:40:09Z
cluster-admin                                                          2022-12-28T06:40:09Z
edit                                                                   2022-12-28T06:40:09Z
kubeadm:get-nodes                                                      2022-12-28T06:40:11Z
system:aggregate-to-admin                                              2022-12-28T06:40:09Z
system:aggregate-to-edit                                               2022-12-28T06:40:09Z
system:aggregate-to-view                                               2022-12-28T06:40:09Z
system:auth-delegator                                                  2022-12-28T06:40:09Z
system:basic-user                                                      2022-12-28T06:40:09Z
system:certificates.k8s.io:certificatesigningrequests:nodeclient       2022-12-28T06:40:09Z
system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   2022-12-28T06:40:09Z
system:certificates.k8s.io:kube-apiserver-client-approver              2022-12-28T06:40:09Z
system:certificates.k8s.io:kube-apiserver-client-kubelet-approver      2022-12-28T06:40:09Z
system:certificates.k8s.io:kubelet-serving-approver                    2022-12-28T06:40:09Z
system:certificates.k8s.io:legacy-unknown-approver                     2022-12-28T06:40:09Z
system:controller:attachdetach-controller                              2022-12-28T06:40:09Z
system:controller:certificate-controller                               2022-12-28T06:40:09Z
system:controller:clusterrole-aggregation-controller                   2022-12-28T06:40:09Z
system:controller:cronjob-controller                                   2022-12-28T06:40:09Z
system:controller:daemon-set-controller                                2022-12-28T06:40:09Z
system:controller:deployment-controller                                2022-12-28T06:40:09Z
system:controller:disruption-controller                                2022-12-28T06:40:09Z
system:controller:endpoint-controller                                  2022-12-28T06:40:09Z
system:controller:endpointslice-controller                             2022-12-28T06:40:09Z
system:controller:endpointslicemirroring-controller                    2022-12-28T06:40:09Z
system:controller:ephemeral-volume-controller                          2022-12-28T06:40:09Z
system:controller:expand-controller                                    2022-12-28T06:40:09Z
system:controller:generic-garbage-collector                            2022-12-28T06:40:09Z
system:controller:horizontal-pod-autoscaler                            2022-12-28T06:40:09Z
system:controller:job-controller                                       2022-12-28T06:40:09Z
system:controller:namespace-controller                                 2022-12-28T06:40:09Z
system:controller:node-controller                                      2022-12-28T06:40:09Z
system:controller:persistent-volume-binder                             2022-12-28T06:40:09Z
system:controller:pod-garbage-collector                                2022-12-28T06:40:09Z
system:controller:pv-protection-controller                             2022-12-28T06:40:09Z
system:controller:pvc-protection-controller                            2022-12-28T06:40:09Z
system:controller:replicaset-controller                                2022-12-28T06:40:09Z
system:controller:replication-controller                               2022-12-28T06:40:09Z
system:controller:resourcequota-controller                             2022-12-28T06:40:09Z
system:controller:root-ca-cert-publisher                               2022-12-28T06:40:09Z
system:controller:route-controller                                     2022-12-28T06:40:09Z
system:controller:service-account-controller                           2022-12-28T06:40:09Z
system:controller:service-controller                                   2022-12-28T06:40:09Z
system:controller:statefulset-controller                               2022-12-28T06:40:09Z
system:controller:ttl-after-finished-controller                        2022-12-28T06:40:09Z
system:controller:ttl-controller                                       2022-12-28T06:40:09Z
system:coredns                                                         2022-12-28T06:40:12Z
system:discovery                                                       2022-12-28T06:40:09Z
system:heapster                                                        2022-12-28T06:40:09Z
system:kube-aggregator                                                 2022-12-28T06:40:09Z
system:kube-controller-manager                                         2022-12-28T06:40:09Z
system:kube-dns                                                        2022-12-28T06:40:09Z
system:kube-scheduler                                                  2022-12-28T06:40:09Z
system:kubelet-api-admin                                               2022-12-28T06:40:09Z
system:monitoring                                                      2022-12-28T06:40:09Z
system:node                                                            2022-12-28T06:40:09Z
system:node-bootstrapper                                               2022-12-28T06:40:09Z
system:node-problem-detector                                           2022-12-28T06:40:09Z
system:node-proxier                                                    2022-12-28T06:40:09Z
system:persistent-volume-provisioner                                   2022-12-28T06:40:09Z
system:public-info-viewer                                              2022-12-28T06:40:09Z
system:service-account-issuer-discovery                                2022-12-28T06:40:09Z
system:volume-scheduler                                                2022-12-28T06:40:09Z
view                                                                   2022-12-28T06:40:09Z
weave-net                                                              2022-12-28T06:43:21Z"
====+++++++# /\/\/\/\/\\\\\\\\\\=================/{}{}/\\/\/\/\\///
anji@master:~$ kubectl describe clusterrole
Name:         admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                       Non-Resource URLs  Resource Names  Verbs
  ---------                                       -----------------  --------------  -----
  leases.coordination.k8s.io                      []                 []              [create delete deletecollection get list patch update watch]
  rolebindings.rbac.authorization.k8s.io          []                 []              [create delete deletecollection get list patch update watch]
  roles.rbac.authorization.k8s.io                 []                 []              [create delete deletecollection get list patch update watch]
  configmaps                                      []                 []              [create delete deletecollection patch update get list watch]
  events                                          []                 []              [create delete deletecollection patch update get list watch]
  persistentvolumeclaims                          []                 []              [create delete deletecollection patch update get list watch]
  pods                                            []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers/scale                    []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers                          []                 []              [create delete deletecollection patch update get list watch]
  services                                        []                 []              [create delete deletecollection patch update get list watch]
  daemonsets.apps                                 []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps/scale                          []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps                                []                 []              [create delete deletecollection patch update get list watch]
  replicasets.apps/scale                          []                 []              [create delete deletecollection patch update get list watch]
  replicasets.apps                                []                 []              [create delete deletecollection patch update get list watch]
  statefulsets.apps/scale                         []                 []              [create delete deletecollection patch update get list watch]
  statefulsets.apps                               []                 []              [create delete deletecollection patch update get list watch]
  horizontalpodautoscalers.autoscaling            []                 []              [create delete deletecollection patch update get list watch]
  cronjobs.batch                                  []                 []              [create delete deletecollection patch update get list watch]
  jobs.batch                                      []                 []              [create delete deletecollection patch update get list watch]
  daemonsets.extensions                           []                 []              [create delete deletecollection patch update get list watch]
  deployments.extensions/scale                    []                 []              [create delete deletecollection patch update get list watch]
  deployments.extensions                          []                 []              [create delete deletecollection patch update get list watch]
  ingresses.extensions                            []                 []              [create delete deletecollection patch update get list watch]
  networkpolicies.extensions                      []                 []              [create delete deletecollection patch update get list watch]
  replicasets.extensions/scale                    []                 []              [create delete deletecollection patch update get list watch]
  replicasets.extensions                          []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers.extensions/scale         []                 []              [create delete deletecollection patch update get list watch]
  ingresses.networking.k8s.io                     []                 []              [create delete deletecollection patch update get list watch]
  networkpolicies.networking.k8s.io               []                 []              [create delete deletecollection patch update get list watch]
  poddisruptionbudgets.policy                     []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps/rollback                       []                 []              [create delete deletecollection patch update]
  deployments.extensions/rollback                 []                 []              [create delete deletecollection patch update]
  pods/eviction                                   []                 []              [create]
  serviceaccounts/token                           []                 []              [create]
  localsubjectaccessreviews.authorization.k8s.io  []                 []              [create]
  pods/attach                                     []                 []              [get list watch create delete deletecollection patch update]
  pods/exec                                       []                 []              [get list watch create delete deletecollection patch update]
  pods/portforward                                []                 []              [get list watch create delete deletecollection patch update]
  pods/proxy                                      []                 []              [get list watch create delete deletecollection patch update]
  secrets                                         []                 []              [get list watch create delete deletecollection patch update]
  services/proxy                                  []                 []              [get list watch create delete deletecollection patch update]
  bindings                                        []                 []              [get list watch]
  endpoints                                       []                 []              [get list watch]
  limitranges                                     []                 []              [get list watch]
  namespaces/status                               []                 []              [get list watch]
  namespaces                                      []                 []              [get list watch]
  persistentvolumeclaims/status                   []                 []              [get list watch]
  pods/log                                        []                 []              [get list watch]
  pods/status                                     []                 []              [get list watch]
  replicationcontrollers/status                   []                 []              [get list watch]
  resourcequotas/status                           []                 []              [get list watch]
  resourcequotas                                  []                 []              [get list watch]
  services/status                                 []                 []              [get list watch]
  controllerrevisions.apps                        []                 []              [get list watch]
  daemonsets.apps/status                          []                 []              [get list watch]
  deployments.apps/status                         []                 []              [get list watch]
  replicasets.apps/status                         []                 []              [get list watch]
  statefulsets.apps/status                        []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling/status     []                 []              [get list watch]
  cronjobs.batch/status                           []                 []              [get list watch]
  jobs.batch/status                               []                 []              [get list watch]
  endpointslices.discovery.k8s.io                 []                 []              [get list watch]
  daemonsets.extensions/status                    []                 []              [get list watch]
  deployments.extensions/status                   []                 []              [get list watch]
  ingresses.extensions/status                     []                 []              [get list watch]
  replicasets.extensions/status                   []                 []              [get list watch]
  ingresses.networking.k8s.io/status              []                 []              [get list watch]
  poddisruptionbudgets.policy/status              []                 []              [get list watch]
  serviceaccounts                                 []                 []              [impersonate create delete deletecollection patch update get list watch] "


Name:         cluster-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  *.*        []                 []              [*]
             [*]                []              [*]


Name:         edit
Labels:       kubernetes.io/bootstrapping=rbac-defaults
              rbac.authorization.k8s.io/aggregate-to-admin=true
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                    Non-Resource URLs  Resource Names  Verbs
  ---------                                    -----------------  --------------  -----
  leases.coordination.k8s.io                   []                 []              [create delete deletecollection get list patch update watch]
  configmaps                                   []                 []              [create delete deletecollection patch update get list watch]
  events                                       []                 []              [create delete deletecollection patch update get list watch]
  persistentvolumeclaims                       []                 []              [create delete deletecollection patch update get list watch]
  pods                                         []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers/scale                 []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers                       []                 []              [create delete deletecollection patch update get list watch]
  services                                     []                 []              [create delete deletecollection patch update get list watch]
  daemonsets.apps                              []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps/scale                       []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps                             []                 []              [create delete deletecollection patch update get list watch]
  replicasets.apps/scale                       []                 []              [create delete deletecollection patch update get list watch]
  replicasets.apps                             []                 []              [create delete deletecollection patch update get list watch]
  statefulsets.apps/scale                      []                 []              [create delete deletecollection patch update get list watch]
  statefulsets.apps                            []                 []              [create delete deletecollection patch update get list watch]
  horizontalpodautoscalers.autoscaling         []                 []              [create delete deletecollection patch update get list watch]
  cronjobs.batch                               []                 []              [create delete deletecollection patch update get list watch]
  jobs.batch                                   []                 []              [create delete deletecollection patch update get list watch]
  daemonsets.extensions                        []                 []              [create delete deletecollection patch update get list watch]
  deployments.extensions/scale                 []                 []              [create delete deletecollection patch update get list watch]
  deployments.extensions                       []                 []              [create delete deletecollection patch update get list watch]
  ingresses.extensions                         []                 []              [create delete deletecollection patch update get list watch]
  networkpolicies.extensions                   []                 []              [create delete deletecollection patch update get list watch]
  replicasets.extensions/scale                 []                 []              [create delete deletecollection patch update get list watch]
  replicasets.extensions                       []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers.extensions/scale      []                 []              [create delete deletecollection patch update get list watch]
  ingresses.networking.k8s.io                  []                 []              [create delete deletecollection patch update get list watch]
  networkpolicies.networking.k8s.io            []                 []              [create delete deletecollection patch update get list watch]
  poddisruptionbudgets.policy                  []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps/rollback                    []                 []              [create delete deletecollection patch update]
  deployments.extensions/rollback              []                 []              [create delete deletecollection patch update]
  pods/eviction                                []                 []              [create]
  serviceaccounts/token                        []                 []              [create]
  pods/attach                                  []                 []              [get list watch create delete deletecollection patch update]
  pods/exec                                    []                 []              [get list watch create delete deletecollection patch update]
  pods/portforward                             []                 []              [get list watch create delete deletecollection patch update]
  pods/proxy                                   []                 []              [get list watch create delete deletecollection patch update]
  secrets                                      []                 []              [get list watch create delete deletecollection patch update]
  services/proxy                               []                 []              [get list watch create delete deletecollection patch update]
  bindings                                     []                 []              [get list watch]
  endpoints                                    []                 []              [get list watch]
  limitranges                                  []                 []              [get list watch]
  namespaces/status                            []                 []              [get list watch]
  namespaces                                   []                 []              [get list watch]
  persistentvolumeclaims/status                []                 []              [get list watch]
  pods/log                                     []                 []              [get list watch]
  pods/status                                  []                 []              [get list watch]
  replicationcontrollers/status                []                 []              [get list watch]
  resourcequotas/status                        []                 []              [get list watch]
  resourcequotas                               []                 []              [get list watch]
  services/status                              []                 []              [get list watch]
  controllerrevisions.apps                     []                 []              [get list watch]
  daemonsets.apps/status                       []                 []              [get list watch]
  deployments.apps/status                      []                 []              [get list watch]
  replicasets.apps/status                      []                 []              [get list watch]
  statefulsets.apps/status                     []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling/status  []                 []              [get list watch]
  cronjobs.batch/status                        []                 []              [get list watch]
  jobs.batch/status                            []                 []              [get list watch]
  endpointslices.discovery.k8s.io              []                 []              [get list watch]
  daemonsets.extensions/status                 []                 []              [get list watch]
  deployments.extensions/status                []                 []              [get list watch]
  ingresses.extensions/status                  []                 []              [get list watch]
  replicasets.extensions/status                []                 []              [get list watch]
  ingresses.networking.k8s.io/status           []                 []              [get list watch]
  poddisruptionbudgets.policy/status           []                 []              [get list watch]
  serviceaccounts                              []                 []              [impersonate create delete deletecollection patch update get list watch]


Name:         kubeadm:get-nodes
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  nodes      []                 []              [get]


Name:         system:aggregate-to-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
              rbac.authorization.k8s.io/aggregate-to-admin=true
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                       Non-Resource URLs  Resource Names  Verbs
  ---------                                       -----------------  --------------  -----
  rolebindings.rbac.authorization.k8s.io          []                 []              [create delete deletecollection get list patch update watch]
  roles.rbac.authorization.k8s.io                 []                 []              [create delete deletecollection get list patch update watch]
  localsubjectaccessreviews.authorization.k8s.io  []                 []              [create]


Name:         system:aggregate-to-edit
Labels:       kubernetes.io/bootstrapping=rbac-defaults
              rbac.authorization.k8s.io/aggregate-to-edit=true
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                Non-Resource URLs  Resource Names  Verbs
  ---------                                -----------------  --------------  -----
  leases.coordination.k8s.io               []                 []              [create delete deletecollection get list patch update watch]
  configmaps                               []                 []              [create delete deletecollection patch update]
  events                                   []                 []              [create delete deletecollection patch update]
  persistentvolumeclaims                   []                 []              [create delete deletecollection patch update]
  pods                                     []                 []              [create delete deletecollection patch update]
  replicationcontrollers/scale             []                 []              [create delete deletecollection patch update]
  replicationcontrollers                   []                 []              [create delete deletecollection patch update]
  services                                 []                 []              [create delete deletecollection patch update]
  daemonsets.apps                          []                 []              [create delete deletecollection patch update]
  deployments.apps/rollback                []                 []              [create delete deletecollection patch update]
  deployments.apps/scale                   []                 []              [create delete deletecollection patch update]
  deployments.apps                         []                 []              [create delete deletecollection patch update]
  replicasets.apps/scale                   []                 []              [create delete deletecollection patch update]
  replicasets.apps                         []                 []              [create delete deletecollection patch update]
  statefulsets.apps/scale                  []                 []              [create delete deletecollection patch update]
  statefulsets.apps                        []                 []              [create delete deletecollection patch update]
  horizontalpodautoscalers.autoscaling     []                 []              [create delete deletecollection patch update]
  cronjobs.batch                           []                 []              [create delete deletecollection patch update]
  jobs.batch                               []                 []              [create delete deletecollection patch update]
  daemonsets.extensions                    []                 []              [create delete deletecollection patch update]
  deployments.extensions/rollback          []                 []              [create delete deletecollection patch update]
  deployments.extensions/scale             []                 []              [create delete deletecollection patch update]
  deployments.extensions                   []                 []              [create delete deletecollection patch update]
  ingresses.extensions                     []                 []              [create delete deletecollection patch update]
  networkpolicies.extensions               []                 []              [create delete deletecollection patch update]
  replicasets.extensions/scale             []                 []              [create delete deletecollection patch update]
  replicasets.extensions                   []                 []              [create delete deletecollection patch update]
  replicationcontrollers.extensions/scale  []                 []              [create delete deletecollection patch update]
  ingresses.networking.k8s.io              []                 []              [create delete deletecollection patch update]
  networkpolicies.networking.k8s.io        []                 []              [create delete deletecollection patch update]
  poddisruptionbudgets.policy              []                 []              [create delete deletecollection patch update]
  pods/eviction                            []                 []              [create]
  serviceaccounts/token                    []                 []              [create]
  pods/attach                              []                 []              [get list watch create delete deletecollection patch update]
  pods/exec                                []                 []              [get list watch create delete deletecollection patch update]
  pods/portforward                         []                 []              [get list watch create delete deletecollection patch update]
  pods/proxy                               []                 []              [get list watch create delete deletecollection patch update]
  secrets                                  []                 []              [get list watch create delete deletecollection patch update]
  services/proxy                           []                 []              [get list watch create delete deletecollection patch update]
  serviceaccounts                          []                 []              [impersonate create delete deletecollection patch update]


Name:         system:aggregate-to-view
Labels:       kubernetes.io/bootstrapping=rbac-defaults
              rbac.authorization.k8s.io/aggregate-to-view=true
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                    Non-Resource URLs  Resource Names  Verbs
  ---------                                    -----------------  --------------  -----
  bindings                                     []                 []              [get list watch]
  configmaps                                   []                 []              [get list watch]
  endpoints                                    []                 []              [get list watch]
  events                                       []                 []              [get list watch]
  limitranges                                  []                 []              [get list watch]
  namespaces/status                            []                 []              [get list watch]
  namespaces                                   []                 []              [get list watch]
  persistentvolumeclaims/status                []                 []              [get list watch]
  persistentvolumeclaims                       []                 []              [get list watch]
  pods/log                                     []                 []              [get list watch]
  pods/status                                  []                 []              [get list watch]
  pods                                         []                 []              [get list watch]
  replicationcontrollers/scale                 []                 []              [get list watch]
  replicationcontrollers/status                []                 []              [get list watch]
  replicationcontrollers                       []                 []              [get list watch]
  resourcequotas/status                        []                 []              [get list watch]
  resourcequotas                               []                 []              [get list watch]
  serviceaccounts                              []                 []              [get list watch]
  services/status                              []                 []              [get list watch]
  services                                     []                 []              [get list watch]
  controllerrevisions.apps                     []                 []              [get list watch]
  daemonsets.apps/status                       []                 []              [get list watch]
  daemonsets.apps                              []                 []              [get list watch]
  deployments.apps/scale                       []                 []              [get list watch]
  deployments.apps/status                      []                 []              [get list watch]
  deployments.apps                             []                 []              [get list watch]
  replicasets.apps/scale                       []                 []              [get list watch]
  replicasets.apps/status                      []                 []              [get list watch]
  replicasets.apps                             []                 []              [get list watch]
  statefulsets.apps/scale                      []                 []              [get list watch]
  statefulsets.apps/status                     []                 []              [get list watch]
  statefulsets.apps                            []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling/status  []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling         []                 []              [get list watch]
  cronjobs.batch/status                        []                 []              [get list watch]
  cronjobs.batch                               []                 []              [get list watch]
  jobs.batch/status                            []                 []              [get list watch]
  jobs.batch                                   []                 []              [get list watch]
  endpointslices.discovery.k8s.io              []                 []              [get list watch]
  daemonsets.extensions/status                 []                 []              [get list watch]
  daemonsets.extensions                        []                 []              [get list watch]
  deployments.extensions/scale                 []                 []              [get list watch]
  deployments.extensions/status                []                 []              [get list watch]
  deployments.extensions                       []                 []              [get list watch]
  ingresses.extensions/status                  []                 []              [get list watch]
  ingresses.extensions                         []                 []              [get list watch]
  networkpolicies.extensions                   []                 []              [get list watch]
  replicasets.extensions/scale                 []                 []              [get list watch]
  replicasets.extensions/status                []                 []              [get list watch]
  replicasets.extensions                       []                 []              [get list watch]
  replicationcontrollers.extensions/scale      []                 []              [get list watch]
  ingresses.networking.k8s.io/status           []                 []              [get list watch]
  ingresses.networking.k8s.io                  []                 []              [get list watch]
  networkpolicies.networking.k8s.io            []                 []              [get list watch]
  poddisruptionbudgets.policy/status           []                 []              [get list watch]
  poddisruptionbudgets.policy                  []                 []              [get list watch]


Name:         system:auth-delegator
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                  Non-Resource URLs  Resource Names  Verbs
  ---------                                  -----------------  --------------  -----
  tokenreviews.authentication.k8s.io         []                 []              [create]
  subjectaccessreviews.authorization.k8s.io  []                 []              [create]


Name:         system:basic-user
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                      Non-Resource URLs  Resource Names  Verbs
  ---------                                      -----------------  --------------  -----
  selfsubjectaccessreviews.authorization.k8s.io  []                 []              [create]
  selfsubjectrulesreviews.authorization.k8s.io   []                 []              [create]


Name:         system:certificates.k8s.io:certificatesigningrequests:nodeclient
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                                  Non-Resource URLs  Resource Names  Verbs
  ---------                                                  -----------------  --------------  -----
  certificatesigningrequests.certificates.k8s.io/nodeclient  []                 []              [create]


Name:         system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                                      Non-Resource URLs  Resource Names  Verbs
  ---------                                                      -----------------  --------------  -----
  certificatesigningrequests.certificates.k8s.io/selfnodeclient  []                 []              [create]


Name:         system:certificates.k8s.io:kube-apiserver-client-approver
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                    Non-Resource URLs  Resource Names                         Verbs
  ---------                    -----------------  --------------                         -----
  signers.certificates.k8s.io  []                 [kubernetes.io/kube-apiserver-client]  [approve]


Name:         system:certificates.k8s.io:kube-apiserver-client-kubelet-approver
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                    Non-Resource URLs  Resource Names                                 Verbs
  ---------                    -----------------  --------------                                 -----
  signers.certificates.k8s.io  []                 [kubernetes.io/kube-apiserver-client-kubelet]  [approve]


Name:         system:certificates.k8s.io:kubelet-serving-approver
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                    Non-Resource URLs  Resource Names                   Verbs
  ---------                    -----------------  --------------                   -----
  signers.certificates.k8s.io  []                 [kubernetes.io/kubelet-serving]  [approve]


Name:         system:certificates.k8s.io:legacy-unknown-approver
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                    Non-Resource URLs  Resource Names                  Verbs
  ---------                    -----------------  --------------                  -----
  signers.certificates.k8s.io  []                 [kubernetes.io/legacy-unknown]  [approve]


Name:         system:controller:attachdetach-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                         Non-Resource URLs  Resource Names  Verbs
  ---------                         -----------------  --------------  -----
  volumeattachments.storage.k8s.io  []                 []              [create delete get list watch]
  events                            []                 []              [create patch update]
  events.events.k8s.io              []                 []              [create patch update]
  nodes                             []                 []              [get list watch]
  csidrivers.storage.k8s.io         []                 []              [get list watch]
  csinodes.storage.k8s.io           []                 []              [get list watch]
  persistentvolumeclaims            []                 []              [list watch]
  persistentvolumes                 []                 []              [list watch]
  pods                              []                 []              [list watch]
  nodes/status                      []                 []              [patch update]


Name:         system:controller:certificate-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                                Non-Resource URLs  Resource Names                                 Verbs
  ---------                                                -----------------  --------------                                 -----
  signers.certificates.k8s.io                              []                 [kubernetes.io/kube-apiserver-client-kubelet]  [approve sign]
  events                                                   []                 []                                             [create patch update]
  events.events.k8s.io                                     []                 []                                             [create patch update]
  subjectaccessreviews.authorization.k8s.io                []                 []                                             [create]
  certificatesigningrequests.certificates.k8s.io           []                 []                                             [delete get list watch]
  signers.certificates.k8s.io                              []                 [kubernetes.io/kube-apiserver-client]          [sign]
  signers.certificates.k8s.io                              []                 [kubernetes.io/kubelet-serving]                [sign]
  signers.certificates.k8s.io                              []                 [kubernetes.io/legacy-unknown]                 [sign]
  certificatesigningrequests.certificates.k8s.io/approval  []                 []                                             [update]
  certificatesigningrequests.certificates.k8s.io/status    []                 []                                             [update]


Name:         system:controller:clusterrole-aggregation-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                               Non-Resource URLs  Resource Names  Verbs
  ---------                               -----------------  --------------  -----
  clusterroles.rbac.authorization.k8s.io  []                 []              [escalate get list patch update watch]


Name:         system:controller:cronjob-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                  Non-Resource URLs  Resource Names  Verbs
  ---------                  -----------------  --------------  -----
  jobs.batch                 []                 []              [create delete get list patch update watch]
  events                     []                 []              [create patch update]
  events.events.k8s.io       []                 []              [create patch update]
  pods                       []                 []              [delete list]
  cronjobs.batch             []                 []              [get list update watch]
  cronjobs.batch/finalizers  []                 []              [update]
  cronjobs.batch/status      []                 []              [update]


Name:         system:controller:daemon-set-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                         Non-Resource URLs  Resource Names  Verbs
  ---------                         -----------------  --------------  -----
  controllerrevisions.apps          []                 []              [create delete get list patch update watch]
  pods                              []                 []              [create delete list patch watch]
  events                            []                 []              [create patch update]
  events.events.k8s.io              []                 []              [create patch update]
  pods/binding                      []                 []              [create]
  daemonsets.apps                   []                 []              [get list watch]
  daemonsets.extensions             []                 []              [get list watch]
  nodes                             []                 []              [list watch]
  daemonsets.apps/finalizers        []                 []              [update]
  daemonsets.apps/status            []                 []              [update]
  daemonsets.extensions/finalizers  []                 []              [update]
  daemonsets.extensions/status      []                 []              [update]


Name:         system:controller:deployment-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                          Non-Resource URLs  Resource Names  Verbs
  ---------                          -----------------  --------------  -----
  replicasets.apps                   []                 []              [create delete get list patch update watch]
  replicasets.extensions             []                 []              [create delete get list patch update watch]
  events                             []                 []              [create patch update]
  events.events.k8s.io               []                 []              [create patch update]
  pods                               []                 []              [get list update watch]
  deployments.apps                   []                 []              [get list update watch]
  deployments.extensions             []                 []              [get list update watch]
  deployments.apps/finalizers        []                 []              [update]
  deployments.apps/status            []                 []              [update]
  deployments.extensions/finalizers  []                 []              [update]
  deployments.extensions/status      []                 []              [update]


Name:         system:controller:disruption-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                           Non-Resource URLs  Resource Names  Verbs
  ---------                           -----------------  --------------  -----
  events                              []                 []              [create patch update]
  events.events.k8s.io                []                 []              [create patch update]
  replicationcontrollers              []                 []              [get list watch]
  deployments.apps                    []                 []              [get list watch]
  replicasets.apps                    []                 []              [get list watch]
  statefulsets.apps                   []                 []              [get list watch]
  deployments.extensions              []                 []              [get list watch]
  replicasets.extensions              []                 []              [get list watch]
  poddisruptionbudgets.policy         []                 []              [get list watch]
  *.*/scale                           []                 []              [get]
  poddisruptionbudgets.policy/status  []                 []              [update]


Name:         system:controller:endpoint-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  endpoints             []                 []              [create delete get list update]
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  endpoints/restricted  []                 []              [create]
  pods                  []                 []              [get list watch]
  services              []                 []              [get list watch]


Name:         system:controller:endpointslice-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                        Non-Resource URLs  Resource Names  Verbs
  ---------                        -----------------  --------------  -----
  endpointslices.discovery.k8s.io  []                 []              [create delete get list update]
  events                           []                 []              [create patch update]
  events.events.k8s.io             []                 []              [create patch update]
  nodes                            []                 []              [get list watch]
  pods                             []                 []              [get list watch]
  services                         []                 []              [get list watch]
  services/finalizers              []                 []              [update]


Name:         system:controller:endpointslicemirroring-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                        Non-Resource URLs  Resource Names  Verbs
  ---------                        -----------------  --------------  -----
  endpointslices.discovery.k8s.io  []                 []              [create delete get list update]
  events                           []                 []              [create patch update]
  events.events.k8s.io             []                 []              [create patch update]
  endpoints                        []                 []              [get list watch]
  services                         []                 []              [get list watch]
  endpoints/finalizers             []                 []              [update]
  services/finalizers              []                 []              [update]


Name:         system:controller:ephemeral-volume-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources               Non-Resource URLs  Resource Names  Verbs
  ---------               -----------------  --------------  -----
  persistentvolumeclaims  []                 []              [create get list watch]
  events                  []                 []              [create patch update]
  events.events.k8s.io    []                 []              [create patch update]
  pods                    []                 []              [get list watch]
  pods/finalizers         []                 []              [update]


Name:         system:controller:expand-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                      Non-Resource URLs  Resource Names  Verbs
  ---------                      -----------------  --------------  -----
  events                         []                 []              [create patch update]
  events.events.k8s.io           []                 []              [create patch update]
  persistentvolumes              []                 []              [get list patch update watch]
  persistentvolumeclaims         []                 []              [get list watch]
  storageclasses.storage.k8s.io  []                 []              [get list watch]
  endpoints                      []                 []              [get]
  secrets                        []                 []              [get]
  services                       []                 []              [get]
  persistentvolumeclaims/status  []                 []              [patch update]


Name:         system:controller:generic-garbage-collector
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  *.*                   []                 []              [delete get list patch update watch]


Name:         system:controller:horizontal-pod-autoscaler
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                    Non-Resource URLs  Resource Names     Verbs
  ---------                                    -----------------  --------------     -----
  events                                       []                 []                 [create patch update]
  events.events.k8s.io                         []                 []                 [create patch update]
  horizontalpodautoscalers.autoscaling         []                 []                 [get list watch]
  *.custom.metrics.k8s.io                      []                 []                 [get list]
  *.external.metrics.k8s.io                    []                 []                 [get list]
  *.*/scale                                    []                 []                 [get update]
  services/proxy                               []                 [http:heapster:]   [get]
  services/proxy                               []                 [https:heapster:]  [get]
  pods                                         []                 []                 [list]
  pods.metrics.k8s.io                          []                 []                 [list]
  horizontalpodautoscalers.autoscaling/status  []                 []                 [update]


Name:         system:controller:job-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources              Non-Resource URLs  Resource Names  Verbs
  ---------              -----------------  --------------  -----
  pods                   []                 []              [create delete list patch watch]
  events                 []                 []              [create patch update]
  events.events.k8s.io   []                 []              [create patch update]
  jobs.batch             []                 []              [get list patch update watch]
  jobs.batch/finalizers  []                 []              [update]
  jobs.batch/status      []                 []              [update]


Name:         system:controller:namespace-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources            Non-Resource URLs  Resource Names  Verbs
  ---------            -----------------  --------------  -----
  *.*                  []                 []              [delete deletecollection get list]
  namespaces           []                 []              [delete get list watch]
  namespaces/finalize  []                 []              [update]
  namespaces/status    []                 []              [update]


Name:         system:controller:node-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                       Non-Resource URLs  Resource Names  Verbs
  ---------                       -----------------  --------------  -----
  clustercidrs.networking.k8s.io  []                 []              [create get list update]
  events                          []                 []              [create patch update]
  events.events.k8s.io            []                 []              [create patch update]
  nodes                           []                 []              [delete get list patch update]
  pods                            []                 []              [delete list]
  nodes/status                    []                 []              [patch update]
  pods/status                     []                 []              [update]


Name:         system:controller:persistent-volume-binder
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                      Non-Resource URLs  Resource Names  Verbs
  ---------                      -----------------  --------------  -----
  persistentvolumes              []                 []              [create delete get list update watch]
  pods                           []                 []              [create delete get list watch]
  endpoints                      []                 []              [create delete get update]
  services                       []                 []              [create delete get]
  events.events.k8s.io           []                 []              [create patch update]
  persistentvolumeclaims         []                 []              [get list update watch]
  storageclasses.storage.k8s.io  []                 []              [get list watch]
  nodes                          []                 []              [get list]
  secrets                        []                 []              [get]
  persistentvolumeclaims/status  []                 []              [update]
  persistentvolumes/status       []                 []              [update]
  events                         []                 []              [watch create patch update]


Name:         system:controller:pod-garbage-collector
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  pods       []                 []              [delete list watch]
  nodes      []                 []              [get list]


Name:         system:controller:pv-protection-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  persistentvolumes     []                 []              [get list update watch]


Name:         system:controller:pvc-protection-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources               Non-Resource URLs  Resource Names  Verbs
  ---------               -----------------  --------------  -----
  events                  []                 []              [create patch update]
  events.events.k8s.io    []                 []              [create patch update]
  persistentvolumeclaims  []                 []              [get list update watch]
  pods                    []                 []              [get list watch]


Name:         system:controller:replicaset-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                          Non-Resource URLs  Resource Names  Verbs
  ---------                          -----------------  --------------  -----
  pods                               []                 []              [create delete list patch watch]
  events                             []                 []              [create patch update]
  events.events.k8s.io               []                 []              [create patch update]
  replicasets.apps                   []                 []              [get list update watch]
  replicasets.extensions             []                 []              [get list update watch]
  replicasets.apps/finalizers        []                 []              [update]
  replicasets.apps/status            []                 []              [update]
  replicasets.extensions/finalizers  []                 []              [update]
  replicasets.extensions/status      []                 []              [update]


Name:         system:controller:replication-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                          Non-Resource URLs  Resource Names  Verbs
  ---------                          -----------------  --------------  -----
  pods                               []                 []              [create delete list patch watch]
  events                             []                 []              [create patch update]
  events.events.k8s.io               []                 []              [create patch update]
  replicationcontrollers             []                 []              [get list update watch]
  replicationcontrollers/finalizers  []                 []              [update]
  replicationcontrollers/status      []                 []              [update]


Name:         system:controller:resourcequota-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources              Non-Resource URLs  Resource Names  Verbs
  ---------              -----------------  --------------  -----
  events                 []                 []              [create patch update]
  events.events.k8s.io   []                 []              [create patch update]
  *.*                    []                 []              [list watch]
  resourcequotas/status  []                 []              [update]


Name:         system:controller:root-ca-cert-publisher
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  configmaps            []                 []              [create update]


Name:         system:controller:route-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  nodes                 []                 []              [list watch]
  nodes/status          []                 []              [patch]


Name:         system:controller:service-account-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  serviceaccounts       []                 []              [create]


Name:         system:controller:service-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  services              []                 []              [get list watch]
  nodes                 []                 []              [list watch]
  services/status       []                 []              [patch update]


Name:         system:controller:statefulset-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                     Non-Resource URLs  Resource Names  Verbs
  ---------                     -----------------  --------------  -----
  controllerrevisions.apps      []                 []              [create delete get list patch update watch]
  persistentvolumeclaims        []                 []              [create get]
  events                        []                 []              [create patch update]
  events.events.k8s.io          []                 []              [create patch update]
  statefulsets.apps             []                 []              [get list watch]
  pods                          []                 []              [list watch create delete get patch update]
  statefulsets.apps/finalizers  []                 []              [update]
  statefulsets.apps/status      []                 []              [update]


Name:         system:controller:ttl-after-finished-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  jobs.batch            []                 []              [delete get list watch]


Name:         system:controller:ttl-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  nodes                 []                 []              [list patch update watch]


Name:         system:coredns
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources                        Non-Resource URLs  Resource Names  Verbs
  ---------                        -----------------  --------------  -----
  nodes                            []                 []              [get]
  endpoints                        []                 []              [list watch]
  namespaces                       []                 []              [list watch]
  pods                             []                 []              [list watch]
  services                         []                 []              [list watch]
  endpointslices.discovery.k8s.io  []                 []              [list watch]


Name:         system:discovery
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
             [/api/*]           []              [get]
             [/api]             []              [get]
             [/apis/*]          []              [get]
             [/apis]            []              [get]
             [/healthz]         []              [get]
             [/livez]           []              [get]
             [/openapi/*]       []              [get]
             [/openapi]         []              [get]
             [/readyz]          []              [get]
             [/version/]        []              [get]
             [/version]         []              [get]


Name:         system:heapster
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources               Non-Resource URLs  Resource Names  Verbs
  ---------               -----------------  --------------  -----
  events                  []                 []              [get list watch]
  namespaces              []                 []              [get list watch]
  nodes                   []                 []              [get list watch]
  pods                    []                 []              [get list watch]
  deployments.extensions  []                 []              [get list watch]


Name:         system:kube-aggregator
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  endpoints  []                 []              [get list watch]
  services   []                 []              [get list watch]


Name:         system:kube-controller-manager
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                  Non-Resource URLs  Resource Names             Verbs
  ---------                                  -----------------  --------------             -----
  secrets                                    []                 []                         [create delete get update]
  serviceaccounts                            []                 []                         [create get update]
  events                                     []                 []                         [create patch update]
  events.events.k8s.io                       []                 []                         [create patch update]
  endpoints                                  []                 []                         [create]
  serviceaccounts/token                      []                 []                         [create]
  tokenreviews.authentication.k8s.io         []                 []                         [create]
  subjectaccessreviews.authorization.k8s.io  []                 []                         [create]
  leases.coordination.k8s.io                 []                 []                         [create]
  endpoints                                  []                 [kube-controller-manager]  [get update]
  leases.coordination.k8s.io                 []                 [kube-controller-manager]  [get update]
  configmaps                                 []                 []                         [get]
  namespaces                                 []                 []                         [get]
  *.*                                        []                 []                         [list watch]


Name:         system:kube-dns
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  endpoints  []                 []              [list watch]
  services   []                 []              [list watch]


Name:         system:kube-scheduler
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                  Non-Resource URLs  Resource Names    Verbs
  ---------                                  -----------------  --------------    -----
  events                                     []                 []                [create patch update]
  events.events.k8s.io                       []                 []                [create patch update]
  bindings                                   []                 []                [create]
  endpoints                                  []                 []                [create]
  pods/binding                               []                 []                [create]
  tokenreviews.authentication.k8s.io         []                 []                [create]
  subjectaccessreviews.authorization.k8s.io  []                 []                [create]
  leases.coordination.k8s.io                 []                 []                [create]
  pods                                       []                 []                [delete get list watch]
  namespaces                                 []                 []                [get list watch]
  nodes                                      []                 []                [get list watch]
  persistentvolumeclaims                     []                 []                [get list watch]
  persistentvolumes                          []                 []                [get list watch]
  replicationcontrollers                     []                 []                [get list watch]
  services                                   []                 []                [get list watch]
  replicasets.apps                           []                 []                [get list watch]
  statefulsets.apps                          []                 []                [get list watch]
  replicasets.extensions                     []                 []                [get list watch]
  poddisruptionbudgets.policy                []                 []                [get list watch]
  csidrivers.storage.k8s.io                  []                 []                [get list watch]
  csinodes.storage.k8s.io                    []                 []                [get list watch]
  csistoragecapacities.storage.k8s.io        []                 []                [get list watch]
  endpoints                                  []                 [kube-scheduler]  [get update]
  leases.coordination.k8s.io                 []                 [kube-scheduler]  [get update]
  pods/status                                []                 []                [patch update]


Name:         system:kubelet-api-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources      Non-Resource URLs  Resource Names  Verbs
  ---------      -----------------  --------------  -----
  nodes/log      []                 []              [*]
  nodes/metrics  []                 []              [*]
  nodes/proxy    []                 []              [*]
  nodes/spec     []                 []              [*]
  nodes/stats    []                 []              [*]
  nodes          []                 []              [get list watch proxy]


Name:         system:monitoring
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
             [/healthz/*]       []              [get]
             [/healthz]         []              [get]
             [/livez/*]         []              [get]
             [/livez]           []              [get]
             [/metrics]         []              [get]
             [/readyz/*]        []              [get]
             [/readyz]          []              [get]


Name:         system:node
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                       Non-Resource URLs  Resource Names  Verbs
  ---------                                       -----------------  --------------  -----
  leases.coordination.k8s.io                      []                 []              [create delete get patch update]
  csinodes.storage.k8s.io                         []                 []              [create delete get patch update]
  nodes                                           []                 []              [create get list watch patch update]
  certificatesigningrequests.certificates.k8s.io  []                 []              [create get list watch]
  events                                          []                 []              [create patch update]
  pods/eviction                                   []                 []              [create]
  serviceaccounts/token                           []                 []              [create]
  tokenreviews.authentication.k8s.io              []                 []              [create]
  localsubjectaccessreviews.authorization.k8s.io  []                 []              [create]
  subjectaccessreviews.authorization.k8s.io       []                 []              [create]
  pods                                            []                 []              [get list watch create delete]
  configmaps                                      []                 []              [get list watch]
  secrets                                         []                 []              [get list watch]
  services                                        []                 []              [get list watch]
  runtimeclasses.node.k8s.io                      []                 []              [get list watch]
  csidrivers.storage.k8s.io                       []                 []              [get list watch]
  persistentvolumeclaims/status                   []                 []              [get patch update]
  endpoints                                       []                 []              [get]
  persistentvolumeclaims                          []                 []              [get]
  persistentvolumes                               []                 []              [get]
  volumeattachments.storage.k8s.io                []                 []              [get]
  nodes/status                                    []                 []              [patch update]
  pods/status                                     []                 []              [patch update]


Name:         system:node-bootstrapper
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                       Non-Resource URLs  Resource Names  Verbs
  ---------                                       -----------------  --------------  -----
  certificatesigningrequests.certificates.k8s.io  []                 []              [create get list watch]


Name:         system:node-problem-detector
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  nodes                 []                 []              [get]
  nodes/status          []                 []              [patch]


Name:         system:node-proxier
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                        Non-Resource URLs  Resource Names  Verbs
  ---------                        -----------------  --------------  -----
  events                           []                 []              [create patch update]
  events.events.k8s.io             []                 []              [create patch update]
  nodes                            []                 []              [get list watch]
  endpoints                        []                 []              [list watch]
  services                         []                 []              [list watch]
  endpointslices.discovery.k8s.io  []                 []              [list watch]


Name:         system:persistent-volume-provisioner
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                      Non-Resource URLs  Resource Names  Verbs
  ---------                      -----------------  --------------  -----
  persistentvolumes              []                 []              [create delete get list watch]
  events.events.k8s.io           []                 []              [create patch update]
  persistentvolumeclaims         []                 []              [get list update watch]
  storageclasses.storage.k8s.io  []                 []              [get list watch]
  events                         []                 []              [watch create patch update]


Name:         system:public-info-viewer
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
             [/healthz]         []              [get]
             [/livez]           []              [get]
             [/readyz]          []              [get]
             [/version/]        []              [get]
             [/version]         []              [get]


Name:         system:service-account-issuer-discovery
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs                    Resource Names  Verbs
  ---------  -----------------                    --------------  -----
             [/.well-known/openid-configuration]  []              [get]
             [/openid/v1/jwks]                    []              [get]


Name:         system:volume-scheduler
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                      Non-Resource URLs  Resource Names  Verbs
  ---------                      -----------------  --------------  -----
  persistentvolumeclaims         []                 []              [get list patch update watch]
  persistentvolumes              []                 []              [get list patch update watch]
  storageclasses.storage.k8s.io  []                 []              [get list watch]


Name:         view
Labels:       kubernetes.io/bootstrapping=rbac-defaults
              rbac.authorization.k8s.io/aggregate-to-edit=true
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                    Non-Resource URLs  Resource Names  Verbs
  ---------                                    -----------------  --------------  -----
  bindings                                     []                 []              [get list watch]
  configmaps                                   []                 []              [get list watch]
  endpoints                                    []                 []              [get list watch]
  events                                       []                 []              [get list watch]
  limitranges                                  []                 []              [get list watch]
  namespaces/status                            []                 []              [get list watch]
  namespaces                                   []                 []              [get list watch]
  persistentvolumeclaims/status                []                 []              [get list watch]
  persistentvolumeclaims                       []                 []              [get list watch]
  pods/log                                     []                 []              [get list watch]
  pods/status                                  []                 []              [get list watch]
  pods                                         []                 []              [get list watch]
  replicationcontrollers/scale                 []                 []              [get list watch]
  replicationcontrollers/status                []                 []              [get list watch]
  replicationcontrollers                       []                 []              [get list watch]
  resourcequotas/status                        []                 []              [get list watch]
  resourcequotas                               []                 []              [get list watch]
  serviceaccounts                              []                 []              [get list watch]
  services/status                              []                 []              [get list watch]
  services                                     []                 []              [get list watch]
  controllerrevisions.apps                     []                 []              [get list watch]
  daemonsets.apps/status                       []                 []              [get list watch]
  daemonsets.apps                              []                 []              [get list watch]
  deployments.apps/scale                       []                 []              [get list watch]
  deployments.apps/status                      []                 []              [get list watch]
  deployments.apps                             []                 []              [get list watch]
  replicasets.apps/scale                       []                 []              [get list watch]
  replicasets.apps/status                      []                 []              [get list watch]
  replicasets.apps                             []                 []              [get list watch]
  statefulsets.apps/scale                      []                 []              [get list watch]
  statefulsets.apps/status                     []                 []              [get list watch]
  statefulsets.apps                            []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling/status  []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling         []                 []              [get list watch]
  cronjobs.batch/status                        []                 []              [get list watch]
  cronjobs.batch                               []                 []              [get list watch]
  jobs.batch/status                            []                 []              [get list watch]
  jobs.batch                                   []                 []              [get list watch]
  endpointslices.discovery.k8s.io              []                 []              [get list watch]
  daemonsets.extensions/status                 []                 []              [get list watch]
  daemonsets.extensions                        []                 []              [get list watch]
  deployments.extensions/scale                 []                 []              [get list watch]
  deployments.extensions/status                []                 []              [get list watch]
  deployments.extensions                       []                 []              [get list watch]
  ingresses.extensions/status                  []                 []              [get list watch]
  ingresses.extensions                         []                 []              [get list watch]
  networkpolicies.extensions                   []                 []              [get list watch]
  replicasets.extensions/scale                 []                 []              [get list watch]
  replicasets.extensions/status                []                 []              [get list watch]
  replicasets.extensions                       []                 []              [get list watch]
  replicationcontrollers.extensions/scale      []                 []              [get list watch]
  ingresses.networking.k8s.io/status           []                 []              [get list watch]
  ingresses.networking.k8s.io                  []                 []              [get list watch]
  networkpolicies.networking.k8s.io            []                 []              [get list watch]
  poddisruptionbudgets.policy/status           []                 []              [get list watch]
  poddisruptionbudgets.policy                  []                 []              [get list watch]


Name:         weave-net
Labels:       name=weave-net
Annotations:  <none>
PolicyRule:
  Resources                          Non-Resource URLs  Resource Names  Verbs
  ---------                          -----------------  --------------  -----
  namespaces                         []                 []              [get list watch]
  nodes                              []                 []              [get list watch]
  pods                               []                 []              [get list watch]
  networkpolicies.extensions         []                 []              [get list watch]
  networkpolicies.networking.k8s.io  []                 []              [get list watch]
  nodes/status                       []                 []              [patch update]
###################################################################++==++++++++++++++++++++++++++++++=========================
anji@master:~$ kubectl describe clusterrole  cluster-admin
Name:         cluster-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  *.*        []                 []              [*]
             [*]                []              [*]

=====
anji@master:~$ kubectl describe clusterrole  view

anji@master:~$ kubectl describe clusterrole  edit

anji@master:~$ kubectl describe clusterrole  admin


====++++++++
anji@master:~$ kubectl create namespace  godfather
namespace/godfather created
anji@master:~$ kubectl get  ns
NAME              STATUS   AGE
default           Active   2d6h
godfather         Active   7s
kube-node-lease   Active   2d6h
kube-public       Active   2d6h
kube-system       Active   2d6h

anji@master:~$ kubectl create deployment nginx  --image=nginx -n godfather
deployment.apps/nginx created

anji@master:~$ kubectl get all 
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   2d6h
anji@master:~$ kubectl get all  -n godfather
NAME                        READY   STATUS    RESTARTS   AGE
pod/nginx-76d6c9b8c-99tnd   1/1     Running   0          33s

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx   1/1     1            1           33s

NAME                              DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-76d6c9b8c   1         1         1       33s
======-----{}{]]]}  =---""  
-------------------------------------------------------------------------------------------------------------"
wipro@master:~$ kubectl get all -n godfather 
Error from server (Forbidden): "pods "is forbidden: User "wipro" cannot list resource "pods" in API group "" in the namespace "godfather"
Error from server (Forbidden): "replicationcontrollers "is forbidden: User "wipro" cannot list resource "replicationcontrollers" in API group "" in the namespace "godfather"
Error from server (Forbidden): s"ervices is forbidden": User "wipro" cannot list resource "services" in API group "" in the namespace "godfather"
Error from server (Forbidden): d"aemonsets.apps" is forbidden: User "wipro" cannot list resource "daemonsets" in API group "apps" in the namespace "godfather"
Error from server (Forbidden): "deployments."apps is forbidden: User "wipro" cannot list resource "deployments" in API group "apps" in the namespace "godfather"
Error from server (Forbidden): r"eplicasets."apps is forbidden: User "wipro" cannot list resource "replicasets" in API group "apps" in the namespace "godfather"
Error from server (Forbidden): "statefulsets."apps is forbidden: User "wipro" cannot list resource "statefulsets" in API group "apps" in the namespace "godfather"
Error from server (Forbidden): "horizontalpodautoscalers".autoscaling is forbidden: User "wipro" cannot list resource "horizontalpodautoscalers" in API group "autoscaling" in the namespace "godfather"
Error from server (Forbidden): "cronjobs."batch is forbidden: User "wipro" cannot list resource "cronjobs" in API group "batch" in the namespace "godfather"
Error from server (Forbidden): j"obs."batch is forbidden: User "wipro" cannot list resource "jobs" in API group "batch" in the namespace "godfather" "

kubectl create rolebinding --help | less

  kubectl create rolebinding admin --clusterrole=admin --user=user1 --user=user2 --group=group1

anji@master:~$   kubectl create rolebinding admin --clusterrole=admin --user=user1 --user=user2 --group=group1
rolebinding.rbac.authorization.k8s.io/admin created
anji@master:~$  kubectl delete rolebinding admin 
'rolebinding.rbac.authorization.k8s.io "admin" deleted


anji@master:~$   kubectl create rolebinding wipro  --clusterrole=view --user=wipro -n godfather
rolebinding.rbac.authorization.k8s.io/wipro created

====\\\\\\\\\\\
wipro@master:~$ kubectl get all -n godfather
NAME                        READY   STATUS    RESTARTS      AGE
pod/nginx-76d6c9b8c-99tnd   1/1     Running   1 (50m ago)   70m

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx   1/1     1            1           70m

NAME                              DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-76d6c9b8c   1         1         1       70m
=----
anji@master:~$ kubectl get rolebinding  
No resources found in default namespace.

anji@master:~$ kubectl get rolebinding  -n godfather  -o wide 
NAME    ROLE               AGE     USERS   GROUPS   SERVICEACCOUNTS
wipro   ClusterRole/view   3m58s   wipro 
=======-------------////////
wipro@master:~$ kubectl edit deployment nginx -n godfather 
error: deployments.apps "nginx" could not be patched: deployments.apps "nginx" is forbidden: User "wipro" cannot patch resource "deployments" in API group "apps" in the namespace "godfather"
You can run `kubectl replace -f /tmp/kubectl-edit-1600269644.yaml` to try this update again.

anji@master:~$ kubectl delete  rolebinding wipro -n godfather
rolebinding.rbac.authorization.k8s.io "wipro" deleted

nji@master:~$ kubectl create rolebinding wipro1 --clusterrole=edit --user=wipro -n godfather 
rolebinding.rbac.authorization.k8s.io/wipro1 created

anji@master:~$ kubectl get rolebinding -n godfather  -o wide 
NAME     ROLE               AGE   USERS   GROUPS   SERVICEACCOUNTS
wipro1   ClusterRole/edit   79s   wipro            
========================#########
anji@master:~$ kubectl get all -n godfather -o wide 
NAME                        READY   STATUS    RESTARTS      AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/nginx-76d6c9b8c-99tnd   1/1     Running   1 (67m ago)   86m   10.44.0.3   worker1   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES   SELECTOR
deployment.apps/nginx   1/1     1            1           86m   nginx        nginx    app=nginx

NAME                              DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR
replicaset.apps/nginx-76d6c9b8c   1         1         1       86m   nginx        nginx    app=nginx,pod-template-hash=76d6c9b8c
anji@master:~$ kubectl get all -n godfather -o wide 
----=========+++++++++++
anji@master:~$ kubectl get all -n godfather -o wide 
NAME                        READY   STATUS    RESTARTS      AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/nginx-76d6c9b8c-99tnd   1/1     Running   1 (67m ago)   86m   10.44.0.3   worker1   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES   SELECTOR
deployment.apps/nginx   1/1     1            1           86m   nginx        nginx    app=nginx

NAME                              DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR
replicaset.apps/nginx-76d6c9b8c   1         1         1       86m   nginx        nginx    app=nginx,pod-template-hash=76d6c9b8c "
#########################################################################################################################"
""=============////////////\\\\\\\\\=\\
https://www.youtube.com/watch?v=rXTpVGIENAQ&list=PL8SR-mMnL9f6KKYDnrG3saVnyLqNDz1GP&index=17
Kubernetes-Day-17
Kubernetes - Secrets and Configmaps

https://medium.com/open-devops-academy/deploy-a-mysql-db-on-kubernetes-b268333a4881
https://bluexp.netapp.com/blog/how-to-set-up-mysql-kubernetes-deployments
https://github.com/kubernetes/examples/blob/master/mysql-wordpress-pd/mysql-deployment.yaml
https://dev.to/musolemasu/deploy-a-mysql-database-server-in-kubernetes-static-dpc
https://phoenixnap.com/kb/kubernetes-mysql


apiVersion: apps/v1
kind: Deployment
metadata: 
   name: mysql
   labels: 
     app: mysql
spec: 
  selector:
    matchLabels:
      app: mysql
      tier: mysql
  strategy:
     type: Recreate
  template:
     metadata: 
       labels: 
         app: mysql
         tier: mysql
     spec: 
       containers: 
       - name: mysql
         image: mysql:5.6
         env:                            #\\\ see look 
          -  name: MYSQL_ROOT_PASSWORD
             value: redhat                   ## look see 
         ports: 
         - containerPort: 3306
           name: mysql
"
anji@master:~$ kubectl exec mysql-cc4cbfff4-99nh2  -it -- /bin/bash

root@mysql-cc4cbfff4-99nh2:/# mysql -p
Enter password: redhat
mysql> 
==========="
anji@master:~$ kubectl get po
NAME                    READY   STATUS    RESTARTS   AGE
mysql-cc4cbfff4-99nh2   1/1     Running   0          80m

anji@master:~$ kubectl describe pod mysql-cc4cbfff4-99nh2 
Name:             mysql-cc4cbfff4-99nh2

Node:             worker1/192.168.122.131
Start Time:       Sun, 01 Jan 2023 15:21:43 +0530
Labels:           app=mysql
                  pod-template-hash=cc4cbfff4
                  tier=mysql
Controlled By:  ReplicaSet/mysql-cc4cbfff4
Containers:
  mysql:
    Image:          mysql:5.6
    Port:           3306/TCP
    Environment:
      MYSQL_ROOT_PASSWORD:  redhat  #////  see look 
=========------"
anji@master:~/test$ kubectl create secret generic book --from-literal=password=redhat
secret/book created

anji@master:~/test$ kubectl get secret
NAME   TYPE     DATA   AGE
book   Opaque   1      16s

anji@master:~/test$ kubectl describe secret
Name:         book
Namespace:    default
Labels:       <none>
Annotations:  <none>
Type:  Opaque
Data
====
password:  6 bytes
====----"
anji@master:~$ kubectl edit secret
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  password: cmVkaGF0         ## look see  encode base64 
kind: Secret
metadata:
  creationTimestamp: "2023-01-01T11:26:01Z"
  name: book
  namespace: default
  resourceVersion: "242656"
  uid: a1461a2a-6e60-42a9-99a5-7616c04af01e
type: Opaque
----===/////
anji@master:~$ echo "redhat" | base64 -w0
cmVkaGF0

anji@master:~$ echo "anjireddy"  | base64
YW5qaXJlZGR5Cg==

anji@master:~$ echo "venkata" | base64
dmVua2F0YQo=

anji@master:~$ echo "a" | base64
YQo=

=========================================+++++++++++++++++++++++"
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: mysql
   labels: 
     app: mysql
spec: 
   selector: 
     matchLabels:
        app: mysql
        tier: mysql
   strategy: 
      type: Recreate
   template: 
      metadata: 
        labels: 
           app: mysql
           tier: mysql
      spec:
        containers:
        - image: mysql
          name: mysql
          env: 
          - name: MYSQL_ROOT_PASSWORD     
            valueFrom:                    ## see look 
              secretKeyRef:            /// see look 
                name: book             ## book 
                key: password
"                                  
anji@master:~$ kubectl get po,secret  -o wide 
NAME                         READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/mysql-85b96fd6d9-zntqr   1/1     Running   0          3s    10.44.0.3   worker1   <none>           <none>

NAME          TYPE     DATA   AGE
"secret/book "  Opaque   1      47m "

anji@master:~$ kubectl describe secret  book 
Name:         book
Namespace:    default
Labels:       <none>
Annotations:  <none>
Type:  Opaque
Data
====
password:  6 bytes "
--------------------
anji@master:~$ kubectl describe pod mysql-85b96fd6d9-zntqr 
Name:             mysql-85b96fd6d9-zntqr
Labels:           app=mysql
                  pod-template-hash=85b96fd6d9
                  tier=mysql
Controlled By:  ReplicaSet/mysql-85b96fd6d9
Containers:
  mysql:
    Image:          mysql                                         ||  see look 
    Environment:
      MYSQL_ROOT_PASSWORD: " <set to the key 'password' in secret 'book'>  Optional: false"

-----=== ## @@@@  BEFORE  BEFORE ---==== ### 
anji@master:~$ kubectl edit secret  BOOK

# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  password: cmVkaGF0    ##  SEE LOOK IF  CHANDED POD IS RE-STARTED 
kind: Secret
metadata:
  creationTimestamp: "2023-01-01T11:26:01Z"
  name: book
  namespace: default
  resourceVersion: "242656"
  uid: a1461a2a-6e60-42a9-99a5-7616c04af01e
type: Opaque
~            
++++ "
anji@master:~/test$ echo  "anjireddy" | base64
YW5qaXJlZGR5Cg==                                  ###  REPLACE THE VALUE 
####"
anji@master:~$ kubectl get po,secret
NAME                         READY   STATUS    RESTARTS   AGE
pod/"mysql-85b96fd6d9-zntqr "  1/1     Running   0          31m     ## restart restart restart 

NAME          TYPE     DATA   AGE
secret/book   Opaque   1      78m
-----   ##  AFTER  AFTER   AFTER  ===  
nji@master:~$ kubectl edit secret  BOOK

# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  password: YW5qaXJlZGR5Cg==   ##  SEE LOOK IF  CHANDED POD IS RE-STARTED 
kind: Secret
metadata:
  creationTimestamp: "2023-01-01T11:26:01Z"
  name: book
  namespace: default
  resourceVersion: "242656"
  uid: a1461a2a-6e60-42a9-99a5-7616c04af01e
type: Opaque

anji@master:~$ kubectl get all 
NAME                         READY   STATUS    RESTARTS   AGE
pod/mysql-85b96fd6d9-zntqr   1/1     Running   0          53m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d6h

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/mysql   1/1     1            1           53m

NAME                               DESIRED   CURRENT   READY   AGE
replicaset.apps/mysql-85b96fd6d9   1         1         1       53m
===============+++++++++++++++++#############################"
" \\\\\\\\\///  configmap  
anji@master:~$ kubectl create configmap devops --from-literal=password=sony
configmap/devops created

anji@master:~$ kubectl get configmap  -o wide 
NAME               DATA   AGE
devops             1      29s
kube-root-ca.crt   1      4d6h

anji@master:~$ kubectl describe configmap devops  "
Name:         devops
Namespace:    default
Labels:       <none>
Annotations:  <none>
Data
====
password:
----
sony       ##  is password is showing appeared 
BinaryData
====
Events:  <none>
++++++++++++++== "
anji@master:~$ kubectl edit configmap devops
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  password: sony    "##  is password is appeared  "
kind: ConfigMap     
metadata:
  creationTimestamp: "2023-01-01T13:24:26Z"
  name: devops
  namespace: default
  resourceVersion: "251914"
  uid: 9bb15c73-18f8-4a15-a040-530368c8adbd
++++++++++++++++++===

apiVersion: apps/v1
kind: Deployment
metadata: 
   name: mysql
   labels: 
      app: mysql
spec: 
  selector: 
     matchLabels: 
       app: mysql
       tier: mysql
  strategy: 
     type: Recreate
  template: 
     metadata:
        labels:
          app: mysql
          tier: mysql
     spec: 
       containers:
       - name: mysql
         image: mysql
         env:
         - name: MYSQL_ROOT_PASSWORD
           valueFrom:  
              configMapKeyRef:
                 name: devops
                 key: password
"                                      
anji@master:~$ kubectl get po 
NAME                     READY   STATUS    RESTARTS   AGE
mysql-7bff89f75f-wfwhw   1/1     Running   0          34m

anji@master:~$ kubectl describe pod  mysql-7bff89f75f-wfwhw 
Name:             mysql-7bff89f75f-wfwhw
Labels:           app=mysql
                  pod-template-hash=7bff89f75f
                  tier=mysql
Containers:
  mysql:
    Image:          mysql
    Environment:
      MYSQL_ROOT_PASSWORD:  <set to the key 'password' of config map 'devops'>  Optional: false "
#########################################+++++++++++++++++++++++++++++++++++++++++++
anji@master:~$ cat ./index.html 
<h1> WISH YOU HAPPYNEWYEAR  <h1>

anji@master:~$ kubectl create secret  generic  htmlfile --from-file=index.html=./index.html
secret/htmlfile created

anji@master:~$ kubectl get secret  "
NAME       TYPE     DATA   AGE
htmlfile   Opaque   1      59s

anji@master:~$ kubectl describe secret  htmlfile
Name:         htmlfile
Namespace:    default
Labels:       <none>
Annotations:  <none>
Type:  Opaque
Data
====
index.html:  33 bytes "
----------------------
anji@master:~$ kubectl edit secret htmlfile
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  index.html: PGgxPiBXSVNIIFlPVSBIQVBQWU5FV1lFQVIgIDxoMT4K
kind: Secret
metadata:
  creationTimestamp: "2023-01-01T14:48:37Z"
  name: htmlfile
  namespace: default
  resourceVersion: "258489"
  uid: eeafef1c-0f3b-4452-986e-a4be244f0614
type: Opaque 
----"
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginx
   labels:
     app: nginx
spec: 
  replicas: 1
  selector:
     matchLabels:
       app: nginx
  template:
    metadata:
       labels:
         app: nginx
    spec: 
      volumes:
        - name: myvol
          secret:
            secretName: htmlfile
      containers: 
       - name: nginx
         image: nginx
         ports:
           - containerPort: 80
         volumeMounts: 
           - name: myvol
             mountPath: /usr/share/nginx/html               
             readOnly: true

======---"
anji@master:~$ kubectl describe pod  nginx-5ddb4f4bdb-cl42n
Name:             nginx-5ddb4f4bdb-cl42n
Labels:           app=nginx
                  pod-template-hash=5ddb4f4bdb
Controlled By:  ReplicaSet/nginx-5ddb4f4bdb
Containers:
  nginx:
    Image:          nginx
      Environment:    <none>
    Mounts:
      /usr/share/nginx/html from myvol (ro)
Volumes:
  myvol:
    Type:        Secret (a volume populated by a Secret)   ## see look 
    SecretName:  htmlfile      ##  see look  "
-------------------------"
https://kubernetes.io/docs/concepts/configuration/secret/

apiVersion: v1
kind: Secret
metadata:
  name: secret-sa-sample
  annotations:
    kubernetes.io/service-account.name: "sa-name"
type: kubernetes.io/service-account-token
data:
  # You can include additional key value pairs as you do with Opaque Secrets
  extra: YmFyCg==
------"
Types of Secret : = ## 

Built-in Type                	                                    Usage
Opaque	                                                          arbitrary user-defined data
kubernetes.io/service-account-token	                              ServiceAccount token
kubernetes.io/dockercfg	                                          serialized ~/.dockercfg file
kubernetes.io/dockerconfigjson                                   	serialized ~/.docker/config.json file
kubernetes.io/basic-auth	                                        credentials for basic authentication
kubernetes.io/ssh-auth	                                          credentials for SSH authentication
kubernetes.io/tls	                                                data for a TLS client or server
bootstrap.kubernetes.io/token	                                    bootstrap token data

apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  username: YWRtaW4=
  password: MWYyZDFlMmU2N2Rm

===----"
apiVersion: v1
kind: Secret
metadata:
  name: indexfile
type: Opaque
data: 
  index.html: PGgxPiBXSVNIIFlPVSBIQVBQWU5FV1lFQVIgIDxoMT4K
  
=====   "
anji@master:~$ kubectl get secret 
NAME        TYPE     DATA   AGE
indexfile   Opaque   1      40s
anji@master:~$ kubectl describe secret 
Name:         indexfile
Namespace:    default
Labels:       <none>
Annotations:  <none>
Type:  Opaque
Data
====
index.html:  33 bytes
=====++++++++++"
anji@master:~$ kubectl edit secret indexfile
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  index.html: PGgxPiBXSVNIIFlPVSBIQVBQWU5FV1lFQVIgIDxoMT4K
kind: Secret
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"index.html":"PGgxPiBXSVNIIFlPVSBIQVBQWU5FV1lFQVIgIDxoMT4K"},"kind":"Secret","metadata":{"annotations":{},"name":"indexfile","namespace":"default"},"type":"Opaque"}
  creationTimestamp: "2023-01-02T07:16:07Z"
  name: indexfile
  namespace: default
  resourceVersion: "286737"
  uid: 17a350fa-ffce-4afa-8c56-2233686d6d9a
type: Opaque
=====================
anji@master:~$ kubectl create configmap indexmap  --from-file=index.html=./index.html
configmap/indexmap created

anji@master:~$ kubectl get configmap 
NAME               DATA   AGE
indexmap           1      15s
kube-root-ca.crt   1      5d1h "
anji@master:~$ kubectl describe configmap  indexmap
Name:         indexmap
Namespace:    default
Labels:       <none>
Annotations:  <none>
Data
====
index.html:
----
<h1> WISH YOU HAPPYNEWYEAR  <h1>
BinaryData
====
Events:  <none>
============+++++++++++"
anji@master:~$ kubectl edit configmap indexmap
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  index.html: |
    <h1> WISH YOU HAPPYNEWYEAR  <h1>
kind: ConfigMap
metadata:
  creationTimestamp: "2023-01-02T07:47:29Z"
  name: indexmap
  namespace: default
  resourceVersion: "289179"
  uid: 6748cd23-23fa-464b-89d3-d19f0f0a0168
############################################ "
---"
anji@master:~$ kubectl create configmap indexmap  --from-file=index.html=./index.html
configmap/indexmap created


apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec: 
  replicas: 1
  selector:
    matchLabels: 
      app: nginx
  template:
     metadata: 
       labels: 
         app: nginx
     spec: 
       volumes: 
         - name: myvol
           configMap:
             name: indexmap
       containers: 
       - name: nginx
         image: nginx
         volumeMounts:
           - name: myvol
             mountPath: /usr/share/nginx/html
             readOnly: true       "
      
anji@master:~$ kubectl get pod,configmap  -o wide 
NAME                         READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
pod/nginx-6bf598886b-dpfkb   1/1     Running   0          5m52s   10.44.0.3   worker1   <none>           <none>

NAME                         DATA   AGE
configmap/"indexmap     "      1      4m23s
configmap/kube-root-ca.crt   1      5d1h "
-========
anji@master:~$ kubectl describe pod nginx-6bf598886b-dpfkb
Name:             nginx-6bf598886b-dpfkb
Labels:           app=nginx
                  pod-template-hash=6bf598886b
Containers:
  nginx:
    Image:          nginx
    Mounts:
      /usr/share/nginx/html from myvol (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nrpkr (ro)
Volumes:
  myvol:
    Type:     " ConfigMap (a volume populated by a ConfigMap)"
    Name:      "indexmap"
    Optional:  false
----------"    ##  BEFORE   BEFORE  BEFORE  
anji@master:~$ kubectl edit configmap indexmap
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  index.html: |
    <h1> WISH YOU HAPPYNEWYEAR  <h1>
kind: ConfigMap
metadata:
  creationTimestamp: "2023-01-02T08:14:50Z"
  name: indexmap
  namespace: default
  resourceVersion: "291333"
  uid: 0d3e9b51-dc20-4401-92e3-4fc397c72444
=====  ## AFTER  AFTER   
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  index.html: |
    <h1> WISH YOU HAPPYNEWYEAR  <h1>        #  see look 
    <h2> THIS IS DEVOPS CLASS ZZZZZZXXXX<h2>    ##  see  look 
kind: ConfigMap
metadata:
  creationTimestamp: "2023-01-02T08:14:50Z"
  name: indexmap
  namespace: default
  resourceVersion: "292980"
  uid: 0d3e9b51-dc20-4401-92e3-4fc397c724443
=====---- "
anji@master:~$ kubectl get all,configmap  -o wide
NAME                         READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/"nginx-6bf598886b-dpfkb "  1/1     Running   0          26m   10.44.0.3   worker1   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   5d1h   <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES   SELECTOR
"deployment.apps/nginx "  1/1     1            1           26m   nginx        nginx    app=nginx

NAME                               DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR
replicaset.apps/nginx-6bf598886b   1         1         1       26m   nginx        nginx    app=nginx,pod-template-hash=6bf598886b

NAME                         DATA   AGE
configmap/"indexmap "          1      25m
configmap/kube-root-ca.crt   1      5d1h
======="
anji@master:~$ kubectl set env deployment nginx  --from=configmap/indexmap
Warning: key index.html transferred to INDEX_HTML
deployment.apps/nginx env updated                                  ##  restart the pod  see look 
+++++
anji@master:~$ kubectl get all,configmap  -o wide
NAME                        READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/"nginx-cc7957db6-5mzj4 "  1/1     Running   0          45s   10.44.0.4   worker1   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   5d2h   <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES   SELECTOR
deployment.apps/nginx   1/1     1            1           31m   nginx        nginx    app=nginx

NAME                               DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR
replicaset.apps/nginx-6bf598886b   0         0         0       31m   nginx        nginx    app=nginx,pod-template-hash=6bf598886b
replicaset.apps/nginx-cc7957db6    1         1         1       45s   nginx        nginx    app=nginx,pod-template-hash=cc7957db6

NAME                         DATA   AGE
configmap/indexmap           1      29m
configmap/kube-root-ca.crt   1      5d2h
##################################################################################################################
https://www.youtube.com/watch?v=mkksJZZmpgE&list=PL8SR-mMnL9f6KKYDnrG3saVnyLqNDz1GP&index=18

Kubernetes-Day-18
Kubernetes Probes, Introduction to HELM

Probe is health chesk , that can be configured to check the health of the container running in the pod,

https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/

a probe may return the following results:
1.success
2.failure
3.unknown

types of probes: 
 1. Liveness Probe
     this probe is used to deternmine if the particular container is running or not , if a container fails  the liveness probe , then controller will 
       try to restart the pod in the same node , it is all based on restart policy -- , the default restart policy is always 

2. Readiness probe:
    this probe is used to deternmine  wheather a particular container is ready to receive the request or not , if this fails then
     kubenretes controller will ensure the pod does't receive any request 


implementaion of the probes:
 1. command probes : in this probe the controller will get the  container to execute specific command in order to perform probe in the  container

2.HTTP REQUEST probes: in this probe , the controller will send GET-HTTP request to the given address to either port or hostname 
   to perform the probe on the container , we can set the following fields to configure the htttp probe
   * port number
   * hostname
   *http header 
   * tcp socket probe  "

apiVersion: v1
kind: Pod
metadata: 
  name: liveness-alwaysrestart
spec:  
  restartPolicy: Always        # see look
  containers:
    - name: ubuntu
      image: ubuntu
      command:
        - /bin/bash
        - -ec
        - touch /tmp/anji;sleep 30; rm /tmp/anji;sleep 600
      livenessProbe:
         exec:
           command:
             - cat
             - /tmp/anji
         initialDelaySeconds: 5
         periodSeconds: 5         "
anji@master:~$ kubectl get pod -o wide 
NAME                     READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES
liveness-alwaysrestart   0/1     ContainerCreating    " 0 "         2s    <none>   worker1   <none>           <none>

anji@master:~$ kubectl get pod -o wide 
NAME                     READY   STATUS    RESTARTS     AGE   IP          NODE      NOMINATED NODE   READINESS GATES
liveness-alwaysrestart   1/1     Running   "1 "(4s ago)   79s   10.44.0.3   worker1   <none>           <none>


anji@master:~$ kubectl get pod -o wide 
NAME                     READY   STATUS    RESTARTS     AGE     IP          NODE      NOMINATED NODE   READINESS GATES
liveness-alwaysrestart   1/1     Running   "2 "(5s ago)   2m35s   10.44.0.3   worker1   <none>           <none>

anji@master:~$ kubectl get pod -o wide 
NAME                     READY   STATUS    RESTARTS      AGE     IP          NODE      NOMINATED NODE   READINESS GATES
liveness-alwaysrestart   1/1     Running   "3 "(39s ago)   4m24s   10.44.0.3   worker1   <none>           <none>

##+++++++++++++++++++++++++++++ 
"
apiVersion: v1
kind: Pod
metadata: 
   name: liveness-norestart
spec:
  restartPolicy: Never               # see look 
  containers: 
    - name: ubuntu
      image: ubuntu
      command:
         - /bin/bash
         - -ec
         - touch /tmp/anji;sleep 30; rm /tmp/anji;sleep 600
      livenessProbe:
        exec: 
          command:
             - cat
             - /tmp/anji
        initialDelaySeconds: 5
        periodSeconds: 5
                   
anji@master:~$ kubectl get pods   -o wide
NAME                 READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
liveness-norestart   1/1     "Running "  0        "  3s  "  10.44.0.3   worker1   <none>           <none>

anji@master:~$ kubectl get pods   -o wide
NAME                 READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
liveness-norestart   1/1     Running   0          76s   10.44.0.3   worker1   <none>           <none>

anji@master:~$ kubectl get pods   -o wide
NAME                 READY   STATUS   RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
liveness-norestart   0/1    "Error  "  0          "77s  " 10.44.0.3   worker1   <none>           <none>

==========++++++++++++++++++++++++++++++++++++++++++++"
apiVersion: v1
kind: Pod
metadata: 
  name: readiness-command-probe
spec: 
  containers: 
    - name: ubuntu
      image: ubuntu
      command: 
        - /bin/bash
        - -ec
        - sleep 30; touch  /tmp/anji; sleep 600
      readinessProbe:        # see look  
        exec: 
          command:
            - cat
            - /tmp/anji
        initialDelaySeconds: 10
        periodSeconds: 5        
"        
anji@master:~$ kubectl get pods -o wide 
NAME                      READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES
readiness-command-probe   0/1     ContainerCreating   0          2s    <none>   worker1   <none>           <none>

======================++++++++++++++++++++++++++++++++++++"
---
apiVersion: v1 
kind: Pod 
metadata: 
  name: liveness-request-nginx
spec: 
  containers: 
  - name: liveness 
    image: nginx
    ports: 
      - containerPort: 80 
    livenessProbe: 
      httpGet: 
        path: / 
        port: 80 
      initialDelaySeconds: 2
      periodSeconds: 2
      timeoutSeconds: 1 
      successThreshold: 1 
      failureThreshold: 3
---
apiVersion: v1 
kind: Pod 
metadata: 
  name: liveness-request-httpd
spec: 
  containers: 
  - name: liveness 
    image: httpd
    ports: 
      - containerPort: 80 
    livenessProbe: 
      httpGet: 
        path: / 
        port: 80 
      initialDelaySeconds: 2
      periodSeconds: 2
      timeoutSeconds: 1 
      successThreshold: 1 
      failureThreshold: 3

===========

---
apiVersion: v1
kind: Pod
metadata: 
   name: http-probe-111
spec: 
  containers: 
  - name: http
    image: httpd
    ports: 
      - containerPort: 80
    livenessProbe:
      httpGet: 
        path: /
        port: 80
      initialDelaySeconds: 2
      periodSeconds: 2
      timeoutSeconds: 1 
      successThreshold: 1
      failureThreshold: 3

-------"
anji@master:~$ kubectl get po -o wide 
NAME             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
http-probe-111   1/1     Running   0          4s    10.44.0.3   worker1   <none>           <none>
---=="
nji@master:~$ kubectl describe pod http-probe-111  
Name:             http-probe-111
Containers:
  http:
    Image:          httpd
    Restart Count:  0
    Liveness:    "   http-get http://:80/ delay=2s timeout=1s period=2s #success=1 #failure=3"
    
#########################################################################################################################
HELM CHARTS "
https://www.youtube.com/watch?v=2dqQcou_MCU&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=2

https://helm.sh/docs/intro/install/
apt-get update -y

root@master:~# wget https://get.helm.sh/helm-v3.10.3-linux-amd64.tar.gz

root@master:~# tar -xvzf helm-v3.10.3-linux-amd64.tar.gz 

root@master:~# mv linux-amd64/helm  /usr/local/bin/helm

========-----------------"
https://jhooq.com/getting-start-with-helm-chart/
https://www.youtube.com/watch?v=jP_PF0mqUHk&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=3

anji@master:~$ helm create helloworld 
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
Creating helloworld

anji@master:~$ ls -lrt | grep helloworld 
drwxr-xr-x 4 anji anji 4096 Jan  3 14:02 helloworld

anji@master:~/helloworld$ ll
total 28
drwxr-xr-x  4 anji anji 4096 Jan  3 14:02 ./
drwxr-xr-x 18 anji anji 4096 Jan  3 14:02 ../
drwxr-xr-x  2 anji anji 4096 Jan  3 14:02 charts/
-rw-r--r--  1 anji anji 1146 Jan  3 14:02 Chart.yaml
-rw-r--r--  1 anji anji  349 Jan  3 14:02 .helmignore
drwxr-xr-x  3 anji anji 4096 Jan  3 14:02 templates/
-rw-r--r--  1 anji anji 1877 Jan  3 14:02 values.yaml
anji@master:~/helloworld$ tree
.
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files
--------+++++++++++++========"
anji@master:~/helloworld$ cat values.yaml 
# Default values for helloworld.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 1
image:
  repository: nginx
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""
imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""
podAnnotations: {}
podSecurityContext: {}
  # fsGroup: 2000
securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000
service:
  type: ClusterIP                  ## changed to clusterip =>> nodeport see look 
  port: 80

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80
nodeSelector: {}
tolerations: []
affinity: {}
==============="
anji@master:~/helloworld$ cat values.yaml 
# Default values for helloworld.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: nginx
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: NodePort   ### see look  service
  port: 80

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}
======================="
helm install <FIRST_ARGUMENT_RELEASE_NAME> <SECOND_ARGUMENT_CHART_NAME>

anji@master:~$ helm install myhworld helloworld
NAME: myhworld
LAST DEPLOYED: Tue Jan  3 14:45:05 2023
NAMESPACE: default
STATUS: "deployed"
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services myhworld-helloworld)
  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
------===="
anji@master:~$ helm list -a 
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART           	APP VERSION
myhworld	default  	1       	2023-01-03 14:45:05.517588358 +0530 IST	"deployed"	"helloworld-0.1.0"	1.16.0     

anji@master:~$ kubectl get pod,deployments  -o wide 
NAME                                      READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/myhworld-helloworld-9f5d78d85-2w4f4   1/1     Running   0          11m   10.44.0.1   worker1   <none>           <none>

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR
deployment.apps/"myhworld-helloworld  " 1/1     1            1           11m   "helloworld "  nginx:1.16.0   app.kubernetes.io/instance=myhworld,app.kubernetes.io/name=helloworld
---==+++++++++++++"
anji@master:~$ kubectl get service  -o wide 
NAME                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE    SELECTOR
kubernetes            ClusterIP   10.96.0.1      <none>        443/TCP        111m   <none>
myhworld-helloworld   "NodePort "   10.104.51.12   <none>        "80:32114/TCP  " 18m    app.kubernetes.io/instance=myhworld,app.kubernetes.io/name=helloworld
"===/////\\
nji@master:~$ kubectl describe pod myhworld-helloworld-9f5d78d85-2w4f4 
Name:             myhworld-helloworld-9f5d78d85-2w4f4
Service Account:  myhworld-helloworld
Labels:           app.kubernetes.io/instance=myhworld
                  app.kubernetes.io/name=helloworld
                  pod-template-hash=9f5d78d85
Controlled By:  ReplicaSet/myhworld-helloworld-9f5d78d85
Containers:
  helloworld:
    Image:          nginx:1.16.0
  "  Liveness:  "     http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3
    "Readiness:  "    http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3

http://192.168.122.15:32114/
anji@master:~$ curl http://192.168.122.15:32114/
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
anji@mast
##########################################3
What are Helm Chart Repositories and how to work with it - Part 3
https://bitnami.com/
https://bitnami.com/stack/redis/helm
https://bitnami.com/stacks

anji@master:~$ helm repo list 
Error: no repositories to show

Usage:  helm repo add [NAME] [URL] [flags]
anji@master:~$ helm repo add bitnami https://charts.bitnami.com/bitnami
"bitnami" has been added to your repositories

anji@master:~$ helm repo list 
NAME   	URL                               
bitnami	https://charts.bitnami.com/bitnami

anji@master:~$ helm search repo bitnami
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
NAME                                        	CHART VERSION	APP VERSION  	DESCRIPTION                                       
bitnami/airflow                             	14.0.6       	2.5.0        	Apache Airflow is a tool to express and execute...
bitnami/apache                              	9.2.9        	2.4.54       	Apache HTTP Server is an open-source HTTP serve...
bitnami/appsmith                            	0.1.7        	1.8.15       	Appsmith is an open source platform for buildin...
bitnami/argo-cd                             	4.4.0        	2.5.5        	Argo CD is a continuous delivery tool for Kuber...
bitnami/argo-workflows                      	5.1.1        	3.4.4        	Argo Workflows is meant to orchestrate Kubernet...
bitnami/aspnet-core                         	4.0.1        	7.0.1        	ASP.NET Core is an open-source framework for we...
bitnami/cassandra                           	10.0.0       	4.1.0        	Apache Cassandra is an open source distributed ...
bitnami/cert-manager                        	0.8.10       	1.10.1       	cert-manager is a Kubernetes add-on to automate...
bitnami/clickhouse                          	2.2.1        	22.12.1      	ClickHouse is an open-source column-oriented OL...
bitnami/common                              	2.2.2        	2.2.2        	A Library Helm Chart for grouping common logic ...
bitnami/concourse                           	2.0.1        	7.8.3        	Concourse is an automation system written in Go...
bitnami/consul                              	10.9.8       	1.14.3       	HashiCorp Consul is a tool for discovering and ...
bitnami/contour                             	10.1.3       	1.23.2       	Contour is an open source Kubernetes ingress co...
bitnami/contour-operator                    	3.0.1        	1.23.0       	The Contour Operator extends the Kubernetes API...
bitnami/dataplatform-bp2                    	12.0.5       	1.0.1        	DEPRECATED This Helm chart can be used for the ...
bitnami/discourse                           	9.0.6        	2.8.13       	Discourse is an open source discussion platform...
bitnami/dokuwiki                            	13.1.9       	20220731.1.0 	DokuWiki is a standards-compliant wiki optimize...
bitnami/drupal                              	12.5.12      	9.4.9        	Drupal is one of the most versatile open source...
bitnami/ejbca                               	6.3.9        	7.10.0-2     	EJBCA is an enterprise class PKI Certificate Au...
bitnami/elasticsearch                       	19.5.5       	8.5.3        	Elasticsearch is a distributed search and analy...
bitnami/etcd                                	8.5.11       	3.5.6        	etcd is a distributed key-value store designed ...
bitnami/external-dns                        	6.12.2       	0.13.1       	ExternalDNS is a Kubernetes addon that configur...
bitnami/fluentd                             	5.5.12       	1.15.3       	Fluentd collects events from various data sourc...
bitnami/geode                               	1.1.8        	1.15.1       	DEPRECATED Apache Geode is a data management pl...
bitnami/ghost                               	19.1.53      	5.26.4       	Ghost is an open source publishing platform des...
bitnami/gitea                               	0.1.3        	1.18.0       	Gitea is a lightweight code hosting solution. W...
bitnami/grafana                             	8.2.21       	9.3.2        	Grafana is an open source metric analytics and ...
bitnami/grafana-loki                        	2.5.2        	2.7.1        	Grafana Loki is a horizontally scalable, highly...
bitnami/grafana-operator                    	2.7.12       	4.8.0        	Grafana Operator is a Kubernetes operator that ...
bitnami/grafana-tempo                       	1.4.6        	1.5.0        	Grafana Tempo is a distributed tracing system t...
bitnami/haproxy                             	0.6.5        	2.7.1        	HAProxy is a TCP proxy and a HTTP reverse proxy...
bitnami/haproxy-intel                       	0.2.10       	2.7.1        	HAProxy is a high-performance, open-source load...
bitnami/harbor                              	16.1.2       	2.7.0        	Harbor is an open source trusted cloud-native r...
bitnami/influxdb                            	5.4.14       	2.6.1        	InfluxDB(TM) is an open source time-series data...
bitnami/jaeger                              	0.1.2        	1.40.0       	Jaeger is a distributed tracing system. It is u...
bitnami/jasperreports                       	14.3.6       	8.1.0        	JasperReports Server is a stand-alone and embed...
bitnami/jenkins                             	11.0.10      	2.375.1      	Jenkins is an open source Continuous Integratio...
bitnami/joomla                              	13.3.12      	4.2.6        	Joomla! is an award winning open source CMS pla...
bitnami/jupyterhub                          	3.0.4        	3.0.0        	JupyterHub brings the power of notebooks to gro...
bitnami/kafka                               	20.0.2       	3.3.1        	Apache Kafka is a distributed streaming platfor...
bitnami/keycloak                            	13.0.0       	20.0.2       	Keycloak is a high performance Java-based ident...
bitnami/kiam                                	1.1.6        	4.2.0        	kiam is a proxy that captures AWS Metadata API ...
bitnami/kibana                              	10.2.10      	8.5.3        	Kibana is an open source, browser based analyti...
bitnami/kong                                	8.0.25       	3.1.1        	Kong is an open source Microservice API gateway...
bitnami/kube-prometheus                     	8.3.1        	0.61.1       	Prometheus Operator provides easy monitoring de...
bitnami/kube-state-metrics                  	3.2.7        	2.7.0        	kube-state-metrics is a simple service that lis...
bitnami/kubeapps                            	12.1.3       	2.6.2        	Kubeapps is a web-based UI for launching and ma...
bitnami/kubernetes-event-exporter           	2.1.4        	1.1.0        	Kubernetes Event Exporter makes it easy to expo...
bitnami/logstash                            	5.1.9        	8.5.3        	Logstash is an open source data processing engi...
bitnami/magento                             	21.1.11      	2.4.5-p1     	Magento is a powerful open source e-commerce pl...
bitnami/mariadb                             	11.4.2       	10.6.11      	MariaDB is an open source, community-developed ...
bitnami/mariadb-galera                      	7.4.10       	10.6.11      	MariaDB Galera is a multi-primary database clus...
bitnami/mastodon                            	0.1.2        	4.0.2        	Mastodon is self-hosted social network server b...
bitnami/matomo                              	0.2.15       	4.13.0       	Matomo, formerly known as Piwik, is a real time...
bitnami/mediawiki                           	14.3.11      	1.39.1       	MediaWiki is the free and open source wiki soft...
bitnami/memcached                           	6.3.2        	1.6.17       	Memcached is an high-performance, distributed m...
bitnami/metallb                             	4.1.13       	0.13.7       	MetalLB is a load-balancer implementation for b...
bitnami/metrics-server                      	6.2.6        	0.6.2        	Metrics Server aggregates resource usage data, ...
bitnami/minio                               	11.10.24     	2022.12.12   	MinIO(R) is an object storage server, compatibl...
bitnami/mongodb                             	13.6.2       	6.0.3        	MongoDB(R) is a relational open source NoSQL da...
bitnami/mongodb-sharded                     	6.2.1        	6.0.3        	MongoDB(R) is an open source NoSQL database tha...
bitnami/moodle                              	14.3.4       	4.1.0        	Moodle(TM) LMS is an open source online Learnin...
bitnami/mxnet                               	3.1.8        	1.9.1        	Apache MXNet (Incubating) is a flexible and eff...
bitnami/mysql                               	9.4.5        	8.0.31       	MySQL is a fast, reliable, scalable, and easy t...
bitnami/nats                                	7.5.5        	2.9.10       	NATS is an open source, lightweight and high-pe...
bitnami/nginx                               	13.2.20      	1.23.3       	NGINX Open Source is a web server that can be a...
bitnami/nginx-ingress-controller            	9.3.24       	1.6.0        	NGINX Ingress Controller is an Ingress controll...
bitnami/nginx-intel                         	2.1.13       	0.4.9        	NGINX Open Source for Intel is a lightweight se...
bitnami/node                                	19.1.7       	16.18.0      	DEPRECATED Node.js is a runtime environment bui...
bitnami/node-exporter                       	3.2.6        	1.5.0        	Prometheus exporter for hardware and OS metrics...
bitnami/oauth2-proxy                        	3.4.2        	7.4.0        	A reverse proxy and static file server that pro...
bitnami/odoo                                	23.0.4       	16.0.20221115	Odoo is an open source ERP and CRM platform, fo...
bitnami/opencart                            	13.0.6       	4.0.1-1      	OpenCart is free open source ecommerce platform...
bitnami/osclass                             	14.2.8       	8.0.2        	Osclass allows you to easily create a classifie...
bitnami/owncloud                            	12.2.10      	10.11.0      	ownCloud is an open source content collaboratio...
bitnami/parse                               	19.1.12      	5.4.0        	Parse is a platform that enables users to add a...
bitnami/phpbb                               	12.3.9       	3.3.9        	phpBB is a popular bulletin board that features...
bitnami/phpmyadmin                          	10.3.8       	5.2.0        	phpMyAdmin is a free software tool written in P...
bitnami/pinniped                            	0.4.7        	0.21.0       	Pinniped is an identity service provider for Ku...
bitnami/postgresql                          	12.1.6       	15.1.0       	PostgreSQL (Postgres) is an open source object-...
bitnami/postgresql-ha                       	10.0.7       	15.1.0       	This PostgreSQL cluster solution includes the P...
bitnami/prestashop                          	16.0.1       	8.0.0        	PrestaShop is a powerful open source eCommerce ...
bitnami/pytorch                             	2.5.11       	1.13.1       	PyTorch is a deep learning platform that accele...
bitnami/rabbitmq                            	11.3.0       	3.11.5       	RabbitMQ is an open source general-purpose mess...
bitnami/rabbitmq-cluster-operator           	3.1.5        	2.0.0        	The RabbitMQ Cluster Kubernetes Operator automa...
bitnami/redis                               	17.4.0       	7.0.7        	Redis(R) is an open source, advanced key-value ...
bitnami/redis-cluster                       	8.3.3        	7.0.7        	Redis(R) is an open source, scalable, distribut...
bitnami/redmine                             	21.0.4       	5.0.4        	Redmine is an open source management applicatio...
bitnami/schema-registry                     	8.0.2        	7.3.1        	Confluent Schema Registry provides a RESTful in...
bitnami/sealed-secrets                      	1.2.1        	0.19.3       	Sealed Secrets are "one-way" encrypted K8s Secr...
bitnami/solr                                	7.1.0        	9.1.0        	Apache Solr is an extremely powerful, open sour...
bitnami/sonarqube                           	2.0.5        	9.8.0        	SonarQube(TM) is an open source quality managem...
bitnami/spark                               	6.3.13       	3.3.1        	Apache Spark is a high-performance engine for l...
bitnami/spring-cloud-dataflow               	15.0.1       	2.9.6        	Spring Cloud Data Flow is a microservices-based...
bitnami/suitecrm                            	11.2.7       	7.12.8       	SuiteCRM is a completely open source, enterpris...
bitnami/tensorflow-resnet                   	3.6.10       	2.11.0       	TensorFlow ResNet is a client utility for use w...
bitnami/thanos                              	11.6.6       	0.29.0       	Thanos is a highly available metrics system tha...
bitnami/tomcat                              	10.5.6       	10.1.4       	Apache Tomcat is an open-source web server desi...
bitnami/wavefront                           	4.2.8        	1.13.0       	Wavefront is a high-performance streaming analy...
bitnami/wavefront-adapter-for-istio         	2.0.6        	0.1.5        	DEPRECATED Wavefront Adapter for Istio is an ad...
bitnami/wavefront-hpa-adapter               	1.3.7        	0.9.9        	Wavefront HPA Adapter for Kubernetes is a Kuber...
bitnami/wavefront-prometheus-storage-adapter	2.1.6        	1.0.5        	Wavefront Storage Adapter is a Prometheus integ...
bitnami/wildfly                             	14.0.2       	27.0.1       	Wildfly is a lightweight, open source applicati...
bitnami/wordpress                           	15.2.22      	6.1.1        	WordPress is the world's most popular blogging ...
bitnami/wordpress-intel                     	2.1.28       	6.1.1        	WordPress for Intel is the most popular bloggin...
bitnami/zookeeper                           	11.0.2       	3.8.0        	Apache ZooKeeper provides a reliable, centraliz...
----==="
anji@master:~$ helm repo update 
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "bitnami" chart repository
Update Complete. âŽˆHappy Helming!âŽˆ "

anji@master:~$ ls
Desktop  Documents  Downloads  "helloworld " Music  Pictures  Public  Templates  Videos
anji@master:~$ tree helloworld/
helloworld/
â”œâ”€â”€ charts         # see look 
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files

anji@master:~$ helm repo index helloworld 

anji@master:~$ tree helloworld/
helloworld/
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ index.yaml                ## see look  index 
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 11 files "

anji@master:~$ cat helloworld/index.yaml 
apiVersion: v1
entries: {}
generated: "2023-01-03T16:34:04.958380726+05:30"   "

anji@master:~$ helm repo remove bitnami 
"bitnami" has been removed from your repositories

anji@master:~$ helm repo list 
Error: no repositories to show
##############################################################################""
"What is Helm Chart ï¼‚Pluginsï¼‚ and how to use it. - Part 4 
https://jhooq.com/helm-chart-plugin/
 https://github.com/databus23/helm-diff

helm plugin install [options] <path|url>... [flags]

anji@master:~$ helm plugin list 
NAME	VERSION	DESCRIPTION

anji@master:~$ helm plugin install https://github.com/databus23/helm-diff
 Use "diff [command] --help" for more information about a command.
  Installed plugin: diff

anji@master:~$ helm plugin list 
NAME	VERSION	DESCRIPTION                           
diff	3.6.0  	Preview helm upgrade changes as a diff  "

anji@master:~$ helm create helloworld 
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
Creating helloworld
WARNING: File "/home/anji/helloworld/Chart.yaml" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/values.yaml" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/.helmignore" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/templates/ingress.yaml" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/templates/deployment.yaml" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/templates/service.yaml" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/templates/serviceaccount.yaml" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/templates/hpa.yaml" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/templates/NOTES.txt" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/templates/_helpers.tpl" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/templates/tests/test-connection.yaml" already exists. Overwriting"

anji@master:~$ helm create helloworld/
Creating helloworld/

anji@master:~$ tree helloworld/
helloworld/
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files "
======-------
anji@master:~$ helm install release1 helloworld/
NAME: release1
LAST DEPLOYED: Tue Jan  3 17:26:29 2023
NAMESPACE: default
STATUS: "deployed"
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=helloworld,app.kubernetes.io/instance=release1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT "

anji@master:~$ helm list -a 
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART           	APP VERSION
myhworld	default  	1       	2023-01-03 14:45:05.517588358 +0530 IST	deployed	helloworld-0.1.0	1.16.0     
"release1	"default  	"1  "     	2023-01-03 17:26:29.598397743 +0530 IST	deployed	helloworld-0.1.0	1.16.0     

anji@master:~$ helm delete myhworld
release "myhworld" uninstalled

anji@master:~$ helm list -a 
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART           	APP VERSION
"release1	"default  	"1  "     	2023-01-03 17:26:29.598397743 +0530 IST	deployed	helloworld-0.1.0	1.16.0      "


anji@master:~$ kubectl get pod,deployment   -o wide 
NAME                                       READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/release1-helloworld-7d4d79f6f9-dlk9s   1/1     Running   0          34s   10.44.0.1   worker1   <none>           <none>

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR
deployment.apps/release1-helloworld   1/1     1            1           34s   helloworld   nginx:1.16.0   app.kubernetes.io/instance=release1,app.kubernetes.io/name=helloworld

=====================\\\\\\///////////////
anji@master:~$ ls
Desktop  Documents  Downloads  helloworld  Music  Pictures  Public  Templates  Videos

anji@master:~$ cd helloworld/
anji@master:~/helloworld$ ls
charts  Chart.yaml  templates  values.yaml

anji@master:~/helloworld$ nano values.yaml 
# Default values for helloworld.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 3             " ## to increase the replicas count 1 to 3 "  see look 
image:
  repository: nginx
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "

anji@master:~/helloworld$ helm upgrade release1 . 
Release "release1" has been upgraded. Happy Helming!
NAME: release1
LAST DEPLOYED: Tue Jan  3 17:47:48 2023
NAMESPACE: default
STATUS: "deployed"
REVISION:" 2"
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=helloworld,app.kubernetes.io/instance=release1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
===-////\\\

anji@master:~$ helm list -a 
NAME    	NAMESPACE	REVISION	UPDATED                               	STATUS  	CHART           	APP VERSION
release1	default  "	2    "   	2023-01-03 17:47:48.50720335 +0530 IST	deployed	helloworld-0.1.0	1.16.0     

anji@master:~$ kubectl get pods -o wide 
NAME                                   READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
release1-helloworld-7d4d79f6f9-dlk9s   1/1     Running   0          23m     10.44.0.1   worker1   <none>           <none>
release1-helloworld-7d4d79f6f9-qkvnv   1/1     Running   0          2m21s   10.44.0.3   worker1   <none>           <none>
release1-helloworld-7d4d79f6f9-wx7pk   1/1     Running   0          2m21s   10.44.0.2   worker1   <none>           <none>

--===--//\\

anji@master:~$ helm diff revision release1  1 2
default, release1-helloworld, Deployment (apps) has changed:
  # Source: helloworld/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: release1-helloworld
    labels:
      helm.sh/chart: helloworld-0.1.0
      app.kubernetes.io/name: helloworld
      app.kubernetes.io/instance: release1
      app.kubernetes.io/version: "1.16.0"
      app.kubernetes.io/managed-by: Helm
  spec:
-   replicas: 1    ### see look 
+   replicas: 3   ###  see look 
    selector:
      matchLabels:
        app.kubernetes.io/name: helloworld
        app.kubernetes.io/instance: release1
    template:
      metadata:
        labels:
          app.kubernetes.io/name: helloworld
          app.kubernetes.io/instance: release1
      spec:
        serviceAccountName: release1-helloworld
        securityContext:
          {}
        containers:
          - name: helloworld
            securityContext:
              {}
            image: "nginx:1.16.0"
            imagePullPolicy: IfNotPresent
            ports:
              - name: http
                containerPort: 80
                protocol: TCP
            livenessProbe:
              httpGet:
                path: /
                port: http
            readinessProbe:
              httpGet:
                path: /
                port: http
            resources:
              {}
=========\\\\////////\\\/\/\//\//\\/ "
"
https://www.youtube.com/watch?v=8hyXDFrWi9w&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=6   
Setup Wordpress using Helm Chart on Kubernetes - Part 5

Usage:  helm repo add [NAME] [URL] [flags]
anji@master:~$ helm repo add bitnami https://charts.bitnami.com/bitnami
"bitnami" has been added to your repositories

anji@master:~$ helm repo list 
NAME   	URL                               
bitnami	https://charts.bitnami.com/bitnami

anji@master:~/helloworld$ helm search repo bitnami

anji@master:~/helloworld$ helm search repo wordpress --versions
NAME                   	CHART VERSION	APP VERSION	DESCRIPTION                                       
bitnami/wordpress      	15.2.22      	6.1.1      	WordPress is the world's most popular blogging ...
bitnami/wordpress      	15.2.21      	6.1.1      	WordPress is the world's most popular blogging ...
bitnami/wordpress      	15.2.20      	6.1.1      	WordPress is the world's most popular blogging ...
bitnami/wordpress      	15.2.19      	6.1.1      	WordPress is the world's most popular blogging ...
bitnami/wordpress      	15.2.18      	6.1.1      	WordPress is the world's most popular blogging ... "+++++++++++++{}{}{{{{{{{{{{{{{}{}}}}}}}}}}}}}
===+++/////\\\\
anji@master:~$ helm show readme bitnami/wordpress  --version 15.2.22                               {}{{[][[][{}{{{{{][{{}{{}{{{}[]}}\}}]}}}}}]]}}
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
<!--- app-name: WordPress -->
# WordPress packaged by Bitnami
WordPress is the world's most popular blogging and content management platform. Powerful yet simple, everyone from students to global corporations use it to build beautiful, functional websites.
[Overview of WordPress](http://www.wordpress.org)
## TL;DR
```console
$ helm repo add my-repo https://charts.bitnami.com/bitnami
$ helm install my-release my-repo/wordpress
``
## Introduction
This chart bootstraps a [WordPress](https://github.com/bitnami/containers/tree/main/bitnami/wordpress) deployment on a [Kubernetes](https://kubernetes.io) cluster using the [Helm](https://helm.sh) package manager.
It also packages the [Bitnami MariaDB chart](https://github.com/bitnami/charts/tree/main/bitnami/mariadb) which is required for bootstrapping a MariaDB deployment for the database requirements of the WordPress application, and the [Bitnami Memcached chart](https://github.com/bitnami/charts/tree/main/bitnami/memcached) that can be used to cache database queries.
Bitnami charts can be used with [Kubeapps](https://kubeapps.dev/) for deployment and management of Helm Charts in clusters.
## Prerequisites
- Kubernetes 1.19+
- Helm 3.2.0+
- PV provisioner support in the underlying infrastructure
- ReadWriteMany volumes for deployment scaling
## Installing the Chart
To install the chart with the release name `my-release`:
```console
helm install my-release my-repo/wordpress
`
The command deploys WordPress on the Kubernetes cluster in the default configuration. The [Parameters](#parameters) section lists the parameters that can be configured during installation.
> **Tip**: List all releases using `helm list`
## Uninstalling the Chart
To uninstall/delete the `my-release` deployment:
```console
helm delete my-release
The command removes all the Kubernetes components associated with the chart and deletes the release.
## Parameters
### Global parameters
| Name                      | Description                                     | Value |
| ------------------------- | ----------------------------------------------- | ----- |
| `global.imageRegistry`    | Global Docker image registry                    | `""`  |
| `global.imagePullSecrets` | Global Docker registry secret names as an array | `[]`  |
| `global.storageClass`     | Global StorageClass for Persistent Volume(s)    | `""`  |
### Common parameters
| Name                     | Description                                                                                  | Value           |
| ------------------------ | -------------------------------------------------------------------------------------------- | --------------- |
| `kubeVersion`            | Override Kubernetes version                                                                  | `""`            |
| `nameOverride`           | String to partially override common.names.fullname template (will maintain the release name) | `""`            |
| `fullnameOverride`       | String to fully override common.names.fullname template                                      | `""`            |
| `commonLabels`           | Labels to add to all deployed resources                                                      | `{}`            |
| `commonAnnotations`      | Annotations to add to all deployed resources                                                 | `{}`            |
| `clusterDomain`          | Kubernetes Cluster Domain                                                                    | `cluster.local` |
| `extraDeploy`            | Array of extra objects to deploy with the release                                            | `[]`            |
| `diagnosticMode.enabled` | Enable diagnostic mode (all probes will be disabled and the command will be overridden)      | `false`         |
| `diagnosticMode.command` | Command to override all containers in the deployment                                         | `["sleep"]`     |
| `diagnosticMode.args`    | Args to override all containers in the deployment                                            | `["infinity"]`  |
### WordPress Image parameters
| Name                | Description                                                                                               | Value                 |
| ------------------- | --------------------------------------------------------------------------------------------------------- | --------------------- |
| `image.registry`    | WordPress image registry                                                                                  | `docker.io`           |
| `image.repository`  | WordPress image repository                                                                                | `bitnami/wordpress`   |
| `image.tag`         | WordPress image tag (immutable tags are recommended)                                                      | `6.1.1-debian-11-r15` |
| `image.digest`      | WordPress image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag | `""`                  |
| `image.pullPolicy`  | WordPress image pull policy                                                                               | `IfNotPresent`        |
| `image.pullSecrets` | WordPress image pull secrets                                                                              | `[]`                  |
| `image.debug`       | Specify if debug values should be set                                                                     | `false`               |
### WordPress Configuration parameters
| Name                                   | Description                                                                           | Value              |
| -------------------------------------- | ------------------------------------------------------------------------------------- | ------------------ |
| `wordpressUsername`                    | WordPress username                                                                    | `user`             |
| `wordpressPassword`                    | WordPress user password                                                               | `""`               |
| `existingSecret`                       | Name of existing secret containing WordPress credentials                              | `""`               |
| `wordpressEmail`                       | WordPress user email                                                                  | `user@example.com` |
| `wordpressFirstName`                   | WordPress user first name                                                             | `FirstName`        |
| `wordpressLastName`                    | WordPress user last name                                                              | `LastName`         |
| `wordpressBlogName`                    | Blog name                                                                             | `User's Blog!`     |
| `wordpressTablePrefix`                 | Prefix to use for WordPress database tables                                           | `wp_`              |
| `wordpressScheme`                      | Scheme to use to generate WordPress URLs                                              | `http`             |
| `wordpressSkipInstall`                 | Skip wizard installation                                                              | `false`            |
| `wordpressExtraConfigContent`          | Add extra content to the default wp-config.php file                                   | `""`               |
| `wordpressConfiguration`               | The content for your custom wp-config.php file (advanced feature)                     | `""`               |
| `existingWordPressConfigurationSecret` | The name of an existing secret with your custom wp-config.php file (advanced feature) | `""`               |
| `wordpressConfigureCache`              | Enable W3 Total Cache plugin and configure cache settings                             | `false`            |
| `wordpressPlugins`                     | Array of plugins to install and activate. Can be specified as `all` or `none`.        | `none`             |
| `apacheConfiguration`                  | The content for your custom httpd.conf file (advanced feature)                        | `""`               |
| `existingApacheConfigurationConfigMap` | The name of an existing secret with your custom httpd.conf file (advanced feature)    | `""`               |
| `customPostInitScripts`                | Custom post-init.d user scripts                                                       | `{}`               |
| `smtpHost`                             | SMTP server host                                                                      | `""`               |
| `smtpPort`                             | SMTP server port                                                                      | `""`               |
| `smtpUser`                             | SMTP username                                                                         | `""`               |
| `smtpPassword`                         | SMTP user password                                                                    | `""`               |
| `smtpProtocol`                         | SMTP protocol                                                                         | `""`               |
| `smtpExistingSecret`                   | The name of an existing secret with SMTP credentials                                  | `""`               |
| `allowEmptyPassword`                   | Allow the container to be started with blank passwords                                | `true`             |
| `allowOverrideNone`                    | Configure Apache to prohibit overriding directives with htaccess files                | `false`            |
| `overrideDatabaseSettings`             | Allow overriding the database settings persisted in wp-config.php                     | `false`            |
| `htaccessPersistenceEnabled`           | Persist custom changes on htaccess files                                              | `false`            |
| `customHTAccessCM`                     | The name of an existing ConfigMap with custom htaccess rules                          | `""`               |
| `command`                              | Override default container command (useful when using custom images)                  | `[]`               |
| `args`                                 | Override default container args (useful when using custom images)                     | `[]`               |
| `extraEnvVars`                         | Array with extra environment variables to add to the WordPress container              | `[]`               |
| `extraEnvVarsCM`                       | Name of existing ConfigMap containing extra env vars                                  | `""`               |
| `extraEnvVarsSecret`                   | Name of existing Secret containing extra env vars                                     | `""`               |

### WordPress Multisite Configuration parameters
| Name                            | Description                                                                                                                        | Value       |
| ------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- | ----------- |
| `multisite.enable`              | Whether to enable WordPress Multisite configuration.                                                                               | `false`     |
| `multisite.host`                | WordPress Multisite hostname/address. This value is mandatory when enabling Multisite mode.                                        | `""`        |
| `multisite.networkType`         | WordPress Multisite network type to enable. Allowed values: `subfolder`, `subdirectory` or `subdomain`.                            | `subdomain` |
| `multisite.enableNipIoRedirect` | Whether to enable IP address redirection to nip.io wildcard DNS. Useful when running on an IP address with subdomain network type. | `false`     |

### WordPress deployment parameters
| Name                                                | Description                                                                                                              | Value            |
| --------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ | ---------------- |
| `replicaCount`                                      | Number of WordPress replicas to deploy                                                                                   | `1`              |
| `updateStrategy.type`                               | WordPress deployment strategy type                                                                                       | `RollingUpdate`  |
| `updateStrategy.rollingUpdate`                      | WordPress deployment rolling update configuration parameters                                                             | `{}`             |
| `schedulerName`                                     | Alternate scheduler                                                                                                      | `""`             |
| `topologySpreadConstraints`                         | Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template | `[]`             |
| `priorityClassName`                                 | Name of the existing priority class to be used by WordPress pods, priority class needs to be created beforehand          | `""`             |
| `hostAliases`                                       | WordPress pod host aliases                                                                                               | `[]`             |
| `extraVolumes`                                      | Optionally specify extra list of additional volumes for WordPress pods                                                   | `[]`             |
| `extraVolumeMounts`                                 | Optionally specify extra list of additional volumeMounts for WordPress container(s)                                      | `[]`             |
| `sidecars`                                          | Add additional sidecar containers to the WordPress pod                                                                   | `[]`             |
| `initContainers`                                    | Add additional init containers to the WordPress pods                                                                     | `[]`             |
| `podLabels`                                         | Extra labels for WordPress pods                                                                                          | `{}`             |
| `podAnnotations`                                    | Annotations for WordPress pods                                                                                           | `{}`             |
| `podAffinityPreset`                                 | Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`                                      | `""`             |
| `podAntiAffinityPreset`                             | Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`                                 | `soft`           |
| `nodeAffinityPreset.type`                           | Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`                                | `""`             |
| `nodeAffinityPreset.key`                            | Node label key to match. Ignored if `affinity` is set                                                                    | `""`             |
| `nodeAffinityPreset.values`                         | Node label values to match. Ignored if `affinity` is set                                                                 | `[]`             |
| `affinity`                                          | Affinity for pod assignment                                                                                              | `{}`             |
| `nodeSelector`                                      | Node labels for pod assignment                                                                                           | `{}`             |
| `tolerations`                                       | Tolerations for pod assignment                                                                                           | `[]`             |
| `resources.limits`                                  | The resources limits for the WordPress containers                                                                        | `{}`             |
| `resources.requests.memory`                         | The requested memory for the WordPress containers                                                                        | `512Mi`          |
| `resources.requests.cpu`                            | The requested cpu for the WordPress containers                                                                           | `300m`           |
| `containerPorts.http`                               | WordPress HTTP container port                                                                                            | `8080`           |
| `containerPorts.https`                              | WordPress HTTPS container port                                                                                           | `8443`           |
| `extraContainerPorts`                               | Optionally specify extra list of additional ports for WordPress container(s)                                             | `[]`             |
| `podSecurityContext.enabled`                        | Enabled WordPress pods' Security Context                                                                                 | `true`           |
| `podSecurityContext.fsGroup`                        | Set WordPress pod's Security Context fsGroup                                                                             | `1001`           |
| `podSecurityContext.seccompProfile.type`            | Set WordPress container's Security Context seccomp profile                                                               | `RuntimeDefault` |
| `containerSecurityContext.enabled`                  | Enabled WordPress containers' Security Context                                                                           | `true`           |
| `containerSecurityContext.runAsUser`                | Set WordPress container's Security Context runAsUser                                                                     | `1001`           |
| `containerSecurityContext.runAsNonRoot`             | Set WordPress container's Security Context runAsNonRoot                                                                  | `true`           |
| `containerSecurityContext.allowPrivilegeEscalation` | Set WordPress container's privilege escalation                                                                           | `false`          |
| `containerSecurityContext.capabilities.drop`        | Set WordPress container's Security Context runAsNonRoot                                                                  | `["ALL"]`        |
| `livenessProbe.enabled`                             | Enable livenessProbe on WordPress containers                                                                             | `true`           |
| `livenessProbe.initialDelaySeconds`                 | Initial delay seconds for livenessProbe                                                                                  | `120`            |
| `livenessProbe.periodSeconds`                       | Period seconds for livenessProbe                                                                                         | `10`             |
| `livenessProbe.timeoutSeconds`                      | Timeout seconds for livenessProbe                                                                                        | `5`              |
| `livenessProbe.failureThreshold`                    | Failure threshold for livenessProbe                                                                                      | `6`              |
| `livenessProbe.successThreshold`                    | Success threshold for livenessProbe                                                                                      | `1`              |
| `readinessProbe.enabled`                            | Enable readinessProbe on WordPress containers                                                                            | `true`           |
| `readinessProbe.initialDelaySeconds`                | Initial delay seconds for readinessProbe                                                                                 | `30`             |
| `readinessProbe.periodSeconds`                      | Period seconds for readinessProbe                                                                                        | `10`             |
| `readinessProbe.timeoutSeconds`                     | Timeout seconds for readinessProbe                                                                                       | `5`              |
| `readinessProbe.failureThreshold`                   | Failure threshold for readinessProbe                                                                                     | `6`              |
| `readinessProbe.successThreshold`                   | Success threshold for readinessProbe                                                                                     | `1`              |
| `startupProbe.enabled`                              | Enable startupProbe on WordPress containers                                                                              | `false`          |
| `startupProbe.initialDelaySeconds`                  | Initial delay seconds for startupProbe                                                                                   | `30`             |
| `startupProbe.periodSeconds`                        | Period seconds for startupProbe                                                                                          | `10`             |
| `startupProbe.timeoutSeconds`                       | Timeout seconds for startupProbe                                                                                         | `5`              |
| `startupProbe.failureThreshold`                     | Failure threshold for startupProbe                                                                                       | `6`              |
| `startupProbe.successThreshold`                     | Success threshold for startupProbe                                                                                       | `1`              |
| `customLivenessProbe`                               | Custom livenessProbe that overrides the default one                                                                      | `{}`             |
| `customReadinessProbe`                              | Custom readinessProbe that overrides the default one                                                                     | `{}`             |
| `customStartupProbe`                                | Custom startupProbe that overrides the default one                                                                       | `{}`             |
| `lifecycleHooks`                                    | for the WordPress container(s) to automate configuration before or after startup                                         | `{}`             |

### Traffic Exposure Parameters
| Name                               | Description                                                                                                                      | Value                    |
| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | ------------------------ |
| `service.type`                     | WordPress service type                                                                                                           | `LoadBalancer`           |
| `service.ports.http`               | WordPress service HTTP port                                                                                                      | `80`                     |
| `service.ports.https`              | WordPress service HTTPS port                                                                                                     | `443`                    |
| `service.httpsTargetPort`          | Target port for HTTPS                                                                                                            | `https`                  |
| `service.nodePorts.http`           | Node port for HTTP                                                                                                               | `""`                     |
| `service.nodePorts.https`          | Node port for HTTPS                                                                                                              | `""`                     |
| `service.sessionAffinity`          | Control where client requests go, to the same pod or round-robin                                                                 | `None`                   |
| `service.sessionAffinityConfig`    | Additional settings for the sessionAffinity                                                                                      | `{}`                     |
| `service.clusterIP`                | WordPress service Cluster IP                                                                                                     | `""`                     |
| `service.loadBalancerIP`           | WordPress service Load Balancer IP                                                                                               | `""`                     |
| `service.loadBalancerSourceRanges` | WordPress service Load Balancer sources                                                                                          | `[]`                     |
| `service.externalTrafficPolicy`    | WordPress service external traffic policy                                                                                        | `Cluster`                |
| `service.annotations`              | Additional custom annotations for WordPress service                                                                              | `{}`                     |
| `service.extraPorts`               | Extra port to expose on WordPress service                                                                                        | `[]`                     |
| `ingress.enabled`                  | Enable ingress record generation for WordPress                                                                                   | `false`                  |
| `ingress.pathType`                 | Ingress path type                                                                                                                | `ImplementationSpecific` |
| `ingress.apiVersion`               | Force Ingress API version (automatically detected if not set)                                                                    | `""`                     |
| `ingress.ingressClassName`         | IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)                                                    | `""`                     |
| `ingress.hostname`                 | Default host for the ingress record                                                                                              | `wordpress.local`        |
| `ingress.path`                     | Default path for the ingress record                                                                                              | `/`                      |
| `ingress.annotations`              | Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations. | `{}`                     |
| `ingress.tls`                      | Enable TLS configuration for the host defined at `ingress.hostname` parameter                                                    | `false`                  |
| `ingress.selfSigned`               | Create a TLS secret for this ingress record using self-signed certificates generated by Helm                                     | `false`                  |
| `ingress.extraHosts`               | An array with additional hostname(s) to be covered with the ingress record                                                       | `[]`                     |
| `ingress.extraPaths`               | An array with additional arbitrary paths that may need to be added to the ingress under the main host                            | `[]`                     |
| `ingress.extraTls`                 | TLS configuration for additional hostname(s) to be covered with this ingress record                                              | `[]`                     |
| `ingress.secrets`                  | Custom TLS certificates as secrets                                                                                               | `[]`                     |
| `ingress.extraRules`               | Additional rules to be covered with this ingress record                                                                          | `[]`                     |

### Persistence Parameters
| Name                                                   | Description                                                                                                   | Value                   |
| ------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------- | ----------------------- |
| `persistence.enabled`                                  | Enable persistence using Persistent Volume Claims                                                             | `true`                  |
| `persistence.storageClass`                             | Persistent Volume storage class                                                                               | `""`                    |
| `persistence.accessModes`                              | Persistent Volume access modes                                                                                | `[]`                    |
| `persistence.accessMode`                               | Persistent Volume access mode (DEPRECATED: use `persistence.accessModes` instead)                             | `ReadWriteOnce`         |
| `persistence.size`                                     | Persistent Volume size                                                                                        | `10Gi`                  |
| `persistence.dataSource`                               | Custom PVC data source                                                                                        | `{}`                    |
| `persistence.existingClaim`                            | The name of an existing PVC to use for persistence                                                            | `""`                    |
| `persistence.selector`                                 | Selector to match an existing Persistent Volume for WordPress data PVC                                        | `{}`                    |
| `persistence.annotations`                              | Persistent Volume Claim annotations                                                                           | `{}`                    |
| `volumePermissions.enabled`                            | Enable init container that changes the owner/group of the PV mount point to `runAsUser:fsGroup`               | `false`                 |
| `volumePermissions.image.registry`                     | Bitnami Shell image registry                                                                                  | `docker.io`             |
| `volumePermissions.image.repository`                   | Bitnami Shell image repository                                                                                | `bitnami/bitnami-shell` |
| `volumePermissions.image.tag`                          | Bitnami Shell image tag (immutable tags are recommended)                                                      | `11-debian-11-r63`      |
| `volumePermissions.image.digest`                       | Bitnami Shell image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag | `""`                    |
| `volumePermissions.image.pullPolicy`                   | Bitnami Shell image pull policy                                                                               | `IfNotPresent`          |
| `volumePermissions.image.pullSecrets`                  | Bitnami Shell image pull secrets                                                                              | `[]`                    |
| `volumePermissions.resources.limits`                   | The resources limits for the init container                                                                   | `{}`                    |
| `volumePermissions.resources.requests`                 | The requested resources for the init container                                                                | `{}`                    |
| `volumePermissions.containerSecurityContext.runAsUser` | User ID for the init container                                                                                | `0`                     |

### Other Parameters

| Name                                          | Description                                                            | Value   |
| --------------------------------------------- | ---------------------------------------------------------------------- | ------- |
| `serviceAccount.create`                       | Enable creation of ServiceAccount for WordPress pod                    | `false` |
| `serviceAccount.name`                         | The name of the ServiceAccount to use.                                 | `""`    |
| `serviceAccount.automountServiceAccountToken` | Allows auto mount of ServiceAccountToken on the serviceAccount created | `true`  |
| `serviceAccount.annotations`                  | Additional custom annotations for the ServiceAccount                   | `{}`    |
| `pdb.create`                                  | Enable a Pod Disruption Budget creation                                | `false` |
| `pdb.minAvailable`                            | Minimum number/percentage of pods that should remain scheduled         | `1`     |
| `pdb.maxUnavailable`                          | Maximum number/percentage of pods that may be made unavailable         | `""`    |
| `autoscaling.enabled`                         | Enable Horizontal POD autoscaling for WordPress                        | `false` |
| `autoscaling.minReplicas`                     | Minimum number of WordPress replicas                                   | `1`     |
| `autoscaling.maxReplicas`                     | Maximum number of WordPress replicas                                   | `11`    |
| `autoscaling.targetCPU`                       | Target CPU utilization percentage                                      | `50`    |
| `autoscaling.targetMemory`                    | Target Memory utilization percentage                                   | `50`    |

### Metrics Parameters
| Name                                         | Description                                                                                                     | Value                     |
| -------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | ------------------------- |
| `metrics.enabled`                            | Start a sidecar prometheus exporter to expose metrics                                                           | `false`                   |
| `metrics.image.registry`                     | Apache exporter image registry                                                                                  | `docker.io`               |
| `metrics.image.repository`                   | Apache exporter image repository                                                                                | `bitnami/apache-exporter` |
| `metrics.image.tag`                          | Apache exporter image tag (immutable tags are recommended)                                                      | `0.11.0-debian-11-r73`    |
| `metrics.image.digest`                       | Apache exporter image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag | `""`                      |
| `metrics.image.pullPolicy`                   | Apache exporter image pull policy                                                                               | `IfNotPresent`            |
| `metrics.image.pullSecrets`                  | Apache exporter image pull secrets                                                                              | `[]`                      |
| `metrics.containerPorts.metrics`             | Prometheus exporter container port                                                                              | `9117`                    |
| `metrics.livenessProbe.enabled`              | Enable livenessProbe on Prometheus exporter containers                                                          | `true`                    |
| `metrics.livenessProbe.initialDelaySeconds`  | Initial delay seconds for livenessProbe                                                                         | `15`                      |
| `metrics.livenessProbe.periodSeconds`        | Period seconds for livenessProbe                                                                                | `10`                      |
| `metrics.livenessProbe.timeoutSeconds`       | Timeout seconds for livenessProbe                                                                               | `5`                       |
| `metrics.livenessProbe.failureThreshold`     | Failure threshold for livenessProbe                                                                             | `3`                       |
| `metrics.livenessProbe.successThreshold`     | Success threshold for livenessProbe                                                                             | `1`                       |
| `metrics.readinessProbe.enabled`             | Enable readinessProbe on Prometheus exporter containers                                                         | `true`                    |
| `metrics.readinessProbe.initialDelaySeconds` | Initial delay seconds for readinessProbe                                                                        | `5`                       |
| `metrics.readinessProbe.periodSeconds`       | Period seconds for readinessProbe                                                                               | `10`                      |
| `metrics.readinessProbe.timeoutSeconds`      | Timeout seconds for readinessProbe                                                                              | `3`                       |
| `metrics.readinessProbe.failureThreshold`    | Failure threshold for readinessProbe                                                                            | `3`                       |
| `metrics.readinessProbe.successThreshold`    | Success threshold for readinessProbe                                                                            | `1`                       |
| `metrics.startupProbe.enabled`               | Enable startupProbe on Prometheus exporter containers                                                           | `false`                   |
| `metrics.startupProbe.initialDelaySeconds`   | Initial delay seconds for startupProbe                                                                          | `10`                      |
| `metrics.startupProbe.periodSeconds`         | Period seconds for startupProbe                                                                                 | `10`                      |
| `metrics.startupProbe.timeoutSeconds`        | Timeout seconds for startupProbe                                                                                | `1`                       |
| `metrics.startupProbe.failureThreshold`      | Failure threshold for startupProbe                                                                              | `15`                      |
| `metrics.startupProbe.successThreshold`      | Success threshold for startupProbe                                                                              | `1`                       |
| `metrics.customLivenessProbe`                | Custom livenessProbe that overrides the default one                                                             | `{}`                      |
| `metrics.customReadinessProbe`               | Custom readinessProbe that overrides the default one                                                            | `{}`                      |
| `metrics.customStartupProbe`                 | Custom startupProbe that overrides the default one                                                              | `{}`                      |
| `metrics.resources.limits`                   | The resources limits for the Prometheus exporter container                                                      | `{}`                      |
| `metrics.resources.requests`                 | The requested resources for the Prometheus exporter container                                                   | `{}`                      |
| `metrics.service.ports.metrics`              | Prometheus metrics service port                                                                                 | `9150`                    |
| `metrics.service.annotations`                | Additional custom annotations for Metrics service                                                               | `{}`                      |
| `metrics.serviceMonitor.enabled`             | Create ServiceMonitor Resource for scraping metrics using Prometheus Operator                                   | `false`                   |
| `metrics.serviceMonitor.namespace`           | Namespace for the ServiceMonitor Resource (defaults to the Release Namespace)                                   | `""`                      |
| `metrics.serviceMonitor.interval`            | Interval at which metrics should be scraped.                                                                    | `""`                      |
| `metrics.serviceMonitor.scrapeTimeout`       | Timeout after which the scrape is ended                                                                         | `""`                      |
| `metrics.serviceMonitor.labels`              | Additional labels that can be used so ServiceMonitor will be discovered by Prometheus                           | `{}`                      |
| `metrics.serviceMonitor.selector`            | Prometheus instance selector labels                                                                             | `{}`                      |
| `metrics.serviceMonitor.relabelings`         | RelabelConfigs to apply to samples before scraping                                                              | `[]`                      |
| `metrics.serviceMonitor.metricRelabelings`   | MetricRelabelConfigs to apply to samples before ingestion                                                       | `[]`                      |
| `metrics.serviceMonitor.honorLabels`         | Specify honorLabels parameter to add the scrape endpoint                                                        | `false`                   |
| `metrics.serviceMonitor.jobLabel`            | The name of the label on the target service to use as the job name in prometheus.                               | `""`                      |

### NetworkPolicy parameters

| Name                                                          | Description                                                                                                                  | Value   |
| ------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ------- |
| `networkPolicy.enabled`                                       | Enable network policies                                                                                                      | `false` |
| `networkPolicy.metrics.enabled`                               | Enable network policy for metrics (prometheus)                                                                               | `false` |
| `networkPolicy.metrics.namespaceSelector`                     | Monitoring namespace selector labels. These labels will be used to identify the prometheus' namespace.                       | `{}`    |
| `networkPolicy.metrics.podSelector`                           | Monitoring pod selector labels. These labels will be used to identify the Prometheus pods.                                   | `{}`    |
| `networkPolicy.ingress.enabled`                               | Enable network policy for Ingress Proxies                                                                                    | `false` |
| `networkPolicy.ingress.namespaceSelector`                     | Ingress Proxy namespace selector labels. These labels will be used to identify the Ingress Proxy's namespace.                | `{}`    |
| `networkPolicy.ingress.podSelector`                           | Ingress Proxy pods selector labels. These labels will be used to identify the Ingress Proxy pods.                            | `{}`    |
| `networkPolicy.ingressRules.backendOnlyAccessibleByFrontend`  | Enable ingress rule that makes the backend (mariadb) only accessible by testlink's pods.                                     | `false` |
| `networkPolicy.ingressRules.customBackendSelector`            | Backend selector labels. These labels will be used to identify the backend pods.                                             | `{}`    |
| `networkPolicy.ingressRules.accessOnlyFrom.enabled`           | Enable ingress rule that makes testlink only accessible from a particular origin                                             | `false` |
| `networkPolicy.ingressRules.accessOnlyFrom.namespaceSelector` | Namespace selector label that is allowed to access testlink. This label will be used to identified the allowed namespace(s). | `{}`    |
| `networkPolicy.ingressRules.accessOnlyFrom.podSelector`       | Pods selector label that is allowed to access testlink. This label will be used to identified the allowed pod(s).            | `{}`    |
| `networkPolicy.ingressRules.customRules`                      | Custom network policy ingress rule                                                                                           | `{}`    |
| `networkPolicy.egressRules.denyConnectionsToExternal`         | Enable egress rule that denies outgoing traffic outside the cluster, except for DNS (port 53).                               | `false` |
| `networkPolicy.egressRules.customRules`                       | Custom network policy rule                                                                                                   | `{}`    |

## Database Parameters
| Name                                       | Description                                                                       | Value               |
| ------------------------------------------ | --------------------------------------------------------------------------------- | ------------------- |
| `mariadb.enabled`                          | Deploy a MariaDB server to satisfy the applications database requirements         | `true`              |
| `mariadb.architecture`                     | MariaDB architecture. Allowed values: `standalone` or `replication`               | `standalone`        |
| `mariadb.auth.rootPassword`                | MariaDB root password                                                             | `""`                |
| `mariadb.auth.database`                    | MariaDB custom database                                                           | `bitnami_wordpress` |
| `mariadb.auth.username`                    | MariaDB custom user name                                                          | `bn_wordpress`      |
| `mariadb.auth.password`                    | MariaDB custom user password                                                      | `""`                |
| `mariadb.primary.persistence.enabled`      | Enable persistence on MariaDB using PVC(s)                                        | `true`              |
| `mariadb.primary.persistence.storageClass` | Persistent Volume storage class                                                   | `""`                |
| `mariadb.primary.persistence.accessModes`  | Persistent Volume access modes                                                    | `[]`                |
| `mariadb.primary.persistence.size`         | Persistent Volume size                                                            | `8Gi`               |
| `externalDatabase.host`                    | External Database server host                                                     | `localhost`         |
| `externalDatabase.port`                    | External Database server port                                                     | `3306`              |
| `externalDatabase.user`                    | External Database username                                                        | `bn_wordpress`      |
| `externalDatabase.password`                | External Database user password                                                   | `""`                |
| `externalDatabase.database`                | External Database database name                                                   | `bitnami_wordpress` |
| `externalDatabase.existingSecret`          | The name of an existing secret with database credentials. Evaluated as a template | `""`                |
| `memcached.enabled`                        | Deploy a Memcached server for caching database queries                            | `false`             |
| `memcached.auth.enabled`                   | Enable Memcached authentication                                                   | `false`             |
| `memcached.auth.username`                  | Memcached admin user                                                              | `""`                |
| `memcached.auth.password`                  | Memcached admin password                                                          | `""`                |
| `memcached.service.port`                   | Memcached service port                                                            | `11211`             |
| `externalCache.host`                       | External cache server host                                                        | `localhost`         |
| `externalCache.port`                       | External cache server port                                                        | `11211`             |

The above parameters map to the env variables defined in [bitnami/wordpress](https://github.com/bitnami/containers/tree/main/bitnami/wordpress). For more information please refer to the [bitnami/wordpress](https://github.com/bitnami/containers/tree/main/bitnami/wordpress) image documentation.

Specify each parameter using the `--set key=value[,key=value]` argument to `helm install`. For example,

```console
helm install my-release \
  --set wordpressUsername=admin \
  --set wordpressPassword=password \
  --set mariadb.auth.rootPassword=secretpassword \
    my-repo/wordpress
``
The above command sets the WordPress administrator account username and password to `admin` and `password` respectively. Additionally, it sets the MariaDB `root` user password to `secretpassword`.

> NOTE: Once this chart is deployed, it is not possible to change the application's access credentials, such as usernames or passwords, using Helm. To change these application credentials after deployment, delete any persistent volumes (PVs) used by the chart and re-deploy it, or use the application's built-in administrative tools if available.

Alternatively, a YAML file that specifies the values for the above parameters can be provided while installing the chart. For example,

```console
helm install my-release -f values.yaml my-repo/wordpress
```
> **Tip**: You can use the default [values.yaml](values.yaml)

## Configuration and installation details

### [Rolling VS Immutable tags](https://docs.bitnami.com/containers/how-to/understand-rolling-tags-containers/)

It is strongly recommended to use immutable tags in a production environment. This ensures your deployment does not change automatically if the same tag is updated with a different image.

Bitnami will release a new chart updating its containers if a new version of the main container, significant changes, or critical vulnerabilities exist.

### Known limitations

When performing admin operations that require activating the maintenance mode (such as updating a plugin or theme), it's activated in only one replica (see: [bug report](https://core.trac.wordpress.org/ticket/50797)). This implies that WP could be attending requests on other replicas while performing admin operations, with unpredictable consequences.

To avoid that, you can manually activate/deactivate the maintenance mode on every replica using the WP CLI. For instance, if you installed WP with three replicas, you can run the commands below to activate the maintenance mode in all of them (assuming that the release name is `wordpress`):

```console
kubectl exec $(kubectl get pods -l app.kubernetes.io/name=wordpress -o jsonpath='{.items[0].metadata.name}') -c wordpress -- wp maintenance-mode activate
kubectl exec $(kubectl get pods -l app.kubernetes.io/name=wordpress -o jsonpath='{.items[1].metadata.name}') -c wordpress -- wp maintenance-mode activate
kubectl exec $(kubectl get pods -l app.kubernetes.io/name=wordpress -o jsonpath='{.items[2].metadata.name}') -c wordpress -- wp maintenance-mode activate
``
### External database support

You may want to have WordPress connect to an external database rather than installing one inside your cluster. Typical reasons for this are to use a managed database service, or to share a common database server for all your applications. To achieve this, the chart allows you to specify credentials for an external database with the [`externalDatabase` parameter](#database-parameters). You should also disable the MariaDB installation with the `mariadb.enabled` option. Here is an example:

```console
mariadb.enabled=false
externalDatabase.host=myexternalhost
externalDatabase.user=myuser
externalDatabase.password=mypassword
externalDatabase.database=mydatabase
externalDatabase.port=3306
```
Refer to the [documentation on using an external database with WordPress](https://docs.bitnami.com/kubernetes/apps/wordpress/configuration/use-external-database/) and the [tutorial on integrating WordPress with a managed cloud database](https://docs.bitnami.com/tutorials/secure-wordpress-kubernetes-managed-database-ssl-upgrades/) for more information.
### Memcached
This chart provides support for using Memcached to cache database queries and objects improving the website performance. To enable this feature, set `wordpressConfigureCache` and `memcached.enabled` parameters to `true`.
When this feature is enabled, a Memcached server will be deployed in your K8s cluster using the Bitnami Memcached chart and the [W3 Total Cache](https://wordpress.org/plugins/w3-total-cache/) plugin will be activated and configured to use the Memcached server for database caching.
It is also possible to use an external cache server rather than installing one inside your cluster. To achieve this, the chart allows you to specify credentials for an external cache server with the [`externalCache` parameter](#database-parameters). You should also disable the Memcached installation with the `memcached.enabled` option. Here is an example:
```console
wordpressConfigureCache=true
memcached.enabled=false
externalCache.host=myexternalcachehost
externalCache.port=11211
```
### Ingress
This chart provides support for Ingress resources. If an Ingress controller, such as nginx-ingress or traefik, that Ingress controller can be used to serve WordPress.
To enable Ingress integration, set `ingress.enabled` to `true`. The `ingress.hostname` property can be used to set the host name. The `ingress.tls` parameter can be used to add the TLS configuration for this host. It is also possible to have more than one host, with a separate TLS configuration for each host. [Learn more about configuring and using Ingress](https://docs.bitnami.com/kubernetes/apps/wordpress/configuration/configure-ingress/).
### TLS secrets

The chart also facilitates the creation of TLS secrets for use with the Ingress controller, with different options for certificate management. [Learn more about TLS secrets](https://docs.bitnami.com/kubernetes/apps/wordpress/administration/enable-tls-ingress/).

### `.htaccess` files

For performance and security reasons, it is a good practice to configure Apache with the `AllowOverride None` directive. Instead of using `.htaccess` files, Apache will load the same directives at boot time. These directives are located in `/opt/bitnami/wordpress/wordpress-htaccess.conf`.

By default, the container image includes all the default `.htaccess` files in WordPress (together with the default plugins). To enable this feature, install the chart with the value `allowOverrideNone=yes`.

[Learn more about working with `.htaccess` files](https://docs.bitnami.com/kubernetes/apps/wordpress/configuration/understand-htaccess/).

## Persistence

The [Bitnami WordPress](https://github.com/bitnami/containers/tree/main/bitnami/wordpress) image stores the WordPress data and configurations at the `/bitnami` path of the container. Persistent Volume Claims are used to keep the data across deployments.

If you encounter errors when working with persistent volumes, refer to our [troubleshooting guide for persistent volumes](https://docs.bitnami.com/kubernetes/faq/troubleshooting/troubleshooting-persistence-volumes/).

### Additional environment variables

In case you want to add extra environment variables (useful for advanced operations like custom init scripts), you can use the `extraEnvVars` property.

```yaml
wordpress:
  extraEnvVars:
    - name: LOG_LEVEL
      value: error
```

Alternatively, you can use a ConfigMap or a Secret with the environment variables. To do so, use the `extraEnvVarsCM` or the `extraEnvVarsSecret` values.

### Sidecars

If additional containers are needed in the same pod as WordPress (such as additional metrics or logging exporters), they can be defined using the `sidecars` parameter. If these sidecars export extra ports, extra port definitions can be added using the `service.extraPorts` parameter. [Learn more about configuring and using sidecar containers](https://docs.bitnami.com/kubernetes/apps/wordpress/configuration/configure-sidecar-init-containers/).

### Pod affinity

This chart allows you to set your custom affinity using the `affinity` parameter. Learn more about Pod affinity in the [kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity).

As an alternative, use one of the preset configurations for pod affinity, pod anti-affinity, and node affinity available at the [bitnami/common](https://github.com/bitnami/charts/tree/main/bitnami/common#affinities) chart. To do so, set the `podAffinityPreset`, `podAntiAffinityPreset`, or `nodeAffinityPreset` parameters.

## Troubleshooting

Find more information about how to deal with common errors related to Bitnami's Helm charts in [this troubleshooting guide](https://docs.bitnami.com/general/how-to/troubleshoot-helm-chart-issues).

## Notable changes

### 13.2.0

Removed support for limiting auto-updates to WordPress core via the `wordpressAutoUpdateLevel` option. To update WordPress core, we recommend you use the `helm upgrade` command to update your deployment instead of using the built-in update functionality.

### 11.0.0

The [Bitnami WordPress](https://github.com/bitnami/containers/tree/main/bitnami/wordpress) image was refactored and now the source code is published in GitHub in the `rootfs` folder of the container image.

In addition, several new features have been implemented:

- Multisite mode is now supported via `multisite.*` options.
- Plugins can be installed and activated on the first deployment via the `wordpressPlugins` option.
- Added support for limiting auto-updates to WordPress core via the `wordpressAutoUpdateLevel` option. In addition, auto-updates have been disabled by default. To update WordPress core, we recommend to swap the container image version for your deployment instead of using the built-in update functionality.

To enable the new features, it is not possible to do it by upgrading an existing deployment. Instead, it is necessary to perform a fresh deploy.

## Upgrading

### To 14.0.0

This major release bumps the MariaDB version to 10.6. Follow the [upstream instructions](https://mariadb.com/kb/en/upgrading-from-mariadb-105-to-mariadb-106/) for upgrading from MariaDB 10.5 to 10.6. No major issues are expected during the upgrade.

### To 13.0.0

This major release renames several values in this chart and adds missing features, in order to be inline with the rest of assets in the Bitnami charts repository.

- `service.port` and `service.httpsPort` have been regrouped under the `service.ports` map.
- `metrics.service.port` has been regrouped under the `metrics.service.ports` map.
- `serviceAccountName` has been deprecated in favor of `serviceAccount` map.

Additionally updates the MariaDB & Memcached subcharts to their newest major `10.x.x` and `6.x.x`, respectively, which contain similar changes.

### To 12.0.0

WordPress version was bumped to its latest major, `5.8.x`. Though no incompatibilities are expected while upgrading from previous versions, WordPress recommends backing up your application first.

Site backups can be easily performed using tools such as [VaultPress](https://vaultpress.com/) or [All-in-One WP Migration](https://wordpress.org/plugins/all-in-one-wp-migration/).

### To 11.0.0

The [Bitnami WordPress](https://github.com/bitnami/containers/tree/main/bitnami/wordpress) image was refactored and now the source code is published in GitHub in the `rootfs` folder of the container image.

Compatibility is not guaranteed due to the amount of involved changes, however no breaking changes are expected.

### To 10.0.0

[On November 13, 2020, Helm v2 support was formally finished](https://github.com/helm/charts#status-of-the-project), this major version is the result of the required changes applied to the Helm Chart to be able to incorporate the different features added in Helm v3 and to be consistent with the Helm project itself regarding the Helm v2 EOL.

[Learn more about this change and related upgrade considerations](https://docs.bitnami.com/kubernetes/apps/wordpress/administration/upgrade-helm3/).

#### Additional upgrade notes

- MariaDB dependency version was bumped to a new major version that introduces several incompatibilities. Therefore, backwards compatibility is not guaranteed unless an external database is used. Check [MariaDB Upgrading Notes](https://github.com/bitnami/charts/tree/main/bitnami/mariadb#to-800) for more information.
- If you want to upgrade to this version from a previous one installed with Helm v3, there are two alternatives:
  - Install a new WordPress chart, and migrate your WordPress site using backup/restore tools such as [VaultPress](https://vaultpress.com/) or [All-in-One WP Migration](https://wordpress.org/plugins/all-in-one-wp-migration/).
  - Reuse the PVC used to hold the MariaDB data on your previous release. To do so, follow the instructions below (the following example assumes that the release name is `wordpress`).

> Warning: please create a backup of your database before running any of these actions. The steps below would be only valid if your application (e.g. any plugins or custom code) is compatible with MariaDB 10.5.

Obtain the credentials and the name of the PVC used to hold the MariaDB data on your current release:

```console
$ export WORDPRESS_PASSWORD=$(kubectl get secret --namespace default wordpress -o jsonpath="{.data.wordpress-password}" | base64 -d)
$ export MARIADB_ROOT_PASSWORD=$(kubectl get secret --namespace default wordpress-mariadb -o jsonpath="{.data.mariadb-root-password}" | base64 -d)
$ export MARIADB_PASSWORD=$(kubectl get secret --namespace default wordpress-mariadb -o jsonpath="{.data.mariadb-password}" | base64 -d)
$ export MARIADB_PVC=$(kubectl get pvc -l app.kubernetes.io/instance=wordpress,app.kubernetes.io/name=mariadb,app.kubernetes.io/component=primary -o jsonpath="{.items[0].metadata.name}")
```

Upgrade your release (maintaining the version) disabling MariaDB and scaling WordPress replicas to 0:

```console
$ helm upgrade wordpress my-repo/wordpress --set wordpressPassword=$WORDPRESS_PASSWORD --set replicaCount=0 --set mariadb.enabled=false --version 9.6.4
```

Finally, upgrade you release to `10.0.0` reusing the existing PVC, and enabling back MariaDB:

```console
$ helm upgrade wordpress my-repo/wordpress --set mariadb.primary.persistence.existingClaim=$MARIADB_PVC --set mariadb.auth.rootPassword=$MARIADB_ROOT_PASSWORD --set mariadb.auth.password=$MARIADB_PASSWORD --set wordpressPassword=$WORDPRESS_PASSWORD
```

You should see the lines below in MariaDB container logs:

```console
$ kubectl logs $(kubectl get pods -l app.kubernetes.io/instance=wordpress,app.kubernetes.io/name=mariadb,app.kubernetes.io/component=primary -o jsonpath="{.items[0].metadata.name}")
...
mariadb 12:13:24.98 INFO  ==> Using persisted data
mariadb 12:13:25.01 INFO  ==> Running mysql_upgrade
...
```

### To 9.0.0

The [Bitnami WordPress](https://github.com/bitnami/containers/tree/main/bitnami/wordpress) image was migrated to a "non-root" user approach. Previously the container ran as the `root` user and the Apache daemon was started as the `daemon` user. From now on, both the container and the Apache daemon run as user `1001`. You can revert this behavior by setting the parameters `securityContext.runAsUser`, and `securityContext.fsGroup` to `0`.
Chart labels and Ingress configuration were also adapted to follow the Helm charts best practices.

Consequences:

- The HTTP/HTTPS ports exposed by the container are now `8080/8443` instead of `80/443`.
- No writing permissions will be granted on `wp-config.php` by default.
- Backwards compatibility is not guaranteed.

To upgrade to `9.0.0`, it's recommended to install a new WordPress chart, and migrate your WordPress site using backup/restore tools such as [VaultPress](https://vaultpress.com/) or [All-in-One WP Migration](https://wordpress.org/plugins/all-in-one-wp-migration/).

### To 8.0.0

Helm performs a lookup for the object based on its group (apps), version (v1), and kind (Deployment). Also known as its GroupVersionKind, or GVK. Changing the GVK is considered a compatibility breaker from Kubernetes' point of view, so you cannot "upgrade" those objects to the new GVK in-place. Earlier versions of Helm 3 did not perform the lookup correctly which has since been fixed to match the spec.

In https://github.com/helm/charts/pulls/12642 the `apiVersion` of the deployment resources was updated to `apps/v1` in tune with the API's deprecated, resulting in compatibility breakage.

This major version signifies this change.

### To 3.0.0

Backwards compatibility is not guaranteed unless you modify the labels used on the chart's deployments.
Use the workaround below to upgrade from versions previous to `3.0.0`. The following example assumes that the release name is `wordpress`:

```console
kubectl patch deployment wordpress-wordpress --type=json -p='[{"op": "remove", "path": "/spec/selector/matchLabels/chart"}]'
kubectl delete statefulset wordpress-mariadb --cascade=false
```

## License

Copyright &copy; 2022 Bitnami

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License  
##########################################################################################################################################################
"
anji@master:~$  helm show values bitnami/wordpress --version  15.2.22 
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
## @section Global parameters
## Global Docker image parameters
## Please, note that this will override the image parameters, including dependencies, configured to use the global value
## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass
##

## @param global.imageRegistry Global Docker image registry
## @param global.imagePullSecrets Global Docker registry secret names as an array
## @param global.storageClass Global StorageClass for Persistent Volume(s)
##
global:
  imageRegistry: ""
  ## E.g.
  ## imagePullSecrets:
  ##   - myRegistryKeySecretName
  ##
  imagePullSecrets: []
  storageClass: ""

## @section Common parameters
##

## @param kubeVersion Override Kubernetes version
##
kubeVersion: ""
## @param nameOverride String to partially override common.names.fullname template (will maintain the release name)
##
nameOverride: ""
## @param fullnameOverride String to fully override common.names.fullname template
##
fullnameOverride: ""
## @param commonLabels Labels to add to all deployed resources
##
commonLabels: {}
## @param commonAnnotations Annotations to add to all deployed resources
##
commonAnnotations: {}
## @param clusterDomain Kubernetes Cluster Domain
##
clusterDomain: cluster.local
## @param extraDeploy Array of extra objects to deploy with the release
##
extraDeploy: []

## Enable diagnostic mode in the deployment
##
diagnosticMode:
  ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)
  ##
  enabled: false
  ## @param diagnosticMode.command Command to override all containers in the deployment
  ##
  command:
    - sleep
  ## @param diagnosticMode.args Args to override all containers in the deployment
  ##
  args:
    - infinity

## @section WordPress Image parameters
##

## Bitnami WordPress image
## ref: https://hub.docker.com/r/bitnami/wordpress/tags/
## @param image.registry WordPress image registry
## @param image.repository WordPress image repository
## @param image.tag WordPress image tag (immutable tags are recommended)
## @param image.digest WordPress image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
## @param image.pullPolicy WordPress image pull policy
## @param image.pullSecrets WordPress image pull secrets
## @param image.debug Specify if debug values should be set
##
image:
  registry: docker.io
  repository: bitnami/wordpress
  tag: 6.1.1-debian-11-r15
  digest: ""
  ## Specify a imagePullPolicy
  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
  ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  ##
  pullPolicy: IfNotPresent
  ## Optionally specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ## e.g:
  ## pullSecrets:
  ##   - myRegistryKeySecretName
  ##
  pullSecrets: []
  ## Enable debug mode
  ##
  debug: false

## @section WordPress Configuration parameters
## WordPress settings based on environment variables
## ref: https://github.com/bitnami/containers/tree/main/bitnami/wordpress#environment-variables
##

## @param wordpressUsername WordPress username
##
wordpressUsername: user
## @param wordpressPassword WordPress user password
## Defaults to a random 10-character alphanumeric string if not set
##
wordpressPassword: ""
## @param existingSecret Name of existing secret containing WordPress credentials
## NOTE: Must contain key `wordpress-password`
## NOTE: When it's set, the `wordpressPassword` parameter is ignored
##
existingSecret: ""
## @param wordpressEmail WordPress user email
##
wordpressEmail: user@example.com
## @param wordpressFirstName WordPress user first name
##
wordpressFirstName: FirstName
## @param wordpressLastName WordPress user last name
##
wordpressLastName: LastName
## @param wordpressBlogName Blog name
##
wordpressBlogName: User's Blog!
## @param wordpressTablePrefix Prefix to use for WordPress database tables
##
wordpressTablePrefix: wp_
## @param wordpressScheme Scheme to use to generate WordPress URLs
##
wordpressScheme: http
## @param wordpressSkipInstall Skip wizard installation
## NOTE: useful if you use an external database that already contains WordPress data
## ref: https://github.com/bitnami/containers/tree/main/bitnami/wordpress#connect-wordpress-docker-container-to-an-existing-database
##
wordpressSkipInstall: false
## @param wordpressExtraConfigContent Add extra content to the default wp-config.php file
## e.g:
## wordpressExtraConfigContent: |
##   @ini_set( 'post_max_size', '128M');
##   @ini_set( 'memory_limit', '256M' );
##
wordpressExtraConfigContent: ""
## @param wordpressConfiguration The content for your custom wp-config.php file (advanced feature)
## NOTE: This will override configuring WordPress based on environment variables (including those set by the chart)
## NOTE: Currently only supported when `wordpressSkipInstall=true`
##
wordpressConfiguration: ""
## @param existingWordPressConfigurationSecret The name of an existing secret with your custom wp-config.php file (advanced feature)
## NOTE: When it's set the `wordpressConfiguration` parameter is ignored
##
existingWordPressConfigurationSecret: ""
## @param wordpressConfigureCache Enable W3 Total Cache plugin and configure cache settings
## NOTE: useful if you deploy Memcached for caching database queries or you use an external cache server
##
wordpressConfigureCache: false
## @param wordpressPlugins Array of plugins to install and activate. Can be specified as `all` or `none`.
## NOTE: If set to all, only plugins that are already installed will be activated, and if set to none, no plugins will be activated
##
wordpressPlugins: none
## @param apacheConfiguration The content for your custom httpd.conf file (advanced feature)
##
apacheConfiguration: ""
## @param existingApacheConfigurationConfigMap The name of an existing secret with your custom httpd.conf file (advanced feature)
## NOTE: When it's set the `apacheConfiguration` parameter is ignored
##
existingApacheConfigurationConfigMap: ""
## @param customPostInitScripts Custom post-init.d user scripts
## ref: https://github.com/bitnami/containers/tree/main/bitnami/wordpress
## NOTE: supported formats are `.sh`, `.sql` or `.php`
## NOTE: scripts are exclusively executed during the 1st boot of the container
## e.g:
## customPostInitScripts:
##   enable-multisite.sh: |
##     #!/bin/bash
##     chmod +w /bitnami/wordpress/wp-config.php
##     wp core multisite-install --url=example.com --title="Welcome to the WordPress Multisite" --admin_user="doesntmatternotreallyused" --admin_password="doesntmatternotreallyused" --admin_email="user@example.com"
##     cat /docker-entrypoint-init.d/.htaccess > /bitnami/wordpress/.htaccess
##     chmod -w bitnami/wordpress/wp-config.php
##   .htaccess: |
##     RewriteEngine On
##     RewriteBase /
##     ...
##
customPostInitScripts: {}
## SMTP mail delivery configuration
## ref: https://github.com/bitnami/containers/tree/main/bitnami/wordpress/#smtp-configuration
## @param smtpHost SMTP server host
## @param smtpPort SMTP server port
## @param smtpUser SMTP username
## @param smtpPassword SMTP user password
## @param smtpProtocol SMTP protocol
##
smtpHost: ""
smtpPort: ""
smtpUser: ""
smtpPassword: ""
smtpProtocol: ""
## @param smtpExistingSecret The name of an existing secret with SMTP credentials
## NOTE: Must contain key `smtp-password`
## NOTE: When it's set, the `smtpPassword` parameter is ignored
##
smtpExistingSecret: ""
## @param allowEmptyPassword Allow the container to be started with blank passwords
##
allowEmptyPassword: true
## @param allowOverrideNone Configure Apache to prohibit overriding directives with htaccess files
##
allowOverrideNone: false
## @param overrideDatabaseSettings Allow overriding the database settings persisted in wp-config.php
##
overrideDatabaseSettings: false
## @param htaccessPersistenceEnabled Persist custom changes on htaccess files
## If `allowOverrideNone` is `false`, it will persist `/opt/bitnami/wordpress/wordpress-htaccess.conf`
## If `allowOverrideNone` is `true`, it will persist `/opt/bitnami/wordpress/.htaccess`
##
htaccessPersistenceEnabled: false
## @param customHTAccessCM The name of an existing ConfigMap with custom htaccess rules
## NOTE: Must contain key `wordpress-htaccess.conf` with the file content
## NOTE: Requires setting `allowOverrideNone=false`
##
customHTAccessCM: ""
## @param command Override default container command (useful when using custom images)
##
command: []
## @param args Override default container args (useful when using custom images)
##
args: []
## @param extraEnvVars Array with extra environment variables to add to the WordPress container
## e.g:
## extraEnvVars:
##   - name: FOO
##     value: "bar"
##
extraEnvVars: []
## @param extraEnvVarsCM Name of existing ConfigMap containing extra env vars
##
extraEnvVarsCM: ""
## @param extraEnvVarsSecret Name of existing Secret containing extra env vars
##
extraEnvVarsSecret: ""

## @section WordPress Multisite Configuration parameters
## ref: https://github.com/bitnami/containers/tree/main/bitnami/wordpress#multisite-configuration
##

## @param multisite.enable Whether to enable WordPress Multisite configuration.
## @param multisite.host WordPress Multisite hostname/address. This value is mandatory when enabling Multisite mode.
## @param multisite.networkType WordPress Multisite network type to enable. Allowed values: `subfolder`, `subdirectory` or `subdomain`.
## @param multisite.enableNipIoRedirect Whether to enable IP address redirection to nip.io wildcard DNS. Useful when running on an IP address with subdomain network type.
##
multisite:
  enable: false
  host: ""
  networkType: subdomain
  enableNipIoRedirect: false

## @section WordPress deployment parameters
##

## @param replicaCount Number of WordPress replicas to deploy
## NOTE: ReadWriteMany PVC(s) are required if replicaCount > 1
##
replicaCount: 1
## @param updateStrategy.type WordPress deployment strategy type
## @param updateStrategy.rollingUpdate WordPress deployment rolling update configuration parameters
## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
## NOTE: Set it to `Recreate` if you use a PV that cannot be mounted on multiple pods
## e.g:
## updateStrategy:
##  type: RollingUpdate
##  rollingUpdate:
##    maxSurge: 25%
##    maxUnavailable: 25%
##
updateStrategy:
  type: RollingUpdate
  rollingUpdate: {}
## @param schedulerName Alternate scheduler
## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
##
schedulerName: ""
## @param topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template
## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
##
topologySpreadConstraints: []
## @param priorityClassName Name of the existing priority class to be used by WordPress pods, priority class needs to be created beforehand
## Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
##
priorityClassName: ""
## @param hostAliases [array] WordPress pod host aliases
## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
##
hostAliases:
  ## Required for Apache exporter to work
  ##
  - ip: "127.0.0.1"
    hostnames:
      - "status.localhost"
## @param extraVolumes Optionally specify extra list of additional volumes for WordPress pods
##
extraVolumes: []
## @param extraVolumeMounts Optionally specify extra list of additional volumeMounts for WordPress container(s)
##
extraVolumeMounts: []
## @param sidecars Add additional sidecar containers to the WordPress pod
## e.g:
## sidecars:
##   - name: your-image-name
##     image: your-image
##     imagePullPolicy: Always
##     ports:
##       - name: portname
##         containerPort: 1234
##
sidecars: []
## @param initContainers Add additional init containers to the WordPress pods
## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
## e.g:
## initContainers:
##  - name: your-image-name
##    image: your-image
##    imagePullPolicy: Always
##    command: ['sh', '-c', 'copy themes and plugins from git and push to /bitnami/wordpress/wp-content. Should work with extraVolumeMounts and extraVolumes']
##
initContainers: []
## @param podLabels Extra labels for WordPress pods
## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
##
podLabels: {}
## @param podAnnotations Annotations for WordPress pods
## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
##
podAnnotations: {}
## @param podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
##
podAffinityPreset: ""
## @param podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
##
podAntiAffinityPreset: soft
## Node affinity preset
## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
##
nodeAffinityPreset:
  ## @param nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ##
  type: ""
  ## @param nodeAffinityPreset.key Node label key to match. Ignored if `affinity` is set
  ##
  key: ""
  ## @param nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set
  ## E.g.
  ## values:
  ##   - e2e-az1
  ##   - e2e-az2
  ##
  values: []
## @param affinity Affinity for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
## NOTE: podAffinityPreset, podAntiAffinityPreset, and nodeAffinityPreset will be ignored when it's set
##
affinity: {}
## @param nodeSelector Node labels for pod assignment
## ref: https://kubernetes.io/docs/user-guide/node-selection/
##
nodeSelector: {}
## @param tolerations Tolerations for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
##
tolerations: []
## WordPress containers' resource requests and limits
## ref: https://kubernetes.io/docs/user-guide/compute-resources/
## @param resources.limits The resources limits for the WordPress containers
## @param resources.requests.memory The requested memory for the WordPress containers
## @param resources.requests.cpu The requested cpu for the WordPress containers
##
resources:
  limits: {}
  requests:
    memory: 512Mi
    cpu: 300m
## Container ports
## @param containerPorts.http WordPress HTTP container port
## @param containerPorts.https WordPress HTTPS container port
##
containerPorts:
  http: 8080
  https: 8443
## @param extraContainerPorts Optionally specify extra list of additional ports for WordPress container(s)
## e.g:
## extraContainerPorts:
##   - name: myservice
##     containerPort: 9090
##
extraContainerPorts: []
## Configure Pods Security Context
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
## @param podSecurityContext.enabled Enabled WordPress pods' Security Context
## @param podSecurityContext.fsGroup Set WordPress pod's Security Context fsGroup
## @param podSecurityContext.seccompProfile.type Set WordPress container's Security Context seccomp profile
##
podSecurityContext:
  enabled: true
  fsGroup: 1001
  seccompProfile:
    type: "RuntimeDefault"
## Configure Container Security Context (only main container)
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
## @param containerSecurityContext.enabled Enabled WordPress containers' Security Context
## @param containerSecurityContext.runAsUser Set WordPress container's Security Context runAsUser
## @param containerSecurityContext.runAsNonRoot Set WordPress container's Security Context runAsNonRoot
## @param containerSecurityContext.allowPrivilegeEscalation Set WordPress container's privilege escalation
## @param containerSecurityContext.capabilities.drop Set WordPress container's Security Context runAsNonRoot
##
containerSecurityContext:
  enabled: true
  runAsUser: 1001
  runAsNonRoot: true
  allowPrivilegeEscalation: false
  capabilities:
    drop: ["ALL"]
## Configure extra options for WordPress containers' liveness, readiness and startup probes
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
## @param livenessProbe.enabled Enable livenessProbe on WordPress containers
## @skip livenessProbe.httpGet
## @param livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
## @param livenessProbe.periodSeconds Period seconds for livenessProbe
## @param livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
## @param livenessProbe.failureThreshold Failure threshold for livenessProbe
## @param livenessProbe.successThreshold Success threshold for livenessProbe
##
livenessProbe:
  enabled: true
  httpGet:
    path: /wp-admin/install.php
    port: '{{ .Values.wordpressScheme }}'
    scheme: '{{ .Values.wordpressScheme | upper }}'
    ## If using an HTTPS-terminating load-balancer, the probes may need to behave
    ## like the balancer to prevent HTTP 302 responses. According to the Kubernetes
    ## docs, 302 should be considered "successful", but this issue on GitHub
    ## (https://github.com/kubernetes/kubernetes/issues/47893) shows that it isn't.
    ## E.g.
    ## httpHeaders:
    ## - name: X-Forwarded-Proto
    ##   value: https
    ##
    httpHeaders: []
  initialDelaySeconds: 120
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1
## @param readinessProbe.enabled Enable readinessProbe on WordPress containers
## @skip readinessProbe.httpGet
## @param readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
## @param readinessProbe.periodSeconds Period seconds for readinessProbe
## @param readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
## @param readinessProbe.failureThreshold Failure threshold for readinessProbe
## @param readinessProbe.successThreshold Success threshold for readinessProbe
##
readinessProbe:
  enabled: true
  httpGet:
    path: /wp-login.php
    port: '{{ .Values.wordpressScheme }}'
    scheme: '{{ .Values.wordpressScheme | upper }}'
    ## If using an HTTPS-terminating load-balancer, the probes may need to behave
    ## like the balancer to prevent HTTP 302 responses. According to the Kubernetes
    ## docs, 302 should be considered "successful", but this issue on GitHub
    ## (https://github.com/kubernetes/kubernetes/issues/47893) shows that it isn't.
    ## E.g.
    ## httpHeaders:
    ## - name: X-Forwarded-Proto
    ##   value: https
    ##
    httpHeaders: []
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1
## @param startupProbe.enabled Enable startupProbe on WordPress containers
## @skip startupProbe.httpGet
## @param startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
## @param startupProbe.periodSeconds Period seconds for startupProbe
## @param startupProbe.timeoutSeconds Timeout seconds for startupProbe
## @param startupProbe.failureThreshold Failure threshold for startupProbe
## @param startupProbe.successThreshold Success threshold for startupProbe
##
startupProbe:
  enabled: false
  httpGet:
    path: /wp-login.php
    port: '{{ .Values.wordpressScheme }}'
    scheme: '{{ .Values.wordpressScheme | upper }}'
    ## If using an HTTPS-terminating load-balancer, the probes may need to behave
    ## like the balancer to prevent HTTP 302 responses. According to the Kubernetes
    ## docs, 302 should be considered "successful", but this issue on GitHub
    ## (https://github.com/kubernetes/kubernetes/issues/47893) shows that it isn't.
    ## E.g.
    ## httpHeaders:
    ## - name: X-Forwarded-Proto
    ##   value: https
    ##
    httpHeaders: []
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1
## @param customLivenessProbe Custom livenessProbe that overrides the default one
##
customLivenessProbe: {}
## @param customReadinessProbe Custom readinessProbe that overrides the default one
##
customReadinessProbe: {}
## @param customStartupProbe Custom startupProbe that overrides the default one
##
customStartupProbe: {}
## @param lifecycleHooks for the WordPress container(s) to automate configuration before or after startup
##
lifecycleHooks: {}

## @section Traffic Exposure Parameters
##

## WordPress service parameters
##
service:
  ## @param service.type WordPress service type
  ##
  type: LoadBalancer
  ## @param service.ports.http WordPress service HTTP port
  ## @param service.ports.https WordPress service HTTPS port
  ##
  ports:
    http: 80
    https: 443
  ## @param service.httpsTargetPort Target port for HTTPS
  ##
  httpsTargetPort: https
  ## Node ports to expose
  ## @param service.nodePorts.http Node port for HTTP
  ## @param service.nodePorts.https Node port for HTTPS
  ## NOTE: choose port between <30000-32767>
  ##
  nodePorts:
    http: ""
    https: ""
  ## @param service.sessionAffinity Control where client requests go, to the same pod or round-robin
  ## Values: ClientIP or None
  ## ref: https://kubernetes.io/docs/user-guide/services/
  ##
  sessionAffinity: None
  ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
  ## sessionAffinityConfig:
  ##   clientIP:
  ##     timeoutSeconds: 300
  ##
  sessionAffinityConfig: {}
  ## @param service.clusterIP WordPress service Cluster IP
  ## e.g.:
  ## clusterIP: None
  ##
  clusterIP: ""
  ## @param service.loadBalancerIP WordPress service Load Balancer IP
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
  ##
  loadBalancerIP: ""
  ## @param service.loadBalancerSourceRanges WordPress service Load Balancer sources
  ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
  ## e.g:
  ## loadBalancerSourceRanges:
  ##   - 10.10.10.0/24
  ##
  loadBalancerSourceRanges: []
  ## @param service.externalTrafficPolicy WordPress service external traffic policy
  ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
  ##
  externalTrafficPolicy: Cluster
  ## @param service.annotations Additional custom annotations for WordPress service
  ##
  annotations: {}
  ## @param service.extraPorts Extra port to expose on WordPress service
  ##
  extraPorts: []
## Configure the ingress resource that allows you to access the WordPress installation
## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
##
ingress:
  ## @param ingress.enabled Enable ingress record generation for WordPress
  ##
  enabled: false
  ## @param ingress.pathType Ingress path type
  ##
  pathType: ImplementationSpecific
  ## @param ingress.apiVersion Force Ingress API version (automatically detected if not set)
  ##
  apiVersion: ""
  ## @param ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
  ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
  ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
  ##
  ingressClassName: ""
  ## @param ingress.hostname Default host for the ingress record
  ##
  hostname: wordpress.local
  ## @param ingress.path Default path for the ingress record
  ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
  ##
  path: /
  ## @param ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
  ## For a full list of possible ingress annotations, please see
  ## ref: https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md
  ## Use this parameter to set the required annotations for cert-manager, see
  ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
  ##
  ## e.g:
  ## annotations:
  ##   kubernetes.io/ingress.class: nginx
  ##   cert-manager.io/cluster-issuer: cluster-issuer-name
  ##
  annotations: {}
  ## @param ingress.tls Enable TLS configuration for the host defined at `ingress.hostname` parameter
  ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.ingress.hostname }}`
  ## You can:
  ##   - Use the `ingress.secrets` parameter to create this TLS secret
  ##   - Rely on cert-manager to create it by setting the corresponding annotations
  ##   - Rely on Helm to create self-signed certificates by setting `ingress.selfSigned=true`
  ##
  tls: false
  ## @param ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
  ##
  selfSigned: false
  ## @param ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
  ## e.g:
  ## extraHosts:
  ##   - name: wordpress.local
  ##     path: /
  ##
  extraHosts: []
  ## @param ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
  ## e.g:
  ## extraPaths:
  ## - path: /*
  ##   backend:
  ##     serviceName: ssl-redirect
  ##     servicePort: use-annotation
  ##
  extraPaths: []
  ## @param ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
  ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
  ## e.g:
  ## extraTls:
  ## - hosts:
  ##     - wordpress.local
  ##   secretName: wordpress.local-tls
  ##
  extraTls: []
  ## @param ingress.secrets Custom TLS certificates as secrets
  ## NOTE: 'key' and 'certificate' are expected in PEM format
  ## NOTE: 'name' should line up with a 'secretName' set further up
  ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
  ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
  ## It is also possible to create and manage the certificates outside of this helm chart
  ## Please see README.md for more information
  ## e.g:
  ## secrets:
  ##   - name: wordpress.local-tls
  ##     key: |-
  ##       -----BEGIN RSA PRIVATE KEY-----
  ##       ...
  ##       -----END RSA PRIVATE KEY-----
  ##     certificate: |-
  ##       -----BEGIN CERTIFICATE-----
  ##       ...
  ##       -----END CERTIFICATE-----
  ##
  secrets: []
  ## @param ingress.extraRules Additional rules to be covered with this ingress record
  ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
  ## e.g:
  ## extraRules:
  ## - host: wordpress.local
  ##     http:
  ##       path: /
  ##       backend:
  ##         service:
  ##           name: wordpress-svc
  ##           port:
  ##             name: http
  ##
  extraRules: []

## @section Persistence Parameters
##

## Persistence Parameters
## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/
##
persistence:
  ## @param persistence.enabled Enable persistence using Persistent Volume Claims
  ##
  enabled: true
  ## @param persistence.storageClass Persistent Volume storage class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is set, choosing the default provisioner
  ##
  storageClass: ""
  ## @param persistence.accessModes [array] Persistent Volume access modes
  ##
  accessModes:
    - ReadWriteOnce
  ## @param persistence.accessMode Persistent Volume access mode (DEPRECATED: use `persistence.accessModes` instead)
  ##
  accessMode: ReadWriteOnce
  ## @param persistence.size Persistent Volume size
  ##
  size: 10Gi
  ## @param persistence.dataSource Custom PVC data source
  ##
  dataSource: {}
  ## @param persistence.existingClaim The name of an existing PVC to use for persistence
  ##
  existingClaim: ""
  ## @param persistence.selector Selector to match an existing Persistent Volume for WordPress data PVC
  ## If set, the PVC can't have a PV dynamically provisioned for it
  ## E.g.
  ## selector:
  ##   matchLabels:
  ##     app: my-app
  ##
  selector: {}
  ## @param persistence.annotations Persistent Volume Claim annotations
  ##
  annotations: {}

## Init containers parameters:
## volumePermissions: Change the owner and group of the persistent volume(s) mountpoint(s) to 'runAsUser:fsGroup' on each node
##
volumePermissions:
  ## @param volumePermissions.enabled Enable init container that changes the owner/group of the PV mount point to `runAsUser:fsGroup`
  ##
  enabled: false
  ## Bitnami Shell image
  ## ref: https://hub.docker.com/r/bitnami/bitnami-shell/tags/
  ## @param volumePermissions.image.registry Bitnami Shell image registry
  ## @param volumePermissions.image.repository Bitnami Shell image repository
  ## @param volumePermissions.image.tag Bitnami Shell image tag (immutable tags are recommended)
  ## @param volumePermissions.image.digest Bitnami Shell image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  ## @param volumePermissions.image.pullPolicy Bitnami Shell image pull policy
  ## @param volumePermissions.image.pullSecrets Bitnami Shell image pull secrets
  ##
  image:
    registry: docker.io
    repository: bitnami/bitnami-shell
    tag: 11-debian-11-r63
    digest: ""
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
  ## Init container's resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## @param volumePermissions.resources.limits The resources limits for the init container
  ## @param volumePermissions.resources.requests The requested resources for the init container
  ##
  resources:
    limits: {}
    requests: {}
  ## Init container' Security Context
  ## Note: the chown of the data folder is done to containerSecurityContext.runAsUser
  ## and not the below volumePermissions.containerSecurityContext.runAsUser
  ## @param volumePermissions.containerSecurityContext.runAsUser User ID for the init container
  ##
  containerSecurityContext:
    runAsUser: 0

## @section Other Parameters
##

## WordPress Service Account
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
##
serviceAccount:
  ## @param serviceAccount.create Enable creation of ServiceAccount for WordPress pod
  ##
  create: false
  ## @param serviceAccount.name The name of the ServiceAccount to use.
  ## If not set and create is true, a name is generated using the common.names.fullname template
  ##
  name: ""
  ## @param serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created
  ## Can be set to false if pods using this serviceAccount do not need to use K8s API
  ##
  automountServiceAccountToken: true
  ## @param serviceAccount.annotations Additional custom annotations for the ServiceAccount
  ##
  annotations: {}
## WordPress Pod Disruption Budget configuration
## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
## @param pdb.create Enable a Pod Disruption Budget creation
## @param pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
## @param pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable
##
pdb:
  create: false
  minAvailable: 1
  maxUnavailable: ""
## WordPress Autoscaling configuration
## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
## @param autoscaling.enabled Enable Horizontal POD autoscaling for WordPress
## @param autoscaling.minReplicas Minimum number of WordPress replicas
## @param autoscaling.maxReplicas Maximum number of WordPress replicas
## @param autoscaling.targetCPU Target CPU utilization percentage
## @param autoscaling.targetMemory Target Memory utilization percentage
##
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 11
  targetCPU: 50
  targetMemory: 50

## @section Metrics Parameters
##

## Prometheus Exporter / Metrics configuration
##
metrics:
  ## @param metrics.enabled Start a sidecar prometheus exporter to expose metrics
  ##
  enabled: false
  ## Bitnami Apache exporter image
  ## ref: https://hub.docker.com/r/bitnami/apache-exporter/tags/
  ## @param metrics.image.registry Apache exporter image registry
  ## @param metrics.image.repository Apache exporter image repository
  ## @param metrics.image.tag Apache exporter image tag (immutable tags are recommended)
  ## @param metrics.image.digest Apache exporter image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  ## @param metrics.image.pullPolicy Apache exporter image pull policy
  ## @param metrics.image.pullSecrets Apache exporter image pull secrets
  ##
  image:
    registry: docker.io
    repository: bitnami/apache-exporter
    tag: 0.11.0-debian-11-r73
    digest: ""
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
  ## @param metrics.containerPorts.metrics Prometheus exporter container port
  ##
  containerPorts:
    metrics: 9117
  ## Configure extra options for Prometheus exporter containers' liveness, readiness and startup probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
  ## @param metrics.livenessProbe.enabled Enable livenessProbe on Prometheus exporter containers
  ## @param metrics.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param metrics.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param metrics.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param metrics.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param metrics.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 15
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
    successThreshold: 1
  ## @param metrics.readinessProbe.enabled Enable readinessProbe on Prometheus exporter containers
  ## @param metrics.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param metrics.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param metrics.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param metrics.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param metrics.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 3
    successThreshold: 1
  ## @param metrics.startupProbe.enabled Enable startupProbe on Prometheus exporter containers
  ## @param metrics.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param metrics.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param metrics.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param metrics.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param metrics.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 15
    successThreshold: 1
  ## @param metrics.customLivenessProbe Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}
  ## @param metrics.customReadinessProbe Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}
  ## @param metrics.customStartupProbe Custom startupProbe that overrides the default one
  ##
  customStartupProbe: {}
  ## Prometheus exporter container's resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## @param metrics.resources.limits The resources limits for the Prometheus exporter container
  ## @param metrics.resources.requests The requested resources for the Prometheus exporter container
  ##
  resources:
    limits: {}
    requests: {}
  ## Prometheus exporter service parameters
  ##
  service:
    ## @param metrics.service.ports.metrics Prometheus metrics service port
    ##
    ports:
      metrics: 9150
    ## @param metrics.service.annotations [object] Additional custom annotations for Metrics service
    ##
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "{{ .Values.metrics.containerPorts.metrics }}"
  ## Prometheus Operator ServiceMonitor configuration
  ##
  serviceMonitor:
    ## @param metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using Prometheus Operator
    ##
    enabled: false
    ## @param metrics.serviceMonitor.namespace Namespace for the ServiceMonitor Resource (defaults to the Release Namespace)
    ##
    namespace: ""
    ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped.
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
    ##
    interval: ""
    ## @param metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
    ##
    scrapeTimeout: ""
    ## @param metrics.serviceMonitor.labels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus
    ##
    labels: {}
    ## @param metrics.serviceMonitor.selector Prometheus instance selector labels
    ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
    ##
    selector: {}
    ## @param metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
    ##
    relabelings: []
    ## @param metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
    ##
    metricRelabelings: []
    ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
    ##
    honorLabels: false
    ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
    ##
    jobLabel: ""

## @section NetworkPolicy parameters
##

## Add networkpolicies
##
networkPolicy:
  ## @param networkPolicy.enabled Enable network policies
  ## If ingress.enabled or metrics.enabled are true, configure networkPolicy.ingress and networkPolicy.metrics selectors respectively to allow communication
  ##
  enabled: false
  ## @param networkPolicy.metrics.enabled Enable network policy for metrics (prometheus)
  ## @param networkPolicy.metrics.namespaceSelector [object] Monitoring namespace selector labels. These labels will be used to identify the prometheus' namespace.
  ## @param networkPolicy.metrics.podSelector [object] Monitoring pod selector labels. These labels will be used to identify the Prometheus pods.
  ##
  metrics:
    enabled: false
    ## e.g:
    ## podSelector:
    ##   label: monitoring
    ##
    podSelector: {}
    ## e.g:
    ## namespaceSelector:
    ##   label: monitoring
    ##
    namespaceSelector: {}
  ## @param networkPolicy.ingress.enabled Enable network policy for Ingress Proxies
  ## @param networkPolicy.ingress.namespaceSelector [object] Ingress Proxy namespace selector labels. These labels will be used to identify the Ingress Proxy's namespace.
  ## @param networkPolicy.ingress.podSelector [object] Ingress Proxy pods selector labels. These labels will be used to identify the Ingress Proxy pods.
  ##
  ingress:
    enabled: false
    ## e.g:
    ## podSelector:
    ##   label: ingress
    ##
    podSelector: {}
    ## e.g:
    ## namespaceSelector:
    ##   label: ingress
    ##
    namespaceSelector: {}
  ## @param networkPolicy.ingressRules.backendOnlyAccessibleByFrontend Enable ingress rule that makes the backend (mariadb) only accessible by testlink's pods.
  ## @param networkPolicy.ingressRules.customBackendSelector [object] Backend selector labels. These labels will be used to identify the backend pods.
  ## @param networkPolicy.ingressRules.accessOnlyFrom.enabled Enable ingress rule that makes testlink only accessible from a particular origin
  ## @param networkPolicy.ingressRules.accessOnlyFrom.namespaceSelector [object] Namespace selector label that is allowed to access testlink. This label will be used to identified the allowed namespace(s).
  ## @param networkPolicy.ingressRules.accessOnlyFrom.podSelector [object] Pods selector label that is allowed to access testlink. This label will be used to identified the allowed pod(s).
  ## @param networkPolicy.ingressRules.customRules [object] Custom network policy ingress rule
  ##
  ingressRules:
    ## mariadb backend only can be accessed from testlink
    ##
    backendOnlyAccessibleByFrontend: false
    ## Additional custom backend selector
    ## e.g:
    ## customBackendSelector:
    ##   - to:
    ##       - namespaceSelector:
    ##           matchLabels:
    ##             label: example
    ##
    customBackendSelector: {}
    ## Allow only from the indicated:
    ##
    accessOnlyFrom:
      enabled: false
      ## e.g:
      ## podSelector:
      ##   label: access
      ##
      podSelector: {}
      ## e.g:
      ## namespaceSelector:
      ##   label: access
      ##
      namespaceSelector: {}
    ## custom ingress rules
    ## e.g:
    ## customRules:
    ##   - from:
    ##       - namespaceSelector:
    ##           matchLabels:
    ##             label: example
    ##
    customRules: {}
  ## @param networkPolicy.egressRules.denyConnectionsToExternal Enable egress rule that denies outgoing traffic outside the cluster, except for DNS (port 53).
  ## @param networkPolicy.egressRules.customRules [object] Custom network policy rule
  ##
  egressRules:
    # Deny connections to external. This is not compatible with an external database.
    denyConnectionsToExternal: false
    ## Additional custom egress rules
    ## e.g:
    ## customRules:
    ##   - to:
    ##       - namespaceSelector:
    ##           matchLabels:
    ##             label: example
    ##
    customRules: {}

## @section Database Parameters
##

## MariaDB chart configuration
## ref: https://github.com/bitnami/charts/blob/main/bitnami/mariadb/values.yaml
##
mariadb:
  ## @param mariadb.enabled Deploy a MariaDB server to satisfy the applications database requirements
  ## To use an external database set this to false and configure the `externalDatabase.*` parameters
  ##
  enabled: true
  ## @param mariadb.architecture MariaDB architecture. Allowed values: `standalone` or `replication`
  ##
  architecture: standalone
  ## MariaDB Authentication parameters
  ## @param mariadb.auth.rootPassword MariaDB root password
  ## @param mariadb.auth.database MariaDB custom database
  ## @param mariadb.auth.username MariaDB custom user name
  ## @param mariadb.auth.password MariaDB custom user password
  ## ref: https://github.com/bitnami/containers/tree/main/bitnami/mariadb#setting-the-root-password-on-first-run
  ##      https://github.com/bitnami/containers/blob/main/bitnami/mariadb/README.md#creating-a-database-on-first-run
  ##      https://github.com/bitnami/containers/blob/main/bitnami/mariadb/README.md#creating-a-database-user-on-first-run
  ##
  auth:
    rootPassword: ""
    database: bitnami_wordpress
    username: bn_wordpress
    password: ""
  ## MariaDB Primary configuration
  ##
  primary: 
    ## MariaDB Primary Persistence parameters
    ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/
    ## @param mariadb.primary.persistence.enabled Enable persistence on MariaDB using PVC(s)
    ## @param mariadb.primary.persistence.storageClass Persistent Volume storage class
    ## @param mariadb.primary.persistence.accessModes [array] Persistent Volume access modes
    ## @param mariadb.primary.persistence.size Persistent Volume size
    ##
    persistence:
      enabled: true
      storageClass: ""
      accessModes:
        - ReadWriteOnce
      size: 8Gi
## External Database Configuration
## All of these values are only used if `mariadb.enabled=false`
##
externalDatabase:
  ## @param externalDatabase.host External Database server host
  ##
  host: localhost
  ## @param externalDatabase.port External Database server port
  ##
  port: 3306
  ## @param externalDatabase.user External Database username
  ##
  user: bn_wordpress
  ## @param externalDatabase.password External Database user password
  ##
  password: ""
  ## @param externalDatabase.database External Database database name
  ##
  database: bitnami_wordpress
  ## @param externalDatabase.existingSecret The name of an existing secret with database credentials. Evaluated as a template
  ## NOTE: Must contain key `mariadb-password`
  ## NOTE: When it's set, the `externalDatabase.password` parameter is ignored
  ##
  existingSecret: ""
## Memcached chart configuration
## ref: https://github.com/bitnami/charts/blob/main/bitnami/memcached/values.yaml
##
memcached:
  ## @param memcached.enabled Deploy a Memcached server for caching database queries
  ##
  enabled: false
  ## Authentication parameters
  ## ref: https://github.com/bitnami/containers/tree/main/bitnami/memcached#creating-the-memcached-admin-user
  ##
  auth:
    ## @param memcached.auth.enabled Enable Memcached authentication
    ##
    enabled: false
    ## @param memcached.auth.username Memcached admin user
    ##
    username: ""
    ## @param memcached.auth.password Memcached admin password
    ##
    password: ""
  ## Service parameters
  ##
  service:
    ## @param memcached.service.port Memcached service port
    ##
    port: 11211
## External Memcached Configuration
## All of these values are only used if `memcached.enabled=false`
##
externalCache:
  ## @param externalCache.host External cache server host
  ##
  host: localhost
  ## @param externalCache.port External cache server port
  ##
  port: 11211 
  ############################################################################################################################################################
  
  "anji@master:~$ mkdir wordpress
  anji@master:~$ cd wordpress/
anji@master:~/wordpress$ ll
anji@master:~/wordpress$ touch wordpress.yaml

anji@master:~/wordpress$ nano wordpress.yaml 
wordpressUsername: admin
wordpressPassword: admin
wordpressEmail: admin@gmail.com
wordpressFirstName: anji
wordpressLastName: reddy
wordpressBlogName: anji.com
service: 
  type: LoadBalancer                 
---==--" 
anji@master:~/wordpress$ kubectl create namespace wordpress
namespace/wordpress created
anji@master:~/wordpress$ kubectl get ns
NAME              STATUS   AGE
default           Active   6h48m
kube-node-lease   Active   6h48m
kube-public       Active   6h48m
kube-system       Active   6h48m
"wordpress  "       Active   10s
===----------"
anji@master:~/wordpress$ helm install wordpress bitnami/wordpress --values=wordpress.yaml --namespace wordpress --version 15.2.22
NAME: wordpress
LAST DEPLOYED: Tue Jan  3 20:18:32 2023
"NAMESPACE": wordpress
STATUS: "deployed"
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: wordpress
CHART VERSION: 15.2.22
APP VERSION: 6.1.1
** Please be patient while the chart is being deployed **
Your WordPress site can be accessed through the following DNS name from within your cluster:
    wordpress.wordpress.svc.cluster.local (port 80)
To access your WordPress site from outside the cluster follow the steps below:
1. Get the WordPress URL by running these commands:
  NOTE: It may take a few minutes for the LoadBalancer IP to be available.
        Watch the status with: 'kubectl get svc --namespace wordpress -w wordpress'
   export SERVICE_IP=$(kubectl get svc --namespace wordpress wordpress --include "{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}")
   echo "WordPress URL: http://$SERVICE_IP/"
   echo "WordPress Admin URL: http://$SERVICE_IP/admin"
2. Open a browser and access WordPress using the obtained URL.
3. Login with the following credentials below to see your blog:
  echo Username: admin
  echo Password: $(kubectl get secret --namespace wordpress wordpress -o jsonpath="{.data.wordpress-password}" | base64 -d)

===\\\\\\//////////
anji@master:~$ kubectl get po -n wordpress
NAME                        READY   STATUS    RESTARTS   AGE
wordpress-8b854d97f-6bkh4   0/1     Pending   0          8m58s
wordpress-mariadb-0         0/1     Pending   0          8m58s

anji@master:~/wordpress$ kubectl get secret --namespace wordpress
NAME                              TYPE                 DATA   AGE
sh.helm.release.v1.wordpress.v1   helm.sh/release.v1   1      9m4s
wordpress                         Opaque               1      9m4s
wordpress-mariadb                 "Opaque  "             2      9m4s  "
\\\\\\///////////===

anji@master:~$ kubectl get svc  -n wordpress -o wide 
NAME                TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE   SELECTOR
wordpress          " LoadBalancer   10.97.108.43    <pending>  "   80:30121/TCP,443:30120/TCP   28m   app.kubernetes.io/instance=wordpress,app.kubernetes.io/name=wordpress
wordpress-mariadb   ClusterIP      10.109.65.243   <none>        3306/TCP                     28m   app.kubernetes.io/component=primary,app.kubernetes.io/instance=wordpress,app.kubernetes.io/name=mariadb
anji@master:~$ 


----
anji@master:~/wordpress$ kubectl describe pod  wordpress-8b854d97f-6bkh4  -n  wordpress
Name:             wordpress-8b854d97f-6bkh4
Namespace:        wordpress
Node:             <none>
Labels:           app.kubernetes.io/instance=wordpress
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=wordpress
                  helm.sh/chart=wordpress-15.2.22
                  pod-template-hash=8b854d97f
Annotations:      <none>
Status:           Pending
Controlled By:    ReplicaSet/wordpress-8b854d97f
Containers:
  wordpress:
    Image:       docker.io/bitnami/wordpress:6.1.1-debian-11-r15
    Ports:       8080/TCP, 8443/TCP
    Host Ports:  0/TCP, 0/TCP
    "Requests:"
      cpu:      300m
      memory:   512Mi
  "  Liveness:"   http-get http://:http/wp-admin/install.php delay=120s timeout=5s period=10s #success=1 #failure=6
   " Readiness:  h"ttp-get http://:http/wp-login.php delay=30s timeout=5s period=10s #success=1 #failure=6
    Environment:
      BITNAMI_DEBUG:                          false
      ALLOW_EMPTY_PASSWORD:                   yes
      MARIADB_HOST:                           wordpress-mariadb
      MARIADB_PORT_NUMBER:                   " 3306"
      WORDPRESS_DATABASE_NAME:                bitnami_wordpress
      WORDPRESS_DATABASE_USER:                bn_wordpress
      WORDPRESS_DATABASE_PASSWORD:            <set to the key 'mariadb-password' in secret 'wordpress-mariadb'>  Optional: false
      WORDPRESS_"USERNAME:                     admin"
      WORDPRESS_PASSWORD:                     <set to the key 'wordpress-password' in secret 'wordpress'>  Optional: false
      WORDPRESS_EMAIL:                        admin@gmail.com
      WORDPRESS_FIRST_NAME:                   anji
      WORDPRESS_LAST_NAME:                    reddy
      WORDPRESS_HTACCESS_OVERRIDE_NONE:       no
      WORDPRESS_ENABLE_HTACCESS_PERSISTENCE:  no
      WORDPRESS_BLOG_NAME:                    anji.com
      WORDPRESS_SKIP_BOOTSTRAP:               no
      WORDPRESS_TABLE_PREFIX:                 wp_
      WORDPRESS_SCHEME:                       http
      WORDPRESS_EXTRA_WP_CONFIG_CONTENT:      
      WORDPRESS_PLUGINS:                      none
      APACHE_HTTP_PORT_NUMBER:                8080
      APACHE_HTTPS_PORT_NUMBER:               8443
    Mounts:
      /"bitnami/wordpress from wordpress-data (rw,path="wordpress")"
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-k4cm5 (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  wordpress-data:
    Type:     "  PersistentVolumeClaim (a reference to a" PersistentVolumeClaim in the same namespace)
   " ClaimName:  wordpress"
    ReadOnly:   false
  kube-api-access-k4cm5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  2m17s (x3 over 12m)  default-scheduler  0/2 nodes are available: 2 pod has unbound immediate PersistentVolumeClaims. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.
\\\\\\/\/\\/\/\/\\\////////////////////\\\\\\\\\\\\\\\\\\\\\\\\\\\//\/\

anji@master:~/wordpress$ kubectl describe pod  wordpress-mariadb-0   -n  wordpress
Name:             wordpress-mariadb-0
Namespace:        wordpress
Service Account:  wordpress-mariadb
Labels:           app.kubernetes.io/component=primary
                  app.kubernetes.io/instance=wordpress
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mariadb
                  controller-revision-hash=wordpress-mariadb-7b6748bdff
                  helm.sh/chart=mariadb-11.4.2
                  statefulset.kubernetes.io/pod-name=wordpress-mariadb-0
Controlled By:    "StatefulSet/wordpress-mariadb"
Containers:
 " mariadb:"
    Image:      docker.io/bitnami/mariadb:10.6.11-debian-11-r12
    Port:       3306/TCP
    Host Port:  0/TCP
    Liveness:   exec [/bin/bash -ec password_aux="${MARIADB_ROOT_PASSWORD:-}"
if [[ -f "${MARIADB_ROOT_PASSWORD_FILE:-}" ]]; then
    password_aux=$(cat "$MARIADB_ROOT_PASSWORD_FILE")
fi
mysqladmin status -uroot -p"${password_aux}"
] delay=120s timeout=1s period=10s #success=1 #failure=3
    Readiness:  exec [/bin/bash -ec password_aux="${MARIADB_ROOT_PASSWORD:-}"
if [[ -f "${MARIADB_ROOT_PASSWORD_FILE:-}" ]]; then
    password_aux=$(cat "$MARIADB_ROOT_PASSWORD_FILE")
fi
mysqladmin status -uroot -p"${password_aux}"
] delay=30s timeout=1s period=10s #success=1 #failure=3
    Environment:
      BITNAMI_DEBUG:          false
      MARIADB_ROOT_PASSWORD:  <set to the key 'mariadb-root-password' in secret 'wordpress-mariadb'>  Optional: false
      MARIADB_USER:           bn_wordpress
      MARIADB_PASSWORD:       <set to the key 'mariadb-password' in secret 'wordpress-mariadb'>  Optional: false
      MARIADB_DATABASE:       bitnami_wordpress
    Mounts:
      /bitnami/mariadb from data (rw)
      /opt/bitnami/mariadb/conf/my.cnf from config (rw,path="my.cnf")
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  data:
    "Type:       PersistentVolumeClaim" (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  data-wordpress-mariadb-0
    ReadOnly:   false
  config:
    Type:        ConfigMap (a volume populated by a ConfigMap)
    Name:        wordpress-mariadb
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  3m23s (x4 over 18m)  default-scheduler  0/2 nodes are available: 2 pod has unbound immediate PersistentVolumeClaims. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.
"=====================================================================
Helm Chart Demo - How to create your first Helm Chart? - Part 6
https://www.youtube.com/watch?v=2dqQcou_MCU&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=2
https://jhooq.com/building-first-helm-chart-with-spring-boot/

anji@master:~$ helm create springboot
Creating springboot

anji@master:~$ ls
Desktop  Documents  Downloads  Music  Pictures  Public  springboot  Templates  Videos  wordpress
anji@master:~$ cd springboot/
anji@master:~/springboot$ ls
charts  Chart.yaml  templates  values.yaml
anji@master:~/springboot$ tree
.
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files
---====//\\\============== 
anji@master:~/springboot$ cat Chart.yaml 
apiVersion: v2   ##  importent 
name: springboot
description: A Helm chart for Kubernetes
type: application
version: 0.1.0
appVersion: "1.16.0"      ##  importent 

====#############\/\/\/\/\/\\\\\\\\// "
anji@master:~/springboot$ cat  values.yaml 
# Default values for springboot.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  "repository: rahulwagh17/kubernetes:jhooq-k8s-springboot "  # see look 
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  "port: 8080     # see look 
"
ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}   "
====--==/\/\==============================\\
anji@master:~/springboot$ cat templates/deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "springboot.fullname" . }}
  labels:
    {{- include "springboot.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "springboot.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "springboot.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "springboot.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: {{ .Values.service.port }}
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
======++++++++++++++++++++++++++++++++"
anji@master:~/springboot/templates$ cat service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: {{ include "springboot.fullname" . }}
  labels:
    {{- include "springboot.labels" . | nindent 4 }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: http
      protocol: TCP
      name: http
  selector:
    {{- include "springboot.selectorLabels" . | nindent 4 }}  "

############################################################################333
anji@master:~$ helm template springboot/
---
# Source: springboot/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: springboot/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: release-name
---
# Source: springboot/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: springboot
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: springboot
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: release-name-springboot
      securityContext:
        {}
      containers:
        - name: springboot
          securityContext:
            {}
          image: "rahulwagh17/kubernetes:jhooq-k8s-springboot"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
#          livenessProbe:
 #           httpGet:
  #            path: /
   #           port: http
    #      readinessProbe:
     #       httpGet:
      #        path: /
       #       port: http
          resources:
            {}
---
# Source: springboot/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-springboot-test-connection"
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['release-name-springboot:8080']
  restartPolicy: Never
###############################################################3
"
anji@master:~$ helm lint springboot/
==> Linting springboot/
[INFO] Chart.yaml: icon is recommended
1 chart(s) linted, 0 chart(s) failed   ## see look 
"###############################################################33
anji@master:~$ helm install mydebug --debug --dry-run springboot/
install.go:209: [debug] CHART PATH: /home/anji/springboot
NAME: mydebug
LAST DEPLOYED: Wed Jan  4 12:10:41 2023
NAMESPACE: default
STATUS:" pending-install"
REVISION: 1
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
fullnameOverride: ""
image:
  pullPolicy: IfNotPresent
  repository: rahulwagh17/kubernetes:jhooq-k8s-springboot
  tag: ""
imagePullSecrets: []
ingress:
  annotations: {}
  className: ""
  enabled: false
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
nameOverride: ""
nodeSelector: {}
podAnnotations: {}
podSecurityContext: {}
replicaCount: 1
resources: {}
securityContext: {}
service:
  port: 8080
  type: ClusterIP
serviceAccount:
  annotations: {}
  create: true
  name: ""
tolerations: []

HOOKS:
---
# Source: springboot/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "mydebug-springboot-test-connection"
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['mydebug-springboot:8080']
  restartPolicy: Never
MANIFEST:
---
# Source: springboot/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mydebug-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: springboot/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mydebug-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
---
# Source: springboot/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydebug-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: springboot
      app.kubernetes.io/instance: mydebug
  template:anji@master:~$ helm install mydebug --debug --dry-run springboot/
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
install.go:192: [debug] Original chart version: ""
install.go:209: [debug] CHART PATH: /home/anji/springboot

NAME: mydebug
LAST DEPLOYED: Wed Jan  4 12:10:41 2023
NAMESPACE: default
STATUS: pending-install
REVISION: 1
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
fullnameOverride: ""
image:
  pullPolicy: IfNotPresent
  repository: rahulwagh17/kubernetes:jhooq-k8s-springboot
  tag: ""
imagePullSecrets: []
ingress:
  annotations: {}
  className: ""
  enabled: false
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
nameOverride: ""
nodeSelector: {}
podAnnotations: {}
podSecurityContext: {}
replicaCount: 1
resources: {}
securityContext: {}
service:
  port: 8080
  type: ClusterIP
serviceAccount:
  annotations: {}
  create: true
  name: ""
tolerations: []

HOOKS:
---
# Source: springboot/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "mydebug-springboot-test-connection"
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['mydebug-springboot:8080']
  restartPolicy: Never
MANIFEST:
---
# Source: springboot/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mydebug-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: springboot/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mydebug-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
---
# Source: springboot/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydebug-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: springboot
      app.kubernetes.io/instance: mydebug
  template:
    metadata:
      labels:
        app.kubernetes.io/name: springboot
        app.kubernetes.io/instance: mydebug
    spec:
      serviceAccountName: mydebug-springboot
      securityContext:
        {}
      containers:
        - name: springboot
          securityContext:
            {}
          image: "rahulwagh17/kubernetes:jhooq-k8s-springboot"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
#          livenessProbe:
 #           httpGet:
  #            path: /
   #           port: http
    #      readinessProbe:
     #       httpGet:
      #        path: /
       #       port: http
          resources:
            {}
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=springboot,app.kubernetes.io/instance=mydebug" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
anji@master:~$ 
    metadata:
      labels:
        app.kubernetes.io/name: springboot
        app.kubernetes.io/instance: mydebug
    spec:
      serviceAccountName: mydebug-springboot
      securityContext:
        {}
      containers:
        - name: springboot
          securityContext:
            {}
          image: "rahulwagh17/kubernetes:jhooq-k8s-springboot"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
#          livenessProbe:
 #           httpGet:
  #            path: /
   #           port: http
    #      readinessProbe:
     #       httpGet:
      #        path: /
       #       port: http
          resources:
            {}
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=springboot,app.kubernetes.io/instance=mydebug" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
########################################################################################33
"anji@master:~$ helm install release1-1  springboot/
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
NAME: "release1-1"
LAST DEPLOYED: Wed Jan  4 12:37:03 2023
NAMESPACE: default
STATUS: "deployed"
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=springboot,app.kubernetes.io/instance=release1-1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT 
=======++++++++++++++++++///\//
anji@master:~$ kubectl get pod,service,deployment  -o wide 
NAME                                         READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
pod/"release1-1-"springboot-85f9fcc4c5-v4cg2   0/1     Pending   0          5s    <none>   <none>   <none>           <none>

NAME                            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE   SELECTOR
service/kubernetes              ClusterIP   10.96.0.1      <none>        443/TCP    17h   <none>
service/release1-1-springboot   ClusterIP   10.98.185.59   <none>        "8080/TCP"   5s    app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=springboot

NAME                                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                                        SELECTOR
deployment.apps/release1-1-springboot   0/1     1            0           5s    springboot   "rahulwagh17/kubernetes:jhooq-k8s-springboot "  app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=springboot "
==\\/\//####
anji@master:~$ helm list -a 
NAME      	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART           	APP VERSION
release1  	default  	2       	2023-01-03 17:47:48.50720335 +0530 IST 	deployed	helloworld-0.1.0	1.16.0     
"release1-1	"default  	"1  "     	2023-01-04 12:42:18.744331019 +0530 IST	deployed	springboot-0.1.0	1.16.0     "
########################################### +++++   BEFORE  BEFORE =====
anji@master:~$ cat springboot/Chart.yaml 
apiVersion: v2
name: springboot
description: A Helm chart for Kubernetes
type: application
version: 0.1.0   ##  SEE LOOK  CHANGED THE VERSION 
appVersion: "1.16.0"
====++++++++

anji@master:~$ cat springboot/Chart.yaml 
apiVersion: v2
name: springboot
description: A Helm chart for Kubernetes
type: application
version: 0.1.1     ## see look changed the value 
appVersion: "1.16.0"
######################################   before  BEFORE 
anji@master:~$ cd  springboot/
anji@master:~/springboot$ cat values.yaml 
# Default values for springboot.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 1    ===  SEE LOOK 
image:
  repository: rahulwagh17/kubernetes:jhooq-k8s-springboot 
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""
==================
anji@master:~$ cd  springboot/
anji@master:~/springboot$ cat values.yaml 
# Default values for springboot.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 2     ===  SEE LOOK 
image:
  repository: rahulwagh17/kubernetes:jhooq-k8s-springboot 
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""
============+++
anji@master:~$ helm upgrade release1-1 springboot/
Release "release1-1" has been upgraded. Happy Helming!
NAME: release1-1
LAST DEPLOYED: Wed Jan  4 13:23:14 2023
NAMESPACE: default
STATUS: deployed
REVISION: 2   # see look changed 
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=springboot,app.kubernetes.io/instance=release1-1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
                         
anji@master:~$ kubectl get all -o wide  --show-labels
NAME                                         READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES   LABELS
pod/release1-1-springboot-85f9fcc4c5-v4cg2   0/1     Pending   0          43m   <none>   <none>   <none>           <none>            app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=springboot,pod-template-hash=85f9fcc4c5

NAME                            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE   SELECTOR                                                                  LABELS
service/kubernetes              ClusterIP   10.96.0.1      "<none>"        443/TCP    18h   <none>                                                                    component=apiserver,provider=kubernetes
service/release1-1-springboot   ClusterIP   10.98.185.59  " <none>   "     8080/TCP   43m   app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=springboot   app.kubernetes.io/instance=release1-1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=springboot,app.kubernetes.io/version=1.16.0,helm.sh/chart=springboot-0.1.1

NAME                                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                                        SELECTOR                                                                  LABELS
deployment.apps/release1-1-springboot   0/1     1            0           43m   springboot   rahulwagh17/kubernetes:jhooq-k8s-springboot   app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=springboot   app.kubernetes.io/instance=release1-1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=springboot,app.kubernetes.io/version=1.16.0,helm.sh/chart=springboot-0.1.1

NAME                                               DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES                                        SELECTOR                                                                                               LABELS
replicaset.apps/release1-1-springboot-85f9fcc4c5   1         1         0       43m   springboot   rahulwagh17/kubernetes:jhooq-k8s-springboot   app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=springboot,pod-template-hash=85f9fcc4c5   app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=springboot,pod-template-hash=85f9fcc4c5
################################################################ -roleback 

anji@master:~$ helm list -a 
NAME      	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART           	  APP VERSION
release1-1	default  	"2  "     	2023-01-04 13:23:14.402246047 +0530 IST	deployed	"springboot-0.1.1	"1.16.0     

anji@master:~$ helm rollback release1-1  1
Rollback was a "success!" Happy Helming!

anji@master:~$ helm list -a 
NAME      	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART           	APP VERSION
release1-1	default  	3       	2023-01-04 13:50:28.742369186 +0530 IST	deployed	s"pringboot-0.1.0"	1.16.0     
===========+++++++++++++++++=================
anji@master:~$ helm diff revision release1-1   1  2 
default, release1-1-springboot, Deployment (apps) has changed:
  # Source: springboot/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: release1-1-springboot
    labels:
-     helm.sh/chart: springboot-0.1.0    ### see look 
+     helm.sh/chart: springboot-0.1.1
      app.kubernetes.io/name: springboot
      app.kubernetes.io/instance: release1-1
      app.kubernetes.io/version: "1.16.0"
      app.kubernetes.io/managed-by: Helm
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/name: springboot
        app.kubernetes.io/instance: release1-1
    template:
      metadata:
        labels:
          app.kubernetes.io/name: springboot
          app.kubernetes.io/instance: release1-1
      spec:
        serviceAccountName: release1-1-springboot
        securityContext:
          {}
        containers:
          - name: springboot
            securityContext:
              {}
            image: "rahulwagh17/kubernetes:jhooq-k8s-springboot"
            imagePullPolicy: IfNotPresent
            ports:
              - name: http
                containerPort: 8080
                protocol: TCP
  #          livenessProbe:
   #           httpGet:
    #            path: /
     #           port: http
      #      readinessProbe:
       #       httpGet:
        #        path: /
         #       port: http
            resources:
              {}
default, release1-1-springboot, Service (v1) has changed:
  # Source: springboot/templates/service.yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: release1-1-springboot
    labels:
-     helm.sh/chart: springboot-0.1.0
+     helm.sh/chart: springboot-0.1.1    ##  see look changed 
      app.kubernetes.io/name: springboot
      app.kubernetes.io/instance: release1-1
      app.kubernetes.io/version: "1.16.0"
      app.kubernetes.io/managed-by: Helm
  spec:
    type: ClusterIP
    ports:
      - port: 8080
        targetPort: http
        protocol: TCP
        name: http
    selector:
      app.kubernetes.io/name: springboot
      app.kubernetes.io/instance: release1-1
default, release1-1-springboot, ServiceAccount (v1) has changed:
  # Source: springboot/templates/serviceaccount.yaml
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: release1-1-springboot
    labels:
-     helm.sh/chart: springboot-0.1.0
+     helm.sh/chart: springboot-0.1.1     ## see look 
      app.kubernetes.io/name: springboot
      app.kubernetes.io/instance: release1-1
      app.kubernetes.io/version: "1.16.0"
      app.kubernetes.io/managed-by: Helm
####################################################################################
How to convert Kubernetes yaml to Helm Chart yaml "
https://www.youtube.com/watch?v=ZZVXXEyEzAs&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=7
https://jhooq.com/convert-kubernetes-yaml-into-helm/

apiVersion: apps/v1
kind: Deployment
metadata: 
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 1
  selector: 
     matchLabels:
       app: nginx
  strategy: {}     
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers: 
       - name: nginx
         image: rahulwagh17/kubernetes:jhooq-k8s-springboot
         ports: 
           - containerPort: 80
         resources: {}
status: {}          
 
anji@master:~$ kubectl get all -o wide  --show-labels
NAME                         READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES   LABELS
pod/nginx-7bc7874b58-tc8dr   0/1     Pending   0          78s   <none>   <none>   <none>           <none>            app=nginx,pod-template-hash=7bc7874b58

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    "<none>  "      443/TCP   87s   <none>     component=apiserver,provider=kubernetes

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                                        SELECTOR    LABELS
deployment.apps/nginx   0/1     1            0           78s   nginx        rahulwagh17/kubernetes:jhooq-k8s-springboot   app=nginx   app=nginx

NAME                               DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES                                        SELECTOR                                 LABELS
replicaset.apps/nginx-7bc7874b58   1         1         0       78s   nginx        rahulwagh17/kubernetes:jhooq-k8s-springboot   app=nginx,pod-template-hash=7bc7874b58   app=nginx,pod-template-hash=7bc7874b58
=============++++++++++++"
apiVersion: v1
kind: Service
metadata: 
  name: service
  labels: 
    app: nginx
spec:
  ports:
    - port: 8080
      protocol: TCP
      targetPort: 8080
  selector:
    app: nginx
  type: LoadBalancer
status: 
  loadBalancer: {}
  
anji@master:~$ kubectl get all -o wide  --show-labels
NAME                         READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES   LABELS
pod/nginx-7bc7874b58-tc8dr   0/1     Pending   0          14m   <none>   <none>   <none>           <none>            app=nginx,pod-template-hash=7bc7874b58

NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE   SELECTOR    LABELS
service/kubernetes   ClusterIP      10.96.0.1       <none>        443/TCP          14m   <none>      component=apiserver,provider=kubernetes
service/service    "  LoadBalancer "  10.107.234.72   <pending>    " 8080:32425/TCP "  5s    app=nginx   app=nginx

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                                        SELECTOR    LABELS
deployment.apps/nginx   0/1     1            0           14m   nginx        rahulwagh17/kubernetes:jhooq-k8s-springboot   app=nginx   app=nginx

NAME                               DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES                                        SELECTOR                                 LABELS
replicaset.apps/nginx-7bc7874b58   1         1         0       14m   nginx        rahulwagh17/kubernetes:jhooq-k8s-springboot   app=nginx,pod-template-hash=7bc7874b58   app=nginx,pod-template-hash=7bc7874b58

###################################################################################
anji@master:~$ helm create velpula
Creating velpula

anji@master:~$ tree reddy/
reddy/
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files
=----++++++++\\/\/
anji@master:~$ tree anji
anji
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files
====================+++++++++++++==
Here is comparision of YAMLs generated by Helm Chart and Kubernetes(k8s) -
YAMLs Generated by Helm Chart	                                Kubernetes(k8s) YAMLs

1	Chart.yaml	
2	helper.tpl	
3	deployment.yaml	                                             k8s-deployment.yaml
4	hpa.yaml	
5	ingress.yaml	
6	service.yaml	                                                k8s-service.yaml
7	serviceaccount.yaml	
8	test-connection.yaml	
9	values.yaml	                                                  k8s-deployment.yaml
                                                                 1. replicas: 1
                                                                 2. docker image =rahulwagh17/kubernetes:jhooq-k8s-springboot
-----========------------++
anji@master:~$ tree velpula/
velpula/
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files

===-----
anji@master:~/velpula$ cat  Chart.yaml 
apiVersion: v2
name: velpula
description: anjireddy-velpula-venkata-test-helm   ## see look  changed
type: application
version: 0.1.0
appVersion: "1.16.0"

anji@master:~/velpula/templates$ cat  deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "velpula.fullname" . }}
  labels:
    {{- include "velpula.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "velpula.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "velpula.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "velpula.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}"                 ## see look changed 
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 8080    ## see look changed 
              protocol: TCP
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
+++++++------------====       
anji@master:~/velpula/templates$ cat service.yaml   ## no changes  no  modifications 
apiVersion: v1
kind: Service
metadata:
  name: {{ include "velpula.fullname" . }}
  labels:
    {{- include "velpula.labels" . | nindent 4 }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: http
      protocol: TCP
      name: http
  selector:
    {{- include "velpula.selectorLabels" . | nindent 4 }}
===---======#######
anji@master:~/velpula$ cat  values.yaml 
# Default values for velpula.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 1
image:
  repository: rahulwagh17/kubernetes:jhooq-k8s-springboot   #  see look changed 
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""
imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""
podAnnotations: {}
podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000
service:
  type: NoadPort    # changed see look  
  port: 8080   see look 
ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local
resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80
nodeSelector: {}
tolerations: []
affinity: {}
####################============+++++++++++=
anji@master:~$ helm template velpula/
---
# Source: velpula/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-velpula
  labels:
    helm.sh/chart: velpula-0.1.0
    app.kubernetes.io/name: velpula
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: velpula/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-velpula
  labels:
    helm.sh/chart: velpula-0.1.0
    app.kubernetes.io/name: velpula
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: NodePort
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: velpula
    app.kubernetes.io/instance: release-name
---
# Source: velpula/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-velpula
  labels:
    helm.sh/chart: velpula-0.1.0
    app.kubernetes.io/name: velpula
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: velpula
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: velpula
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: release-name-velpula
      securityContext:
        {}
      containers:
        - name: velpula
          securityContext:
            {}
          image: "rahulwagh17/kubernetes:jhooq-k8s-springboot"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          resources:
            {}
---
# Source: velpula/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-velpula-test-connection"
  labels:
    helm.sh/chart: velpula-0.1.0
    app.kubernetes.io/name: velpula
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['release-name-velpula:8080']
  restartPolicy: Never
--=========++++=
anji@master:~$ helm lint velpula/
==> Linting velpula/
[INFO] Chart.yaml: icon is recommended
1 chart(s) linted, 0 chart(s) failed "
##################################################################################3
anji@master:~$ helm install sony velpula/
NAME: "sony"
LAST DEPLOYED: Wed Jan  4 16:12:21 2023
NAMESPACE: default
STATUS:" deployed"
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services sony-velpula)
  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT

anji@master:~$ kubectl get all -o wide  --show-labels
NAME                                READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES   LABELS
pod/sony-velpula-5cdc66bdd6-nk78r   0/1     Pending   0          17s   <none>   <none>   <none>           <none>            app.kubernetes.io/instance=sony,app.kubernetes.io/name=velpula,pod-template-hash=5cdc66bdd6

NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE   SELECTOR                                                         LABELS
service/kubernetes     ClusterIP   10.96.0.1       <none>        443/TCP          54m   <none>                                                           component=apiserver,provider=kubernetes
service/sony-velpula   "NodePort "   10.110.40.140   <none>        "8080:32460/TCP "  17s   app.kubernetes.io/instance=sony,app.kubernetes.io/name=velpula   app.kubernetes.io/instance=sony,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=velpula,app.kubernetes.io/version=1.16.0,helm.sh/chart=velpula-0.1.0

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                                        SELECTOR                                                         LABELS
deployment.apps/sony-velpula   0/1     1            0           17s   velpula      rahulwagh17/kubernetes:jhooq-k8s-springboot   app.kubernetes.io/instance=sony,app.kubernetes.io/name=velpula   app.kubernetes.io/instance=sony,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=velpula,app.kubernetes.io/version=1.16.0,helm.sh/chart=velpula-0.1.0

NAME                                      DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES                                        SELECTOR                                                                                      LABELS
replicaset.apps/sony-velpula-5cdc66bdd6   1         1         0       17s   velpula      rahulwagh17/kubernetes:jhooq-k8s-springboot   app.kubernetes.io/instance=sony,app.kubernetes.io/name=velpula,pod-template-hash=5cdc66bdd6   app.kubernetes.io/instance=sony,app.kubernetes.io/name=velpula,pod-template-hash=5cdc66bdd6
==========++++++"
anji@master:~$ helm list -a
NAME      	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART           	APP VERSION
release1-1	default  	3       	2023-01-04 13:50:28.742369186 +0530 IST	deployed	springboot-0.1.0	1.16.0     
sony      	default  	1       	2023-01-04 16:12:21.864548832 +0530 IST	deployed	velpula-0.1.0   	1.16.0     

anji@master:~$ helm delete release1-1  sony    
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
release "release1-1" uninstalled
release "sony" uninstalled

anji@master:~$ helm list -a
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
NAME	NAMESPACE	REVISION	UPDATED	STATUS	CHART	APP VERSION
#################################################################################################################
"
anji@master:~$ helm create wipro
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
Creating wipro
anji@master:~$ tree wipro/
wipro/
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files
========+++ "
anji@master:~/wipro$ cat values.yaml 
# Default values for wipro.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 1
image:
  repository: nginx
service:
  type: NodePort
  port: 80
=====+++++ "
anji@master:~$ helm lint wipro/
==> Linting wipro/
[INFO] Chart.yaml: icon is recommended
1 chart(s) linted, 0 chart(s) failed
###########"
anji@master:~$ helm list -a 
NAME	NAMESPACE	REVISION	UPDATED	STATUS	CHART	APP VERSION
==----
anji@master:~$ helm install nginx1-1  wipro/
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
NAME:" nginx1-1"
LAST DEPLOYED: Wed Jan  4 17:43:49 2023
NAMESPACE: default
STATUS:" deployed"
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services nginx1-1-wipro)
  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT  
==--+++++= "
anji@master:~$ kubectl get all -o wide --show-labels 
NAME                                  READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginx1-1-wipro-5587c64dfb-2h9xl   1/1     Running   0          112s   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=nginx1-1,app.kubernetes.io/name=wipro,pod-template-hash=5587c64dfb

NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE    SELECTOR                                                           LABELS
service/kubernetes       ClusterIP   10.96.0.1       <none>        443/TCP        13m    <none>                                                             component=apiserver,provider=kubernetes
service/nginx1-1-wipro   "NodePort"    10.107.65.182   <none>        "80:30711/TCP "  112s   app.kubernetes.io/instance=nginx1-1,app.kubernetes.io/name=wipro   app.kubernetes.io/instance=nginx1-1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=wipro,app.kubernetes.io/version=1.16.0,helm.sh/chart=wipro-0.1.0

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES         SELECTOR                                                           LABELS
deployment.apps/nginx1-1-wipro   1/1     1            1           112s   wipro        nginx:1.16.0   app.kubernetes.io/instance=nginx1-1,app.kubernetes.io/name=wipro   app.kubernetes.io/instance=nginx1-1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=wipro,app.kubernetes.io/version=1.16.0,helm.sh/chart=wipro-0.1.0

NAME                                        DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES         SELECTOR                                                                                        LABELS
replicaset.apps/nginx1-1-wipro-5587c64dfb   1         1         1       112s   wipro       " nginx:1.16.0"   app.kubernetes.io/instance=nginx1-1,app.kubernetes.io/name=wipro,pod-template-hash=5587c64dfb   app.kubernetes.io/instance=nginx1-1,app.kubernetes.io/name=wipro,pod-template-hash=5587c64dfb

anji@master:~$ curl http://192.168.122.172:30711/   ## success 
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
=====++++++++++//
anji@master:~$ helm list -a 
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART      	APP VERSION
nginx1-1	default  	1       	2023-01-04 19:01:14.307709222 +0530 IST	deployed	wipro-0.1.0	1.16.0   
anji@master:~$ helm delete nginx1-1
release "nginx1-1" uninstalled
=============#######################################333
anji@master:~/wipro$ cat values.yaml 
# Default values for wipro.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 1
anji12image:      === modify data key  see look
  repository: nginx
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""
--=====----"
anji@master:~$ helm lint wipro/
==> Linting wipro/
[INFO] Chart.yaml: icon is recommended
[ERROR] templates/: template: wipro/templates/deployment.yaml:34:28: executing "wipro/templates/deployment.yaml" at <.Values.image.repository>: nil pointer evaluating interface {}.repository
Error: 1 chart(s) linted, 1 chart(s) failed

anji@master:~$ helm lint wipro/
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
==> Linting wipro/
[INFO] Chart.yaml: icon is recommended
[ERROR] values.yaml: unable to parse YAML: error converting YAML to JSON: yaml: line 41: mapping values are not allowed in this context
[ERROR] templates/: cannot load values.yaml: error converting YAML to JSON: yaml: line 41: mapping values are not allowed in this context
[ERROR] : unable to load chart
	cannot load values.yaml: error converting YAML to JSON: yaml: line 41: mapping values are not allowed in this context
Error: 1 chart(s) linted, 1 chart(s) failed "
##########################################################################################################
https://www.youtube.com/watch?v=VW1JCmBr8H0&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=9
Helm template | How to use "helm template" command for your helm chart
anji@master:~$ helm create motherboard
Creating motherboard
anji@master:~$ tree motherboard/
motherboard/
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files
+++---=====
anji@master:~$ helm template motherboard/
---
# Source: motherboard/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-motherboard
  labels:
    helm.sh/chart: motherboard-0.1.0
    app.kubernetes.io/name: motherboard
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: motherboard/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-motherboard
  labels:
    helm.sh/chart: motherboard-0.1.0
    app.kubernetes.io/name: motherboard
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: motherboard
    app.kubernetes.io/instance: release-name
---
# Source: motherboard/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-motherboard
  labels:
    helm.sh/chart: motherboard-0.1.0
    app.kubernetes.io/name: motherboard
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: motherboard
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: motherboard
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: release-name-motherboard
      securityContext:
        {}
      containers:
        - name: motherboard
          securityContext:
            {}
          image: "nginx:1.16.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {}
---
# Source: motherboard/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-motherboard-test-connection"
  labels:
    helm.sh/chart: motherboard-0.1.0
    app.kubernetes.io/name: motherboard
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['release-name-motherboard:80']
  restartPolicy: Never
==++++++++++"
anji@master:~$ helm install  intel motherboard/
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
NAME: intel
LAST DEPLOYED: Wed Jan  4 19:52:40 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=motherboard,app.kubernetes.io/instance=intel" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
anji@master:~$ helm list -a 
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
NAME 	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART            	APP VERSION
intel	default  	1       	2023-01-04 19:52:40.166847269 +0530 IST	deployed	motherboard-0.1.0	1.16.0     " 

anji@master:~$ cd motherboard/
anji@master:~/motherboard$ ls
charts  Chart.yaml  templates  values.yaml
anji@master:~/motherboard$ cd templates/
anji@master:~/motherboard/templates$ ls
deployment.yaml  _helpers.tpl  hpa.yaml  ingress.yaml  NOTES.txt  serviceaccount.yaml  service.yaml  tests

anji@master:~$ helm install red motherboard/
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
NAME: red
LAST DEPLOYED: Wed Jan  4 20:08:36 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=motherboard,app.kubernetes.io/instance=red" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT

anji@master:~/motherboard/templates$  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
anji@master:~/motherboard/templates$   echo "Visit http://127.0.0.1:8080 to use your application"
Visit http://127.0.0.1:8080 to use your application
anji@master:~/motherboard/templates$   kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80
Handling connection for 8080

anji@master:~$ curl http://127.0.0.1:8080
+++++++++++++++++++###################################################
"
https://www.youtube.com/watch?v=gb5nYiWAIUs&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=10
How to use "helm --debug --dry-run" command to validate and verify kubernetes resources
https://jhooq.com/helm-dry-run-install/

anji@master:~$ helm create samsung 
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
Creating samsung

anji@master:~$ ls 
Desktop    Downloads                        linux-amd64  Pictures  samsung    Videos
Documents  helm-v3.10.3-linux-amd64.tar.gz  Music        Public    Templates

anji@master:~$ cd samsung/
anji@master:~/samsung$ tree
.
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files
==---++++-- "
anji@master:~$  helm install apple  --debug --dry-run samsung/    #  see look importent
install.go:192: [debug] Original chart version: ""
install.go:209: [debug] CHART PATH: /home/anji/samsung
NAME: apple
LAST DEPLOYED: Wed Jan  4 20:35:08 2023
NAMESPACE: default
STATUS: pending-install
REVISION: 1
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
fullnameOverride: ""
image:
  pullPolicy: IfNotPresent
  repository: nginx
  tag: ""
imagePullSecrets: []
ingress:
  annotations: {}
  className: ""
  enabled: false
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
nameOverride: ""
nodeSelector: {}
podAnnotations: {}
podSecurityContext: {}
replicaCount: 1
resources: {}
securityContext: {}
service:
  port: 80
  type: ClusterIP
serviceAccount:
  annotations: {}
  create: true
  name: ""
tolerations: []

HOOKS:
---
# Source: samsung/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "apple-samsung-test-connection"
  labels:
    helm.sh/chart: samsung-0.1.0
    app.kubernetes.io/name: samsung
    app.kubernetes.io/instance: apple
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['apple-samsung:80']
  restartPolicy: Never
MANIFEST:
---
# Source: samsung/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: apple-samsung
  labels:
    helm.sh/chart: samsung-0.1.0
    app.kubernetes.io/name: samsung
    app.kubernetes.io/instance: apple
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: samsung/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: apple-samsung
  labels:
    helm.sh/chart: samsung-0.1.0
    app.kubernetes.io/name: samsung
    app.kubernetes.io/instance: apple
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: samsung
    app.kubernetes.io/instance: apple
---
# Source: samsung/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apple-samsung
  labels:
    helm.sh/chart: samsung-0.1.0
    app.kubernetes.io/name: samsung
    app.kubernetes.io/instance: apple
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: samsung
      app.kubernetes.io/instance: apple
  template:
    metadata:
      labels:
        app.kubernetes.io/name: samsung
        app.kubernetes.io/instance: apple
    spec:
      serviceAccountName: apple-samsung
      securityContext:
        {}
      containers:
        - name: samsung
          securityContext:
            {}
          image: "nginx:1.16.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {}

NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=samsung,app.kubernetes.io/instance=apple" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT

anji@master:~$ helm list -a
NAME	NAMESPACE	REVISION	UPDATED	STATUS	CHART	APP VERSION

##############################################################################################
"How to use/pull environment variables into Helm Chart
https://www.youtube.com/watch?v=dKKMJseMiGU&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=12

https://jhooq.com/helm-pass-environment-variables/

anji@master:~$ helm  create cpu
Creating cpu

anji@master:~$ cd cpu/
anji@master:~/cpu$ ls
charts  Chart.yaml  templates  values.yaml
----
anji@master:~$ helm install ssd cpu
NAME: ssd
LAST DEPLOYED: Thu Jan  5 11:09:24 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=cpu,app.kubernetes.io/instance=ssd" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT

anji@master:~$ helm list -a 
NAME	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART    	APP VERSION
ssd 	default  	1       	2023-01-05 11:09:24.442124942 +0530 IST	deployed	cpu-0.1.0	1.16.0     

anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/ssd-cpu-5dfc6bd7d7-795n8   1/1     Running   0          27s   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=ssd,app.kubernetes.io/name=cpu,pod-template-hash=5dfc6bd7d7

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR                                                    LABELS
deployment.apps/ssd-cpu   1/1     1            1           27s   cpu          nginx:1.16.0   app.kubernetes.io/instance=ssd,app.kubernetes.io/name=cpu   app.kubernetes.io/instance=ssd,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=cpu,app.kubernetes.io/version=1.16.0,helm.sh/chart=cpu-0.1.0
===="

anji@master:~$ helm install --set replicaCount=2  ssd cpu/
Error: INSTALLATION FAILED: cannot re-use a name that is still in use

anji@master:~$ helm install --set replicaCount=2 ssd1 cpu/
NAME: ssd1
LAST DEPLOYED: Thu Jan  5 11:17:42 2023
NAMESPACE: default
STATUS: "deployed"
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=cpu,app.kubernetes.io/instance=ssd1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT

anji@master:~$ helm list 
NAME	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART    	APP VERSION
ssd 	default  	1       	2023-01-05 11:09:24.442124942 +0530 IST	deployed	cpu-0.1.0	1.16.0     
ssd1	default  	1       	2023-01-05 11:17:42.044590608 +0530 IST	deployed	cpu-0.1.0	1.16.0     "
anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                            READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/ssd-cpu-5dfc6bd7d7-795n8    1/1     Running   0          12m     10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=ssd,app.kubernetes.io/name=cpu,pod-template-hash=5dfc6bd7d7
pod/ssd1-cpu-7666c44784-cms6z   1/1     Running   0          3m43s   10.44.0.3   worker1   <none>           <none>            app.kubernetes.io/instance=ssd1,app.kubernetes.io/name=cpu,pod-template-hash=7666c44784
pod/ssd1-cpu-7666c44784-r6xp6   1/1     Running   0          3m43s   10.44.0.2   worker1   <none>           <none>            app.kubernetes.io/instance=ssd1,app.kubernetes.io/name=cpu,pod-template-hash=7666c44784

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES         SELECTOR                                                     LABELS
deployment.apps/ssd-cpu    1/1     1            1           12m     cpu          nginx:1.16.0   app.kubernetes.io/instance=ssd,app.kubernetes.io/name=cpu    app.kubernetes.io/instance=ssd,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=cpu,app.kubernetes.io/version=1.16.0,helm.sh/chart=cpu-0.1.0
deployment.apps/ssd1-cpu   2/2     2            2           3m43s   cpu          nginx:1.16.0   app.kubernetes.io/instance=ssd1,app.kubernetes.io/name=cpu   app.kubernetes.io/instance=ssd1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=cpu,app.kubernetes.io/version=1.16.0,helm.sh/chart=cpu-0.1.0
"""
anji@master:~$ helm install  --set replicaCount=1   --set replicaCount=3  ap1  india/ 
NAME: ap1
LAST DEPLOYED: Thu Jan  5 12:46:48 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=india,app.kubernetes.io/instance=ap1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT "
====\\\//\//\/\//
anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/ap-india-f7dcc47b-tfj49      1/1     Running   0          47m   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=ap,app.kubernetes.io/name=india,pod-template-hash=f7dcc47b
pod/ap1-india-7b69dffb47-jz98f   1/1     Running   0          6s    10.44.0.3   worker1   <none>           <none>            app.kubernetes.io/instance=ap1,app.kubernetes.io/name=india,pod-template-hash=7b69dffb47
pod/ap1-india-7b69dffb47-mbp4q   1/1     Running   0          6s    10.44.0.2   worker1   <none>           <none>            app.kubernetes.io/instance=ap1,app.kubernetes.io/name=india,pod-template-hash=7b69dffb47
pod/ap1-india-7b69dffb47-w7h9w   1/1     Running   0          6s    10.44.0.4   worker1   <none>           <none>            app.kubernetes.io/instance=ap1,app.kubernetes.io/name=india,pod-template-hash=7b69dffb47

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR                                                      LABELS
deployment.apps/ap-india    1/1     1            1           47m   india        nginx:1.16.0   app.kubernetes.io/instance=ap,app.kubernetes.io/name=india    app.kubernetes.io/instance=ap,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=india,app.kubernetes.io/version=1.16.0,helm.sh/chart=india-0.1.0
deployment.apps/ap1-india   3/3     3            3           6s    india        nginx:1.16.0   app.kubernetes.io/instance=ap1,app.kubernetes.io/name=india   app.kubernetes.io/instance=ap1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=india,app.kubernetes.io/version=1.16.0,helm.sh/chart=india-0.1.0

============##########+++++++++++++++++++++++++++=="
nano myvalues.yaml 
  replicaCount: 3
anji@master:~$ ls
Desktop    Downloads                        laptop       Music          Pictures  Public     Videos
Documents  helm-v3.10.3-linux-amd64.tar.gz  linux-amd64  "myvalues.yaml " project   Templates

anji@master:~$ helm install -f myvalues.yaml  release1-1 project/
NAME: release1-1
LAST DEPLOYED: Thu Jan  5 13:07:53 2023
NAMESPACE: default
STATUS: "deployed"
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=project,app.kubernetes.io/instance=release1-1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT

nji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                                     READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/release1-1-project-584d48d5f-5twgp   1/1     Running   0          8s    10.44.0.2   worker1   <none>           <none>            app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=project,pod-template-hash=584d48d5f
pod/release1-1-project-584d48d5f-j7rvd   1/1     Running   0          8s    10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=project,pod-template-hash=584d48d5f
pod/release1-1-project-584d48d5f-zkmbv   1/1     Running   0          8s    10.44.0.3   worker1   <none>           <none>            app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=project,pod-template-hash=584d48d5f

NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR                                                               LABELS
deployment.apps/release1-1-project   "3/3 "    3            3           8s    project      nginx:1.16.0   app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=project   app.kubernetes.io/instance=release1-1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=project,app.kubernetes.io/version=1.16.0,helm.sh/chart=project-0.1.0
===+++++++++++""
anji@master:~$ helm create gold 
anji@master:~$ ls
Desktop    Downloads  helm-v3.10.3-linux-amd64.tar.gz  Music          Pictures  Public     Videos
Documents  "gold "      linux-amd64                     project   Templates

anji@master:~$ cat env-values.yaml 
examplemap:
 - name: "USERNAME"
   value: "test1"
 - name: "PASSWORD"
   value: "test2"
++\\\/// ///// BEFORE   BEFORE  ""
"anji@master:~$ ls
Desktop    Downloads        gold                             linux-amd64  Pictures  Public     Videos
Documents  env-values.yaml  helm-v3.10.3-linux-amd64.tar.gz  Music        project   Templates

anji@master:~$ cat gold/
charts/      Chart.yaml   .helmignore  templates/   values.yaml  

anji@master:~$ cat gold/templates/
deployment.yaml      hpa.yaml             NOTES.txt            service.yaml         
_helpers.tpl         ingress.yaml         serviceaccount.yaml  tests/          "

anji@master:~$ cat gold/templates/deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "gold.fullname" . }}
  labels:
    {{- include "gold.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "gold.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "gold.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "gold.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}     ##  see look   modify 
          ports:
            - name: http
              containerPort: {{ .Values.service.port }}
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
###################+++
anji@master:~$ cat gold/templates/deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
           image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }} 
          env:                          ###    =-   see look  
            {{- range .Values.examplemap }}
            - name: {{ .name }}
              value: {{ .value }}
            {{- end }} 
          ports: "
---======
anji@master:~$ helm template -f env-values.yaml  gold/
---
# Source: gold/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:

---
# Source: gold/templates/service.yaml
apiVersion: v1
kind: Service
metadata:

---
# Source: gold/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment

        {}
      containers:
        - name: gold
          securityContext:
            {}
          image: "nginx:1.16.0"
          imagePullPolicy: IfNotPresent 
          env:
            - name: USERNAME     ## "see look "
              value: test1
            - name: PASSWORD
              value: test2 

            {}
---
# Source: gold/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-gold-test-connection"

    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['release-name-gold:80']
  restartPolicy: Never
=====++++"
anji@master:~$ helm install -f env-values.yaml release1 gold/
NAME: release1
LAST DEPLOYED: Thu Jan  5 14:10:33 2023
NAMESPACE: default
STATUS: "deployed"
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=gold,app.kubernetes.io/instance=release1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT "

anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                                 READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/release1-gold-7f4d4cc84b-hk8mb   1/1     Running   0          3m24s   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=release1,app.kubernetes.io/name=gold,pod-template-hash=7f4d4cc84b

NAME                            READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES         SELECTOR                                                          LABELS
deployment.apps/release1-gold   1/1     1            1           3m24s   gold         nginx:1.16.0   app.kubernetes.io/instance=release1,app.kubernetes.io/name=gold   app.kubernetes.io/instance=release1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=gold,app.kubernetes.io/version=1.16.0,helm.sh/chart=gold-0.1.0

anji@master:~$ kubectl describe pod release1-gold-7f4d4cc84b-hk8mb 
Name:             release1-gold-7f4d4cc84b-hk8mb
Labels:           app.kubernetes.io/instance=release1
                  app.kubernetes.io/name=gold
                  pod-template-hash=7f4d4cc84b
IPs:
  IP:           10.44.0.1
Controlled By:  ReplicaSet/release1-gold-7f4d4cc84b
Containers:
  gold:
    Image:          nginx:1.16.0
    Liveness:       http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:      http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
   "   USERNAME:  test1
      PASSWORD:  test2"
####################################################################################################################
Helm Upgrade Command | How to use Helm Upgrade Command - Part 12
https://www.youtube.com/watch?v=6ua2LBzPfxo&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=13
https://jhooq.com/building-first-helm-chart-with-spring-boot/#8-upgrade-helm-release
8. Upgrade helm release "
anji@master:~$ helm create jio 
Creating jio

anji@master:~$ ls
Desktop  Documents  Downloads  gold  helm-v3.10.3-linux-amd64.tar.gz  jio  linux-amd64  Music  Pictures  project  Public  Templates  Videos

anji@master:~$ tree jio 
jio
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files
====|||||||||||||||||//////////\\\\\ 
"anji@master:~$ helm install aoc jio/
NAME: aoc
LAST DEPLOYED: Thu Jan  5 14:34:25 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=jio,app.kubernetes.io/instance=aoc" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT

"anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                          READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/aoc-jio-9d9cbcc78-cw7vn   1/1     Running   0          98s   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=aoc,app.kubernetes.io/name=jio,pod-template-hash=9d9cbcc78

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR                                                    LABELS
deployment.apps/aoc-jio   1/1     1            1           98s   jio          nginx:1.16.0   app.kubernetes.io/instance=aoc,app.kubernetes.io/name=jio   app.kubernetes.io/instance=aoc,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=jio,app.kubernetes.io/version=1.16.0,helm.sh/chart=jio-0.1.0
" anji@master:~$ cd jio/
anji@master:~/jio$ ls
charts  Chart.yaml  templates  values.yaml

anji@master:~/jio$ nano values.yaml 
# Default values for jio.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 2
image:
===-----"
anji@master:~$ helm upgrade aoc  jio/
Release "aoc" has been upgraded. Happy Helming!
NAME: "aoc"
LAST DEPLOYED: Thu Jan  5 15:12:46 2023
NAMESPACE: default
STATUS: deployed
REVISION: "2"
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=jio,app.kubernetes.io/instance=aoc" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT "

anji@master:~$ helm list 
NAME	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART    	APP VERSION
aoc 	default  	2       	2023-01-05 15:12:46.709636937 +0530 IST	deployed	jio-0.1.0	1.16.0     
++=="
anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                          READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/aoc-jio-9d9cbcc78-cw7vn   1/1     Running   0          38m   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=aoc,app.kubernetes.io/name=jio,pod-template-hash=9d9cbcc78
pod/aoc-jio-9d9cbcc78-xj6v4   1/1     Running   0          31s   10.44.0.2   worker1   <none>           <none>            app.kubernetes.io/instance=aoc,app.kubernetes.io/name=jio,pod-template-hash=9d9cbcc78

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR                                                    LABELS
deployment.apps/aoc-jio   2/2     2            2           38m   jio          nginx:1.16.0   app.kubernetes.io/instance=aoc,app.kubernetes.io/name=jio   app.kubernetes.io/instance=aoc,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=jio,app.kubernetes.io/version=1.16.0,helm.sh/chart=jio-0.1.0
#################################################################################################################################
"9. Rollback Helm release
Helm Rollback Command | How to use Helm Rollback Command - Part 13
https://jhooq.com/building-first-helm-chart-with-spring-boot/#9-rollback-helm-release
anji@master:~/jio$ cd
anji@master:~$ helm create book 
Creating book

anji@master:~$ ls
book  Desktop  Documents  Downloads  linux-amd64  Music  Pictures  project  Public  Templates  Videos

anji@master:~$ helm install version1 book/
NAME: version1
LAST DEPLOYED: Thu Jan  5 16:10:54 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=book,app.kubernetes.io/instance=version1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT

anji@master:~$ helm list -a 
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART     	APP VERSION
version1	default  	"1"       	2023-01-05 16:10:54.409921948 +0530 IST	deployed	book-0.1.0	1.16.0     

anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                                 READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/version1-book-76d96ffb74-dn7gp   1/1     Running   0          8m54s   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=version1,app.kubernetes.io/name=book,pod-template-hash=76d96ffb74

NAME                            READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES         SELECTOR                                                          LABELS
deployment.apps/version1-book   1/1     1            1           8m54s   book         nginx:1.16.0   app.kubernetes.io/instance=version1,app.kubernetes.io/name=book   app.kubernetes.io/instance=version1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=book,app.kubernetes.io/version=1.16.0,helm.sh/chart=book-0.1.0
=======++++++++--------"
anji@master:~$ nano book/values.yaml 
anji@master:~$ cat book/values.yaml 
# Default values for book.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 5 "
---
anji@master:~$ helm upgrade version1 book/
Release "version1" has been upgraded. Happy Helming!
NAME: version1
LAST DEPLOYED: Thu Jan  5 16:24:05 2023
NAMESPACE: default
STATUS: deployed
REVISIO"N: 2"
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=book,app.kubernetes.io/instance=version1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT

anji@master:~$ helm list 
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART     	APP VERSION
version1	default  	"2 "      	2023-01-05 16:24:05.600255414 +0530 IST	deployed	book-0.1.0	1.16.0     

anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                                 READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/version1-book-76d96ffb74-9mgvk   1/1     Running   0          14s   10.44.0.5   worker1   <none>           <none>            app.kubernetes.io/instance=version1,app.kubernetes.io/name=book,pod-template-hash=76d96ffb74
pod/version1-book-76d96ffb74-dn7gp   1/1     Running   0          13m   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=version1,app.kubernetes.io/name=book,pod-template-hash=76d96ffb74
pod/version1-book-76d96ffb74-hllvz   1/1     Running   0          14s   10.44.0.4   worker1   <none>           <none>            app.kubernetes.io/instance=version1,app.kubernetes.io/name=book,pod-template-hash=76d96ffb74
pod/version1-book-76d96ffb74-kncbn   1/1     Running   0          14s   10.44.0.2   worker1   <none>           <none>            app.kubernetes.io/instance=version1,app.kubernetes.io/name=book,pod-template-hash=76d96ffb74
pod/version1-book-76d96ffb74-m8bfp   1/1     Running   0          14s   10.44.0.3   worker1   <none>           <none>            app.kubernetes.io/instance=version1,app.kubernetes.io/name=book,pod-template-hash=76d96ffb74

NAME                            READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR                                                          LABELS
deployment.apps/version1-book  " 5/5    " 5            5           13m   book         nginx:1.16.0   app.kubernetes.io/instance=version1,app.kubernetes.io/name=book   app.kubernetes.io/instance=version1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=book,app.kubernetes.io/version=1.16.0,helm.sh/chart=book-0.1.0
"anji@master:~$ helm list 
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART     	APP VERSION
version1	default  	"2   "    	2023-01-05 16:24:05.600255414 +0530 IST	deployed	book-0.1.0	1.16.0     

anji@master:~$ helm rollback version1 1
Rollback was a success! Happy Helming!

anji@master:~$ helm list 
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART     	APP VERSION
version1	default  "	3 "      	2023-01-05 16:29:09.061914373 +0530 IST	deployed	book-0.1.0	1.16.0     

anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                                 READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/version1-book-76d96ffb74-dn7gp   1/1     Running   0          18m   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=version1,app.kubernetes.io/name=book,pod-template-hash=76d96ffb74

NAME                            READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR                                                          LABELS
deployment.apps/version1-book   "1/1  "   1            1           18m   book         nginx:1.16.0   app.kubernetes.io/instance=version1,app.kubernetes.io/name=book   app.kubernetes.io/instance=version1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=book,app.kubernetes.io/version=1.16.0,helm.sh/chart=book-0.1.0
"
#####################################################################################################################################
https://linuxhostsupport.com/blog/how-to-install-grafana-on-ubuntu-20-04/
How to Install Grafana on Ubuntu 20.04

root@grafana:~# apt-get install wget curl gnupg2 apt-transport-https software-properties-common -y

root@grafana:~# wget -q -O - https://packages.grafana.com/gpg.key | apt-key add -

root@grafana:~# echo "deb https://packages.grafana.com/oss/deb stable main" | tee -a /etc/apt/sources.list.d/grafana.list

apt-get update -y

apt-get install grafana -y


 root@grafana:~# grafana-server -v
   Version 9.3.2 (commit: 21c1d14e91, branch: HEAD)

systemctl start grafana-server
systemctl enable grafana-server

systemctl status grafana-server

root@grafana:~# ss -antpl | grep 3000
LISTEN    0         4096                     *:3000                   *:*        users:(("grafana-server",pid=4279,fd=9)) 

http://localhost:3000/login

################   or  or  or  second method {}{}
" https://grafana.com/grafana/download?pg=get&plcmt=selfmanaged-box1-cta1

root@anji:~# wget https://dl.grafana.com/enterprise/release/grafana-enterprise-9.3.2.linux-amd64.tar.gz
root@anji:~# tar -zxvf grafana-enterprise-9.3.2.linux-amd64.tar.gz
root@anji:~# tar -zxvf grafana-enterprise-9.3.2.linux-amd64.tar.gz
root@anji:~# cd grafana-9.3.2/
root@anji:~/grafana-9.3.2# ls
bin  conf  LICENSE  NOTICE.md  plugins-bundled  public  README.md  scripts  VERSION
root@anji:~/grafana-9.3.2# cd bin/
root@anji:~/grafana-9.3.2/bin# la
grafana-cli  grafana-cli.md5  grafana-server  grafana-server.md5
root@anji:~/grafana-9.3.2/bin# ./grafana-server 


++++++++++=============+++++++==================================//////\\\
Configure Nginx as a Reverse Proxy for Grafana "
Next, you will need to install the Nginx as a reverse proxy for Grafana. First, install the Nginx package using the following command:
apt-get install nginx -y
nano /etc/nginx/conf.d/grafana.conf

root@grafana:~# cat  /etc/nginx/conf.d/grafana.conf
Server {
        server_name grafana.example.com;
        listen 80 ;
        access_log /var/log/nginx/grafana.log;

    location / {
                proxy_pass http://localhost:3000;
        proxy_set_header Host $http_host;
                proxy_set_header X-Forwarded-Host $host:$server_port;
                proxy_set_header X-Forwarded-Server $host;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }
}
-----------------------
nginx -t
systemctl restart nginx
"
########################################################################################
How to Install Prometheus on Ubuntu 20.04
https://linuxopsys.com/topics/install-prometheus-on-ubuntu

sudo mkdir -p /etc/prometheus
sudo mkdir -p /var/lib/prometheus

https://prometheus.io/download/

https://github.com/prometheus/prometheus/releases/download/v2.41.0/prometheus-2.41.0.linux-amd64.tar.gz

root@grafana:~# tar -xvzf prometheus-2.41.0.linux-amd64.tar.gz 

root@grafana:~# cd prometheus-2.41.0.linux-amd64/

root@grafana:~/prometheus-2.41.0.linux-amd64# ls
console_libraries  consoles  LICENSE  NOTICE  prometheus  prometheus.yml  promtool
root@anji:~/prometheus-2.41.0.linux-amd64# ./prometheus 
     ##\\ || second model == under --------=====================-------------------------//////////

root@grafana:~/prometheus-2.41.0.linux-amd64# sudo mv prometheus promtool /usr/local/bin/

root@grafana:~/prometheus-2.41.0.linux-amd64# mkdir -p  /etc/prometheus/
root@grafana:~/prometheus-2.41.0.linux-amd64# sudo mv consoles/ console_libraries/ /etc/prometheus/

root@grafana:~/prometheus-2.41.0.linux-amd64# sudo mv prometheus.yml /etc/prometheus/prometheus.yml

root@grafana:~# prometheus --version
prometheus, version 2.41.0 (branch: HEAD, revision: c0d8a56c69014279464c0e15d8bfb0e153af0dab)
  build user:       root@d20a03e77067
  build date:       20221220-10:40:45
  go version:       go1.19.4
  platform:         linux/amd64
-----
root@grafana:~# promtool --version
promtool, version 2.41.0 (branch: HEAD, revision: c0d8a56c69014279464c0e15d8bfb0e153af0dab)
  build user:       root@d20a03e77067
  build date:       20221220-10:40:45
  go version:       go1.19.4
  platform:         linux/amd64
-----
root@grafana:~# sudo groupadd --system prometheus

root@grafana:~# sudo useradd -s /sbin/nologin --system -g prometheus prometheus

root@grafana:~# sudo chown -R prometheus:prometheus /etc/prometheus/ /var/lib/prometheus/

root@grafana:~# sudo chmod -R 775 /etc/prometheus/ /var/lib/prometheus/

root@grafana:~# sudo nano /etc/systemd/system/prometheus.service
root@grafana:~# cat  nano /etc/systemd/system/prometheus.service
cat: nano: No such file or directory
[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target

[Service]
User=prometheus
Group=prometheus
Restart=always
Type=simple
ExecStart=/usr/local/bin/prometheus \
    --config.file=/etc/prometheus/prometheus.yml \
    --storage.tsdb.path=/var/lib/prometheus/ \
    --web.console.templates=/etc/prometheus/consoles \
    --web.console.libraries=/etc/prometheus/console_libraries \
    --web.listen-address=0.0.0.0:9090

[Install]
WantedBy=multi-user.target
---------
sudo systemctl start prometheus
sudo systemctl enable prometheus
sudo systemctl status prometheus

root@grafana:~# sudo ufw allow 9090/tcp
Rules updated
Rules updated (v6)

root@grafana:~# systemctl start firewalld
root@grafana:~# sudo ufw allow 9090/tcp
Skipping adding existing rule
Skipping adding existing rule (v6)
http://localhost:9090/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
################################################################################3
https://jhooq.com/prometheous-grafan-setup/

http://localhost:9090/metrics

2.1 How to Install Node exporter
After installing the Prometheus in the previous step the next package we are going to install is **Node Exporter **. Node exported is used for collecting various hardware and kernel-level metrics of your machine.

https://prometheus.io/download/#node_exporter
root@anji:~# wget https://github.com/prometheus/node_exporter/releases/download/v1.5.0/node_exporter-1.5.0.linux-amd64.tar.gz

root@anji:~# tar -xvzf node_exporter-1.5.0.linux-amd64.tar.gz 
node_exporter-1.5.0.linux-amd64/
node_exporter-1.5.0.linux-amd64/LICENSE
node_exporter-1.5.0.linux-amd64/NOTICE
node_exporter-1.5.0.linux-amd64/node_exporter

root@anji:~# cd node_exporter-1.5.0.linux-amd64

root@anji:~/node_exporter-1.5.0.linux-amd64# ls
LICENSE  node_exporter  NOTICE

root@anji:~/node_exporter-1.5.0.linux-amd64# ./node_exporter 
ts=2023-01-05T13:38:16.088Z caller=node_exporter.go:180 level=info msg="Starting node_exporter" version="(version=1.5.0, branch=HEAD, revision=1b48970ffcf5630534fb00bb0687d73c66d1c959)"
ts=2023-01-05T13:38:16.088Z caller=node_exporter.go:181 level=info msg="Build context" build_context="(go=go1.19.3, user=root@6e7732a7b81b, date=20221129-18:59:09)"
ts=2023-01-05T13:38:16.095Z caller=node_exporter.go:117 level=info collector=zfs
ts=2023-01-05T13:38:16.096Z caller=tls_config.go:232 level=info msg="Listening on" "address=[::]:9100"
ts=2023-01-05T13:38:16.096Z caller=tls_config.go:235 level=info msg="TLS is disabled." http2=false address=[::]:9100

http://192.168.122.205:9100/
http://192.168.122.205:9100/metrics

root@worker1:/etc/kubernetes# export PS1='$'
$
$

https://prometheus.io/docs/guides/node-exporter/

global:
  scrape_interval: 15s

scrape_configs:
- job_name: node
  static_configs:
  - targets: ['localhost:9100']
root@anji:~# nano exporter-config.yaml 
root@anji:~# cat exporter-config.yaml 
global:
  scrape_interval: 15s

scrape_configs:
- job_name: node
  static_configs:
  - targets: ['localhost:9100']
---====
 root@anji:~# ls
exporter-config.yaml             node_exporter-1.5.0.linux-amd64.tar.gz  snap
git                              prometheus-2.41.0.linux-amd64
node_exporter-1.5.0.linux-amd64  prometheus-2.41.0.linux-amd64.tar.gz

root@anji:~# cd prometheus-2.41.0.linux-amd64/
root@anji:~/prometheus-2.41.0.linux-amd64# ls
console_libraries  consoles  LICENSE  NOTICE  prometheus  prometheus.yml  promtool

root@anji:~/prometheus-2.41.0.linux-amd64# cp /root/
.bash_history                           node_exporter-1.5.0.linux-amd64/
.bashrc                                 node_exporter-1.5.0.linux-amd64.tar.gz
.cache/                                 .profile
exporter-config.yaml                    prometheus-2.41.0.linux-amd64/
git/                                    prometheus-2.41.0.linux-amd64.tar.gz
.gitconfig                              snap/
.local/                                 .wget-hsts
root@anji:~/prometheus-2.41.0.linux-amd64# cp /root/exporter-config.yaml  .

root@anji:~/prometheus-2.41.0.linux-amd64# ls
console_libraries  exporter-config.yaml  NOTICE      prometheus.yml
consoles           LICENSE               prometheus  promtool

root@anji:~/prometheus-2.41.0.linux-amd64# ./prometheus --config.file=exporter-config.yaml       \\\\\\\\  importent 
ts=2023-01-05T14:09:58.534Z caller=main.go:512 level=info msg="No time or size retention was set so using the default time retention" duration=15d
ts=2023-01-05T14:09:58.534Z caller=main.go:556 level=info msg="Starting Prometheus Server" mode=server version="(version=2.41.0, branch=HEAD, revision=c0d8a56c69014279464c0e15d8bfb0e153af0dab)"
ts=2023-01-05T14:09:58.534Z caller=main.go:561 level=info build_context="(go=go1.19.4, platform=linux/amd64, user=root@d20a03e77067, date=20221220-10:40:45)"
ts=2023-01-05T14:09:58.534Z caller=main.go:562 level=info host_details="(Linux 5.15.0-56-generic #62~20.04.1-Ubuntu SMP Tue Nov 22 21:24:20 UTC 2022 x86_64 anji (none))"
ts=2023-01-05T14:09:58.535Z caller=main.go:563 level=info fd_limits="(soft=1048576, hard=1048576)"
ts=2023-01-05T14:09:58.535Z caller=main.go:564 level=info vm_limits="(soft=unlimited, hard=unlimited)"
ts=2023-01-05T14:09:58.537Z caller=web.go:559 level=info component=web msg="Start listening for connections" "address=0.0.0.0:9090"
ts=2023-01-05T14:13:06.563Z caller=main.go:1234 level=info msg="Completed loading of configuration file"" filename=exporter-config.yaml "totalDuration=407.773Âµs db_storage=1.04Âµs remote_storage=1.623Âµs web_handler=334ns query_engine=671ns scrape=186.24Âµs scrape_sd=16.487Âµs notify=780ns notify_sd=1.527Âµs rules=1.129Âµs tracing=10.52Âµs
ts=2023-01-05T14:13:06.563Z caller=main.go:978 level=info msg="Server is ready to receive web requests."
ts=2023-01-05T14:13:06.563Z caller=manager.go:953 level=info component="rule manager" msg="Starting rule manager..."

{{}{{}{}}}    OR  OR  OR  
root@anji:~/prometheus-2.41.0.linux-amd64# ls
console_libraries  data                  LICENSE  prometheus      promtool
consoles           exporter-config.yaml  NOTICE   prometheus.yml
root@anji:~/prometheus-2.41.0.linux-amd64# cat prom
prometheus      prometheus.yml  promtool        
root@anji:~/prometheus-2.41.0.linux-amd64# cat prometheus.yml 
# my global config
global:
  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
          # - alertmanager:9093

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
  # - "first_rules.yml"
  # - "second_rules.yml"

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: "prometheus"

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
      - targets: ["localhost:9090","192.168.29.152:9100","192.168.122.232:9100"]      "best see look  follow "

=======================+++++++++++
add the  node1  to  prometheus 
 https://prometheus.io/download/#node_exporter
 anji@worker1:~$ mkdir nodeexporter
anji@worker1:~$ cd nodeexporter/

anji@worker1:~/nodeexporter$ wget https://github.com/prometheus/node_exporter/releases/download/v1.5.0/node_exporter-1.5.0.linux-amd64.tar.gz
anji@worker1:~/nodeexporter$ ls
node_exporter-1.5.0.linux-amd64.tar.gz

anji@worker1:~/nodeexporter$ tar -xvzf node_exporter-1.5.0.linux-amd64.tar.gz 
node_exporter-1.5.0.linux-amd64/
node_exporter-1.5.0.linux-amd64/LICENSE
node_exporter-1.5.0.linux-amd64/NOTICE
node_exporter-1.5.0.linux-amd64/node_exporter

anji@worker1:~/nodeexporter$ ls
node_exporter-1.5.0.linux-amd64  node_exporter-1.5.0.linux-amd64.tar.gz

anji@worker1:~/nodeexporter$ cd node_exporter-1.5.0.linux-amd64/

anji@worker1:~/nodeexporter/node_exporter-1.5.0.linux-amd64$ ls
LICENSE  node_exporter  NOTICE

anji@worker1:~/nodeexporter/node_exporter-1.5.0.linux-amd64$ ./node_exporter 
ts=2023-01-05T14:33:29.309Z caller=node_exporter.go:117 level=info collector=uname
ts=2023-01-05T14:33:29.309Z caller=node_exporter.go:117 level=info collector=vmstat
ts=2023-01-05T14:33:29.309Z caller=node_exporter.go:117 level=info collector=xfs
ts=2023-01-05T14:33:29.309Z caller=node_exporter.go:117 level=info collector=zfs
ts=2023-01-05T14:33:29.314Z caller=tls_config.go:232 level=info msg="Listening on" "address=[::]:9100"
ts=2023-01-05T14:33:29.314Z caller=tls_config.go:235 level=info msg="TLS is disabled." http2=false address=[::]:9100

http://192.168.122.170:9100/   or  or  http://worker1:9100/

------------------------
root@anji:~/prometheus-2.41.0.linux-amd64# cat exporter-config.yaml 
global:
  scrape_interval: 15s

scrape_configs:
- job_name: node
  static_configs:
  - targets: ['localhost:9100','192.168.122.170:9100']
root@anji:~/prometheus-2.41.0.linux-amd64# 

{}{{{{{{{}{{{{{{{{{}}}}}}}}}}{{{{{{{{}}}}}}}}}}}}}  error "error "
msg="Unable to start web listener" err="listen tcp 0.0.0.0:9090: bind: address already in use"
sudo lsof -i -P -n | grep 9090

https://stackoverflow.com/questions/47552888/prometheus-error-starting-web-server-address-already-in-use
root@anji:~# lsof -i :9090
COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
prometheu 3521 root    7u  IPv6 102416      0t0  TCP *:9090 (LISTEN)
prometheu 3521 root   12u  IPv4 102643      0t0  TCP localhost:60932->localhost:9090 (ESTABLISHED)
prometheu 3521 root   13u  IPv6 101242      0t0  TCP localhost:9090->localhost:60932 (ESTABLISHED)

root@anji:~# kill -9 3521
root@anji:~# lsof -i :9090

{}{{{{{{{}{{{{{{{}{}}}}}}}}}}}}}  ERROR 
Get "http://192.168.122.232:9100/metrics": dial tcp 192.168.122.232:9100: connect: no route to host

https://www.henryxieblogs.com/2018/11/how-to-fix-no-route-to-host-in.html

wget -O- localhost:9100/metrics
firewall-cmd --add-port=9100/tcp --permanent
systemctl restart firewalld

########################
"  GRAFANA DASH BOARD ADD 
https://grafana.com/grafana/dashboards/
https://grafana.com/grafana/dashboards/2747-linux-memory/
ID: 2747      "
{}{}{}{[]]{}{{}{}}{][[[{{{]][[{}{}{[][[[][{}{}{{{{{{{{{{}{}}}}}}}}}}]]]}]]}}}  error  ERROR  
N: Skipping acquire of configured file 'main/binary-i386/Packages' as repository 
 'https://packages.grafana.com/oss/deb stable InRelease' doesn't support architecture 'i386'

https://askubuntu.com/questions/741410/skipping-acquire-of-configured-file-main-binary-i386-packages-as-repository-x

dpkg --print-foreign-architectures
dpkg --print-architecture 
sudo dpkg --remove-architecture i386
=========++++++++  {{{}}{}{}[][[]{}{}{}[[]]][[[[[]]{}{}{{}}]]]]]}    error error 
add prometheus data source to grafana error message Error reading Prometheus: An error occurred within the plugin

http://192.168.122.48:9090/metrics  check once 

https://community.grafana.com/t/getting-403-forbidden-error-when-adding-data-source-by-ip/78400
root@anji:~# cat /var/log/grafana/grafana.log
logger=settings t=2023-01-06T09:36:58.922634288+05:30 level=info msg="Starting Grafana" version=9.3.2 commit=21c1d14e91 branch=HEAD compiled=2022-12-14T16:10:18+05:30
logger=settings t=2023-01-06T09:36:58.923330109+05:30 level=inf
-----
https://www.youtube.com/watch?v=4WWW2ZLEg74&t=1s    nice videos 

root@anji:~# kill -s HUP 
kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]
root@anji:~# kill -s HUP   pid no 

dash boards 
https://grafana.com/grafana/dashboards/?search=pod
https://grafana.com/orgs/grafana/plugins
https://grafana.com/orgs/grafana/dashboards

https://grafana.com/grafana/dashboards/
https://grafana.com/grafana/dashboards/7743-node-exporter-server-metrics-template/
https://grafana.com/grafana/dashboards/704-node-exporter-single-server/
https://grafana.com/grafana/dashboards/11757-hpc-node-exporter-server-simple-metrics-v2/
https://grafana.com/grafana/dashboards/9894-node-exporter-0-16-for-prometheus-monitoring-display-board/    super nice 

https://grafana.com/grafana/dashboards/10242-node-exporter-full/   best super exlent
https://grafana.com/grafana/dashboards/11206-traffic-analysis/
https://grafana.com/grafana/dashboards/12619-network-traffic/
https://grafana.com/grafana/dashboards/17229-hardware-sentry-site/
https://grafana.com/grafana/dashboards/9969-zookeeper-dashboard/
https://grafana.com/grafana/dashboards/6293-traefik/
https://grafana.com/grafana/dashboards/4475-traefik/
https://grafana.com/grafana/dashboards/8396-libvirt-kvm/
https://grafana.com/grafana/dashboards/15682-libvirt/
=={}{{}{}{}  }  ONLY  LINUX 
https://grafana.com/grafana/dashboards/13052-node-stats-and-alerts/
https://grafana.com/grafana/dashboards/12542-power-information/
https://grafana.com/grafana/dashboards/14513-linux-exporter-node/       === BEST OK 
https://grafana.com/grafana/dashboards/14731-1-linux-stats-with-node-exporter/      ==== BEST SUPER 
https://grafana.com/grafana/dashboards/10301-grafana/
https://grafana.com/grafana/dashboards/13021-system-metrics-for-the-linux-hosts/
https://grafana.com/grafana/dashboards/2747-linux-memory/                           == BEST
https://grafana.com/grafana/dashboards/6490-linux-node-exporter/
https://grafana.com/grafana/dashboards/8378-system-processes-metrics/
https://grafana.com/grafana/dashboards/5984-alerts-linux-nodes/

=============================================================#####################################3
grafana in kubenrets  
https://www.youtube.com/watch?v=HgjTUiU0Ihk      --  aws

The Istio service mesh
https://istio.io/latest/docs/setup/getting-started/


apiVersion: apps/v1
kind: Deployment
metadata: 
  name: kubectl-istio-deployment
spec:
  replicas: 1
  selector:
     matchLabels: 
       chapter: istio
       topic: istio-installation
  template:
     metadata:
         name: kubectl-istio-pod
         labels: 
           chapeter: istio
           topic: istio-installation
     spec: 
       containers: 
       - name: kubectl-container
         image: deepcloud2208/kubectlcontainer            
"---------=======================================================================
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: app-deployment
spec: 
  replicas: 1
  selector:
    matchLabels: 
      chapter: istio
      topic: traffic-management
      app: app
      version: v2
  template:
    metadata: 
      name: app-pod
      labels:
        chapter: istio
        topic: traffic-management
        app: app
        version: v2
    spec: 
       containers: 
       - name: app-container
         image: deepcloud2208/app:v2
         ports:
         - containerPort: 80
---
apiVersion: v1
kind: Service
metadata: 
  name: app-svc
  labels: 
     chapter: istio
     topic: trafic-management
     app: app
     version: v2
spec: 
   type: ClusterIp
   ports: 
   - targetPort: 80
     port: 8080
   selector: 
      chapter: istio
      topic: traffic-management
      app: app
      version: v2
---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: app2-deployment
spec:
  replicas: 1
  selector: 
     matchLabels:
        chapter: istio
        topic: traffic-management
        app: app2
        version: v2
  template:
    metadata:
       name: app2-pod
       labels:
          chapter: istio
          topic: traffic-management
          app: app2
          version: v2
    spec: 
      containers: 
      - name: app2-container
        image:   deepcloud2208/app:v2
        ports: 
        - containerPort: 80
---
apiVersion: 
kind: Service
metadata:    
  name: app2-svc
  labels: 
    chapter: istio
    topic: traffic-management
    app: app2
    version: v2
spec:
  type: ClusterIp
  ports: 
  - targetPort: 80
    port: 8080
    selector:
       chapter: istio
       topic: traffic-management
       app: app2
       version: v2
========+++++++++++++++++++++++++++================================================="
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata: 
  name: istio-virtual-svc
spec: 
  hosts: 
  - app-svc.prod.svc.cluster.local
  http: 
  - name: virtual-service-routes
    route:
    - destination: 
         host: app-svc.prod.svc.cluster.local
         subset: app-service-subset
      weight: 80
    - destination:
         host: app2-svc.prod.svc.cluster.local
         subset: app2-service-subset
      weight: 20
---
apiVersion: networking.istio.io/v1beta1        
kind: DestinationRule
metadata: 
  name: istio-destination-rule1
spec: 
  host: app-svc.prod.svc.cluster.local
  subsets:
  - name: app-service-subset
    labels: 
      app: app
      version: v2
      chapter: istio
      topic: traffic-management
    trafficPolicy:
      loadBalancer:
        simple: ROUND_ROBIN
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata: 
  name: istio-destination-rule1
    - name: app2-service-subset
      labels: 
         app: app2
         version: v2
         chapter: istio
         topic: traffic-management
      trafficPolicy:
        loadBalancer
          simple: ROUND_ROBIN
+++++++++++++++++++=+++++++++++++++++======================++++++++++++++"
while true
do
curl http:app:svc.prod.local:8080
sleep 1 
done
++++++++++===================================================
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: books-depl
spec: 
  replicas: 1
  selector: 
     matchLabels: 
        chapter: istio
        topic: traffic-management
        app: books  
        version: v1
  template: 
    metadata: 
       name: books-pod
       labels: 
          chapter: istio
          topic: traffic-management
          app: books
          version: v1
    spec: 
      containers:
      - name: books-cont
        image: deepcloud2208/books:v1
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata: 
  name: books-svc
  labels:
    chapter: istio
    topic: traffic-management
    app: books
    version: v1
spec: 
  type: ClusterIp
  ports: 
  - targetPort: 80
    port: 8000
  selector: 
    chapter: istio
    topic: traffic-management
    app: books
    version: v1

---
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: electronics-depl
spec:
  replicas: 1
  selector:
    matchLabels:
      chapter: istio
      topic: traffic-management
      app: electronics
      version: v1
  template: 
    metadata:
      name: electronics-pod
      labels:
        chapter: istio
        topic: trafficmanagement
        app: electronics
        version: v1
    spec: 
      containers:
      - name: electronics-cont
        image: deeplcoud2208/electronics:v1
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata: 
   name: electronics-svc
   labels: 
      chapter: istio
      toopic: traffic-management
      app: electronics
      version: v1
spec: 
  type: ClusterIP
  ports: 
  - targetPort: 80
    port: 8000
  selector: 
    chapter: istio
    topic: traffic-management
    app: electronics
    version: v1
---
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: retail-depl
spec: 
   replicas: 1
   selector: 
     matchLabels: 
       chapter: istio
       topic: traffic-management
       app: retail
       version: v1
   template: 
      metadata: 
         name: retail-pod
         labels: 
            chapter: istio
            topic: traffic-management
            app: retail
            version: v1
      spec: 
        containers: 
        - name: retail-cont
          image: deepcloud2208/retail:v1
          ports: 
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata: 
  name: retail-svc
  labels: 
     chapter: istio
     topic: traffic-management
     app: retail
     version: v1
spec: 
  type: ClusterIP
  ports: 
  - targetPort: 80
    port: 8000
  selector: 
     chapter: istio
     topic: traffic-management
     app: retail
     version: v1
###############################3++++++++++++++++++++++++==================" 
apiVersion: networking.istio.io/v1beta1
kind: VirtuvalService
metadata: 
  name: retail-portal-vs
spec: 
  hosts: 
  - retail-svc.prod.svc.cluster.local
  http: 
  - name: books-routes
    match:
    - uri:
       prefix: "/books"
    rewrite:
       uri: "/"           ## optional  
    route: 
    - destination:
        host: books-svc.prod.svc.cluster.local
        subset: books-service-subset

  - name: electronics-routes
    match: 
    - uri: 
       prefix: "/electronics"
    rewrite:
       uri: "/"           ## optional  
    route:
    - destination:
         host: electronics-svc.prod.svc.cluster.local
         subset: electronics-service-subset
  - name: retail-routes
    route: 
    - destination: 
         host: retail-svc.prod.svc.cluster.local
         subset: retail-service-subset
  - name: retail-routes
    route:
    - destination:
          host: retail-svc.prod.svc.cluster.local
          subset: retails-service-subset
---
apiVersion: networking.istio.io/v1beta1  
kind: DestinationRule
metadata: 
   name: books-destination-rule
spec: 
  host: books-svc.prod.svc.cluster.local
  subsets:
  - name: books-service-subset
    labels: 
      app: books
      version: v1
      chapter: istio
      topic: traffic-management
    trafficPolicy:
       loadBalancer: 
          simple: ROUND_ROBIN
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata: 
  name: electronics-destination-rule
spec: 
  host: electronics-svc.prod.svc.cluster.local
  subsets:
  -  name: electronics-service-subset
     labels: 
       app: electronics
       version: v1
       chapter: istio
       topic: traffic-management
     trafficPolicy:
       loadBlancer:
         simple: ROUND_ROBIN
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: retail-destination-rule
spec: 
  host: retail-svc.prod.svc.cluster.local
  subsets:
  - name: retail-service-subset
    labels:
      app: retail
      version: v1
      chapter: istio
      topic: traffic-management
    trafficPolicy
       loadBalancer:
         simple: ROUND_ROBIN
#################################################++++++++++++++++++++++++
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: ab-depl1              
spec: 
  replicas: 1
  selector: 
      matchLabels:
         chapter: istio
         topic: traffic-management
         app: ab
         version: v1
  template:
    metadata: 
       name: ab-pod
       labels:
         chapter: istio
         topic: traffic-management
         app: ab
         version: v1
    spec: 
      containers:
      - name: ab-cont
        image: deepcloud2208/ab:v1
        ports: 
        - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: ab-depl2
spec: 
  replicas: 1
  selector: 
      matchLabels:
         chapter: istio
         topic: traffic-management
         app: ab
         version: v2
  template:
    metadata: 
       name: ab-pod
       labels:
          chapter: istio
          topic: traffic-management
          app: ab
          verasion: v2
    spec: 
      containers: 
      - name: ab-cont
        image: deepcloud2208/ab:v2
        ports: 
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata: 
   name: ab-svc
   labels: 
      chapter: istio
      topic: traffic-management
      app: ab
spec: 
  type: ClusterIP
  ports: 
  - targetPort: 80
    port: 8000
  selector:
     chapter: istio
     topic: traffic-management
     app: ab
##########################################+++++++++++++++++++++++++++
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata: 
  name: istio-ingress-gateway
spec: 
  selector: 
    app: istio-ingressgateway
    istio: ingressgateway


  servers: 
  - port:
      number: 80
      name: app
      ptotocol: HTTP
    hosts:
    - "anji.com"
    - "app-svc.prod.svc.cluster.local"

=======================================================
https://istio.io/latest/docs/reference/config/networking/service-entry/
Service Entry
apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata:
  name: external-svc-https
spec:
  hosts:
  - api.dropboxapi.com
  - www.googleapis.com
  - api.facebook.com
  location: MESH_EXTERNAL
  ports:
  - number: 443
    name: https
    protocol: TLS
  resolution: DNS
https://github.com/kubernetes/kubectl/issues/837
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/
kubectl get configmap istio -n anjireddy -o yaml | sed 's/mode: ALLOW_ANY/mode: REGISTRY_ONLY/g' | kubectl replace -n anjireddy -f -


apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata: 
  name: webpage-on-ec2
spec:
  hosts: 
  - ec2-54.87.21.45.compute-1.amazonaws.com
  ports: 
  - number: 80
    name: http
    protocol: HTTP
  - number: 443
    name: https
    protocol: HTTPS
  resolution: DNS
  location: MESH_EXTERNAL
---------------------------===================
kubectl get se -n prod

apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata: 
  name: kiali-ingress-gateway
  namespace: istio-system
spec: 
  selector: 
    app: istio-ingressgateway
    istio: ingressgateway
  servers:
  - port:
      number: 15029
      name: http-kiali
      protocol: HTTP
    hosts:
    - "*"      
===============+++++++++++++++++++++
apiVersion: networking.istio.io/v1beta1
kind: VirtuvalService
metadata: 
  name: kiali-vurtual-service
  namespace: istio-system
spec: 
  hosts:
  - "*"
  gateways:
  - kiali-ingress-gateway    
  http:
  - match:
    - port: 15029
    route:
    - destination:
      host: kiali.istio-system.svc.cluster.local
      port:
        number: 20001
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata: 
  name: kiali-destination-rule
  namespace: istio-system
spec:
  host: kiali.istio-system.svc.cluster.local
  trafficPolicy:
    tls:
      mode: DISABLE
==================================++++++++++++++++++###########3
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata: 
  name: prometheus-gateway
  namespace: istio-system
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 15030
      name: http-prom
      protocol: HTTP
    hosts:
    - "*"
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata: 
  name: prometheous-vs
  namespace: istio-system
  spec: 
    hosts: 
    - "*"
    gateway:
    - prometheus-gateway
    http:
    - match:
      - port: 15030
      route:
      - destination:
          host: prometheus
          port:
            number: 9090
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata: 
  name: prometheus
  namespace: istio-system
spec: 
 host: prometheus
 trafficPolicy:
   tls: 
     mode: DISABLE
###############################################################################3
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata: 
   name: grafana-gateway
   namespace: istio-system
spec: 
  selector: 
    istio: ingressgateway
  servers: 
  - port: 
      number: 15031
      name: http-grafana
      protocol: HTTP
    hosts:
    - "*"                
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata: 
  name: grafana-vs
  namespace: istio-system
spec: 
  hosts: 
  - "*"
  gateways:
  - grafana-gateway
  http:
  - match:
    - port: 15031
    route: 
    - destination: 
         host: grafana
         port: 
           number: 3000
---
apiVersion: networking.istio.io/v1alpha3
kind: Destination
metadata: 
  name: grafana
  namespace: istio-system
spec: 
  host: grafana
  trafficPolicy:
     tls:
       mode: DISABLE
############################################################################33
kubectl get gw -n  anjireddy   11.15 
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: app1
spec: 
  replicas: 1
  selector: 
    matchLabels: 
       chapter: istio
       topic: kiali
       app: app1
       version: v1
  template: 
     metadata: 
       name: app1-pod
       labels: 
          chapter: istio
          topic: kiali
          app: app1
          version: v1
     spec: 
       containers: 
       - name: app1-container
         image: deepcloud2208/app1:v1
         ports: 
         - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: app2
spec: 
  replicas: 1
  selector: 
    matchLabels: 
       chapter: istio
       topic: kiali
       app: app2
       version: v1
  template: 
     metadata: 
       name: app1-pod
       labels: 
          chapter: istio
          topic: kiali
          app: app2
          version: v1
     spec: 
       containers: 
       - name: app1-container
         image: deepcloud2208/app2:v1
         ports: 
         - containerPort: 80         
---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: app3
spec: 
  replicas: 1
  selector: 
    matchLabels: 
       chapter: istio
       topic: kiali
       app: app3
       version: v1
  template: 
     metadata: 
       name: app1-pod
       labels: 
          chapter: istio
          topic: kiali
          app: app3
          version: v1
     spec: 
       containers: 
       - name: app1-container
         image: deepcloud2208/app1:v1
         ports: 
         - containerPort: 80                 
---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: app4
spec: 
  replicas: 1
  selector: 
    matchLabels: 
       chapter: istio
       topic: kiali
       app: app4
       version: v1
  template: 
     metadata: 
       name: app1-pod
       labels: 
          chapter: istio
          topic: kiali
          app: app4
          version: v1
     spec: 
       containers: 
       - name: app1-container
         image: deepcloud2208/app1:v1
         ports: 
         - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: app5
spec: 
  replicas: 1
  selector: 
    matchLabels: 
       chapter: istio
       topic: kiali
       app: app5
       version: v1
  template: 
     metadata: 
       name: app1-pod
       labels: 
          chapter: istio
          topic: kiali
          app: app5
          version: v1
     spec: 
       containers: 
       - name: app1-container
         image: deepcloud2208/app1:v1
         ports: 
         - containerPort: 80
---


apiVersion: v1
kind: Service
metadata: 
  name: app5
  labels: 
     chapter: istio
     topic: kiali
     app: app5
     version: v1
spec: 
  type: ClusterIP
  ports: 
  - targetPort: 80
    port: 8080
  selector: 
     chapter: istio
     topic: kiali
     app: app5
     version: v1
---
apiVersion: v1
kind: Service
metadata: 
  name: app4
  labels: 
     chapter: istio
     topic: kiali
     app: app4
     version: v1
spec: 
  type: ClusterIP
  ports: 
  - targetPort: 80
    port: 8080
  selector: 
     chapter: istio
     topic: kiali
     app: app4
     version: v1
---
apiVersion: v1
kind: Service
metadata: 
  name: app3
  labels: 
     chapter: istio
     topic: kiali
     app: app3
     version: v1
spec: 
  type: ClusterIP
  ports: 
  - targetPort: 80
    port: 8080
  selector: 
     chapter: istio
     topic: kiali
     app: app3
     version: v1
---
apiVersion: v1
kind: Service
metadata: 
  name: app2
  labels: 
     chapter: istio
     topic: kiali
     app: app2
     version: v1
spec: 
  type: ClusterIP
  ports: 
  - targetPort: 80
    port: 8080
  selector: 
     chapter: istio
     topic: kiali
     app: app2
     version: v1
---
apiVersion: v1
kind: Service
metadata: 
  name: app1
  labels: 
     chapter: istio
     topic: kiali
     app: app1
     version: v1
spec: 
  type: ClusterIP
  ports: 
  - targetPort: 80
    port: 8080
  selector:       ###  11.15  videos 
     chapter: istio
     topic: kiali
     app: app1
     version: v1
####################################################################################################333
apiVersion: networking.istio.io/v1beta1      ## 11.16 video VSDR.YAML
kind: VirtuvalService
metadata: 
  name: app1
spec:                                                   
  hosts: 
  - app1.prod.svc.cluster.local
  http: 
  - name: app1-service-routes
    route:
    - destination:
        host: app1.prod.svc.cluster.local
        subset: app1-service-subset
      weight: 90
    - destination: 
        host: app2.prod.svc.cluster.local    
        subset: app2-service-subset
      weight: 10
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata: 
   name: app1-destination-rule
spec: 
  host: app1.prod.svc.cluster.local
  subsets:
  - name: app1-service-subset
    labels:
       app: app1
       version: v1
       chapter: istio
       topic: kiali
    trafficPolicy:
       loadBalancer: 
         simple: ROUND_ROBIN
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata: 
   name: app2-destination-rule
spec: 
  host: app2.prod.svc.cluster.local
  subsets:
  - name: app2-service-subset
    labels:
       app: app1
       version: v1
       chapter: istio
       topic: kiali
    trafficPolicy:
       loadBalancer: 
         simple: ROUND_ROBIN                  
---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata: 
  name: app3
spec: 
  hosts: 
  - app3.prod.svc.cluster.local
  http: 

  - name: app3-routes
    match: 
    - uri: 
       prefix: "/app3"
    rewrite:
      uri: "/"
    route:
    - destination: 
         host: app3.prod.svc.cluster.local
         subset: app3-service-subset
  - name: app4-routes
    match: 
    - uri: 
       prefix: "/app4"
    rewrite: 
      uri: "/"
    route:
    - destination:
         host: app4.prod.svc.cluster.local
         subset: app4-service-subset
    - name: app5-routes
      match: 
      - uri:
         prefix: "/app5"
      rewrite: 
      - destination: 
           host: app5.prod.svc.cluster.local
           subset: app5-service-subset
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata: 
   name: app3-destination-rule
spec: 
  host: app3.prod.svc.cluster.local
  subsets: 
  - name: app3-service-subset
    labels: 
      app: app3
      version: v1
      chapter: istio
      topic: kiali
    trafficPolicy:
      loadBalancer:
         simple: ROUND_ROBIN
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata: 
   name: app4-destination-rule
spec: 
   host: app4.prod.svc.cluster.local
   subsets:
   - name: app4-service-subset
     labels: 
       app: app4
       version: v1
       chapter: istio
       topic: kiali
     trafficPolicy: 
       loadBalancer: 
         simple: ROUND_ROBIN
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata: 
   name: app5-destination-rule
spec: 
   host: app5.prod.svc.cluster.local
   subsets: 
   - labels: 
       app: app5
       version: v1
       chapter: istio
       topic: kiali
     trafficPolicy: 
       loadBalancer: 
          simple: ROUND_ROBIN
#######################################################################################################3
11.17 appgateway
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata: 
   name: allapp-ingress-gateway
spec: 
  selector: 
    app: istio-ingressgateway
    istio:  ingressgateway
  servers: 
  - port: 
      number: 80
      name: app
      protocol: HTTP
    hosts: 
    - "deep.com"
    - "app.com"    
===============================================================
gateway virtualService
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata: 
   name: allapp-virtual-svc-for-ingress-gtw
spec: 
  hosts: 
  - "deep.com"
  - "app.com"
  gateway: 
  - allapp-ingress-gateway
  http:
  - name: allapp-service-routes
    route: 
    - destination: 
         host: app1.prod.svc.cluster.local
         subset: app-service-subset
      weight: 0
    - destination: 
        host: app2.prod.svc.cluster.local
        subset: app2-service-subset
      weight: 100
---
apiVersion: network.istio.io/vbeta1
kind: DestinationRule
metadata: 
   name: ingress-gtw-destination-rule1
spec: 
  host: app1.prod.svc.cluster.local
  subsets:
  - name: app1-service-subset
    labels: 
       app: app1
       version: v1
       chapter: istio
       topic: kiali
    trafficPolicy: 
       loadBalancer: 
         simple: ROUND_ROBIN
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata: 
   name: ingress-gtw-destination-rule2
spec: 
   host: app2.prod.svc.cluster.local
   subsets: 
   - name: app2-service-subset
     labels: 
       app: app2
       version: v1
       chapter: istio
       topic: kiali
     trafficPolicy:
        loadBalancer: 
           simple: ROUND_ROBIN
######################################################################################################3
11.6 - video,    11.19 SERVICEENTRY
apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata: 
   name: 
spec: 
  hosts:
  - "*.compute-1.amazoneaws.com"
  - www.amazon.com
  - www.anji.com
  ports: 
  - number: 80
    name: http
    protocol: HTTP
  - number: 443
    name: https
    protocol: HTTPS
  resolution: NONE
  location: MESH_EXTERNAL
#########################################################################################################################3

centos  9 
sudo dnf install java-11-openjdk-devel

https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-centos-7
curl -fsSL https://get.docker.com/ | sh
sudo systemctl start docker
sudo systemctl enable docker
[root@localhost ~]# docker --version

==========================================================================
kubeadm install in centos = 9
https://upcloud.com/resources/tutorials/install-kubernetes-cluster-centos-8
https://tayeh.me/posts/install-kubernetes-cluster-on-centos-8-with-kubeadm-crio/

[root@localhost ~]# nano /etc/fstab 

[root@localhost ~]# setenforce 0
[root@localhost ~]# nano /etc/selinux/config    ### SELINUX=disabled     or  
[root@localhost ~]# sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux


[root@localhost ~]# modprobe overlay
[root@localhost ~]# modprobe br_netfilter
tee /etc/sysctl.d/k8s.conf<<EOF
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF

[root@localhost ~]# sysctl --system



[root@localhost ~]# firewall-cmd --reload

curl -fsSL https://get.docker.com/ | sh
sudo systemctl start docker
sudo systemctl enable docker
systemctl restart docker
[root@localhost ~]# docker --version


[root@localhost ~]# echo '{
  "exec-opts": ["native.cgroupdriver=systemd"]
}' > /etc/docker/daemon.json

tee /etc/yum.repos.d/kubernetes.repo<<EOF
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF


[root@localhost ~]# yum -y install kubelet kubeadm kubectl --disableexcludes=kubernetes epel-release
[root@localhost ~]# dnf  -y install kubelet kubeadm kubectl --disableexcludes=kubernetes epel-release     latest
sudo yum-mark hold kubeadm kubelet kubectl

[root@localhost ~]# kubectl version --client
[root@localhost ~]# systemctl enable kubelet
[root@localhost ~]# systemctl start kubelet

[root@localhost ~]# firewall-cmd --zone=public --permanent --add-port={6443,2379,2380,10250,10251,10252}/tcp

firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=worker-IP-address/32 accept' 
firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=172.17.0.0/16 accept'
[root@localhost ~]# firewall-cmd --reload
[root@master kubernetes]# systemctl restart containerd 

[aws@master ~]$ sudo kubeadm init --pod-network-cidr 10.244.0.0/16
[init] Using Kubernetes version: v1.26.0
[preflight] Running pre-flight checks
	[WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
	[WARNING Hostname]: hostname "master" could not be reached
	[WARNING Hostname]: hostname "master": lookup master on 192.168.122.1:53: server misbehaving
[preflight] Pulling images required for setting up a Kubernetes cluster
     ###  OR  OR  OR 
 kubeadm init --ignore-preflight-errors all

###  OR   OR OR 
 
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.122.5:6443 --token fsn8g9.045q38upvmkriqem \
	--discovery-token-ca-cert-hash sha256:57d3ba8ea0d375e7b7957203c239c041901ce75fa51ef15a3ac34349c75672ea 
[root@master ~]# 




 kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
  kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml


================================================================================================

How to install helm?
https://helm.sh/docs/intro/install/
https://get.helm.sh/helm-v3.10.3-linux-amd64.tar.gz

Download your desired version
Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz)
(mv linux-amd64/helm /usr/local/bin/helm)

===============================================================
rancher install in DOCKER 

anji@master:~$ sudo docker run --privileged -d --restart=unless-stopped -p 800:80 -p 443:443 rancher/rancher
723ab7b978600b308299c4acfb8c90d5380faab56278bf7bea26acc03edc1aae

anji@master:~$ sudo docker images
REPOSITORY        TAG       IMAGE ID       CREATED       SIZE
rancher/rancher   latest    b8d45566fae9   7 weeks ago   1.56GB

anji@master:~$ sudo docker ps
CONTAINER ID   IMAGE             COMMAND           CREATED          STATUS          PORTS                                                                        NAMES
723ab7b97860   rancher/rancher   "entrypoint.sh"   28 seconds ago   Up 26 seconds   0.0.0.0:443->443/tcp, :::443->443/tcp, 0.0.0.0:800->80/tcp, :::800->80/tcp   wizardly_northcutt

docker logs   723ab7b97860  2>&1 | grep "Bootstrap Password:"

root@master:~# docker logs   723ab7b97860  2>&1 | grep "Bootstrap Password:"
2023/01/10 11:13:47 [INFO] Bootstrap Password: 95vwrsldjpvb84d464ncpp9nd59kk2ss254fpdmw58b2qkjtms9rgl

https://192.168.122.244/dashboard/

+++=========////////\\\\\\\\\\\\\\\\\

rancher installed in ubuntu  kubenretes cluster

https://www.devopsschool.com/blog/rancher-installation-deployment-in-kubernetes-clustor/
https://ranchermanager.docs.rancher.com/v2.5/pages-for-subheaders/install-upgrade-on-a-kubernetes-cluster

Add the Helm chart repository
Create a namespace for Rancher
Choose your SSL configuration
Install cert-manager (unless you are bringing your own certificates, or TLS will be terminated on a load balancer)
Install Rancher with Helm and your chosen certificate option
Verify that the Rancher server is successfully deployed
Save your options

anji@master:~$ helm repo add rancher-latest https://releases.rancher.com/server-charts/latest

anji@master:~$ helm repo add rancher-stable https://releases.rancher.com/server-charts/stable

anji@master:~$ helm repo add rancher-stable https://releases.rancher.com/server-charts/stable

anji@master:~$ kubectl create namespace rancher
namespace/rancher created
anji@master:~$ kubectl get ns
NAME              STATUS   AGE
default           Active   3d
kube-node-lease   Active   3d
kube-public       Active   3d
kube-system       Active   3d
"rancher   "        Active   9s

anji@master:~$ kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.5.1/cert-manager.crds.yaml

anji@master:~$ helm repo add jetstack https://charts.jetstack.io

anji@master:~$ helm repo update

anji@master:~$ helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.5.1


NAME:" cert-manager"
LAST DEPLOYED: Tue Jan 10 12:13:16 2023
NAMESPACE: cert-manager
STATUS: "deployed"
REVISION: 1
TEST SUITE: None
NOTES:
cert-manager v1.5.1 has been deployed successfully!
In order to begin issuing certificates, you will need to set up a ClusterIssuer
or Issuer resource (for example, by creating a 'letsencrypt-staging' issuer).
More information on the different types of issuers and how to configure them
can be found in our documentation:
https://cert-manager.io/docs/configuration/
For information on how to configure cert-manager to automatically provision
Certificates for Ingress resources, take a look at the `ingress-shim`
documentation:
https://cert-manager.io/docs/usage/ingress/

anji@master:~$  kubectl get pods --namespace cert-manager
NAME                                       READY   STATUS    RESTARTS   AGE
cert-manager-5879b6cc6b-mwc8f              1/1     Running   0          62s
cert-manager-cainjector-6f875446dc-sd2km   1/1     Running   0          62s
cert-manager-webhook-65745fbb58-jbn4m      1/1     Running   0          62s

anji@master:~$ kubectl get pods --all-namespaces  --show-labels  -o wide
NAMESPACE      NAME                                       READY   STATUS    RESTARTS      AGE     IP                NODE      NOMINATED NODE   READINESS GATES   LABELS
"cert-manager   cert-manager-5879b6cc6b-mwc8f              1/1     Running   0             6m42s   10.44.0.2         worker1   <none>           <none>            app.kubernetes.io/component=controller,app.kubernetes.io/instance=cert-manager,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=cert-manager,app.kubernetes.io/version=v1.5.1,app=cert-manager,helm.sh/chart=cert-manager-v1.5.1,pod-template-hash=5879b6cc6b
cert-manager   cert-manager-cainjector-6f875446dc-sd2km   1/1     Running   0             6m42s   10.44.0.1         worker1   <none>           <none>            app.kubernetes.io/component=cainjector,app.kubernetes.io/instance=cert-manager,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=cainjector,app.kubernetes.io/version=v1.5.1,app=cainjector,helm.sh/chart=cert-manager-v1.5.1,pod-template-hash=6f875446dc
cert-manager   cert-manager-webhook-65745fbb58-jbn4m"      1/1     Running   0             6m42s   10.44.0.3         worker1   <none>           <none>            app.kubernetes.io/component=webhook,app.kubernetes.io/instance=cert-manager,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=webhook,app.kubernetes.io/version=v1.5.1,app=webhook,helm.sh/chart=cert-manager-v1.5.1,pod-template-hash=65745fbb58
kube-system    coredns-565d847f94-fsdjv                   1/1     Running   0             20m     10.32.0.2         master    <none>           <none>            k8s-app=kube-dns,pod-template-hash=565d847f94
kube-system    coredns-565d847f94-nczt7                   1/1     Running   0             20m     10.32.0.3         master    <none>           <none>            k8s-app=kube-dns,pod-template-hash=565d847f94
kube-system    etcd-master                                1/1     Running   0             21m     192.168.122.151   master    <none>           <none>            component=etcd,tier=control-plane
kube-system    kube-apiserver-master                      1/1     Running   0             21m     192.168.122.151   master    <none>           <none>            component=kube-apiserver,tier=control-plane
kube-system    kube-controller-manager-master             1/1     Running   0             21m     192.168.122.151   master    <none>           <none>            component=kube-controller-manager,tier=control-plane
kube-system    kube-proxy-gs8d4                           1/1     Running   0             20m     192.168.122.151   master    <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-system    kube-proxy-wkm2r                           1/1     Running   0             15m     192.168.122.212   worker1   <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-system    kube-scheduler-master                      1/1     Running   0             21m     192.168.122.151   master    <none>           <none>            component=kube-scheduler,tier=control-plane
kube-system    weave-net-57s4z                            2/2     Running   1 (14m ago)   15m     192.168.122.212   worker1   <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
kube-system    weave-net-sc862                            2/2     Running   1 (14m ago)   15m     192.168.122.151   master    <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
-------=======

helm install rancher1 rancher \
  --namespace cattle-system \
  --set hostname=rancher.my.org \
  --set replicas=3
NAME: rancher1
LAST DEPLOYED: Tue Jan 10 13:10:00 2023
NAMESPACE: cattle-system
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace cattle-system -l "app.kubernetes.io/name=rancher,app.kubernetes.io/instance=rancher1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace cattle-system $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace cattle-system port-forward $POD_NAME 8080:$CONTAINER_PORT

anji@master:~$ helm list -a  -n rancher
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART        	APP VERSION
rancher1	rancher  	1       	2023-01-10 12:33:30.367563213 +0530 IST	deployed	rancher-0.1.0	1.16.0     

anji@master:~$ kubectl get  all  --all-namespaces
NAMESPACE       NAME                                           READY   STATUS    RESTARTS      AGE
cattle-system   pod/rancher1-b5bf85f65-6cxq8                   1/1     Running   0             83s
cert-manager    pod/cert-manager-5879b6cc6b-6g2xr              1/1     Running   0             4m23s
cert-manager    pod/cert-manager-cainjector-6f875446dc-ztt4b   1/1     Running   0             4m23s
cert-manager    pod/cert-manager-webhook-65745fbb58-q5hdt      1/1     Running   0             4m23s
kube-system     pod/coredns-565d847f94-fsdjv                   1/1     Running   0             72m
kube-system     pod/coredns-565d847f94-nczt7                   1/1     Running   0             72m
kube-system     pod/etcd-master                                1/1     Running   0             72m
kube-system     pod/kube-apiserver-master                      1/1     Running   0             72m
kube-system     pod/kube-controller-manager-master             1/1     Running   0             72m
kube-system     pod/kube-proxy-gs8d4                           1/1     Running   0             72m
kube-system     pod/kube-proxy-wkm2r                           1/1     Running   0             66m
kube-system     pod/kube-scheduler-master                      1/1     Running   0             72m
kube-system     pod/weave-net-57s4z                            2/2     Running   1 (66m ago)   66m
kube-system     pod/weave-net-sc862                            2/2     Running   1 (66m ago)   66m

NAMESPACE       NAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
cattle-system   service/rancher1               ClusterIP   10.101.146.39   <none>        80/TCP                   83s
cert-manager    service/cert-manager           ClusterIP   10.99.246.26    <none>        9402/TCP                 4m23s
cert-manager    service/cert-manager-webhook   ClusterIP   10.100.41.108   <none>        443/TCP                  4m23s
default         service/kubernetes             ClusterIP   10.96.0.1       <none>        443/TCP                  72m
kube-system     service/kube-dns               ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP,9153/TCP   72m

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system   daemonset.apps/kube-proxy   2         2         2       2            2           kubernetes.io/os=linux   72m
kube-system   daemonset.apps/weave-net    2         2         2       2            2           <none>                   66m

NAMESPACE       NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
cattle-system   deployment.apps/rancher1                  1/1     1            1           83s
cert-manager    deployment.apps/cert-manager              1/1     1            1           4m23s
cert-manager    deployment.apps/cert-manager-cainjector   1/1     1            1           4m23s
cert-manager    deployment.apps/cert-manager-webhook      1/1     1            1           4m23s
kube-system     deployment.apps/coredns                   2/2     2            2           72m

NAMESPACE       NAME                                                 DESIRED   CURRENT   READY   AGE
cattle-system   replicaset.apps/rancher1-b5bf85f65                   1         1         1       83s
cert-manager    replicaset.apps/cert-manager-5879b6cc6b              1         1         1       4m23s
cert-manager    replicaset.apps/cert-manager-cainjector-6f875446dc   1         1         1       4m23s
cert-manager    replicaset.apps/cert-manager-webhook-65745fbb58      1         1         1       4m23s
kube-system     replicaset.apps/coredns-565d847f94                   2         2         2       72m "

====
anji@master:~$ kubectl get pod,deployments  -n cert-manager
NAME                                           READY   STATUS    RESTARTS   AGE
pod/cert-manager-5879b6cc6b-6g2xr              1/1     Running   0          9m25s
pod/cert-manager-cainjector-6f875446dc-ztt4b   1/1     Running   0          9m25s
pod/cert-manager-webhook-65745fbb58-q5hdt      1/1     Running   0          9m25s

NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cert-manager              1/1     1            1           9m25s
deployment.apps/cert-manager-cainjector   1/1     1            1           9m25s
deployment.apps/cert-manager-webhook      1/1     1            1           9m25s



anji@master:~$ kubectl -n cattle-system rollout status deploy/rancher1
deployment "rancher1" successfully rolled out

nji@master:~$ kubectl -n cattle-system rollout status deploy/rancher1
deployment "rancher1" successfully rolled out

anji@master:~$ kubectl -n cattle-system get deploy rancher1
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
rancher1   1/1     1            1           14m "
======++++++++++++++++++
anji@master:~$ kubectl get svc -n cattle-system
NAME       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
rancher1   "ClusterIP "  10.101.146.39   <none>        80/TCP    45m

anji@master:~$ kubectl edit svc rancher1  -n cattle-system
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2023-01-10T07:40:01Z"
  labels:
spec:
  clusterIP: 10.101.146.39
  clusterIPs:
  - 10.101.146.39
======OR   OR  

anji@master:~$ kubectl patch svc rancher1 -p  '{"spec":  {"type": "NodePort"}}' -n cattle-system
service/rancher1 patched

anji@master:~$ kubectl get svc -n cattle-system
NAME       TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
rancher1   "NodePort "  10.101.146.39   <none>        80:"32642/"TCP   59m  "
--=====
                 ip                          192.168.122.151:32642

anji@master:~$ curl http://192.168.122.151:32642/
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
anji@master:~$ 
==================
https://www.youtube.com/watch?v=LK6KbAlQRIg
https://gist.github.com/rritsoft/a60f3976799aa7170b33758818bf097e

helm repo upgrade

helm upgrade --install \
    rancher rancher/rancher \
    --namespace cattle-system \
    --create-namespace \
    --set hostname=$RANCHER_ADDR \
    --set ingress.tls.source=letsEncrypt \
    --set letsEncrypt.email=$EMAIL \
    --wait
anji@master:~$ helm upgrade --install \
>     rancher rancher/rancher \
>     --namespace cattle-system \
>     --create-namespace \
>     --set hostname=$RANCHER_ADDR \
>     --set ingress.tls.source=letsEncrypt \
>     --set letsEncrypt.email=$EMAIL \
>     --wait
Release "rancher" does not exist. Installing it now.
coalesce.go:223: warning: destination for rancher.ingress.tls is a table. Ignoring non-table value ([])
NAME: rancher
LAST DEPLOYED: Tue Jan 10 16:17:00 2023
NAMESPACE: cattle-system
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace cattle-system -l "app.kubernetes.io/name=rancher,app.kubernetes.io/instance=rancher" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace cattle-system $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace cattle-system port-forward $POD_NAME 8080:$CONTAINER_PORT
================================================================================
How To Install Jenkins on Ubuntu 20.04  
https://www.digitalocean.com/community/tutorials/how-to-install-jenkins-on-ubuntu-20-04

wget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add -

sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list' 
sudo apt update
sudo apt install jenkins

sudo systemctl start jenkins
sudo systemctl status jenkins
sudo ufw allow 8080
sudo ufw allow OpenSSH
sudo ufw enable
sudo ufw status

root@anji:~# cat /var/lib/jenkins/secrets/initialAdminPassword
608116a35ffc4daf8fc77202a899ec5c

================+++++++++++++++++++]][[]]

https://www.digitalocean.com/community/tutorials/how-to-install-jenkins-on-ubuntu-22-04
https://www.digitalocean.com/community/tutorials/how-to-install-jenkins-on-ubuntu-22-04

wget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key |sudo gpg --dearmor -o /usr/share/keyrings/jenkins.gpg

sudo sh -c 'echo deb [signed-by=/usr/share/keyrings/jenkins.gpg] http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list'

sudo apt update

sudo apt install jenkins

sudo systemctl start jenkins.service

sudo systemctl status jenkins

sudo ufw allow 8080

sudo ufw allow OpenSSH
sudo ufw enable

sudo ufw status

sudo cat /var/lib/jenkins/secrets/initialAdminPassword 

========================================================================++++++++++++++++++++++++++++++
Install Docker Engine on Ubuntu  Ubuntu Focal 20.04 (LTS)
https://docs.docker.com/engine/install/ubuntu/

sudo apt-get remove docker docker-engine docker.io containerd runc
sudo apt-get update

sudo apt-get install \
    ca-certificates \
    curl \
    gnupg \
    lsb-release

sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

sudo apt-get update

sudo chmod a+r /etc/apt/keyrings/docker.gpg
sudo apt-get update

sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin
=============++++++++++++++++++++++++++++
anji@anji:~$ docker pull nginx
Using default tag: latest
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post 
"http://%2Fvar%2Frun%2Fdocker.sock/v1.24/images/create?fromImage=nginx&tag=latest": dial unix /var/run/docker.sock: connect: permission denied


visudo  add super user permistions
root    ALL=(ALL:ALL) ALL
anji  ALL=(ALL:ALL)     ALL
     or 
anji   ALL=(ALL)  NOPASSWORD:ALL

root@anji:/var/run# ll | grep -i docker.sock 
srw-rw----  1 root              docker    0 Jan 26 14:32 docker.sock=
root@anji:/var/run# chown -R anji docker.sock 
root@anji:/var/run# ll | grep -i docker.sock 
srw-rw----  1 anji              docker    0 Jan 26 14:32 docker.sock=
root@anji:/var/run# 




=================+++++++++++++++++++++++++++++++++++++++##################################################3
How to Deploy To Kubernetes with Jenkins GitOps GitHub Pipeline
Jenkins Kubernetes Integration - Mithun Technologies - 9980923226
https://www.youtube.com/watch?v=IluhOk86prA&t=3s
https://faun.pub/ci-cd-pipeline-using-jenkins-to-deploy-on-kubernetes-cf2fd5e185b8


pipeline {
agent any
stages{
  stage(" checkout from gitgub") {
    steps{
      checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[url: 'https://github.com/rritsoft/jenkins-with-kuberntes.git']])
      }

  }
      
    stage('Deploy App') {
      steps {
        script {
          kubernetesDeploy(configs: "pod.yaml", kubeconfigId: "k8oldplugin")
        }
      }
    }  



}

}
================="   "mutliple manifest files"
pipeline {
agent any
stages{
  stage(" checkout from gitgub") {
    steps{
      checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[url: 'https://github.com/rritsoft/jenkins-with-kuberntes.git']])
      }

  }
      
    stage('Deploy App') {
      steps {
        script {
          kubernetesDeploy(configs: "pod.yaml,nginx1.yaml",  kubeconfigId: "k8oldplugin")
     
        }
      }
    }  


}

}

==========================++++++++++++++++++++==============================

pipeline {
agent any
stages{
  stage(" checkout from gitgub") {
    steps{
      checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[url: 'https://github.com/rritsoft/jenkins-with-kuberntes.git']])
      }

  }
      
//     stage('Deploy App') {
//       steps {
//         script {
//           kubernetesDeploy(configs: "pod.yaml,nginx1.yaml,dp3.yaml",    kubeconfigId: "k8oldplugin")
           
//         }
//       }
//     }  

  stage( " cli commands through deployment" ) {
    steps{
      sh 'kubectl apply -f dp3.yaml'
    
    }  
  }

}

}
====================+++++++++++++++++++++++++++++++++++++++++============ "
https://www.youtube.com/watch?v=adG0vq5boL8&t=9336s
https://github.com/DeekshithSN/CICD_Java_gradle_application/blob/main/Jenkinsfile

pipeline{
    agent any 
    environment{
        VERSION = "${env.BUILD_ID}"
    }
    stages{
        stage("sonar quality check"){
            agent {
                docker {
                    image 'openjdk:11'
                }
            }
            steps{
                script{
                    withSonarQubeEnv(credentialsId: 'sonar-token') {
                            sh 'chmod +x gradlew'
                            sh './gradlew sonarqube'
                    }

                    timeout(time: 1, unit: 'HOURS') {
                      def qg = waitForQualityGate()
                      if (qg.status != 'OK') {
                           error "Pipeline aborted due to quality gate failure: ${qg.status}"
                      }
                    }

                }  
            }
        }
        stage("docker build & docker push"){
            steps{
                script{
                    withCredentials([string(credentialsId: 'docker_pass', variable: 'docker_password')]) {
                             sh '''
                                docker build -t 34.125.214.226:8083/springapp:${VERSION} .
                                docker login -u admin -p $docker_password 34.125.214.226:8083 
                                docker push  34.125.214.226:8083/springapp:${VERSION}
                                docker rmi 34.125.214.226:8083/springapp:${VERSION}
                            '''
                    }
                }
            }
        }
        stage('indentifying misconfigs using datree in helm charts'){
            steps{
                script{

                    dir('kubernetes/') {
                        withEnv(['DATREE_TOKEN=GJdx2cP2TCDyUY3EhQKgTc']) {
                              sh 'helm datree test myapp/'
                        }
                    }
                }
            }
        }
        stage("pushing the helm charts to nexus"){
            steps{
                script{
                    withCredentials([string(credentialsId: 'docker_pass', variable: 'docker_password')]) {
                          dir('kubernetes/') {
                             sh '''
                                 helmversion=$( helm show chart myapp | grep version | cut -d: -f 2 | tr -d ' ')
                                 tar -czvf  myapp-${helmversion}.tgz myapp/
                                 curl -u admin:$docker_password http://34.125.214.226:8081/repository/helm-hosted/ --upload-file myapp-${helmversion}.tgz -v
                            '''
                          }
                    }
                }
            }
        }

        stage('manual approval'){
            steps{
                script{
                    timeout(10) {
                        mail bcc: '', body: "<br>Project: ${env.JOB_NAME} <br>Build Number: ${env.BUILD_NUMBER} <br> Go to build url and approve the deployment request <br> URL de build: ${env.BUILD_URL}", cc: '', charset: 'UTF-8', from: '', mimeType: 'text/html', replyTo: '', subject: "${currentBuild.result} CI: Project name -> ${env.JOB_NAME}", to: "deekshith.snsep@gmail.com";  
                        input(id: "Deploy Gate", message: "Deploy ${params.project_name}?", ok: 'Deploy')
                    }
                }
            }
        }

        stage('Deploying application on k8s cluster') {
            steps {
               script{
                   withCredentials([kubeconfigFile(credentialsId: 'kubernetes-config', variable: 'KUBECONFIG')]) {
                        dir('kubernetes/') {
                          sh 'helm upgrade --install --set image.repository="34.125.214.226:8083/springapp" --set image.tag="${VERSION}" myjavaapp myapp/ ' 
                        }
                    }
               }
            }
        }

        stage('verifying app deployment'){
            steps{
                script{
                     withCredentials([kubeconfigFile(credentialsId: 'kubernetes-config', variable: 'KUBECONFIG')]) {
                         sh 'kubectl run curl --image=curlimages/curl -i --rm --restart=Never -- curl myjavaapp-myapp:8080'

                     }
                }
            }
        }
    }

    post {
		always {
			mail bcc: '', body: "<br>Project: ${env.JOB_NAME} <br>Build Number: ${env.BUILD_NUMBER} <br> URL de build: ${env.BUILD_URL}", cc: '', charset: 'UTF-8', from: '', mimeType: 'text/html', replyTo: '', subject: "${currentBuild.result} CI: Project name -> ${env.JOB_NAME}", to: "deekshith.snsep@gmail.com";  
		 }
	   }
}
Footer

==================================================
https://www.youtube.com/watch?v=o4QG_kqYvHk&t=1112s
https://github.com/saha-rajdeep/kubernetescode/blob/main/Jenkinsfile

node {
    def app

    stage('Clone repository') {
      

        checkout scm
    }

    stage('Build image') {
  
       app = docker.build("raj80dockerid/test")
    }

    stage('Test image') {
  

        app.inside {
            sh 'echo "Tests passed"'
        }
    }

    stage('Push image') {
        
        docker.withRegistry('https://registry.hub.docker.com', 'dockerhub') {
            app.push("${env.BUILD_NUMBER}")
        }
    }
    
    stage('Trigger ManifestUpdate') {
                echo "triggering updatemanifestjob"
                build job: 'updatemanifest', parameters: [string(name: 'DOCKERTAG', value: env.BUILD_NUMBER)]
        }
}

=====================++++++++++++++++++++++====
https://github.com/ankit630/unixcloudfusion/tree/main/jenkins-dynamic-slaves-kubernetes      ===+  different jobs eecucted  freestyle jobs , one by one
https://www.youtube.com/watch?v=ksz0Eqyttgo     
==========================================================================================================================================================
https://github.com/DeekshithSN/CICD_Java_gradle_application/blob/main/Jenkinsfile
https://www.youtube.com/watch?v=adG0vq5boL8&t=9460s

pipeline{
    agent any 
    environment{
        VERSION = "${env.BUILD_ID}"
    }
    stages{
        stage("sonar quality check"){
            agent {
                docker {
                    image 'openjdk:11'
                }
            }
            steps{
                script{
                    withSonarQubeEnv(credentialsId: 'sonar-token') {
                            sh 'chmod +x gradlew'
                            sh './gradlew sonarqube'
                    }

                    timeout(time: 1, unit: 'HOURS') {
                      def qg = waitForQualityGate()
                      if (qg.status != 'OK') {
                           error "Pipeline aborted due to quality gate failure: ${qg.status}"
                      }
                    }

                }  
            }
        }
        stage("docker build & docker push"){
            steps{
                script{
                    withCredentials([string(credentialsId: 'docker_pass', variable: 'docker_password')]) {
                             sh '''
                                docker build -t 34.125.214.226:8083/springapp:${VERSION} .
                                docker login -u admin -p $docker_password 34.125.214.226:8083 
                                docker push  34.125.214.226:8083/springapp:${VERSION}
                                docker rmi 34.125.214.226:8083/springapp:${VERSION}
                            '''
                    }
                }
            }
        }
        stage('indentifying misconfigs using datree in helm charts'){
            steps{
                script{

                    dir('kubernetes/') {
                        withEnv(['DATREE_TOKEN=GJdx2cP2TCDyUY3EhQKgTc']) {
                              sh 'helm datree test myapp/'
                        }
                    }
                }
            }
        }
        stage("pushing the helm charts to nexus"){
            steps{
                script{
                    withCredentials([string(credentialsId: 'docker_pass', variable: 'docker_password')]) {
                          dir('kubernetes/') {
                             sh '''
                                 helmversion=$( helm show chart myapp | grep version | cut -d: -f 2 | tr -d ' ')
                                 tar -czvf  myapp-${helmversion}.tgz myapp/
                                 curl -u admin:$docker_password http://34.125.214.226:8081/repository/helm-hosted/ --upload-file myapp-${helmversion}.tgz -v
                            '''
                          }
                    }
                }
            }
        }

        stage('manual approval'){
            steps{
                script{
                    timeout(10) {
                        mail bcc: '', body: "<br>Project: ${env.JOB_NAME} <br>Build Number: ${env.BUILD_NUMBER} <br> Go to build url and approve the deployment request <br> URL de build: ${env.BUILD_URL}", cc: '', charset: 'UTF-8', from: '', mimeType: 'text/html', replyTo: '', subject: "${currentBuild.result} CI: Project name -> ${env.JOB_NAME}", to: "deekshith.snsep@gmail.com";  
                        input(id: "Deploy Gate", message: "Deploy ${params.project_name}?", ok: 'Deploy')
                    }
                }
            }
        }

        stage('Deploying application on k8s cluster') {
            steps {
               script{
                   withCredentials([kubeconfigFile(credentialsId: 'kubernetes-config', variable: 'KUBECONFIG')]) {
                        dir('kubernetes/') {
                          sh 'helm upgrade --install --set image.repository="34.125.214.226:8083/springapp" --set image.tag="${VERSION}" myjavaapp myapp/ ' 
                        }
                    }
               }
            }
        }

        stage('verifying app deployment'){
            steps{
                script{
                     withCredentials([kubeconfigFile(credentialsId: 'kubernetes-config', variable: 'KUBECONFIG')]) {
                         sh 'kubectl run curl --image=curlimages/curl -i --rm --restart=Never -- curl myjavaapp-myapp:8080'

                     }
                }
            }
        }
    }

    post {
		always {
			mail bcc: '', body: "<br>Project: ${env.JOB_NAME} <br>Build Number: ${env.BUILD_NUMBER} <br> URL de build: ${env.BUILD_URL}", cc: '', charset: 'UTF-8', from: '', mimeType: 'text/html', replyTo: '', subject: "${currentBuild.result} CI: Project name -> ${env.JOB_NAME}", to: "deekshith.snsep@gmail.com";  
		 }
	   }
}

====================++++++++++++@@@@@@@@@@\\\///////\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
error  error  {}{{}{[]{}{{}{{{}}}}}}{{{{{{{{{}{{{{{{{{{{{{}{{{{[]{}|}{}}}}}}}}}}}}}}}}}}}}}}}

root@master:~# kubectl exec -it nginx --  /bin/bash
Error from server: error dialing backend: dial tcp 192.168.122.122:10250: connect: no route to host

root@master:~# ss -tnpl |grep 10250
LISTEN 0      4096                 *:10250            *:*    users:(("kubelet",pid=835,fd=15))        
root@master:~# systemctl disable firewalld && systemctl stop firewalld

  Removed /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service. 
  Removed /etc/systemd/system/multi-user.target.wants/firewalld.service.

root@master:~# ss -tnpl | grep 10250
LISTEN 0      4096                 *:10250            *:*    users:(("kubelet",pid=835,fd=15))        
root@master:~# 

root@master:~# kubectl get pods -n kube-system -o wide
NAME                             READY   STATUS    RESTARTS        AGE    IP                NODE     NOMINATED NODE   READINESS GATES
coredns-787d4945fb-6whjd         1/1     Running   5 (137m ago)    3d7h   10.32.0.2         master   <none>           <none>
coredns-787d4945fb-9f8fs         1/1     Running   5 (137m ago)    3d7h   10.32.0.3         master   <none>           <none>
etcd-master                      1/1     Running   5 (137m ago)    3d7h   192.168.122.103   master   <none>           <none>
kube-apiserver-master            1/1     Running   33 (137m ago)   3d7h   192.168.122.103   master   <none>           <none>
kube-controller-manager-master   1/1     Running   9 (137m ago)    3d7h   192.168.122.103   master   <none>           <none>
kube-proxy-2mh4b                 1/1     Running   5 (137m ago)    3d7h   192.168.122.103   master   <none>           <none>
kube-proxy-x64fm                 1/1     Running   5 (137m ago)    3d7h   192.168.122.122   dev      <none>           <none>
kube-scheduler-master            1/1     Running   9 (137m ago)    3d7h   192.168.122.103   master   <none>           <none>
weave-net-2v8tq                  2/2     Running   11 (137m ago)   3d7h   192.168.122.103   master   <none>           <none>
weave-net-8pnpm                  2/2     Running   10 (137m ago)   3d7h   192.168.122.122   dev      <none>           <none>
=================="
systemctl daemon-reload
systemctl restart kube-apiserver
    results look see "   TO  CHECK  THE NODE/WORKER  
dev@dev:~$ systemctl status firewalld 
\u25cf firewalld.service - firewalld - dynamic firewall daemon
     Loaded: loaded (/lib/systemd/system/firewalld.service; enabled; vendor pre>
     Active: active (running) since Thu 2023-01-26 20:33:40 IST; 3min 50s ago
       Docs: man:firewalld(1)               ####  stop firewalld  in      WORKER OR NODE 
   Main PID: 607 (firewalld)
      Tasks: 2 (limit: 4626)
     Memory: 31.6M
        CPU: 401ms
     CGroup: /system.slice/firewalld.service
             \u2514\u2500607 /usr/bin/python3 /usr/sbin/firewalld --nofork --nopid

Jan 26 20:33:39 dev systemd[1]: Starting firewalld - dynamic firewall daemon...
Jan 26 20:33:40 dev systemd[1]: Started firewalld - dynamic firewall daemon.

dev@dev:~$ systemctl stop firewalld 
dev@dev:~$ lsof -i | grep :10250

root@master:~# kubectl exec -it  nginx -- /bin/bash                 ####  SUCCESS RESULTS OK 
root@nginx:/# ls 
bin   dev		   docker-entrypoint.sh  home  lib64  mnt  proc  run   srv  tmp  var
boot  docker-entrypoint.d  etc			 lib   media  opt  root  sbin  sys  usr
root@nginx:/# cd /usr/share/nginx/html/
root@nginx:/usr/share/nginx/html# ls
50x.html  index.html
root@nginx:/usr/share/nginx/html# 
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30008

curl <node-ip>:<node-port>        # curl <node-ip>:31000
curl <service-ip>:<service-port>  # curl <svc-ip>:8090
curl <pod-ip>:<target-port>       # curl <pod-ip>:80

https://komodor.com/learn/kubernetes-service-examples-basic-usage-and-troubleshooting/
apiVersion: v1
kind: Service
metadata:
  name: my-clusterip-service
spec:
  type: ClusterIP
  clusterIP: 10.10.5.10
  ports:
 â€”name: http
    protocol: TCP
    port: 80
    targetPort: 8080
-----=========
apiVersion: v1
kind: Service
metadata:
  name: my-nodeport-service
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
 â€”name: http
    protocol: TCP
    port: 80
    targetPort: 8080
    nodePort: 30000
----========
apiVersion: v1
kind: Service
metadata:
  name: my-loadbalancer-service
spec:
  type: LoadBalancer
  clusterIP: 10.0.160.135
  loadBalancerIP: 168.196.90.10
  selector:
    app: nginx
  ports:
 â€”name: http
    protocol: TCP
    port: 80
    targetPort: 8080
----================
 apiVersion: v1
kind: Service
metadata:
  name: my-externalname-service
spec:
  type: ExternalName
  externalName: my.database.domain.com
======-----------------


######################################\\\\//\/\/////////////////////////\/\\\\\\\\\\\\\\\\\\/\/\\\/\\/\\\\\\\\\\//////\\\\\\\\\
PROJECT ONE =   =   =  PROJECT -1  ########@@@@@@@@@@@@@@============
pipeline {
agent any
stages{
  stage("checkout from git "){
    steps {
        checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[url: 'https://github.com/rritsoft/jenkins-with-kuberntes.git']])
    }
  }       
 stage("docker build image") {
    steps{
        sh 'docker build -t  anjireddy3993/nginx:latest . '
        sh 'docker images'
        sh 'docker login -u anjireddy3993  -p ASDasd123$'
        sh 'docker push  anjireddy3993/nginx:latest'
        echo " all is success ok  "
    }
 }
 stage (" kube manifest files"){
  steps{
    script {
      kubernetesDeploy(configs: "pod.yaml,nginx1.yaml,dp3.yaml", kubeconfigId: "k8oldplugin")
     }
  }
 }  
    }
}
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
FROM  nginx:latest
COPY  default.conf /etc/nginx/conf.d/  
COPY   index.html /usr/share/nginx/html/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
## default.conf
server {
    listen       80;
    server_name  localhost;
    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
    ssi on;
}
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: nginx-dp
  labels: 
    app: nginx-dp
spec: 
  replicas: 3
  selector:
    matchLabels: 
       app: nginx-dp
  template:    
      metadata: 
        labels: 
         app: nginx-dp
      spec: 
       containers: 
         - name: nginx-dp
           image: anjireddy3993/nginx:latest
           ports: 
            - containerPort: 80        
---
apiVersion: v1
kind: Service
metadata:
  name: outside
spec: 
  selector: 
    name: nginx-dp
  type: NodePort
  ports: 
  - protocol: TCP
    port: 80  
    targetPort: 80
    nodePort: 30008
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
<!doctype html>
<html>
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <h>  ANJIREDDY VELPULS XXXXXXtyyXyyyyyX  </h>
    <title>Simple Transactional Email</title>
    <style>

<html>             
++++++++++++++++++++++++++++++++++++++++++++++++
anji@anji:~$ kubectl get pod,deployment,service  -o wide 
NAME                            READY   STATUS    RESTARTS   AGE   IP          NODE   NOMINATED NODE   READINESS GATES
pod/nginx                       1/1     Running   0          45s   10.44.0.4   dev    <none>           <none>
pod/nginx-dp-56d55b9676-6f2h5   1/1     Running   0          45s   10.44.0.1   dev    <none>           <none>
pod/nginx-dp-56d55b9676-kckdh   1/1     Running   0          45s   10.44.0.3   dev    <none>           <none>
pod/nginx-dp-56d55b9676-mjh4x   1/1     Running   0          45s   10.44.0.2   dev    <none>           <none>
pod/nginx1                      1/1     Running   0          45s   10.44.0.5   dev    <none>           <none>

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                       SELECTOR
deployment.apps/nginx-dp   3/3     3            3           45s   nginx-dp     anjireddy3993/nginx:latest   app=nginx-dp

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE     SELECTOR
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP        3m22s   <none>
service/outside      NodePort    10.104.205.255   <none>        80:30008/TCP   45s     name=nginx-dp
anji@anji:~$ 

=================================================================================================================
https://blog.tekspace.io/exposing-pod-as-a-nodeport-service/
https://katharharshal1.medium.com/steps-to-access-the-pod-from-outside-the-cluster-using-nodeport-e9c194b986df


W: http://apt.kubernetes.io/dists/kubernetes-xenial/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.

dev@dev:~$ apt-key list gazebo
Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).

dev@dev:~$ apt-key export gazebo | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/gazebo-key.gpg
Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).
gpg: WARNING: nothing exported
gpg: no valid OpenPGP data found.

dev@dev:~$ cd /etc/apt

dev@dev:/etc/apt$ sudo cp trusted.gpg trusted.gpg.d

dev@dev:/etc/apt$ apt update 

===================================================


dev@master:~$ kubectl run ubuntu --rm -i --tty --restart=Never --image=ubuntu -- /bin/sh 

If you don't see a command prompt, try pressing enter.

# ls
bin   dev  home  lib32	libx32	mnt  proc  run	 srv  tmp  var
boot  etc  lib	 lib64	media	opt  root  sbin  sys  usr
# apt update -y
Ign:1 http://security.ubuntu.com/ubuntu jammy-security InRelease         
      
Err:1 http://security.ubuntu.com/ubuntu jammy-security InRelease         
  Temporary failure resolving 'security.ubuntu.com'
Err:2 http://archive.ubuntu.com/ubuntu jammy InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Err:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Err:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease
All packages are up to date.
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-updates/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-backports/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://security.ubuntu.com/ubuntu/dists/jammy-security/InRelease  Temporary failure resolving 'security.ubuntu.com'
W: Some index files failed to download. They have been ignored, or old ones used instead.


root@master:~# sudo service docker restart
Failed to restart docker.service: Unit docker.service not found.

root@master:~# sudo /etc/init.d/docker restart
sudo: /etc/init.d/docker: command not found

https://get.docker.com/
curl -fsSL https://get.docker.com -o get-docker.sh
sh get-docker.sh

results : 

root@master:~#  sudo service docker restart

root@master:~# sudo /etc/init.d/docker restart
Restarting docker (via systemctl): docker.service.

=================++++++++++++++++++++++++++++++++++++++++++
errors  [[][][][][][][[][][[[[][[]]]]]]]]]]]]]]]][[]][[[[[[]]]]]]]]]][][]  errors ERRORS  

W: http://apt.kubernetes.io/dists/kubernetes-xenial/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.

dev@dev:~$ apt-key list gazebo
Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).

dev@dev:~$ apt-key export gazebo | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/gazebo-key.gpg
Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).
gpg: WARNING: nothing exported
gpg: no valid OpenPGP data found.

dev@dev:~$ cd /etc/apt

dev@dev:/etc/apt$ sudo cp trusted.gpg trusted.gpg.d

dev@dev:/etc/apt$ apt update 

===================================================


dev@master:~$ kubectl run ubuntu --rm -i --tty --restart=Never --image=ubuntu -- /bin/sh 

If you don't see a command prompt, try pressing enter.

# ls
bin   dev  home  lib32	libx32	mnt  proc  run	 srv  tmp  var
boot  etc  lib	 lib64	media	opt  root  sbin  sys  usr
# apt update -y
Ign:1 http://security.ubuntu.com/ubuntu jammy-security InRelease         
      
Err:1 http://security.ubuntu.com/ubuntu jammy-security InRelease         
  Temporary failure resolving 'security.ubuntu.com'
Err:2 http://archive.ubuntu.com/ubuntu jammy InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Err:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Err:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease
All packages are up to date.
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-updates/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-backports/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://security.ubuntu.com/ubuntu/dists/jammy-security/InRelease  Temporary failure resolving 'security.ubuntu.com'
W: Some index files failed to download. They have been ignored, or old ones used instead.


root@master:~# sudo service docker restart
Failed to restart docker.service: Unit docker.service not found.

root@master:~# sudo /etc/init.d/docker restart
sudo: /etc/init.d/docker: command not found

https://get.docker.com/
curl -fsSL https://get.docker.com -o get-docker.sh
sh get-docker.sh

results : 

root@master:~#  sudo service docker restart

root@master:~# sudo /etc/init.d/docker restart
Restarting docker (via systemctl): docker.service.

=================++++++++++++++++++++++++++++++++++++++++++

systemctl stop firewalld 

sudo rm  -rf /etc/containerd/config.toml

sudo  systemctl restart containerd

sudo kubeadm init --pod-network-cidr=10.244.0.0/16

---------------------
https://stackoverflow.com/questions/65954590/want-to-running-nodeport-on-localhost-kube-services

dell@dell:~$ kubectl get pods 
NAME                        READY   STATUS    RESTARTS   AGE
nginx-dp-56d55b9676-h5jwg   1/1     Running   0          62m
nginx-dp-56d55b9676-qc2l7   1/1     Running   0          62m
nginx-dp-56d55b9676-qd4bm   1/1     Running   0          62m
dell@dell:~$    kubectl port-forward     nginx-dp-56d55b9676-h5jwg   8089:80  --address 0.0.0.0
Forwarding from 0.0.0.0:8089 -> 80
Handling connection for 8089

kubectl expose deployment  deploymentname --type=NodePort  --port  80  --target-port 8086

kubectl expose deployment   nginx-dp   --type=NodePort  --port  80  --target-port 8086


  kubectl port-forward    webserver-744d6b7964-pqnws    8089:80  --address 0.0.0.0

   kubectl port-forward    <podname>    8089:80  --address 0.0.0.0

   kubectl port-forward     nginx-dp-56d55b9676-h5jwg   8089:80  --address 0.0.0.0

   dell@master:~/.kube$  kubectl port-forward nginx-dp-56d55b9676-2rnsw  8089:80 --address 0.0.0.0     ==  important only run in  master ok see 
Forwarding from 0.0.0.0:8089 -> 80                    run in only master 
Handling connection for 8089
^Cdell@master:~/.kube$ 

  ==============================================+++++++++++-------------[][{[}{}{}{{{][}[[[]]]]]]]]}}}]}]

https://www.jenkins.io/doc/book/pipeline/docker/

node {
    checkout scm

    docker.withServer('tcp://swarm.example.com:2376', 'swarm-certs') {
        docker.image('mysql:5').withRun('-p 3306:3306') {
            /* do things */
        }
    }
}



node {
    checkout scm

    docker.withRegistry('https://registry.example.com', 'credentials-id') {

        def customImage = docker.build("my-image:${env.BUILD_ID}")

        /* Push the container to the custom Registry */
        customImage.push()
    }
}

---------------------
https://morioh.com/p/0d410225b84e
   script {
                 withCredentials([string(credentialsId: 'dockerhub-pwd', variable: 'dockerhubpwd')]) {
                    sh 'docker login -u devopshint -p ${dockerhubpwd}'
                 }  
-----
https://medium.com/@dibaekhanal101/jenkins-to-push-docker-image-to-docker-hub-5aefeb48a6c2
withCredentials([string(credentialsId: â€˜json101â€™, variable: â€˜dockerhubpwdâ€™)]) {
sh â€˜docker login -u json101 -p ${dockerhubpwd}â€™
sh â€˜docker push json101/javappâ€™
}
----------------------------
https://www.liatrio.com/blog/building-with-docker-using-jenkins-pipelines
  steps {
      	withCredentials([usernamePassword(credentialsId: 'dockerHub', passwordVariable: 'dockerHubPassword', usernameVariable: 'dockerHubUser')]) {
        	sh "docker login -u ${env.dockerHubUser} -p ${env.dockerHubPassword}"
          sh 'docker push shanem/spring-petclinic:latest'
        }
      }

----------------------
https://iot4beginners.com/how-to-push-a-docker-image-to-the-docker-hub-using-jenkins-pipeline-2022-ci-cd/
        withDockerRegistry([ credentialsId: "dockerhubaccount", url: "" ]) {
        dockerImage.push()
        }
    }   
https://stackoverflow.com/questions/59254492/how-to-pass-credentials-for-jenkins-to-push-a-docker-image-to-my-own-registry
=============================================================================================================================
   PYTHON PLASK APP BUILD IMAGE DOCKER PUSH 
https://runnable.com/docker/python/dockerize-your-flask-application
https://www.digitalocean.com/community/tutorials/how-to-build-and-deploy-a-flask-application-using-docker-on-ubuntu-20-04
https://stuartmccoll.github.io/posts/add-a-flask-application-to-a-docker-container/
https://blog.logrocket.com/build-deploy-flask-app-using-docker/
https://synchronizing.medium.com/running-a-simple-flask-application-inside-a-docker-container-b83bf3e07dd5
https://www.freecodecamp.org/news/how-to-dockerize-a-flask-app/
https://stackoverflow.com/questions/62908052/exposing-flask-app-within-docker-to-internet

=============================================================================
https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/
kubectl port-forward pods/mongo-75f59d57f4-4nd6q 28015:27017
or
kubectl port-forward deployment/mongo 28015:27017
or
kubectl port-forward replicaset/mongo-75f59d57f4 28015:27017
or
kubectl port-forward service/mongo 28015:27017
==================
https://github.com/rritsoft/cicd-docker-kubernetes-project
python flask project  cicd jenkins 
app_service.py
--------------
import json

class AppService:
    
    tasks = [
        {
            'id': 1,
            'name': "task1",
            "description": "This is task 1"
        },
        {
            "id": 2,
            "name": "task2",
            "description": "This is task 2"
        },
        {
            "id": 3,
            "name": "task3",
            "description": "This is task 3"
        }
    ]

    def __init__(self):
        self.tasksJSON = json.dumps(self.tasks)

    def get_tasks(self):
        return self.tasksJSON

    def create_task(self,task):
        tasksData = json.loads(self.tasksJSON)
        tasksData.append(task)
        self.tasksJSON = json.dumps(tasksData)
        return self.tasksJSON

    def update_task(self, request_task):
        tasksData = json.loads(self.tasksJSON)
        for task in tasksData:
            if task["id"] == request_task['id']:
                task.update(request_task)
                return json.dumps(tasksData);
        return json.dumps({'message': 'task id not found'});

    def delete_task(self, request_task_id):
        tasksData = json.loads(self.tasksJSON)
        for task in tasksData:
            if task["id"] == request_task_id:
                tasksData.remove(task)
                return json.dumps(tasksData);
        return json.dumps({'message': 'task id not found'});

------ 
app.py

from flask import Flask, request
from app_service import AppService
import json

app = Flask(__name__)
appService = AppService();


@app.route('/')
def home():
    return "App Works!!!"


@app.route('/api/tasks')
def tasks():
    return appService.get_tasks()

@app.route('/api/task', methods=['POST'])
def create_task():
    request_data = request.get_json()
    task = request_data['task']
    return appService.create_task(task)


@app.route('/api/task', methods=['PUT'])
def update_task():
    request_data = request.get_json()
    return appService.update_task(request_data['task'])


@app.route('/api/task/<int:id>', methods=['DELETE'])
def delete_task(id):
    return appService.delete_task(id)

-----------------------
dockerfile
FROM python:3.7

WORKDIR /opt/app

COPY . .

RUN pip install --no-cache-dir -r requirements-prod.txt

EXPOSE 5000

CMD ["python3", "-m", "flask", "run", "--host=0.0.0.0"]
-------------------------
requirements.txt
flask
python-dotenv
--------
requirements-prod.txt

 -r requirements.txt
-------------------------------
.flaskenv
FLASK_APP=app.py
FLASK_ENV=development
--------
pipeline {
agent any 
stages {
   stage("checkout github"){
     steps{
       checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[url: 'https://github.com/rritsoft/cicd-docker-kubernetes-project.git']])

     }
   }
  stage(" docker build and push image") {
    steps{

      withCredentials([usernamePassword(credentialsId: 'dockerhub', passwordVariable: 'dockerhubPassword', usernameVariable: 'dockerhubUser')]) {
          sh 'docker build -t  anjireddy3993/flaskpro:latest   .'
          sh "docker login -u ${env.dockerhubUser} -p ${env.dockerhubPassword}"
          // sh 'docker run -d -p 5000:5000 anjireddy3993/flaskpro'
          sh 'docker push  anjireddy3993/flaskpro:latest '
      }
   
    }
  }
  stage(" kubectl run commands   "){
    steps{
       withKubeConfig(caCertificate: '', clusterName: 'kubernetes', contextName: 'kubernetes-admin@kubernetes', credentialsId: 'k8file', namespace: '', restrictKubeConfigAccess: false, serverUrl: ' https://192.168.122.31:6443') {
          sh 'kubectl get nodes'
          sh ' kubectl get pods'
          // sh 'kubectl apply -f dp3.yaml'
          // sh 'kubectl apply -f pod.yaml '
          sh 'kubectl apply -f nginx1.yaml'
          sh 'kubectl port-forward  nginx1  5001:5000 '
    }



    }
  }  


}    
}
--------------------------------------

running multiple steps - jenkins pipeline paused stage 3 minutes

https://www.jenkins.io/doc/pipeline/tour/running-multiple-steps/

Jenkinsfile (Declarative Pipeline)

pipeline {
    agent any
    stages {
        stage('Deploy') {
            steps {
                timeout(time: 3, unit: 'MINUTES') {
                    retry(5) {
                        sh './flakey-deploy.sh'
                    }
                }
            }
        }
    }
}
---------
https://stackoverflow.com/questions/63498513/jenkins-pipeline-start-a-stage-2-hours-after-the-1st-one


You can use "sleep" within a stage to pause its execution.

stage("B") {
    steps {
        echo "Pausing stage B"
        sleep(time: 2, unit: "HOURS")
    }
}

----------------------
https://e.printstacktrace.blog/how-to-time-out-jenkins-pipeline-stage-and-keep-the-pipeline-running/
pipeline {
    agent any

    stages {
        stage("A") {
            options {
                timeout(time: 3, unit: "SECONDS")
            }

            steps {
                script {
                    Exception caughtException = null

                    catchError(buildResult: 'SUCCESS', stageResult: 'ABORTED') { 
                        try { 
                            echo "Started stage A"
                            sleep(time: 5, unit: "SECONDS")
                        } catch (org.jenkinsci.plugins.workflow.steps.FlowInterruptedException e) {
                            error "Caught ${e.toString()}" 
                        } catch (Throwable e) {
                            caughtException = e
                        }
                    }

                    if (caughtException) {
                        error caughtException.message
                    }
                }
            }
        }

        stage("B") {
            steps {
                echo "Started stage B"
            }
        }
    }
}
========================================================================================


Manage Docker as a non-root userðŸ”—

sudo groupadd docker
sudo usermod -aG docker $USER

newgrp docker
docker run hello-world

WARNING: Error loading config file: /home/user/.docker/config.json -
stat /home/user/.docker/config.json: permission denied

sudo chown "$USER":"$USER" /home/"$USER"/.docker -R
sudo chmod g+rwx "$HOME/.docker" -R

sudo systemctl enable docker.service
sudo systemctl enable containerd.service

sudo systemctl disable docker.service
sudo systemctl disable containerd.service
--------------------------------------------------
Docker: Temporary failure resolving 'deb.debian.org'
https://docs.docker.com/engine/install/linux-postinstall/#specify-dns-servers-for-docker


root@nginx:/# apt-get update 
Err:1 http://deb.debian.org/debian bullseye InRelease
  Temporary failure resolving 'deb.debian.org'
Err:2 http://deb.debian.org/debian-security bullseye-security InRelease
  Temporary failure resolving 'deb.debian.org'
Err:3 http://deb.debian.org/debian bullseye-updates InRelease
  Temporary failure resolving 'deb.debian.org'
Reading package lists... Done    
W: Failed to fetch http://deb.debian.org/debian/dists/bullseye/InRelease  Temporary failure resolving 'deb.debian.org'
W: Failed to fetch http://deb.debian.org/debian-security/dists/bullseye-security/InRelease  Temporary failure resolving 'deb.debian.org'
W: Failed to fetch http://deb.debian.org/debian/dists/bullseye-updates/InRelease  Temporary failure resolving 'deb.debian.org'
W: Some index files failed to download. They have been ignored, or old ones used instead.
root@nginx:/# 

https://stackoverflow.com/questions/61567404/docker-temporary-failure-resolving-deb-debian-org
https://stackoverflow.com/questions/61567404/docker-temporary-failure-resolving-deb-debian-org

docker run -i -t nginx:latest /bin/bash

apt-get update
apt-get install nano
export TERM=xterm

sudo service docker restart or sudo /etc/init.d/docker restart




Specifying a DNS server for docker containers helped me.

Create a /etc/docker/daemon.json file with this content:

{
  "dns": ["8.8.8.8", "8.8.4.4"]
}

sudo service docker restart




Perhaps the network on the VM is not communicating with the default network created by docker during the build (bridge), so try "host" network :

docker build --network host -t [image_name]

https://www.digitalocean.com/community/questions/apt-error-temporary-failure-resolving-deb-debian-org

echo -e "nameserver 8.8.8.8\nnameserver 8.8.4.4" |sudo tee -a /etc/resolv.conf



I easily resolved it via:

- docker exec -it nginx bash (Go inside container)
- ping google.com (if not working)
- exit (Exit from container)
- sudo service docker restart

Please also confirms /etc/sysctl.conf

- net.ipv4.ip_forward = 1

sudo sysctl -p /etc/sysctl.conf
/etc/init.d/docker restart




I was having the wrong DNS IP address in my /etc/docker/daemon.json. In my case, it was my home router DNS IP address and I was trying from the office network. I found out my office DNS and updated with that.

{ 
  "dns": ["192.168.1.1"] 
}

============================================================+++++++++++++++
https://phoenixnap.com/kb/temporary-failure-in-name-resolution
ping  google.com

Method 1: Badly Configured resolv.conf File

resolv.conf is a file for configuring DNS servers on Linux systems.

To start, open the file in a text editor such as nano.

sudo nano /etc/resolv.conf

sudo systemctl restart systemd-resolved.service

sudo chown root:root /etc/resolv.conf

sudo chmod 777 /etc/resolv.conf

Method 2: Firewall Restrictions:::---

sudo ufw allow 43/tcp
sudo ufw allow 53/tcp

root@master:~# sudo ufw reload
Firewall not enabled (skipping reload)
root@master:~# ufw enable 

root@master:~# sudo ufw reload
Firewall reloaded

root@master:~# sudo firewall-cmd --add-port=43/tcp --permanent
FirewallD is not running
root@master:~# systemctl start firewalld 
root@master:~# sudo firewall-cmd --add-port=43/tcp --permanent
success
root@master:~# sudo firewall-cmd --add-port=53/tcp --permanent

sudo firewall-cmd --reload

-----------------------------------------
https://forum.linuxfoundation.org/discussion/855667/network-does-not-works-on-a-bash-shell-inside-the-new-pod

root@master:~# kubectl exec nginx -it -- /bin/bash 
root@nginx:/# cat /etc/resolv.conf 
search default.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.96.0.10
options ndots:5
root@nginx:/# ping 4.2.2.2 
bash: ping: command not found

root@nginx:/# cat /etc/resolv.conf
search default.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.96.0.10
options ndots:5
root@nginx:/# ping 4.2.2.2
bash: ping: command not found
root@nginx:/# "echo nameserver 8.8.8.8 > /etc/resolv.conf"
bash: echo nameserver 8.8.8.8 > /etc/resolv.conf: No such file or directory
root@nginx:/# echo nameserver 8.8.8.8 > /etc/resolv.conf
root@nginx:/# apt update -y
Err:1 http://deb.debian.org/debian bullseye InRelease
Temporary failure resolving 'deb.debian.org'
0% [Connecting to deb.debian.org]
----------------------------------------
root@master:~# kubectl run ubuntu -it --image=ubuntu  /bin/bash
If you don't see a command prompt, try pressing enter.
root@ubuntu:/# ls
bin  boot  dev  etc  home  lib  lib32  lib64  libx32  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
root@ubuntu:/# apt update -t
root@ubuntu:/# cat /etc/lsb-release 
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=22.04
DISTRIB_CODENAME=jammy
DISTRIB_DESCRIPTION="Ubuntu 22.04.1 LTS"
root@ubuntu:/# exit
exit
Session ended, resume using 'kubectl attach ubuntu -c ubuntu -i -t' command when the pod is running
root@master:~# kubectl attach ubuntu -c ubuntu -i -t     =  best help this command exit
If you don't see a command prompt, try pressing enter.

oot@master:~# kubectl run ubuntu1  -it --image=ubuntu:18.04
If you don't see a command prompt, try pressing enter.
root@ubuntu1:/# apt update -y
Err:1 http://archive.ubuntu.com/ubuntu bionic InRelease                  
  Temporary failure resolving 'archive.ubuntu.com'
Err:2 http://security.ubuntu.com/ubuntu bionic-security InRelease        
  Temporary failure resolving 'security.ubuntu.com'
Err:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease          
  Temporary failure resolving 'archive.ubuntu.com'
0% [Connecting to archive.ubuntu.com]^C
root@ubuntu1:/# cat /etc/lsb-release 
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=18.04
DISTRIB_CODENAME=bionic
DISTRIB_DESCRIPTION="Ubuntu 18.04.6 LTS"
root@ubuntu1:/# 
=======================================================================
root@master:~# ufw status 
Status: inactive
root@master:~# ufw enable
Firewall is active and enabled on system startup
root@master:~# systemctl status ufw 
â— ufw.service - Uncomplicated firewall
     Loaded: loaded (/lib/systemd/system/ufw.service; enabled; vendor preset: enabled)
     Active: active (exited) since Sat 2023-01-28 09:12:20 IST; 10h ago
       Docs: man:ufw(8)
   Main PID: 267 (code=exited, status=0/SUCCESS)
      Tasks: 0 (limit: 7644)
     Memory: 0B
     CGroup: /system.slice/ufw.service

Warning: journal has been rotated since unit was started, output may be incomplete.
root@master:~# systemctl status firewalld 
â— firewalld.service - firewalld - dynamic firewall daemon
     Loaded: loaded (/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled)
     Active: inactive (dead) since Sat 2023-01-28 19:42:06 IST; 23min ago
       Docs: man:firewalld(1)
    Process: 51001 ExecStart=/usr/sbin/firewalld --nofork --nopid (code=exited, status=0/SUCCESS)
   Main PID: 51001 (code=exited, status=0/SUCCESS)

Jan 28 18:42:58 master systemd[1]: Starting firewalld - dynamic firewall daemon...
Jan 28 18:42:58 master systemd[1]: Started firewalld - dynamic firewall daemon.
Jan 28 19:42:05 master systemd[1]: Stopping firewalld - dynamic firewall daemon...
Jan 28 19:42:06 master systemd[1]: firewalld.service: Succeeded.
Jan 28 19:42:06 master systemd[1]: Stopped firewalld - dynamic firewall daemon.
root@master:~# systemctl start  firewalld 
root@master:~# systemctl status firewalld 
â— firewalld.service - firewalld - dynamic firewall daemon
     Loaded: loaded (/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled)
     Active: active (running) since Sat 2023-01-28 20:05:36 IST; 7s ago
       Docs: man:firewalld(1)
   Main PID: 53508 (firewalld)
      Tasks: 2 (limit: 7644)
     Memory: 20.2M
     CGroup: /system.slice/firewalld.service
             â””â”€53508 /usr/bin/python3 /usr/sbin/firewalld --nofork --nopid

Jan 28 20:05:35 master systemd[1]: Starting firewalld - dynamic firewall daemon...
Jan 28 20:05:36 master systemd[1]: Started firewalld - dynamic firewall daemon.
root@master:~# ufw status 
Status: inactive
root@master:~# ufw enable
Firewall is active and enabled on system startup
root@master:~# ufw status 
Status: active

To                         Action      From
--                         ------      ----
43/tcp                     ALLOW       Anywhere                  
53/tcp                     ALLOW       Anywhere                  
43/tcp (v6)                ALLOW       Anywhere (v6)             
53/tcp (v6)                ALLOW       Anywhere (v6)             

root@master:~# 
=======================================================================
https://stackoverflow.com/questions/24991136/docker-build-could-not-resolve-archive-ubuntu-com-apt-get-fails-to-install-a 
https://robinwinslow.uk/fix-docker-networking-dns

root@master:~#  docker run --rm busybox nslookup google.com 
Unable to find image 'busybox:latest' locally
latest: Pulling from library/busybox
205dae5015e7: Pull complete 
Digest: sha256:7b3ccabffc97de872a30dfd234fd972a66d247c8cfc69b0550f276481852627c
Status: Downloaded newer image for busybox:latest
;; connection timed out; no servers could be reached

nslookup: write to '8.8.8.8': No route to host
nslookup: write to '8.8.4.4': No route to host
root@master:~#  nmcli dev show | grep 'IP4.DNS'
IP4.DNS[1]:                             192.168.122.1
root@master:~#  cd /etc/docker
root@master:/etc/docker# touch daemon.json

Put this in /etc/docker/daemon.json:

{                                                                          
    "dns": ["10.0.0.2", "10.0.0.3"]                                                                           
}     
     before ######"

     root@master:/etc/docker# cat daemon.json 
{
  "dns": ["8.8.8.8", "8.8.4.4"]
}
root@master:/etc/docker# 

--   after 


{                                                                          
    "dns": ["192.168.122.1", "192.168.122.1"]                                                                           
}  

root@master:/etc/docker# cat daemon.json 

{                                                                          
    "dns": ["192.168.122.1", "192.168.122.1"]                                                                           
}  

root@master:/etc/docker# 
root@master:/etc/docker# sudo service docker restart        main importent one step all solutions

     after   after   
 root@master:~# docker run --rm busybox nslookup google.com     success  
Server:		192.168.122.1  
Address:	192.168.122.1:53

Non-authoritative answer:
Name:	google.com
Address: 172.217.167.174

Non-authoritative answer:
Name:	google.com
Address: 2404:6800:4007:828::200e

--------------
$ vim /etc/default/docker # (uncomment the DOCKER_OPTS and add DNS IP)
DOCKER_OPTS="--dns 172.168.7.2 --dns 8.8.8.8 --dns 8.8.4.4"

$ rm `docker ps --no-trunc -aq` # (remove all the containers to avoid DNS cache)

$ docker rmi $(docker images -q) # (remove all the images)

$ service docker restart #(restart the docker to pick up dns setting)


               %%%%%######XXXXXXX  success   success ==   


 root@master:~# docker run  -it  ubuntu:16.04 /bin/bash 
Unable to find image 'ubuntu:16.04' locally
16.04: Pulling from library/ubuntu
58690f9b18fc: Pull complete 
b51569e7c507: Pull complete 
da8ef40b9eca: Pull complete 
fb15d46c38dc: Pull complete 
Digest: sha256:1f1a2d56de1d604801a9671f301190704c25d604a416f59e03c04f5c6ffee0d6
Status: Downloaded newer image for ubuntu:16.04
root@3672a327b550:/# ls
bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
root@3672a327b550:/# cat /etc/lsb-release 
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=16.04
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION="Ubuntu 16.04.7 LTS"
root@3672a327b550:/# apt update -y
Err:1 http://archive.ubuntu.com/ubuntu xenial InRelease                  
  Temporary failure resolving 'archive.ubuntu.com'
Err:2 http://security.ubuntu.com/ubuntu xenial-security InRelease        
  Temporary failure resolving 'security.ubuntu.com'
Err:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease          
  Temporary failure resolving 'archive.ubuntu.com'
Err:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Reading package lists... Done        
Building dependency tree       
Reading state information... Done
All packages are up to date.
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/xenial/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/xenial-updates/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/xenial-backports/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://security.ubuntu.com/ubuntu/dists/xenial-security/InRelease  Temporary failure resolving 'security.ubuntu.com'
W: Some index files failed to download. They have been ignored, or old ones used instead.
root@3672a327b550:/# ip a    
bash: ip: command not found
root@3672a327b550:/# cat /etc/host
cat: /etc/host: No such file or directory
root@3672a327b550:/# cat /etc/hosts
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters
172.17.0.2	3672a327b550
root@3672a327b550:/# ping 172.17.0.2
bash: ping: command not found
root@3672a327b550:/# apt install ping
Reading package lists... Done
Building dependency tree       
Reading state information... Done
E: Unable to locate package ping
root@3672a327b550:/# exit
root@master:~# docker ps 
CONTAINER ID   IMAGE          COMMAND       CREATED         STATUS         PORTS     NAMES
b6a7424c3c18   b6f507652425   "/bin/bash"   4 minutes ago   Up 4 minutes             gifted_sutherland
root@master:~# docker images 
REPOSITORY   TAG       IMAGE ID       CREATED         SIZE
busybox      latest    66ba00ad3de8   3 weeks ago     4.87MB
ubuntu       latest    6b7dfa7e8fdb   7 weeks ago     77.8MB
ubuntu       16.04     b6f507652425   17 months ago   135MB
root@master:~# docker attach b6a7424c3c18  /bin/bash 
"docker attach" requires exactly 1 argument.
See 'docker attach --help'.

Usage:  docker attach [OPTIONS] CONTAINER

Attach local standard input, output, and error streams to a running container
root@master:~# docker attach b6a7424c3c18  
^C
root@b6a7424c3c18:/# 
root@b6a7424c3c18:/# 
root@b6a7424c3c18:/# ls 
bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
root@b6a7424c3c18:/# apt update -y 

0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
root@b6a7424c3c18:/# ip a 
bash: ip: command not found
root@b6a7424c3c18:/# apt install net-tools 
Reading package lists... Done

Setting up net-tools (1.60-26ubuntu1) ...
root@b6a7424c3c18:/# ip a 
bash: ip: command not found
root@b6a7424c3c18:/# apt install iproute2
Reading package lists... Done
Building dependency tree       

Fetched 586 kB in 1s (316 kB/s)    
debconf: delaying package configuration, since apt-utils is not installed
Selecting previously unselected package libatm1:amd64.
(Reading database ... 4833 files and directories currently installed.)

Processing triggers for libc-bin (2.23-0ubuntu11.3) ...
root@b6a7424c3c18:/# ip  a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
29: eth0@if30: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever               

==============================================================================================
https://www.techrepublic.com/article/how-to-deploy-and-use-a-mysql-docker-container/

=================================================================================
https://stackoverflow.com/questions/62072977/whats-default-password-in-docker-container-mysql-server-when-you-dont-set-one


root@anji:~# docker run --name mysql  mysql/mysql-server:latest 
Unable to find image 'mysql/mysql-server:latest' locally
latest: Pulling from mysql/mysql-server
6a4a3ef82cdc: Pull complete 
5518b09b1089: Pull complete 
b6b576315b62: Pull complete 
349b52643cc3: Pull complete 
abe8d2406c31: Pull complete 
c7668948e14a: Pull complete 
c7e93886e496: Pull complete 
Digest: sha256:d6c8301b7834c5b9c2b733b10b7e630f441af7bc917c74dba379f24eeeb6a313
Status: Downloaded newer image for mysql/mysql-server:latest
[Entrypoint] MySQL Docker Image 8.0.32-1.2.11-server
[Entrypoint] Initializing database
[Entrypoint] No password option specified for new database.
[Entrypoint]   A random onetime password will be generated.
2023-01-30T14:22:42.404089Z 0 [Warning] [MY-011068] [Server] The syntax '--skip-host-cache' is deprecated and will be removed in a future release. Please use SET GLOBAL host_cache_size=0 instead.
2023-01-30T14:22:42.404182Z 0 [System] [MY-013169] [Server] /usr/sbin/mysqld (mysqld 8.0.32) initializing of server in progress as process 17
2023-01-30T14:22:42.410396Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2023-01-30T14:22:42.789815Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
2023-01-30T14:22:44.102987Z 6 [Warning] [MY-010453] [Server] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.
[Entrypoint] Database initialized
2023-01-30T14:22:47.511339Z 0 [Warning] [MY-011068] [Server] The syntax '--skip-host-cache' is deprecated and will be removed in a future release. Please use SET GLOBAL host_cache_size=0 instead.
2023-01-30T14:22:47.512792Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.32) starting as process 60
2023-01-30T14:22:47.527652Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2023-01-30T14:22:47.645254Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
2023-01-30T14:22:47.876279Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
2023-01-30T14:22:47.876367Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
2023-01-30T14:22:47.929408Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: /var/run/mysqld/mysqlx.sock
2023-01-30T14:22:47.929457Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.32'  socket: '/var/lib/mysql/mysql.sock'  port: 0  MySQL Community Server - GPL.
Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it.
Warning: Unable to load '/usr/share/zoneinfo/leapseconds' as time zone. Skipping it.
Warning: Unable to load '/usr/share/zoneinfo/tzdata.zi' as time zone. Skipping it.
Warning: Unable to load '/usr/share/zoneinfo/zone.tab' as time zone. Skipping it.
Warning: Unable to load '/usr/share/zoneinfo/zone1970.tab' as time zone. Skipping it.
[Entrypoint] GENERATED ROOT PASSWORD: OOMH2O6B+2@;Ko_x95F53&XON=K2.L;v                       ### this is random password 

Entrypoint] ignoring /docker-entrypoint-initdb.d/

2023-01-30T14:22:48.994168Z 11 [System] [MY-013172] [Server] Received SHUTDOWN from user root. Shutting down mysqld (Version: 8.0.32).
2023-01-30T14:22:50.586052Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.32)  MySQL Community Server - GPL.
[Entrypoint] Server shut down
[Entrypoint] Setting root user as expired. Password will need to be changed before database can be used.

[Entrypoint] MySQL init process done. Ready for start up.

[Entrypoint] Starting MySQL 8.0.32-1.2.11-server
2023-01-30T14:22:51.291152Z 0 [Warning] [MY-011068] [Server] The syntax '--skip-host-cache' is deprecated and will be removed in a future release. Please use SET GLOBAL host_cache_size=0 instead.
2023-01-30T14:22:51.292674Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.32) starting as process 1
2023-01-30T14:22:51.298884Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2023-01-30T14:22:51.426700Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
2023-01-30T14:22:51.555423Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
2023-01-30T14:22:51.555446Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
2023-01-30T14:22:51.574382Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: '::' port: 33060, socket: /var/run/mysqld/mysqlx.sock
2023-01-30T14:22:51.574399Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.32'  socket: '/var/lib/mysql/mysql.sock'  port: 3306  MySQL Community Server - GPL.
----------
root@anji:~# docker logs mysql 2>&1 | grep -i  GENERATED 
[Entrypoint]   A random onetime password will be generated.
[Entrypoint] GENERATED ROOT PASSWORD: OOMH2O6B+2@;Ko_x95F53&XON=K2.L;v
root@anji:~# 

----
root@anji:~# docker exec -it mysql  mysql -uroot -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 40
Server version: 8.0.32

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> 

-----------
Because the MYSQL_ONETIME_PASSWORD option is true by default,

 ALTER USER 'root'@'localhost' IDENTIFIED BY 'password';
  ALTER USER 'root'@'localhost' IDENTIFIED BY 'asd123';

mysql>  ALTER USER 'root'@'localhost' IDENTIFIED BY 'asd123';
Query OK, 0 rows affected (0.01 sec)

mysql> 
===============================================================================
                 second method 
https://dbschema.com/2020/03/31/how-to-run-mysql-in-docker/

docker run --name=mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql/mysql-server:8.0

    run - will run a new command in a new Docker container
    â€“name - will give a name to the new container created
    -p - will make the internal docker port visible outside docker
    -e - will change the root password. Here you can insert whatever password you want
    mysql/mysql-server:8.0 - will specify what image to run in the newly created container

root@anji:~# docker exec -it mysql mysql -uroot -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 14
Server version: 8.0.32 MySQL Community Server - GPL

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> 

https://www.tecmint.com/gliding-through-database-mysql-in-a-nutshell-part-i/

mysql> create database anji;
Query OK, 1 row affected (0.01 sec)

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| anji               |
| information_schema |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.01 sec)


mysql> 
mysql> use anji;
Database changed
mysql> CREATE TABLE anji (
    -> id Int(3),
    -> first_name  Varchar (15),
    -> email Varchar(20)
    -> );
Query OK, 0 rows affected, 1 warning (0.02 sec)

mysql> show tables;
+----------------+
| Tables_in_anji |
+----------------+
| anji           |
+----------------+
1 row in set (0.01 sec)

mysql> show columns from  anji;
+------------+-------------+------+-----+---------+-------+
| Field      | Type        | Null | Key | Default | Extra |
+------------+-------------+------+-----+---------+-------+
| id         | int         | YES  |     | NULL    |       |
| first_name | varchar(15) | YES  |     | NULL    |       |
| email      | varchar(20) | YES  |     | NULL    |       |
+------------+-------------+------+-----+---------+-------+
3 rows in set (0.01 sec)



    Int is Integer
    Varchar is char having variable length as defined. The value after Type is the length of field up-to which it can store data.

mysql> ALTER TABLE anji ADD  last_name varchar (20)
    -> AFTER first_name ;
Query OK, 0 rows affected (0.04 sec)
Records: 0  Duplicates: 0  Warnings: 0

mysql> show columns from anji;
+------------+-------------+------+-----+---------+-------+
| Field      | Type        | Null | Key | Default | Extra |
+------------+-------------+------+-----+---------+-------+
| id         | int         | YES  |     | NULL    |       |
| first_name | varchar(15) | YES  |     | NULL    |       |
| last_name  | varchar(20) | YES  |     | NULL    |       |
| email      | varchar(20) | YES  |     | NULL    |       |
+------------+-------------+------+-----+---------+-------+
4 rows in set (0.01 sec)

mysql> ALTER TABLE anji ADD country varchar (15)
    -> AFTER email;
Query OK, 0 rows affected (0.02 sec)
Records: 0  Duplicates: 0  Warnings: 0

mysql> show columns from anji;
+------------+-------------+------+-----+---------+-------+
| Field      | Type        | Null | Key | Default | Extra |
+------------+-------------+------+-----+---------+-------+
| id         | int         | YES  |     | NULL    |       |
| first_name | varchar(15) | YES  |     | NULL    |       |
| last_name  | varchar(20) | YES  |     | NULL    |       |
| email      | varchar(20) | YES  |     | NULL    |       |
| country    | varchar(15) | YES  |     | NULL    |       |
+------------+-------------+------+-----+---------+-------+
5 rows in set (0.00 sec)

mysql> INSERT INTO anji VALUES ('1' , 'reddy' , 'venkata' , 'sdx@gmail.com',
    -> 'india' );
Query OK, 1 row affected (0.01 sec)

mysql> INSERT INTO anji VALUES ('2', 'ashok', 'kamatam' , 'as@gmail.com' , 'usa'),
    ->     ('3', 'hanu', 'velpula' , 'v@gmail.com', 'japan');
Query OK, 2 rows affected (0.00 sec)
Records: 2  Duplicates: 0  Warnings: 0

mysql> select * from anji;
+------+------------+-----------+---------------+---------+
| id   | first_name | last_name | email         | country |
+------+------------+-----------+---------------+---------+
|    1 | reddy      | venkata   | sdx@gmail.com | india   |
|    2 | ashok      | kamatam   | as@gmail.com  | usa     |
|    3 | hanu       | velpula   | v@gmail.com   | japan   |
+------+------------+-----------+---------------+---------+
3 rows in set (0.00 sec)

mysql> DELETE FROM anji WHERE id = 3;
Query OK, 1 row affected (0.01 sec)

mysql> select * from anji;
+------+------------+-----------+---------------+---------+
| id   | first_name | last_name | email         | country |
+------+------------+-----------+---------------+---------+
|    1 | reddy      | venkata   | sdx@gmail.com | india   |
|    2 | ashok      | kamatam   | as@gmail.com  | usa     |
+------+------------+-----------+---------------+---------+
2 rows in set (0.00 sec)


mysql> UPDATE anji SET id = 200 WHERE first_name = 'ashok';
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0

mysql> select * from anji;
+------+------------+-----------+---------------+---------+
| id   | first_name | last_name | email         | country |
+------+------------+-----------+---------------+---------+
|    1 | reddy      | venkata   | sdx@gmail.com | india   |
|  200 | ashok      | kamatam   | as@gmail.com  | usa     |
+------+------------+-----------+---------------+---------+
2 rows in set (0.00 sec)

ysql> ALTER TABLE anji drop country; 
Query OK, 0 rows affected (0.02 sec)
Records: 0  Duplicates: 0  Warnings: 0

mysql> select  * from anji;
+------+------------+-----------+---------------+
| id   | first_name | last_name | email         |
+------+------------+-----------+---------------+
|    1 | reddy      | venkata   | sdx@gmail.com |
|  200 | ashok      | kamatam   | as@gmail.com  |
+------+------------+-----------+---------------+
2 rows in set (0.00 sec)

mysql> RENAME TABLE  anji  TO reddy;
Query OK, 0 rows affected (0.02 sec)

mysql> show tables;
+----------------+
| Tables_in_anji |
+----------------+
| reddy          |
+----------------+
1 row in set (0.01 sec)

mysql> mysqldump -u root -p 123456 > new.sql


mysql> drop database anji;
Query OK, 1 row affected (0.03 sec)


mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
4 rows in set (0.00 sec)

mysql> 

mysql> show databases; 

+--------------------+ 
| Database           | 
+--------------------+ 
| information_schema | 
| my_database        | 
| mysql              | 
| performance_schema | 
| phpmyadmin         | 
| sisso              | 
| test               | 
+--------------------+

7 rows in set (0.00 sec) 

# mysql -u root -p tecmint < tecmint.sql
Enter password:
ERROR 1049 (42000): Unknown database 'tecmint'

mysql> create database tecmint; 
Query OK, 1 row affected (0.00 sec) 

# mysql -u root -p tecmint < tecmint.sql 
Enter password:

mysql> show databases; 

+--------------------+ 
| Database           | 
+--------------------+ 
| information_schema | 
| mysql              | 
| performance_schema | 
| tecmint            | 
| test               | 
+--------------------+ 
8 rows in set (0.00 sec)


mysql> show tables from tecmint;

+-------------------+ 
| Tables_in_tecmint | 
+-------------------+ 
| tecmint_table     | 
+-------------------+ 
1 row in set (0.00 sec)

mysql> select * from tecmint_table; 

+------+------------+-----------+-------------------+ 
| id   | first_name | last_name | email             | 
+------+------------+-----------+-------------------+ 
|    1 | Ravi       | Saive     | raivsaive@xyz.com | 
|    2 | Narad      | Shrestha  | narad@xyz.com     | 
|    6 | tecmint    | [dot]com  | tecmint@gmail.com | 
+------+------------+-----------+-------------------+

3 rows in set (0.00 sec)


============================================================================="
anji@anji:~/docker$ docker run -it mysql1  /bin/bash
bash-4.4# mysql -u root -p 
Enter password: 
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2)
bash-4.4# 

https://medium.com/@alef.duarte/cant-connect-to-local-mysql-server-through-socket-var-run-mysqld-mysqld-sock-155d580f3a06    = onpermise is ok 

https://stackoverflow.com/questions/25920029/setting-up-mysql-and-importing-dump-within-dockerfile


docker compose run [OPTIONS] SERVICE [COMMAND] [ARGS...]
docker compose run web bash
https://docs.docker.com/engine/reference/commandline/compose_run/
docker compose run --service-ports web python manage.py shell
docker compose run db psql -h db -U docker

#docker-compose.yml

data:
  build: docker/data/.
mysql:
  image: mysql
  ports:
    - "3307:3306"
  environment:
    MYSQL_ROOT_PASSWORD: 1234
  volumes:
    - ./docker/data:/docker-entrypoint-initdb.d
  volumes_from:
    - data

----------
Solution 1: use a one-line RUN

RUN /bin/bash -c "/usr/bin/mysqld_safe --skip-grant-tables &" && \
  sleep 5 && \
  mysql -u root -e "CREATE DATABASE mydb" && \
  mysql -u root mydb < /tmp/dump.sql

Solution 2: use a script
Create an executable script init_db.sh:

#!/bin/bash
/usr/bin/mysqld_safe --skip-grant-tables &
sleep 5
mysql -u root -e "CREATE DATABASE mydb"
mysql -u root mydb < /tmp/dump.sql


Add these lines to your Dockerfile:
ADD init_db.sh /tmp/init_db.sh
RUN /tmp/init_db.sh

mysql:
 image: mysql:5.6
 environment:
   MYSQL_ROOT_PASSWORD: pass
 ports:
   - 3306:3306
 volumes:
   - ./db-dump:/docker-entrypoint-initdb.d

----------
mysql:
 image: mysql:5.6
 environment:
   MYSQL_ROOT_PASSWORD: pass
 ports:
   - 3306:3306
 volumes:
   - ./db-dump:/docker-entrypoint-initdb.d

When I run docker-compose up for the first time, the dump is restored in the db.

Here is a working version using v3 of docker-compose.yml. The key is the volumes directive:

mysql:
  image: mysql:5.6
  ports:
    - "3306:3306"
  environment:
    MYSQL_ROOT_PASSWORD: root
    MYSQL_USER: theusername
    MYSQL_PASSWORD: thepw
    MYSQL_DATABASE: mydb
  volumes:
    - ./data:/docker-entrypoint-initdb.d



I used docker-entrypoint-initdb.d approach (Thanks to @Kuhess) But in my case I want to create my DB based on some parameters I defined in .env file so I did these

1) First I define .env file something like this in my docker root project directory

MYSQL_DATABASE=my_db_name
MYSQL_USER=user_test
MYSQL_PASSWORD=test
MYSQL_ROOT_PASSWORD=test
MYSQL_PORT=3306

2) Then I define my docker-compose.yml file. So I used the args directive to define my environment variables and I set them from .env file

version: '2'
services:
### MySQL Container
    mysql:
        build:
            context: ./mysql
            args:
                - MYSQL_DATABASE=${MYSQL_DATABASE}
                - MYSQL_USER=${MYSQL_USER}
                - MYSQL_PASSWORD=${MYSQL_PASSWORD}
                - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
        ports:
            - "${MYSQL_PORT}:3306"

3) Then I define a mysql folder that includes a Dockerfile. So the Dockerfile is this

FROM mysql:5.7
RUN chown -R mysql:root /var/lib/mysql/

ARG MYSQL_DATABASE
ARG MYSQL_USER
ARG MYSQL_PASSWORD
ARG MYSQL_ROOT_PASSWORD

ENV MYSQL_DATABASE=$MYSQL_DATABASE
ENV MYSQL_USER=$MYSQL_USER
ENV MYSQL_PASSWORD=$MYSQL_PASSWORD
ENV MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD

ADD data.sql /etc/mysql/data.sql
RUN sed -i 's/MYSQL_DATABASE/'$MYSQL_DATABASE'/g' /etc/mysql/data.sql
RUN cp /etc/mysql/data.sql /docker-entrypoint-initdb.d

EXPOSE 3306

4) Now I use mysqldump to dump my db and put the data.sql inside mysql folder

mysqldump -h <server name> -u<user> -p <db name> > data.sql

The file is just a normal sql dump file but I add 2 lines at the beginning so the file would look like this

--
-- Create a database using `MYSQL_DATABASE` placeholder
--
CREATE DATABASE IF NOT EXISTS `MYSQL_DATABASE`;
USE `MYSQL_DATABASE`;

-- Rest of queries
DROP TABLE IF EXISTS `x`;
CREATE TABLE `x` (..)
LOCK TABLES `x` WRITE;
INSERT INTO `x` VALUES ...;
...
...
...

So what happening is that I used "RUN sed -i 's/MYSQL_DATABASE/'$MYSQL_DATABASE'/g' /etc/mysql/data.sql" command to replace the MYSQL_DATABASE placeholder with the name of my DB that I have set it in .env file.

|- docker-compose.yml
|- .env
|- mysql
     |- Dockerfile
     |- data.sql


anji@anji:~/docker/compo$ docker-compose up -d 
ERROR: 
        Can't find a suitable configuration file in this directory or any
        parent. Are you in the right directory?

        Supported filenames: docker-compose.yml, docker-compose.yaml
        
anji@anji:~/docker/compo$ mv a.yaml docker-compose.yaml
anji@anji:~/docker/compo$ docker-compose up -d 

anji@anji:~/docker$ tree -a
.
â”œâ”€â”€ data.sql
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â””â”€â”€ .env

0 directories, 4 files

====================================================================================================================
                3 rd method 
https://www.percona.com/blog/2019/11/19/installing-mysql-with-docker/
anji@anji:~/docker$ docker run --name mysql-latest  \
> -p 3306:3306 -p 33060:33060  \
> -e MYSQL_ROOT_HOST='%' -e MYSQL_ROOT_PASSWORD='strongpassword'   \
> -d mysql/mysql-server:latest

anji@anji:~/docker$ docker images 
REPOSITORY           TAG       IMAGE ID       CREATED       SIZE
mysql/mysql-server   latest    1d9c2219ff69   13 days ago   496MB 

anji@anji:~/docker$ docker ps 
CONTAINER ID   IMAGE                       COMMAND                  CREATED          STATUS                    PORTS                                                                                                 NAMES
f7d80c678608   mysql/mysql-server:latest   "/entrypoint.sh mysqâ€¦"   58 seconds ago   Up 57 seconds (healthy)   0.0.0.0:3306->3306/tcp, :::3306->3306/tcp, 0.0.0.0:33060->33060/tcp, :::33060->33060/tcp, 33061/tcp   mysql-latest
anji@anji:~/docker$ 

anji@anji:~/docker$  docker logs mysql-latest
[Entrypoint] MySQL Docker Image 8.0.32-1.2.11-server
[Entrypoint] Initializing database
ssword ! Please consider switching off the --initialize-insecure option.
[Entrypoint] Database initialized
2023-01-31T10:32:26.287068Z 0 [Warning] [MY-011068] [Server] The syntax '--skip-host-cache' is deprecated and will be removed in a future release. Please use SET GLOBAL host_cache_size=0 instead.
rsion: '8.0.32'  socket: '/var/lib/mysql/mysql.sock'  port: 0  MySQL Community Server - GPL.
Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it.
[Entrypoint] ignoring /docker-entrypoint-initdb.d/*

-----
anji@anji:~/docker$ docker exec -it mysql-latest mysql -uroot -pstrongpassword
mysql: [Warning] Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 18
Server version: 8.0.32 MySQL Community Server - GPL

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

=============================================================================================
https://geshan.com.np/blog/2022/02/mysql-docker-compose/
--rm 
The --rm flag is there to tell the Docker Daemon to clean up the container and remove the file system after the container exits.
 This helps you save disk space after running short-lived containers like this one, that we only started to print "Hello, World!

anji@anji:~/docker$ mkdir /tmp/mysql-data
mkdir: cannot create directory â€˜/tmp/mysql-dataâ€™: File exists

anji@anji:~/docker$ docker run --name basic-mysql --rm -v /tmp/mysql-data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=ANSKk08aPEDbFjDO -e MYSQL_DATABASE=testing -p 3306:3306 -it mysql:8.0

root@anji:/home/anji/docker# docker exec -it basic-mysql /bin/bash
bash-4.4# 
bash-4.4# mysql -u root  -p 
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 8
Server version: 8.0.32 MySQL Community Server - GPL
Copyright (c) 2000, 2023, Oracle and/or its affiliates.
Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owner.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sys                |
| testing            |
+--------------------+
5 rows in set (0.01 sec)

====================================================================================
https://ostechnix.com/setup-mysql-with-docker-in-linux/

root@anji:~# docker run --name mysql -p 3306:3306 -v mysql_volume:/var/lib/mysql/ -d -e "MYSQL_ROOT_PASSWORD=temp123" mysql
Unable to find image 'mysql:latest' locally
latest: Pulling from library/mysql

root@anji:~# netstat -plant | grep -i 3306
tcp        0      0 0.0.0.0:3306            0.0.0.0:*               LISTEN      97432/docker-proxy  
tcp6       0      0 :::3306                 :::*                    LISTEN      97438/docker-proxy  
root@anji:~# 
v â†’ Attach a volume to the container. The default behavior of docker is it will not persist the data once the container is removed,so you will lose all your data.

To create persistent storage, I have created volume named "mysql_volume". MySQL stores the data in /var/lib/mysql/ inside the container and here it is mapped to localhost directory /var/lib/docker/volumes/mysql_volume1/_data, so your data will be persistent.

If you wish to know more about docker volumes take a look at our detailed article on same.

-d â†’ Will start and run the container in detached mode. If you omit the -d flag, then you will see the container startup logs in the terminal and you have to open a new terminal session to connect to the container.

-e â†’ Environmental variables. You have to set up mysql root user password using any one of the below parameters.

    MYSQL_ROOT_PASSWORD â†’ Setup your own password using this environment variable.
    MYSQL_ALLOW_EMPTY_PASSWORD â†’ Blank or Null password will be set. You have to set MYSQL_ALLOW_EMPTY_PASSWORD=1.
    MYSQL_RANDOM_ROOT_PASSWORD â†’ random password will be generated when the container is started. You have to set MYSQL_RANDOM_ROOT_PASSWORD=1 to generate the random password.


root@anji:~# docker exec -it mysql bash
bash-4.4# mysql -u root -p
Enter password:   temp123
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 10
Server version: 8.0.32 MySQL Community Server - GPL

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
4 rows in set (0.00 sec)

anji@anji:~/docker$ docker logs mysql 

mysql> ALTER USER 'root'@'localhost' IDENTIFIED BY 'anji123';
Query OK, 0 rows affected (0.00 sec)

mysql> CREATE DATABASE IF NOT EXISTS football;
Query OK, 1 row affected (0.00 sec)

mysql> 
mysql> USE football;
Database changed
mysql> 
mysql> CREATE TABLE IF NOT EXISTS players (
    ->     player_name     VARCHAR(16)     NOT NULL,
    ->     player_age      INT             NOT NULL,
    ->     player_club     VARCHAR(16)     NOT NULL,
    ->     player_country  VARCHAR(16)     NOT NULL
    -> );
Query OK, 0 rows affected (0.02 sec)

mysql> 
mysql> INSERT INTO players VALUES ("Messi",34,"PSG","Argentina");
Query OK, 1 row affected (0.01 sec)

mysql> INSERT INTO players VALUES ("Ronaldo",36,"MANU","Portugal");
Query OK, 1 row affected (0.00 sec)

mysql> INSERT INTO players VALUES ("Neymar",29,"PSG","Brazil");
Query OK, 1 row affected (0.00 sec)

mysql> INSERT INTO players VALUES ("Kane",28,"SPURS","England");
Query OK, 1 row affected (0.01 sec)

mysql> INSERT INTO players VALUES ("E Hazard",30,"MADRID","Belgium");
Query OK, 1 row affected (0.00 sec)

OR  OR  OR                                   -------------------\\\\\\///
anji@anji:$ docker cp data.sql mysql:/tmp

$ docker exec -it mysql bash
bash-4.4# ls -l /tmp/
total 548
-rw-r--r-- 1 1000 1000 558791 Dec 31 23:05 data.sql
bash-4.4# 

mysql> source /tmp/data.sql
Query OK, 0 rows affected (0.00 sec)

Query OK, 0 rows affected (0.00 sec)

or  or   or  
bash-4.4# mysql -u root -p  <  /tmp/data.sql 
Enter password: 
bash-4.4# 

bash-4.4# mysql -u root -p 
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 13
Server version: 8.0.32 MySQL Community Server - GPL

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> use football;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> 

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| football           |
| information_schema |
| mysql              |
| performance_schema |
| sys                |
| world_x            |
+--------------------+
6 rows in set (0.00 sec)

mysql> 

mysql> use football;
Database changed
mysql> 

mysql> show tables;
+--------------------+
| Tables_in_football |
+--------------------+
| players            |
+--------------------+
1 row in set (0.01 sec)

mysql> 

mysql> select * from players;
+-------------+------------+-------------+----------------+
| player_name | player_age | player_club | player_country |
+-------------+------------+-------------+----------------+
| Messi       |         34 | PSG         | Argentina      |
| Ronaldo     |         36 | MANU        | Portugal       |
| Neymar      |         29 | PSG         | Brazil         |
| Kane        |         28 | SPURS       | England        |
| E Hazard    |         30 | MADRID      | Belgium        |
+-------------+------------+-------------+----------------+
5 rows in set (0.00 sec)

secoond method    === = =  ------====== second method`"
`
root@anji:~# docker exec -i mysql  mysql -u root -p < /home/anji/Downloads/world_x-db/data.sql 
Enter password: ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES)

     https://dba.stackexchange.com/questions/315142/error-1045-28000-access-denied-for-user-rootlocalhost
     https://askubuntu.com/questions/401449/error-104528000-access-denied-for-user-rootlocalhost-using-password-no
       https://www.javamadesoeasy.com/2015/11/how-to-resolve-error-1045-28000-access.html
      https://www.stechies.com/error-1045-28000-access-denied-user-root-localhost/
       https://stackoverflow.com/questions/21944936/error-1045-28000-access-denied-for-user-rootlocalhost-using-password-y
-----==

Setup MySQL Container Using Docker-Compose  

version: '3.8'
services:
  database:
    image: mysql:latest
    container_name: mysql
    environment:
      MYSQL_ROOT_PASSWORD: temp1234
    ports:
      - "3306:3306"
    volumes:
      - mysql_volume:/var/lib/mysql
volumes:
  mysql_compose_volume:

                
root@anji:~# docker-compose up
ERROR: Version in "./docker-compose.yaml" is unsupported. You might be seeing this error because you're using the wrong Compose file version. Either specify a supported version (e.g "2.2" or "3.3") and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.
For more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/
root@anji:~#  sudo apt-get remove docker-compose

https://docs.docker.com/compose/install/other/

curl -SL https://github.com/docker/compose/releases/download/v2.15.1/docker-compose-linux-x86_64 -o /usr/local/bin/docker-compose

sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose

root@anji:~# chmod 777 /usr/bin/docker-compose 

root@anji:~# docker-compose -v
Docker Compose version v2.15.1

root@anji:~# docker-compose up
service "database" refers to undefined volume mysql_volume: invalid compose project
root@anji:~# 

   volumes:
            - "./data/db/mysql:/var/lib/mysql"
                volumes:
        - ./symfony/docker/mysql:/var/mysql/data:ro
        - db-data:/var/lib/mysql:rw
$ docker-compose ps

Name              Command             State                          Ports                       
-------------------------------------------------------------------------------------------------
mysql   docker-entrypoint.sh mysqld   Up      0.0.0.0:3306->3306/tcp,:::3306->3306/tcp, 33060/tcp

-=============================================================================
https://earthly.dev/blog/docker-mysql/

docker run --name mysql -d \
    -p 3306:3306 \
    -e MYSQL_ROOT_PASSWORD=change-me \
    --restart unless-stopped \
    mysql:8


anji@anji:~$ docker exec -it mysql  mysql  -p 
Enter password:  change-me

docker stop mysql
docker rm mysql
---

anji@anji:~$ docker run --name mysql -d \
>     -p 3306:3306 \
>     -e MYSQL_ROOT_PASSWORD=change-me \
>     -v mysql:/var/lib/mysql \
>     mysql:8
695b2e47615a85924f520d3d0a22d70b1c5f066209a4802edc8460aa5a88c1ec
anji@anji:~$ 

anji@anji:~$ docker stop mysql
mysql
anji@anji:~$ docker rm mysql
mysql
anji@anji:~$ 
----

anji@anji:~$ docker network create anji
62f813904d43a711e6224cf306d5a7ceea9a0caae1c79b3df363a171565dde0e
anji@anji:~$ 
anji@anji:~$ docker network ls 
NETWORK ID     NAME      DRIVER    SCOPE
62f813904d43   anji      bridge    local
3de69a1fdebc   bridge    bridge    local
94661259418a   host      host      local
e7293d9ceb94   none      null      local
anji@anji:~$ 

---
docker run --name mysql -d \
    -e MYSQL_ROOT_PASSWORD=change-me \
    -v mysql:/var/lib/mysql \
    --network anji \
    mysql:8



docker run --name api-server -d \
    -p 80:80 \
    --network anji \
    example-api-server:latest
-------------------
root@anji:~# mkdir secrets 
root@anji:~# echo "P@$$w0rd" > secrets/mysql-root-password
root@anji:~# ls -a secrets/
.  ..  mysql-root-password
root@anji:~# ll secrets/
total 12
drwxr-xr-x  2 root root 4096 Jan 31 18:51 ./
drwx------ 16 root root 4096 Jan 31 18:50 ../
-rw-r--r--  1 root root   12 Jan 31 18:51 mysql-root-password
root@anji:~# cat secrets/mysql-root-password 
P@93018w0rd
root@anji:~# 

docker run --name mysql -d \
    -p 3306:3306 \
    -e MYSQL_ROOT_PASSWORD_FILE=/run/secrets/mysql-root-password \
    -v ./secrets:/run/secrets \
    --restart unless-stopped \
    mysql:8

Digest: sha256:19b05df6eb4b7ed6f274c0552f053ff0c00842a40dcf05941225c429a716683d
Status: Downloaded newer image for mysql:8
docker: Error response from daemon: create ./secrets: "./secrets" includes invalid characters for a local volume name, only "[a-zA-Z0-9][a-zA-Z0-9_.-]" are allowed. If you intended to pass a host directory, use absolute path.
See 'docker run --help'.
root@anji:~# 

============================================================================================="
https://www.appsdeveloperblog.com/how-to-start-mysql-in-docker-container/

docker run -d -p 3306:3306 --name mysql -e MYSQL_ROOT_PASSWORD=anji123  -e MYSQL_DATABASE=python  -e MYSQL_USER=anji -e MYSQL_PASSWORD=anji123  mysql/mysql-server:latest

root@anji:~# docker exec -it mysql bash
bash-4.4# ls -a 
.  ..  .dockerenv  bin	boot  dev  docker-entrypoint-initdb.d  entrypoint.sh  etc  healthcheck.sh  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
bash-4.4# 

--
anji@anji:~$ watch docker logs mysql 
Every 2.0s: docker logs mysql                                                                                                                                                               anji: Tue Jan 31 19:45:54 2023
[Entrypoint] MySQL Docker Image 8.0.32-1.2.11-server
[Entrypoint] Initializing database
2023-01-31T14:11:17.656170Z 0 [Warning] [MY-011068] [Server] The syntax '--skip-host-cache' is deprecated and will be removed in a future release. Please use SET GLOBAL host_cache_size=0 instead.
[Entrypoint] Database initialized
2023-01-31T14:11:22.775834Z 0 [Warning] [MY-011068] [Server] The syntax '--skip-host-cache' is deprecated and will be removed in a future release. Please use SET GLOBAL host_cache_size=0 instead.
2023-01-31T14:11:23.111628Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.32'  socket: '/var/lib/mysql/mysql.sock'  port: 0  MySQL Community Server - GPL.
Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it.
[Entrypoint] ignoring /docker-entrypoint-initdb.d/
2023-01-31T14:11:24.208412Z 14 [System] [MY-013172] [Server] Received SHUTDOWN from user root. Shutting down mysqld (Version: 8.0.32).
2023-01-31T14:11:25.836441Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.32)  MySQL Community Server - GPL.
[Entrypoint] Server shut down
[Entrypoint] MySQL init process done. Ready for start up.
[Entrypoint] Starting MySQL 8.0.32-1.2.11-server
2023-01-31T14:11:26.455847Z 0 [Warning] [MY-011068] [Server] The syntax '--skip-host-cache' is deprecated and will be removed in a future release. Please use SET GLOBAL host_cache_size=0 instead.
================
mysql> SELECT user FROM mysql.user;
+------------------+
| user             |
+------------------+
| anji             |
| healthchecker    |
| mysql.infoschema |
| mysql.session    |
| mysql.sys        |
| root             |
+------------------+
6 rows in set (0.00 sec)

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| python             |
| sys                |
+--------------------+
5 rows in set (0.00 sec)

mysql> create user 'new_user'@'localhost' identified by 'welcome';
Query OK, 0 rows affected (0.01 sec)

mysql> grant all privileges on *.* to 'new_user'@'localhost';
Query OK, 0 rows affected, 1 warning (0.01 sec)

mysql> flush privileges;
Query OK, 0 rows affected (0.00 sec)

mysql> select user from mysql.user;
+------------------+
| user             |
+------------------+
| anji             |
| healthchecker    |
| mysql.infoschema |
| mysql.session    |
| mysql.sys        |
| new_user         |
| root             |
+------------------+
7 rows in set (0.01 sec)
======================================================================================="
https://dev.to/musolemasu/deploy-a-mysql-database-server-in-kubernetes-static-dpc     success 

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
spec:
  selector:
    matchLabels:
      app: mysql
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
        - image: mysql:8.0
          name: mysql
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: password
          ports:
            - containerPort: 3306
              name: mysql
          volumeMounts:
            - name: mysql-persistent-storage
              mountPath: /var/lib/mysql
      volumes:
        - name: mysql-persistent-storage
          persistentVolumeClaim:
            claimName: mysql-pv-claim
---
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  ports:
    - port: 3306
  selector:
    app: mysql
  clusterIP: None

anji@anji:~/deployment$ kubectl exec mysql-55dd4c6c84-4dzf5  -it -- /bin/bash
bash-4.4# ls 
bin   dev			  entrypoint.sh  home  lib64  mnt  proc  run   srv  tmp  var
boot  docker-entrypoint-initdb.d  etc		 lib   media  opt  root  sbin  sys  usr
bash-4.4# 

anji@anji:~$ kubectl cp  /home/anji/Downloads/world_x-db/data.sql  mysql-55dd4c6c84-4dzf5:/var/lib/mysql

bash-4.4# pwd
/var/lib/mysql
bash-4.4# ls -al  | grep -i data.sql 
-rw-r--r-- 1  1000  1000   558791 Feb  1 09:38 data.sql
bash-4.4# 

mysql> source /var/lib/mysql/data.sql 

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sys                |
| world_x            |
+--------------------+
5 rows in set (0.00 sec)


mysql> show tables;
+-------------------+
| Tables_in_world_x |
+-------------------+
| city              |
| country           |
| countryinfo       |
| countrylanguage   |
+-------------------+
4 rows in set (0.01 sec)

mysql> use world_x;
Database changed
mysql> show tables;
+-------------------+
| Tables_in_world_x |
+-------------------+
| city              |
| country           |
| countryinfo       |
| countrylanguage   |
+-------------------+
4 rows in set (0.00 sec)

mysql> select * from city;

mysql> select * from city;
+------+-----------------------------------+-------------+----------------------+--------------------------+
| ID   | Name                              | CountryCode | District             | Info                     |
+------+-----------------------------------+-------------+----------------------+--------------------------+
|    1 | Kabul                             | AFG         | Kabol                | {"Population": 1780000}  |
|    2 | Qandahar                          | AFG         | Qandahar             | {"Population": 237500}   |
|    3 | Herat                             | AFG         | Herat                | {"Population": 186800}   |
|    4 | Mazar-e-Sharif                    | AFG         | Balkh                | {"Population": 127800}   |
|    5 | Amsterdam                         | NLD         | Noord-Holland        | {"Population": 731200}   |
|    6 | Rotterdam                         | NLD         | Zuid-Holland         | {"Population": 593321}   |
|    7 | Haag                              | NLD         | Zuid-Holland         | {"Population": 440900}   |
|    8 | Utrecht                           | NLD         | Utrecht              | {"Population": 234323}   |
|    9 | Eindhoven                         | NLD         | Noord-Brabant        | {"Population": 201843}   |
|   10 | Tilburg                           | NLD         | Noord-Brabant        | {"Population": 193238}   |
|   11 | Groningen                         | NLD         | Groningen            | {"Population": 172701}   |
|   12 | Breda                             | NLD         | Noord-Brabant        | {"Population": 160398}   |
|   13 | Apeldoorn                         | NLD         | Gelderland           | {"Population": 153491}   |
|   14 | Nijmegen                          | NLD         | Gelderland           | {"Population": 152463}   |
|   15 | Enschede                          | NLD         | Overijssel           | {"Population": 149544}   |
|   16 | Haarlem                           | NLD         | Noord-Holland        | {"Population": 14

mysql> select * from city;
+------+-----------------------------------+-------------+----------------------+--------------------------+
| ID   | Name                              | CountryCode | District             | Info                     |
+------+-----------------------------------+-------------+----------------------+--------------------------+
|    1 | Kabul                             | AFG         | Kabol                | {"Population": 1780000}  |
|    2 | Qandahar                          | AFG         | Qandahar             | {"Population": 237500}   |
|    3 | Herat                             | AFG         | Herat                | {"Population": 186800}   |
|    4 | Mazar-e-Sharif                    | AFG         | Balkh                | {"Population": 127800}   |
|    5 | Amsterdam                         | NLD         | Noord-Holland        | {"Population": 731200}   |
|    6 | Rotterdam                         | NLD         | Zuid-Holland         | {"Population": 593321}   |
|    7 | Haag                              | NLD         | Zuid-Holland         | {"Population": 440900}   |
|    8 | Utrecht                           | NLD         | Utrecht              | {"Population": 234323}   |
|    9 | Eindhoven                         | NLD         | Noord-Brabant        | {"Population": 201843}   |
|   10 | Tilburg                           | NLD         | Noord-Brabant        | {"Population": 193238}   |
|   11 | Groningen                         | NLD         | Groningen            | {"Population": 172701}   |
|   12 | Breda                             | NLD         | Noord-Brabant        | {"Population": 160398}   |
|   13 | Apeldoorn                         | NLD         | Gelderland           | {"Population": 153491}   |
|   14 | Nijmegen                          | NLD         | Gelderland           | {"Population": 152463}   |
|   15 | Enschede                          | NLD         | Overijssel           | {"Population": 149544}   |
|   16 | Haarlem                           | NLD         | Noord-Holland        | {"Population": 148772}   |
|   17 | Almere                            | NLD         | Flevoland            | {"Population": 142465}   |
|   18 | Arnhem                            | NLD         | Gelderland           | {"Population": 138020}   |
|   19 | Zaanstad                          | NLD         | Noord-Holland        | {"Population": 135621}   |

-----"
mysql> select * from countryinfo;
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------+--------------------+
| doc                                                                                                                                                                                                                                                                                                                                                                                                           | _id                                                        | _json_schema       |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------+--------------------+
| {"GNP": 828, "_id": "00005de917d80000000000000000", "Code": "ABW", "Name": "Aruba", "IndepYear": null, "geography": {"Region": "Caribbean", "Continent": "North America", "SurfaceArea": 193}, "government": {"HeadOfState": "Beatrix", "GovernmentForm": "Nonmetropolitan Territory of The Netherlands"}, "demographics": {"Population": 103000, "LifeExpectancy": 78.4000015258789}}                        | 0x30303030356465393137643830303030303030303030303030303030 | {"type": "object"} |
| {"GNP": 5976, "_id": "00005de917d80000000000000001", "Code": "AFG", "Name": "Afghanistan", "IndepYear": 1919, "geography": {"Region": "Southern and Central Asia", "Continent": "Asia", "SurfaceArea": 652090}, "government": {"HeadOfState": "Mohammad Omar", "GovernmentForm": "Islamic Emirate"}, "demographics": {"Population": 22720000, "LifeExpectancy": 45.900001525878906}}                          | 0x30303030356465393137643830303030303030303030303030303031 | {"type": "object"} |
| {"GNP": 6648, "_id": "00005de917d80000000000000002", "Code": "AGO", "Name": "Angola", "IndepYear": 1975, "geography": {"Region": "Central Africa", "Continent": "Africa", "SurfaceArea": 1246700}, "government": {"HeadOfState": "Josï¿½ Eduardo dos Santos", "GovernmentForm": "Republic"}, "demographics": {"Population": 12878000, "LifeExpectancy": 38.29999923706055}}                                     | 0x30303030356465393137643830303030303030303030303030303032 | {"type": "object"} |
| {"GNP": 63.20000076293945, "_id": "00005de917d80000000000000003", "Code": "AIA", "Name": "Anguilla", "IndepYear": null, "geography": {"Region": "Caribbean", "Continent": "North America", "SurfaceArea": 96}, "government": {"HeadOfState": "Elizabeth II", "GovernmentForm": "Dependent Territory of the UK"}, "demographics": {"Population": 8000, "LifeExpectancy": 76.0999984741211}}                    | 0x30303030356465393137643830303030303030303030303030303033 | {"type": "object"} |
| {"GNP": 3205, "_id": "00005de917d80000000000000004", "Code": "ALB", "Name": "Albania", "IndepYear": 1912, "geography": {"Region": "Southern Europe", "Continent": "Europe", "SurfaceArea": 28748}, "government": {"HeadOfState": "Rexhep Mejdani", "GovernmentForm": "Republic"}, "demographics": {"Population": 3401200, "LifeExpectancy": 71.5999984741211}}                                                | 0x30303030356465393137643830303030303030303030303030303034 | {"type": "object"} |
| {"GNP": 1630, "_id": "00005de917d80000000000000005", "Code": "AND", "Name": "Andorra", "IndepYear": 1278, "geography": {"Region": "Southern Europe", "Continent": "Europe", "SurfaceArea": 468}, "government": {"HeadOfState": "", "GovernmentForm": "Parliamentary Coprincipality"}, "demographics": {"Population": 78000, "LifeExpectancy": 83.5}}                                                          | 0x30303030356465393137643830303030303030303030303030303035 | {"type": "object"} |
| {"GNP": 1941, "_id": "00005de917d80000000000000006", "Code": "ANT", "Name": "Netherlands Antilles", "IndepYear": null, "geography": {"Region": "Caribbean", "Continent": "North America", "SurfaceArea": 800}, "government": {"HeadOfState": "Beatrix", "GovernmentForm": "Nonmetropolitan Territory of The Netherlands"}, "demographics": {"Population": 217000, "LifeExpectancy": 74.69999694824219}}       | 0x30303030356465393137643830303030303030303030303030303036 | {"type": "object"} |
| {"GNP": 37966, "_id": "00005de917d80000000000000007", "Code": "ARE", "Name": "United Arab Emirates", "IndepYear": 1971, "geography": {"Region": "Middle East", "Continent": "Asia", "SurfaceArea": 83600}, "government": {"HeadOfState": "Zayid bin Sultan al-Nahayan", "GovernmentForm": "Emirate Federation"}, "demographics": {"Population": 2441000, "LifeExpectancy": 74.0999984741211}}                 | 0x30303030356465393137643830303030303030303030303030303037 | {"type": "object"} |
| {"GNP": 340238, "_id": "00005de917d80000000000000008", "Code": "ARG", "Name": "Argentina", "IndepYear": 1816, "geography": {"Region": "South America", "Continent": "South America", "SurfaceArea": 2780400}, "government": {"HeadOfState": "Fernando de la Rï¿½a", "GovernmentForm": "Federal Republic"}, "demographics": {"Population": 37032000, "LifeExpectancy": 75.0999984741211}}                        | 0x30303030356465393137643830303030303030303030303030303038 | {"type": "object"} |
| {"GNP": 1813, "_id": "00005de917d80000000000000009", "Code": "ARM", "Name": "Armenia", "IndepYear": 1991, "geography": {"Region": "Middle East", "Continent": "Asia", "SurfaceArea": 29800}, "government": {"HeadOfState": "Robert Kotï¿½arjan", "GovernmentForm": "Republic"}, "demographics": {"Population": 3520000, "LifeExpectancy": 66.4000015258789}}                                                    | 0x30303030356465393137643830303030303030303030303030303039 | {"type": "object"} |
| {"GNP": 334, "_id": "00005de917d8000000000000000a", "Code": "ASM", "Name": "American Samoa", "IndepYear": null, "geography": {"Region": "Polynesia", "Continent": "Oceania", "SurfaceArea": 199}, "government": {"HeadOfState": "George W. Bush", "GovernmentForm": "US Territory"}, "demographics": {"Population": 68000, "LifeExpectancy": 75.0999984741211}}                                               | 0x30303030356465393137643830303030303030303030303030303061 | {"type": "object"} |
| {"GNP": 0, "_id": "00005de917d8000000000000000b", "Code": "ATA", "Name": "Antarctica", "IndepYear": null, "geography": {"Region": "Antarctica", "Continent": "Antarctica", "SurfaceArea": 13120000}, "government": {"HeadOfState": "", "GovernmentForm": "Co-administrated"}, "demographics": {"Population": 0, "LifeExpectancy": null}}                                                                      | 0x30303030356465393137643830303030303030303030303030303062 | {"type": "object"} |
| {"GNP": 0, "_id": "00005de917d8000000000000000c", "Code": "ATF", "Name": "French Southern territories", "IndepYear": null, "geography": {"Region": "Antarctica", "Continent": "Antarctica", "SurfaceArea": 7780}, "government": {"HeadOfState": "Jacques Chirac", "GovernmentForm": "Nonmetropolitan Territory of France"}, "demographics": {"Population": 0, "LifeExpectancy": null}}                        | 0x30303030356465393137643830303030303030303030303030303063 | {"type": "object"} |
| {"GNP": 612, "_id": "00005de917d8000000000000000d", "Code": "ATG", "Name": "Antigua and Barbuda", "IndepYear": 1981, "geography": {"Region": "Caribbean", "Continent": "North America", "SurfaceArea": 442}, "government": {"HeadOfState": "Elizabeth II", "GovernmentForm": "Constitutional Monarchy"}, "demographics": {"Population": 68000, "LifeExpectancy": 70.5}}                                       | 0x30303030356465393137643830303030303030303030303030303064 | {"type": "object"} |
| {"GNP": 351182, "_id": "00005de917d8000000000000000e", "Code": "AUS", "Name": "Australia", "IndepYear": 1901, "geography": {"Region": "Australia and New Zealand", "Continent": "Oceania", "SurfaceArea": 7741220}, "government": {"HeadOfState": "Elizabeth II", "GovernmentForm": "Constitutional Monarchy, Federation"}, "demographics": {"Population": 18886000, "LifeExpectancy": 79.80000305175781}}    | 0x30303030356465393137643830303030303030303030303030303065 | {"type": "object"} |
| {"GNP": 211860, "_id": "00005de917d8000000000000000f", "Code": "AUT", "Name": "Austria", "IndepYear": 1918, "geography": {"Region": "Western Europe", "Continent": "Europe", "SurfaceArea": 83859}, "government": {"HeadOfState": "Thomas Klestil", "GovernmentForm": "Federal Republic"}, "demographics": {"Population": 8091800, "LifeExpectancy": 77.69999694824219}}                                      | 0x30303030356465393137643830303030303030303030303030303066 | {"type": "object"} |
| {"GNP": 4127, "_id": "00005de917d80000000000000010", "Code": "AZE", "Name": "Azerbaijan", "IndepYear": 1991, "geography": {"Region": "Middle East", "Continent": "Asia", "SurfaceArea": 86600}, "government": {"HeadOfState": "Heydï¿½r ï¿½liyev", "GovernmentForm": "Federal Republic"}, "demographics": {"Population": 7734000, "LifeExpectancy": 62.900001525878906}}                                          | 0x30303030356465393137643830303030303030303030303030303130 | {"type": "object"} |
| {"GNP": 903, "_id": "00005de917d80000000000000011", "Code": "BDI", "Name": "Burundi", "IndepYear": 1962, "geography": {"Region": "Eastern Africa", "Continent": "Africa", "SurfaceArea": 27834}, "government": {"HeadOfState": "Pierre Buyoya", "GovernmentForm": "Republic"}, "demographics": {"Population": 6695000, "LifeExpectancy": 46.20000076293945}}                                                  | 0x30303030356465393137643830303030303030303030303030303131 | {"type": "object"} |
| {"GNP": 249704, "_id": "00005de917d80000000000000012", "Code": "BEL", "Name": "Belgium", "IndepYear": 1830, "geography": {"Region": "Western Europe", "Continent": "Europe", "SurfaceArea": 30518}, "government": {"HeadOfState": "Albert II", "GovernmentForm": "Constitutional Monarchy, Federation"}, "demographics": {"Population": 10239000, "LifeExpectancy": 77.80000305175781}}                       | 0x30303030356465393137643830303030303030303030303030303132 | {"type": "object"} |
| {"GNP": 2357, "_id": "00005de917d80000000000000013", "Code": "BEN", "Name": "Benin", "IndepYear": 1960, "geography": {"Region": "Western Africa", "Continent": "Africa", "SurfaceArea": 112622}, "government": {"HeadOfState": "Mathieu Kï¿½rï¿½kou", "GovernmentForm": "Republic"}, "demographics": {"Population": 6097000, "LifeExpectancy": 50.20000076293945}}                                                | 0x30303030356465393137643830303030303030303030303030303133 | {"type": "object"} |
| {"GNP": 2425, "_id": "00005de917d80000000000000014", "Code": "BFA", "Name": "Burkina Faso", "IndepYear": 1960, "geography": {"Region": "Western Africa", "Continent": "Africa", "SurfaceArea": 274000}, "government": {"HeadOfState": "Blaise Compaorï¿½", "GovernmentForm": "Republic"}, "demographics": {"Population": 11937000, "LifeExpectancy": 46.70000076293945}}                                        | 0x30303030356465393137643830303030303030303030303030303134 | {"type": "object"} |
| {"GNP": 32852, "_id": "00005de917d80000000000000015", "Code": "BGD", "Name": "Bangladesh", "IndepYear": 1971, "geography": {"Region": "Southern and Central Asia", "Continent": "Asia", "SurfaceArea": 143998}, "government": {"HeadOfState": "Shahabuddin Ahmad", "GovernmentForm": "Republic"}, "demographics": {"Population": 129155000, "LifeExpectancy": 60.20000076293945}}                             | 0x30303030356465393137643830303030303030303030303030303135 | {"type": "object"} |
| {"GNP": 12178, "_id": "00005de917d80000000000000016", "Code": "BGR", "Name": "Bulgaria", "IndepYear": 1908, "geography": {"Region": "Eastern Europe", "Continent": "Europe", "SurfaceArea": 110994}, "government": {"HeadOfState": "Petar Stojanov", "GovernmentForm": "Republic"}, "demographics": {"Population": 8190900, "LifeExpectancy": 70.9000015258789}}                                              | 0x30303030356465393137643830303030303030303030303030303136 | {"type": "object"} |
| {"GNP": 6366, "_id": "00005de917d80000000000000017", "Code": "BHR", "Name": "Bahrain", "IndepYear": 1971, "geography": {"Region": "Middle East", "Continent": "Asia", "SurfaceArea": 694}, "government": {"HeadOfState": "Hamad ibn Isa al-Khalifa", "GovernmentForm": "Monarchy (Emirate)"}, "demographics": {"Population": 617000, "LifeExpectancy": 73}}                                                   | 0x30303030356465393137643830303030303030303030303030303137 | {"type": "object"} |
| {"GNP": 3527, "_id": "00005de917d80000000000000018", "Code": "BHS", "Name": "Bahamas", "IndepYear": 1973, "geography": {"Region": "Caribbean", "Continent": "North America", "SurfaceArea": 13878}, "government": {"HeadOfState": "Elizabeth II", "GovernmentForm": "Constitutional Monarchy"}, "demographics": {"Population": 307000, "LifeExpectancy": 71.0999984741211}}                                   | 0x30303030356465393137643830303030303030303030303030303138 | {"type": "object"} |
| {"GNP": 2841, "_id": "00005de917d80000000000000019", "Code": "BIH", "Name": "Bosnia and Herzegovina", "IndepYear": 1992, "geography": {"Region": "Southern Europe", "Continent": "Europe", "SurfaceArea": 51197}, "government": {"HeadOfState": "Ante Jelavic", "GovernmentForm": "Federal Republic"}, "demographics": {"Population": 3972000, "LifeExpectancy": 71.5}}                                       | 0x30303030356465393137643830303030303030303030303030303139 | {"type": "object"} |
| {"GNP": 13714, "_id": "00005de917d8000000000000001a", "Code": "BLR", "Name": "Belarus", "IndepYear": 1991, "geography": {"Region": "Eastern Europe", "Continent": "Europe", "SurfaceArea": 207600}, "government": {"HeadOfState": "Aljaksandr Lukaï¿½enka", "GovernmentForm": "Republic"}, "demographics": {"Population": 10236000, "LifeExpectancy": 68}}                                                      | 0x30303030356465393137643830303030303030303030303030303161 | {"type": "object"} |
| {"GNP": 630, "_id": "00005de917d8000000000000001b", "Code": "BLZ", "Name": "Belize", "IndepYear": 1981, "geography": {"Region": "Central America", "Continent": "North America", "SurfaceArea": 22696}, "government": {"HeadOfState": "Elizabeth II", "GovernmentForm": "Constitutional Monarchy"}, "demographics": {"Population": 241000, "LifeExpectancy": 70.9000015258789}}                               | 0x30303030356465393137643830303030303030303030303030303162 | {"type": "object"} |
| {"GNP": 2328, "_id": "00005de917d8000000000000001c", "Code": "BMU", "Name": "Bermuda", "IndepYear": null, "geography": {"Region": "Nor
------"

mysql> select * from countrylanguage;
+-------------+---------------------------+------------+------------+
| CountryCode | Language                  | IsOfficial | Percentage |
+-------------+---------------------------+------------+------------+
| AFG         | Dari                      | T          |       32.1 |
| AFG         | Pashto                    | T          |       52.4 |
| AFG         | Turkmenian                | F          |        1.9 |
| AFG         | Uzbek                     | F          |        8.8 |
| AGO         | Ambo                      | F          |        2.4 |
| AGO         | Chokwe                    | F          |        4.2 |
| AGO         | Mbundu                    | F          |       21.6 |
| AGO         | Nyaneka-nkhumbi           | F          |        5.4 |
| AGO         | Ovimbundu                 | F          |       37.2 |
| AIA         | English                   | T          |        0.0 |
| ANT         | Papiamento                | T          |       86.2 |
| ARE         | Arabic                    | T          |       42.0 |
| ARE         | Hindi                     | F          |        0.0 |
| ARG         | Indian Languages          | F          |        0.3 |
| ARG         | Italian                   | F          |        1.7 |
| ARG         | Spanish                   | T          |       96.8 |
| ARM         | Armenian                  | T          |       93.4 |
| ARM         | Azerbaijani               | F          |        2.6 |
| ASM         | English                   | T          |        3.1 |
| ASM         | Samoan                    | T          |       90.6 |
| ASM         | Tongan                    | F          |        3.1 |
| ATG         | Creole English            | F          |       95.7 |
| ATG         | English                   | T          |        0.0 |
| AUS         | Serbo-Croatian            | F          |        0.6 |
| AUS         | Vietnamese                | F          |        0.8 |
| AUT         | Czech                     | F          |        0.2 |


mysql> select user from mysql.user;
+------------------+
| user             |
+------------------+
| root             |
| mysql.infoschema |
| mysql.session    |
| mysql.sys        |
| root             |
+------------------+
5 rows in set (0.00 sec)

root@worker:~# ls -al  /mnt/data/             == \\\\/ data test  mount valumes data refelect data 
total 102112
drwxr-xr-x 10 systemd-coredump root                 4096 Feb  1 15:51  .
drwxr-xr-x  3 root             root                 4096 Feb  1 14:20  ..
-rw-r-----  1 systemd-coredump systemd-coredump       56 Feb  1 14:20  auto.cnf
-rw-r-----  1 systemd-coredump systemd-coredump  3032843 Feb  1 14:20  binlog.000001
-rw-r-----  1 systemd-coredump systemd-coredump  1908824 Feb  1 15:13  binlog.000002
-rw-r-----  1 systemd-coredump systemd-coredump       32 Feb  1 14:20  binlog.index
-rw-------  1 systemd-coredump systemd-coredump     1676 Feb  1 14:20  ca-key.pem
-rw-r--r--  1 systemd-coredump systemd-coredump     1112 Feb  1 14:20  ca.pem
-rw-r--r--  1 systemd-coredump systemd-coredump     1112 Feb  1 14:20  client-cert.pem
-rw-------  1 systemd-coredump systemd-coredump     1676 Feb  1 14:20  client-key.pem
-rw-r--r--  1 dell             dell               558791 Feb  1 15:08  data.sql
drwxr-xr-x  2 root             root                 4096 Feb  1 15:50  I-AM-FROM-MISQL-POD
-rw-r-----  1 systemd-coredump systemd-coredump   196608 Feb  1 15:25 '#ib_16384_0.dblwr'
-rw-r-----  1 systemd-coredump systemd-coredump  8585216 Feb  1 14:20 '#ib_16384_1.dblwr'
-rw-r-----  1 systemd-coredump systemd-coredump     5687 Feb  1 14:20  ib_buffer_pool
-rw-r-----  1 systemd-coredump systemd-coredump 12582912 Feb  1 15:25  ibdata1
-rw-r-----  1 systemd-coredump systemd-coredump 12582912 Feb  1 14:20  ibtmp1
drwxr-x---  2 systemd-coredump systemd-coredump     4096 Feb  1 15:12 '#innodb_redo'
drwxr-x---  2 systemd-coredump systemd-coredump     4096 Feb  1 14:20 '#innodb_temp'
drwxr-x---  2 systemd-coredump systemd-coredump     4096 Feb  1 14:20  mysql
-rw-r-----  1 systemd-coredump systemd-coredump 31457280 Feb  1 15:13  mysql.ibd
lrwxrwxrwx  1 systemd-coredump systemd-coredump       27 Feb  1 14:20  mysql.sock -> /var/run/mysqld/mysqld.sock
drwxr-x---  2 systemd-coredump systemd-coredump     4096 Feb  1 14:20  performance_schema
-rw-------  1 systemd-coredump systemd-coredump     1680 Feb  1 14:20  private_key.pem
-rw-r--r--  1 systemd-coredump systemd-coredump      452 Feb  1 14:20  public_key.pem
-rw-r--r--  1 systemd-coredump systemd-coredump     1112 Feb  1 14:20  server-cert.pem
-rw-------  1 systemd-coredump systemd-coredump     1680 Feb  1 14:20  server-key.pem
drwxr-x---  2 systemd-coredump systemd-coredump     4096 Feb  1 14:20  sys
drwxr-xr-x  2 root             root                 4096 Feb  1 15:47  test-node-to-msqlpod
-rw-r--r--  1 root             root                    0 Feb  1 15:51  TEST-VOLUMEMOUNTS-TEST-VOLUMOUNTPATHS-DATA-REFELECT
-rw-r-----  1 systemd-coredump systemd-coredump 16777216 Feb  1 15:13  undo_001
-rw-r-----  1 systemd-coredump systemd-coredump 16777216 Feb  1 15:13  undo_002
drwxr-x---  2 systemd-coredump systemd-coredump     4096 Feb  1 15:13  world_x
=========================================================================================
https://dev.to/musolemasu/deploy-a-mysql-database-server-in-kubernetes-static-dpc     success code
https://phoenixnap.com/kb/kubernetes-mysql
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  ports:
  - port: 3306
  selector:
    app: mysql

https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/
apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    app: mysql
    app.kubernetes.io/name: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  clusterIP: None
  selector:
    app: mysql

---
# Client service for connecting to any MySQL instance for reads.
# For writes, you must instead connect to the primary: mysql-0.mysql.
apiVersion: v1
kind: Service
metadata:
  name: mysql-read
  labels:
    app: mysql
    app.kubernetes.io/name: mysql
    readonly: "true"
spec:
  ports:
  - name: mysql
    port: 3306
  selector:
    app: mysql
----------------------------      ====   \\\\    success pod deploy in kubernetes 
---
apiVersion: v1
kind: PersistentVolume
metadata: 
  name: pv
spec: 
  storageClassName: ssd
  capacity: 
    storage: 5Gi
  accessModes: 
    - ReadWriteOnce
  hostPath: 
    path: "/tmp/data"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: pvc
spec: 
  storageClassName: ssd
  accessModes: 
    - ReadWriteOnce
  resources:
     requests:
       storage: 4Gi
---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: mysql
  labels: 
    app: mysql
spec: 
   replicas: 3
   selector: 
      matchLabels: 
        app: mysql
   template: 
     metadata:
       labels:   
         app: mysql
     spec: 
        containers: 
          - name: mysql
            image: mysql
            env: 
             - name: MYSQL_ROOT_PASSWORD
               value: anji123
            volumeMounts:
              - name: mysql1
                mountPath: /var/lib/mysql/                               
            ports: 
              - containerPort: 3306
        volumes: 
          - name: mysql1
            persistentVolumeClaim: 
               claimName: pvc
---
apiVersion: v1
kind: Service
metadata: 
   name: outside
spec: 
  selector: 
    app: mysql
  ports: 
    - port: 3306
  clusterIP: None    
anji@anji:~/deployment$  kubectl delete  pod,deployment,service,pv,pvc --all 
==================================================================================="
https://sesamedisk.com/deploy-wordpress-on-k8s/
              kustomization.yaml           -  importent  ok  
secretGenerator:
- name: mysql-password
  literals:
  - password=Mysql.Root2021@
- name: mysql-user
  literals:
  - username=userwp
- name: mysql-user-password
  literals:
  - passworduser=Mysql.User2021@
- name: mysql-database
  literals:
  - database=multitenant_wp
------
       anji@anji:~/deployment$ nano kustomization.yaml
anji@anji:~/deployment$ kubectl apply -k .
secret/mysql-database-4f74mgddt5 created
secret/mysql-password-f547bhm8mc created
secret/mysql-user-4t5mcf8dkm created
secret/mysql-user-password-9m7k5b4k2m created
anji@anji:~/deployment$ 
=========================================================================="
https://www.rancher.cn/running-highly-available-wordpress-mysql-kubernetes

# storage-class.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: slow
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
  zone: us-central1-a

---
# mysql-services.yaml
# Headless service for stable DNS entries of StatefulSet members.
apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  clusterIP: None
  selector:
    app: mysql
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: mysql
spec:
  selector:
    matchLabels:
      app: mysql
  serviceName: mysql
  replicas: 3
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
      - name: init-mysql
        image: mysql:5.7
        command:
        - bash
        - "-c"
        - |
          set -ex
          # Generate mysql server-id from pod ordinal index.
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          echo [mysqld] > /mnt/conf.d/server-id.cnf
          # Add an offset to avoid reserved server-id=0 value.
          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf
          # Copy appropriate conf.d files from config-map to emptyDir.
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map
      - name: clone-mysql
        image: gcr.io/google-samples/xtrabackup:1.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          # Skip the clone if data already exists.
          [[ -d /var/lib/mysql/mysql ]] && exit 0
          # Skip the clone on master (ordinal index 0).
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          [[ $ordinal -eq 0 ]] && exit 0
          # Clone data from previous peer.
          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
          # Prepare the backup.
          xtrabackup --prepare --target-dir=/var/lib/mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
      containers:
      - name: mysql
        image: mysql:5.7
        env:
        - name: MYSQL_ALLOW_EMPTY_PASSWORD
          value: "1"
        ports:
        - name: mysql
          containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
        livenessProbe:
          exec:
            command: ["mysqladmin", "ping"]
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          exec:
            # Check we can execute queries over TCP (skip-networking is off).
            command: ["mysql", "-h", "127.0.0.1", "-e", "SELECT 1"]
          initialDelaySeconds: 5
          periodSeconds: 2
          timeoutSeconds: 1
      - name: xtrabackup
        image: gcr.io/google-samples/xtrabackup:1.0
        ports:
        - name: xtrabackup
          containerPort: 3307
        command:
        - bash
        - "-c"
        - |
          set -ex
          cd /var/lib/mysql

          # Determine binlog position of cloned data, if any.
          if [[ -f xtrabackup_slave_info ]]; then
            # XtraBackup already generated a partial "CHANGE MASTER TO" query
            # because we're cloning from an existing slave.
            mv xtrabackup_slave_info change_master_to.sql.in
            # Ignore xtrabackup_binlog_info in this case (it's useless).
            rm -f xtrabackup_binlog_info
          elif [[ -f xtrabackup_binlog_info ]]; then
            # We're cloning directly from master. Parse binlog position.
            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
            rm xtrabackup_binlog_info
            echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
                  MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
          fi

          # Check if we need to complete a clone by starting replication.
          if [[ -f change_master_to.sql.in ]]; then
            echo "Waiting for mysqld to be ready (accepting connections)"
            until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done

            echo "Initializing replication from clone position"
            # In case of container restart, attempt this at-most-once.
            mv change_master_to.sql.in change_master_to.sql.orig
            mysql -h 127.0.0.1 <<EOF
          $(<change_master_to.sql.orig),
            MASTER_HOST='mysql-0.mysql',
            MASTER_USER='root',
            MASTER_PASSWORD='',
            MASTER_CONNECT_RETRY=10;
          START SLAVE;
          EOF
          fi

          # Start a server to send backups when requested by peers.
          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
            "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
---
# nfs.yaml
# Define the persistent volume claim
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs
  labels:
    demo: nfs
  annotations:
    volume.alpha.kubernetes.io/storage-class: any
spec:
  accessModes: [ "ReadWriteOnce" ]
  resources:
    requests:
      storage: 200Gi

---
# Define the Replication Controller
apiVersion: v1
kind: ReplicationController
metadata:
  name: nfs-server
spec:
  replicas: 1
  selector:
    role: nfs-server
  template:
    metadata:
      labels:
        role: nfs-server
    spec:
      containers:
      - name: nfs-server
        image: gcr.io/google_containers/volume-nfs:0.8
        ports:
          - name: nfs
            containerPort: 2049
          - name: mountd
            containerPort: 20048
          - name: rpcbind
            containerPort: 111
        securityContext:
          privileged: true
        volumeMounts:
          - mountPath: /exports
            name: nfs-pvc
      volumes:
        - name: nfs-pvc
          persistentVolumeClaim:
            claimName: nfs

---
# Define the Service
kind: Service
apiVersion: v1
metadata:
  name: nfs-server
spec:
  ports:
    - name: nfs
      port: 2049
    - name: mountd
      port: 20048
    - name: rpcbind
      port: 111
  selector:
    role: nfs-server
---
# wordpress.yaml
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  type: LoadBalancer

---

apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs
spec:
  capacity:
    storage: 20G
  accessModes:
    - ReadWriteMany
  nfs:
    # FIXME: use the right IP
    server: <IP of the NFS Service>
    path: "/"

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: ""
  resources:
    requests:
      storage: 20G

---

apiVersion: apps/v1beta1 # for versions before 1.8.0 use apps/v1beta1
kind: Deployment
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: frontend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: frontend
    spec:
      containers:
      - image: wordpress:4.9-apache
        name: wordpress
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql
        - name: WORDPRESS_DB_PASSWORD
          value: ""
        ports:
        - containerPort: 80
          name: wordpress
        volumeMounts:
        - name: wordpress-persistent-storage
          mountPath: /var/www/html
      volumes:
      - name: wordpress-persistent-storage
        persistentVolumeClaim:
            claimName: nfs

===========================================================================================

https://gopensource.com/deploy-wordpress-blog-with-mysql-on-a-bare-metal-kubernetes-cluster-8a9323c0f4c9


cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: mysql-pass
type: Opaque
data:
  password: TXlQYXNzd29yZCEkJHRyMG5
EOF

---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: wp-pv
  labels:
    type: local
spec:
  capacity:
    storage: 3Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/home/shashank/wordpress"
---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: wp-mysql-pv
  labels:
    type: local
spec:
  capacity:
    storage: 3Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/home/shashank/wordpress-mysql"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wp-pv-claim
  labels:
    app: wordpress
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pv-claim
  labels:
    app: wordpress
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
---
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  type: NodePort
---
apiVersion: v1
kind: Service
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterIP: 10.96.0.100
---
apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: frontend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: frontend
    spec:
      containers:
      - image: wordpress:5.4.1-apache
        name: wordpressenv:
        - name: WORDPRESS_DB_HOST
          value: 10.96.0.100
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass
              key: password
        ports:
        - containerPort: 80
          name: wordpress
        volumeMounts:
        - name: wordpress-persistent-storage
          mountPath: /var/www/html
      volumes:
      - name: wordpress-persistent-storage
        persistentVolumeClaim:
          claimName: wp-pv-claim
---
apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: mysql
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: mysql
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass
              key: password
        ports:
        - containerPort: 3306
          name: mysql
        readinessProbe:
          tcpSocket:
            port: 3306
          initialDelaySeconds: 5
          periodSeconds: 10
        livenessProbe:
          tcpSocket:
            port: 3306
          initialDelaySeconds: 15
          periodSeconds: 20
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pv-claim
#################################################################################################33333"
https://blog.litespeedtech.com/2022/09/26/wordpress-woocommerce-and-kubernetes-with-litespeed-ingress-controller/  
https://docs.portworx.com/operations/operate-kubernetes/application-install-with-kubernetes/wordpress/
https://hewlettpackard.github.io/hpe-solutions-hpecp/5.0/Validate%20HPE%20Ezmeral%20Container%20Platform%20deployment/Validate%20HPE%20Ezmeral%20Container%20Platform%20deployment.html#deploying-wordpress-application-with-hpe-ezmeral-data-fabric
https://www.goglides.dev/roshan_thapa/how-to-deploy-wordpress-and-mysql-on-kubernetes-5f4d
https://github.com/kubernetes/examples/blob/master/mysql-wordpress-pd/wordpress-deployment.yaml
https://medium.com/codex/how-to-deploy-wordpress-on-kubernetes-part-2-df1cc9cbaa2e
https://phoenixnap.com/kb/kubernetes-wordpress  ok 
       

apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv
  namespace: woo
spec:
  storageClassName: do-block-storage
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/var/lib/mysql"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pv-claim
  namespace: woo
spec:
  storageClassName: do-block-storage
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: wordpress-pv
  namespace: woo
spec:
  storageClassName: do-block-storage
  capacity: 
    storage: 30Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/var/www"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wordpress-pv-claim
  namespace: woo
spec:
  storageClassName: do-block-storage
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 30Gi
---
apiVersion: v1
kind: Service
metadata: 
  name: mysql-wp
  namespace: woo
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-wp
  namespace: woo
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: mysql
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: mysql
    spec:
      containers:
      - image: mysql:latest
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-password
              key: password
        - name: MYSQL_USER
          valueFrom:
            secretKeyRef:
              name: mysql-user
              key: username
        - name: MYSQL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-user-password
              key: password
        - name: MYSQL_DATABASE
          valueFrom:
            secretKeyRef:
              name: mysql-database
              key: database
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pv-claim
---
apiVersion: v1
kind: Service
metadata: 
  name: wordpress
  namespace: woo
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: web
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
  namespace: woo
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: web
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: web
    spec:
      containers:
      - image: wordpress:php8.1-apache
        name: wordpress
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql-wp:3306
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-user-password
              key: password
        - name: WORDPRESS_DB_USER
          valueFrom:
            secretKeyRef:
              name: mysql-user
              key: username
        - name: WORDPRESS_DB_NAME
          valueFrom:
            secretKeyRef:
              name: mysql-database
              key: database
        ports:
        - containerPort: 80
          name: wordpress
        volumeMounts:
        - name: persistent-storage
          mountPath: /var/www/html
      volumes:
      - name: persistent-storage
        persistentVolumeClaim:
          claimName: wordpress-pv-claim
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: wordpress
  namespace: woo
  annotations:
    kubernetes.io/ingress.class: litespeedtech.com/lslbd
    cert-manager.io/cluster-issuer: letsencrypt-production
spec:
  rules:
  - host: YOUR DNS NAME
    http:
     paths:
     - path: "/"
       pathType: Prefix
       backend:
         service:
           name: wordpress
           port:
             number: 80
  tls:
  - hosts:
    - YOUR DNS NAME
    secretName: YOUR DNS NAME
 
++++++++++++++++++++++++++++++++++++++++++   

---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: mysql
  labels: 
    app: mysql
spec: 
  replicas: 2
  selector: 
    matchLabels:
       app: wordpress
  template: 
    metadata: 
       labels: 
         app: wordpress
    spec: 
       containers: 
         - name: mysql
           image: mysql
           env: 
           - name: MYSQL_ROOT_PASSWORD
             valueFrom:
               secretKeyRef:
                 name: secret
                 key: mysqlrootpwd
          
           - name: MYSQL_USER
             valueFrom: 
               secretKeyRef: 
                 name: secret
                 key:  mysqlusername
       
           - name: MYSQL_PASSWORD
             valueFrom: 
               secretKeyRef:
                  name: secret
                  key: mysqluserpasswd
          
           - name: MYSQL_DATABASE
             valueFrom:
                secretKeyRef:
                  name: secret
                  key: mydqldbname
           ports: 
            - containerPort: 3306
           volumeMounts: 
             - name: mysql
               mountPath: /var/lib/mysql
       volumes:
         - name: mysql
           persistentVolummeClaim: 
               claimName: pvc                                       
---
apiVersion: v1
kind: Secret
metadata: 
  name: secret
type:  Opaque  
data: 
  mysqlrootpwd: "8fsdfdsfd7fd"
  mysqlusername: "anji"
  mysqluserpasswd: "admin@123"
  mydqldbname: "olxdb"

---                   
apiVersion: v1
kind: ConfigMap
metadata:
  name: auth-configmap
data:
  MYSQL_HOST: host.minikube.internal
  MYSQL_USER: auth_user
  MYSQL_DB: auth
  MYSQL_PORT: "3306"

---
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: wordpress
   labels: 
     app: wordpress
spec: 
  replicas: 3
  selector: 
    matchLbels: 
       app: wordpress
  template: 
    metadata: 
       labels: 
          app: wordpress
    spec: 
      containers: 
        - name: wordpress
          image: wordpress
          env:
            - name: WORDPRESS_DB_HOST
              value: localhost
            - name: WORDPRESS_DB_USER
              value: anji
            - name: WORDPRESS_DB_PASSWORD
              value: anji@123
            - name: WORDPRESS_DB_NAME
              value: anjidb    
          ports: 
            - containerPort: 80
---
apiVersion: v1  
kind: Secret  
metadata:  
  name: mysql-secrets
type: Opaque  
data:  
  mysql-root-password: cm9vdHBhc3N3b3Jk
  mysql-user: Y29uZmx1ZW5jZQ==
  mysql-password: Y29uZmx1ZW5jZXBhc3N3b3Jk
---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: mysql
  labels: 
    app: mysql
spec: 
  replicas: 2
  selector: 
    matchLabels:
       app: wordpress
  template: 
    metadata: 
       labels: 
         app: wordpress
    spec: 
       containers: 
         - name: mysql
           image: mysql
           env: 
           - name: MYSQL_ROOT_PASSWORD
             valueFrom:
               secretKeyRef:
                 name: secret
                 key: mysqlrootpwd
          
           - name: MYSQL_USER
             valueFrom: 
               secretKeyRef: 
                 name: secret
                 key:  mysqlusername
       
           - name: MYSQL_PASSWORD
             valueFrom: 
               secretKeyRef:
                  name: secret
                  key: mysqluserpasswd
          
           - name: MYSQL_DATABASE
             valueFrom:
                secretKeyRef:
                  name: secret
                  key: mydqldbname
           ports: 
            - containerPort: 3306
           volumeMounts: 
             - name: mysql
               mountPath: /var/lib/mysql
       volumes:
         - name: mysql
           persistentVolummeClaim: 
               claimName: pvc                                       
---
apiVersion: v1
kind: Secret
metadata: 
  name: secret
type:  Opaque  
data: 
  mysqlrootpwd: "8fsdfdsfd7fd"
  mysqlusername: "anji"
  mysqluserpasswd: "admin@123"
  mydqldbname: "olxdb"

---                   
apiVersion: v1
kind: ConfigMap
metadata:
  name: auth-configmap
data:
  MYSQL_HOST: host.minikube.internal
  MYSQL_USER: auth_user
  MYSQL_DB: auth
  MYSQL_PORT: "3306"
---
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginx
   labels: 
     app: nginx
spec: 
  replicas: 3
  selector: 
    matchLabels:
      app: nginx
  template:
    metadata: 
      labels: 
        app: nginx
    spec: 
      containers:
         - name: nginx
           image: nginx
           ports: 
             - containerPort: 80
           volumeMounts:
              - name: nginx
                mountPath: /usr/share/nginx/http
      volumes: 
        - name: nginx
          persistentVolumeClaim:
            claimName: pvc
---
apiVersion: v1
kind: PersistentVolume
metadata: 
   name: pv
spec: 
  storageClassName: ssd
  capacity:
    storage: 5Gi
  accessModes:
    - RWO
  hostPath:
   path: "/opt/data"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: pvc
spec: 
  storageClassName: ssd
  accessModes:
   - RWO
  resources:
     requests:
       storage: 2Gi  
---
apiVersion: v1
kind: Service
metadata: 
  name: nginx
spec: 
  selector: 
     app: nginx
  ports: 
     - port: 80
       targetPort: 8080
  type: LoadBalancer
---
apiVeraion: networking.k8s.io/v1
kind: Ingress
metadata: 
   name: myingress
spec: 
  rules: 
   - host: anji.com
     http: 
       paths:

         - path: /books
           pathType: Prefix
           backend: 
             service: 
               name: nginx
               port: 
                 number: 80
         - path: /apps
           pathType: Prefix
           backend: 
              name: flsk
              port: 500

         - path: /data
           pathType: Prefix
           backend: 
             name: mysql
             port: 
               number: 3306
         - path: /details
           pathType: Prefix
           backend: 
             name: tomcat
             port: 
               number: 8080
         - path: /newoffers
           pathType: prifix
           backend:
             name: postgresql
             port: 
               number: 5432
---

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress-1
spec:
  ingressClassName: nginx
  rules:
  - host: test.com
    http:
      paths:
      - path: /foo/bar
        pathType: Prefix
        backend:
          service:
            name: service1
            port:
              number: 80
      - path: /foo/bar/
        pathType: Prefix
        backend:
          service:
            name: service2
            port:
              number: 80


################################################################################################################"
https://www.serverlab.ca/tutorials/containers/kubernetes/deploying-wordpress-on-kubernetes/
---
apiVersion: v1
kind: Pod
metadata:
  name: wordpress-blog
spec:
  containers:
  - name: wordpress-blog
    image: wordpress:5.2
    env:
    - name: WORDPRESS_DB_HOST
      value: database.host.name:3306
    - name: WORDPRESS_DB_NAME
      value: wordpress
    - name: WORDPRESS_DB_PREFIX
      value: myblog_ 
---
---
apiVersion: v1
kind: Service
metadata:
  name: wordpress-blog-service
spec:
  type: LoadBalancer
  selector:
    application: wordpress-blog
  ports:
  - port: 80
  targetPort:
  - port: 80 
####################################################################33
https://matthewdavis.io/highly-available-wordpress-on-kubernetes/
https://github.com/anjilinux/project--mysql        
https://github.com/anjilinux/project-nfs-kubernetes
https://github.com/anjilinux/project-k8-byexamples-cert-manager
https://github.com/anjilinux/project-ingress-controller

apiVersion: apps/v1                                         ==--  success wordpress 
kind: Deployment
metadata: 
  name: wordpress
  labels: 
    app: wordpress
spec: 
   replicas: 1
   selector: 
     matchLabels: 
       app: wordpress
   template: 
     metadata: 
        labels: 
           app: wordpress
     spec: 
       containers: 
           - name: wordpress
             image: wordpress
             env:
               - name: WORDPRESS_DB_HOST
                 value: mysql
               - name: WORDPRESS_DB_USER
                 value: wordpress
               - name: WORDPRESS_DB_PASSWORD
                 value: wordpress
               - name: WORDPRESS_DB_NAME
                 value: wordpress
             ports: 
               - containerPort: 80
======================================================
docker pull wordpress

-e WORDPRESS_DB_HOST=...
-e WORDPRESS_DB_USER=...
-e WORDPRESS_DB_PASSWORD=...
-e WORDPRESS_DB_NAME=...
-e WORDPRESS_TABLE_PREFIX=...

docker run --name wordpress -p 8085:80 -d wordpress

root@anji:~# docker run --name wordpress -p 8085:80 -d wordpress
ad4c8635296a180c1e6e6f9a023af104b354dc8e932dccfd058881587f70d1d1
root@anji:~# docker images 
REPOSITORY           TAG       IMAGE ID       CREATED       SIZE
mysql/mysql-server   latest    1d9c2219ff69   2 weeks ago   496MB
wordpress            latest    fcd4967b9728   3 weeks ago   615MB
root@anji:~# docker ps 
CONTAINER ID   IMAGE       COMMAND                  CREATED          STATUS          PORTS                                   NAMES
ad4c8635296a   wordpress   "docker-entrypoint.sâ€¦"   30 seconds ago   Up 30 seconds   0.0.0.0:8085->80/tcp, :::8085->80/tcp   wordpress

http://localhost:8085/wp-admin/setup-config.php?step=1


          docker run --name wordpress -p 8085:80 -d wordpress
root@anji:~# docker  ps 
CONTAINER ID   IMAGE       COMMAND                  CREATED          STATUS          PORTS                                   NAMES
7a909a002009   wordpress   "docker-entrypoint.sâ€¦"   48 seconds ago   Up 46 seconds   0.0.0.0:8085->80/tcp, :::8085->80/tcp   wordpress "

https://hub.docker.com/_/wordpress
version: '3.1'                            \\/   success full k 

services:

  wordpress:
    image: wordpress
    restart: always
    ports:
      - 8082:80
    environment:
      WORDPRESS_DB_HOST: db
      WORDPRESS_DB_USER: anjireddy
      WORDPRESS_DB_PASSWORD: anjireddy
      WORDPRESS_DB_NAME: anjireddy
    volumes:
      - wordpress:/var/www/html

  db:
    image: mysql:5.7
    restart: always
    environment:
      MYSQL_DATABASE: anjireddy
      MYSQL_USER: anjireddy
      MYSQL_PASSWORD: anjireddy
      MYSQL_RANDOM_ROOT_PASSWORD: '1'
    volumes:
      - db:/var/lib/mysql

volumes:
  wordpress:
  db:

docker-compose -f stack.yaml up 

anji@anji:~/deployment$ docker ps
CONTAINER ID   IMAGE       COMMAND                  CREATED          STATUS          PORTS                                   NAMES
e2e0e2795cf1   wordpress   "docker-entrypoint.sâ€¦"   15 minutes ago   Up 15 minutes   0.0.0.0:8088->80/tcp, :::8088->80/tcp   secrete-wordpress-1
d63a2e6e6ada   mysql:5.7   "docker-entrypoint.sâ€¦"   22 minutes ago   Up 15 minutes   3306/tcp, 33060/tcp                     secrete-db-1
anji@anji:~/deployment$ 


====
[+] Running 2/2
 â ¿ Container secrete-db-1         Stopped                                                                                           1.8s
 â ¿ Container secrete-wordpress-1  Stopped                                                                                           1.4s
canceled



==========
FROM wordpress:apache
WORKDIR /usr/src/wordpress
RUN set -eux; \
	find /etc/apache2 -name '*.conf' -type f -exec sed -ri -e "s!/var/www/html!$PWD!g" -e "s!Directory /var/www/!Directory $PWD!g" '{}' +; \
	cp -s wp-config-docker.php wp-config.php
COPY custom-theme/ ./wp-content/themes/custom-theme/
COPY custom-plugin/ ./wp-content/plugins/custom-plugin/
-----------------
$ docker run ... \
	--read-only \
	--tmpfs /tmp \
	--tmpfs /run \
	--mount type=...,src=...,dst=/usr/src/wordpress/wp-content/uploads \
	... \
	--env WORDPRESS_DB_HOST=... \
	--env WORDPRESS_AUTH_KEY=... \
	--env ... \
	custom-wordpress:tag
  ------------
   docker run -it --rm \
	--volumes-from some-wordpress \
	--network container:some-wordpress \
	-e WORDPRESS_DB_USER=... \
	-e WORDPRESS_DB_PASSWORD=... \
	# [and other used environment variables]
	wordpress:cli user list
----------------------------------------
 docker run --name wordpress -e WORDPRESS_DB_PASSWORD_FILE=/run/secrete/pwd ... -d wordpress

 docker run --name wordpress -e WORDPRESS_DB_PASSWORD_FILE=/run/secrete/pwd  -d wordpress

root@anji:/run/secrete# docker run --name wordpress -p 8086:80 -e WORDPRESS_DB_PASSWORD_FILE=/run/secrete/pwd -d wordpress"
#######################################################################################################################################
https://medium.com/@containerum/how-to-deploy-wordpress-and-mysql-on-kubernetes-bda9a3fdd2d5

---
apiVersion: v1
kind: Secret
metadata:
  name: mysql-pass
type: Opaque
data:
  password: YWRtaW4=
---
# Create PersistentVolume
# change the ip of NFS server
apiVersion: v1
kind: PersistentVolume
metadata:
  name: wordpress-persistent-storage
  labels:
    app: wordpress
    tier: frontend
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 172.31.39.63
    # Exported path of your NFS server
    path: "/html"

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-persistent-storage
  labels:
    app: wordpress
    tier: mysql
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 172.31.39.63
    # Exported path of your NFS server
    path: "/mysql"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wordpress-persistent-storage
  labels:
    app: wordpress
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 6Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-persistent-storage
  labels:
    app: wordpress
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 6Gi
---
apiVersion: v1
kind: Service
metadata:
  name: wordpress-mysql   # will be used as a value in
  labels:                 # WORDPRESS_DB_HOST in wordpress-deploy.yml
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterIP: None
---
apiVersion:  apps/v1beta2 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: mysql
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: mysql
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass        # the one generated before in secret.yml
              key: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage  # which data will be stored
          mountPath: "/var/lib/mysql"
      volumes:
      - name: mysql-persistent-storage    # PVC
        persistentVolumeClaim:
          claimName: mysql-persistent-storage                  
---
# create a service for wordpress
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  type: ClusterIP---
apiVersion: apps/v1beta2 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: frontend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: frontend
    spec:
      containers:
      - image: wordpress:4.8-apache
        name: wordpress
        env:
        - name: WORDPRESS_DB_HOST
          value: wordpress-mysql
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass          # generated before in secret.yml
              key: password
        ports:
        - containerPort: 80
          name: wordpress
        volumeMounts:
        - name: wordpress-persistent-storage
          mountPath: "/var/www/html"          # which data will be stored
      volumes:
      - name: wordpress-persistent-storage
        persistentVolumeClaim:
          claimName: wordpress-persistent-storage
======+++
apiVersion: v1
kind: Service
metadata:
  name: wordpress-mysql   # will be used as a value in
  labels:                 # WORDPRESS_DB_HOST in wordpress-deploy.yml
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterIP: None         ##  see diffrent
           
---
# create a service for wordpress
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  type: ClusterIP---        ## see  diffrent 

+++++++++=
apiVersion: v1
kind: Service
metadata:
  name: mwithword  # will be used as a value in
  labels:                 # WORDPRESS_DB_HOST in wordpress-deploy.yml
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterIP: None         ##  see diffrent
           
---
# create a service for wordpress
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  type: NodePort        ## see  diffrent 
==========

---
apiVersion: v1
kind: Service
metadata:
  name: mwithw   # will be used as a value in
  labels:                 # WORDPRESS_DB_HOST in wordpress-deploy.yml
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterIP: None         ##  see diffrent
           
---
# create a service for wordpress
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  type:       ## see  diffrent 
-------------------------------

  selector:
    app: mariadb
  ports:
    - protocol: TCP
      port: 3306
      targetPort: 3306
  clusterIP: None

 spec:
  selector:
    app: wordpress
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP #default
      nodePort: 31000
  type: LoadBalancer 
  -----
https://clearlinux.org/blogs-news/deploy-scalable-wordpress-kubernetes-clear-linux-os-containers    = best helpfull
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
      nodePort: 30180
  selector:
    app: wordpress
    tier: frontend
#  type: LoadBalancer
  type: NodePort

======================
https://github.com/anjilinux/Scalable-WordPress-deployment-on-Kubernetes/edit/master/wordpress-deployment.yaml
https://github.com/anjilinux/Scalable-WordPress-deployment-on-Kubernetes
---
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
    tier: frontend
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  type: NodePort
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: wp-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/tmp/wp/data"
  persistentVolumeReclaimPolicy: Recycle
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wp-pv-claim
  labels:
    app: wordpress
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
---
apiVersion: apps/v1   # versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: wordpress
  labels:
    app: wordpress
    tier: frontend
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: frontend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: frontend
    spec:
      containers:
        - image: wordpress:5.8.1-php7.4
          name: wordpress
          env:
            - name: WORDPRESS_DB_HOST
              value: wordpress-mysql
            - name: WORDPRESS_DB_NAME
              value: wordpress
            - name: WORDPRESS_DB_USER
              value: root
            - name: WORDPRESS_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mysql-pass
                  key: password
          ports:
            - containerPort: 80
              name: wordpress
          volumeMounts:
            - name: wordpress-persistent-storage
              mountPath: /var/www/html
      volumes:
        - name: wordpress-persistent-storage
          persistentVolumeClaim:
            claimName: wp-pv-claim

#########################################################################################################
      successfully   successs  mysql with wordpress  with mysql and  services  nodport 

apiVersion: v1
kind: PersistentVolume
metadata: 
   name: pvmysql
spec: 
   storageClassName: ssd
   capacity: 
      storage: 5Gi
   accessModes: 
     - ReadWriteOnce
   hostPath: 
     path: "/opt/data/"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: pvcmysql
spec: 
  storageClassName: ssd
  accessModes: 
     - ReadWriteOnce
  resources: 
     requests:
        storage: 4Gi
---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: mysql
spec: 
   replicas: 1
   selector: 
     matchLabels: 
        app: wordpress
   template: 
     metadata: 
       labels: 
         app: wordpress
     spec: 
        containers: 
          - name: mysql
            image: mysql
            env: 
             - name:    MYSQL_DATABASE  
               value: anjireddy
             - name: MYSQL_USER 
               value: anjireddy
             - name:  MYSQL_PASSWORD  
               value: anjireddy
             - name: MYSQL_RANDOM_ROOT_PASSWORD
               value:  '1'                
            ports: 
               - containerPort: 3306
            volumeMounts:
              - name: mysql
                mountPath: /var/lib/mysql
        volumes:
           - name: mysql
             persistentVolumeClaim: 
                claimName: pvcmysql           
---
apiVersion: v1
kind: PersistentVolume
metadata: 
  name: pvword
spec:
  storageClassName: nvme
  capacity: 
    storage: 5Gi
  accessModes: 
    - ReadWriteOnce
  hostPath:
     path: "/media/data/"      
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:  
  name: pvcword
spec: 
  storageClassName: nvme
  accessModes:
     - ReadWriteOnce
  resources: 
    requests: 
      storage: 4Gi

---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: wordpress
  labels: 
    app: wordpress
spec: 
  replicas: 1
  selector:
    matchLabels: 
      app: wordpress
  template:
    metadata: 
      labels: 
        app: wordpress
    spec: 
      containers:
        - name:  wordpress
          image: wordpress
          env:  
            - name:  WORDPRESS_DB_HOST 
              value: db
            - name:  WORDPRESS_DB_USER 
              value: anjireddy
            - name:  WORDPRESS_DB_PASSWORD 
              value: anjireddy
            - name:  WORDPRESS_DB_NAME 
              value: anjireddy       
          volumeMounts: 
            - name: wordpress
              mountPath: /var/www/
      volumes:
          - name: wordpress
            persistentVolumeClaim: 
              claimName: pvcword          
---
apiVersion: v1
kind: Service
metadata: 
  name: wordservice
spec: 
   selector: 
      app: wordpress

   ports: 
     - port: 80
       nodePort: 32000
   type: NodePort
---
apiVersion: v1
kind: Service
metadata: 
   name: mwithword
spec: 
  selector: 
    app: wordpress
  ports: 
    - port: 3306   
  clusterIP: None           



+++
anji@anji:~$  kubectl get pod,deployment,pv,pvc,service  -o wide 
NAME                             READY   STATUS    RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES
pod/mysql-5f49cd996c-ddzc2       1/1     Running   0          4m13s   10.44.0.1   worker   <none>           <none>
pod/wordpress-79fd469969-498ms   1/1     Running   0          4m13s   10.44.0.2   worker   <none>           <none>

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES      SELECTOR
deployment.apps/mysql       1/1     1            1           4m13s   mysql        mysql       app=wordpress
deployment.apps/wordpress   1/1     1            1           4m14s   wordpress    wordpress   app=wordpress

NAME                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   REASON   AGE     VOLUMEMODE
persistentvolume/pvmysql   5Gi        RWO            Retain           Bound    default/pvcmysql   ssd                     4m14s   Filesystem
persistentvolume/pvword    5Gi        RWO            Retain           Bound    default/pvcword    nvme                    4m14s   Filesystem

NAME                             STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE     VOLUMEMODE
persistentvolumeclaim/pvcmysql   Bound    pvmysql   5Gi        RWO            ssd            4m14s   Filesystem
persistentvolumeclaim/pvcword    Bound    pvword    5Gi        RWO            nvme           4m14s   Filesystem

NAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE     SELECTOR
service/kubernetes    ClusterIP   10.96.0.1        <none>        443/TCP        148m    <none>
service/mwithword     ClusterIP   None             <none>        3306/TCP       4m14s   app=wordpress
service/wordservice   NodePort    10.106.117.167   <none>        80:32000/TCP   4m14s   app=wordpress
#####################################################################################################33

Error establishing a database connection This either means that the username and password information in your wp-config. php file is incorrect or we can't contact the database server at localhost:3306. This could mean your host's database server is down. Are you sure you have the correct username and password?

he default credentials for your newly created database are:

    DB_NAME: Your chosen db name (case sensitive)
    DB_USERNAME: root
    DB_PASSWORD: ""
    DB_HOST: localhost

/** The name of the database for WordPress */
define('DB_NAME', 'case sensitive chosen db name');

/** MySQL database username */
define('DB_USER', 'root');

/** MySQL database password */
define('DB_PASSWORD', '');

/** MySQL hostname */
define('DB_HOST', 'localhost');

----------------
How To Fix The â€œError Establishing a Database Connectionâ€?

    Check Your Database Login Credentials.
    Repair Corrupt WordPress Database.
    Fix Corrupt WordPress Files.
    Check for Issues With Your Database Server.
    Restore Latest Backup.
https://www.youtube.com/watch?v=yX1ZxOab7Cs&t=89s

// ** MySQL settings - You can get this info from your web host ** //

/** The name of the database for WordPress */
define( 'DB_NAME', 'database_name_here' );

/** MySQL database username */
define( 'DB_USER', 'username_here' );

/** MySQL database password */
define( 'DB_PASSWORD', 'password_here' );

/** MySQL hostname */
define( 'DB_HOST', 'localhost' );

-----------------------------
try create on mysql pod:
mysql -u root -p
mysql> CREATE DATABASE wordpress;

    Add in wordpress-deployment.yaml additional env with user for DB
    env:
    - name: WORDPRESS_DB_USER
    value: "root"

for me worked

Regards

--------------------------

https://stackoverflow.com/questions/54446112/error-establishing-a-database-connection-using-wordpress-mysql-nginx-on-kube

"Error establishing a database connection" using Wordpress, MySQL, Nginx on Kubernetes

deploy mysql using helm
    expose the pod as a clusterIP service and dump my tables onto the database
    deploy my wordpress/php app in the same pod with an nginx container
    expose clusterIP service and set up ingress / TLS

The database seems to be working, I can connect to it and see my tables using the following command: echo "mysql -pXXX" | kubectl exec -it <mysql-pod>. Step 4 (ssl cert and ingress) is also working and no problem there. Creating the two pods (my app and mysql), and adding the config files result in this message when I try to access my domain:


helm install --name mysql --set \
mysqlRootPassword=xxx,mysqlUser=xxx,mysqlPassword=xxx, \
mysqlDatabase=xxx,persistence.size=50Gi \
stable/mysql

apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  replicas: 1
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
      - image: nginx:alpine
        name: nginx
        env:
        - name: WP_HOST
          value: wordpress
        - name: DB_HOST
          value: mysql:3306
        - name: DB_NAME
          value: xxx
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql
              key: password
        ports:
        - containerPort: 443
        - containerPort: 80
        volumeMounts:
          - name: wordpress-persistent-storage
            mountPath: /var/www/html
          - name: wp-config
            mountPath: "/etc/nginx/conf.d"
      - image: my-wordpress-php-app
        name: wordpress
        env:
        - name: MY_DB_HOST
          value: mysql:3306
        - name: MY_DB_NAME
          value: xxx
        - name: MY_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql
              key: password
        - name: MY_WP_HOME
          value: "https://example.com"
        - name: MY_WP_SITEURL
          value: "https://example.com"
        - name: WP_DEBUG_LOG
          value: "true"
        - name: WP_DEBUG
          value: "true"
        ports:
        - containerPort: 9000
        volumeMounts:
        - name: wordpress-persistent-storage
          mountPath: /var/www/html
      volumes:
      - name: wordpress-persistent-storage
        persistentVolumeClaim:
          claimName: wordpress-volumeclaim
      - name: wp-config
        configMap:
          name: wp-config
          items:
          - key: wp.conf
            path: wp.conf
      imagePullSecrets:
      - name: regcred

---
  listen 80;
    listen 443 ssl;
    server_name $SITE_URL;

    root /var/www/html;
    index index.php;

    access_log /var/log/nginx/access.log;
    error_log /var/log/nginx/error.log;

    gzip off;

    types {
        ...
    }

    location xxx {
        rewrite .* /index.php;
        ...
    }

    location ~ '\.php$' {
        try_files $uri =404;
        fastcgi_split_path_info ^(.+\.php)(/.+)$;
        fastcgi_pass wordpress:9000;
        fastcgi_index index.php;
        include fastcgi_params;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        fastcgi_param PATH_INFO $fastcgi_path_info;
        ...
    }

    location / {
        autoindex off;
        ...
    }
}


    Both of my services (mysql and wordpress/nginx) are ClusterIP type. In my mysql service I have the following:

    - port: 3306
      targetPort: 3306

And in my wordpress service I have the following:

  - name: wordpress
    port: 9000
    targetPort: 9000
  - port: 80
    targetPort: 80
    protocol: TCP
    name: http
  - port: 443
    targetPort: 443
    protocol: TCP
    name: https


Mysql pod logs

    MySQL init process in progress...
    Warning: Unable to load '/usr/share/zoneinfo/Factory' as time zone. Skipping it.
    Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it.
    Warning: Unable to load '/usr/share/zoneinfo/leap-seconds.list' as time zone. Skipping it.
    Warning: Unable to load '/usr/share/zoneinfo/posix/Factory' as time zone. Skipping it.
    Warning: Unable to load '/usr/share/zoneinfo/right/Factory' as time zone. Skipping it.
    Warning: Unable to load '/usr/share/zoneinfo/zone.tab' as time zone. Skipping it.
    mysql: [Warning] Using a password on the command line interface can be insecure.
    MySQL init process done. Ready for start up.

Nginx container logs

    [11:15:03 +0000] "GET /robots.txt HTTP/1.1" 500 262 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
    10.20.0.128 - - [12:17:48 +0000] "GET / HTTP/1.1" 500 262 "-" "Python/3.6 aiohttp/3.4.4"
    10.20.0.128 - - [16:04:42 +0000] "GET / HTTP/1.1" 500 262 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/x Safari/537.36"
    10.20.0.128 - - [16:04:42 +0000] "GET /favicon.ico HTTP/1.1" 200 5 "https://example.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/x Safari/537.36"

Wordpress container logs

    127.0.0.1 - 16:04:42 +0000 "GET /index.php" 500
    127.0.0.1 - 16:04:42 +0000 "GET /index.php" 200


helm install --name mysql-helm --set \
mysqlRootPassword=xxx,mysqlUser=xxx,mysqlPassword=xxx, \
mysqlDatabase=xxx,persistence.size=50Gi \
stable/mysql

Use the created password secret like this:

        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-helm
              key: password
======================================================================================================
https://discuss.kubernetes.io/t/error-establishing-a-database-connection-error-kubernates-wordpress-mysql/15407

Error establishing a database connection Error Kubernates wordpress & mysql

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-deployment
  labels:
    app: mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        ports:
        - containerPort: 80
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: DEVOPS1
        - name: MYSQL_USER
          value: wpuser
        - name: MYSQL_PASSWORD
          value: DEVOPS12345
        - name: MYSQL_DATABASE
          value: wpdb
---
 apiVersion: v1
    kind: Service
    metadata:
      name: mysql-service
    spec:
      selector:
        app: mysql
      ports:
        - protocol: TCP
          port: 3306
          targetPort: 3306
---
        apiVersion: apps/v1
                kind: Deployment
                metadata:
                  name: wordpress-deployment
                  labels:
                    app: wordpress
                spec:
                  replicas: 3
                  selector:
                    matchLabels:
                      app: wordpress
                  template:
                    metadata:
                      labels:
                        app: wordpress
                    spec:
                      containers:
                      - name: wordpress
                        image: wordpress
                        ports:
                        - containerPort: 80
                        env:
                        - name: WORDPRESS_DB_HOST
                          value: mysql-service
                        - name: WORDPRESS_DB_USER
                          value: wpuser
                        - name: WORDPRESS_DB_PASSWORD
                          value: DEVOPS12345
                        - name: WORDPRESS_DB_NAME
                          value: wpdb
                        - name: WORDPRESS_DEBUG
                          value: "1"
---
apiVersion: v1
    kind: Service
    metadata:
      name: wordpress-service
    spec:
      type: NodePort
      selector:
        app: wordpress
      ports:
        - protocol: TCP
          port: 80
          targetPort: 80
---
++++
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress-deployment
  labels:
    app: wordpress
spec:
  replicas: 3
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
      - name: wordpress
        image: wordpress
        ports:
        - containerPort: 80
        env:
          - name: WORDPRESS_DB_HOST
            value: mysql-service
          - name: WORDPRESS_DB_USER
            value: wpuser
          - name: WORDPRESS_DB_PASSWORD
            value: DEVOPS12345
          - name: WORDPRESS_DB_NAME
            value: wpdb
          - name: WORDPRESS_DEBUG
            value: â€œ1â€
---
apiVersion: v1
kind: Service
metadata:
  name: wordpress-service
spec:
  type: NodePort
  selector:
    app: wordpress
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++==
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-deployment
  labels:
    app: mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        ports:
        - containerPort: 80
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: DEVOPS1
        - name: MYSQL_USER
          value: wpuser
        - name: MYSQL_PASSWORD
          value: DEVOPS12345
        - name: MYSQL_DATABASE
          value: wpdb
---
mysql-service.yaml

apiVersion: v1
kind: Service
metadata:
  name: mysql-service
spec:
  selector:
    app: mysql
  ports:
    - protocol: TCP
      port: 3306
      targetPort: 3306
---      
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress-deployment
  labels:
    app: wordpress
spec:
  replicas: 3
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
      - name: wordpress
        image: wordpress
        ports:
        - containerPort: 80
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql-service
        - name: WORDPRESS_DB_USER
          value: wpuser
        - name: WORDPRESS_DB_PASSWORD
          value: wpdb


wp-service.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: wordpress-service
spec:
  type: NodePort
  selector:
    app: wordpress
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
++++++++++++++++++++++++++++++++++++++++++++++++++++
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-deployment
  labels:
    app: mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        ports:
        - containerPort: 80
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: DEVOPS1
        - name: MYSQL_USER
          value: wpuser
        - name: MYSQL_PASSWORD
          value: DEVOPS12345
        - name: MYSQL_DATABASE
          value: wpdb

mysql-service.yaml

apiVersion: v1
apiVersion: v1
kind: Service
metadata:
  name: mysql-service
spec:
  selector:
    app: mysql
  ports:
    - protocol: TCP
      port: 3306
      targetPort: 3306
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress-deployment
  labels:
    app: wordpress
spec:
  replicas: 3
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
      - name: wordpress
        image: wordpress
        ports:
        - containerPort: 80
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql-service
        - name: WORDPRESS_DB_USER
          value: wpuser
        - name: WORDPRESS_DB_PASSWORD
          value: wpdb

wp-service.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: wordpress-service
spec:
  type: NodePort
  selector:
    app: wordpress
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
---
======================================================================================================
          ingress kubernetes     =  ingress kubernetes
https://github.com/anjilinux/project-ingress-Kubernetes-Tutorial

ingress kubernetes 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: department-service-app
  labels:
    app: department-service-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: department-service-app
  template:
    metadata:
      labels:
        app: department-service-app
    spec:
      containers:
        - name: department-service-app
          image: dailycodebuffer/department-service:0.0.1
          imagePullPolicy: Always
          ports:
            - containerPort: 9001

---
apiVersion: v1
kind: Service
metadata:
  name: department-service-svc
spec:
  ports:
    - targetPort: 9001
      port: 80
  selector:
    app: department-service-app

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service-app
  labels:
    app: user-service-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: user-service-app
  template:
    metadata:
      labels:
        app: user-service-app
    spec:
      containers:
        - name: user-service-app
          image: dailycodebuffer/user-service:0.0.1
          imagePullPolicy: Always
          ports:
            - containerPort: 9002

---
apiVersion: v1
kind: Service
metadata:
  name: user-service-svc
spec:
  ports:
    - targetPort: 9002
      port: 80
  selector:
    app: user-service-app
---
#ingress svc.yaml
# https://kubernetes.io/docs/concepts/services-networking/ingress/
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: dcbapp.com
    http:
      paths:

      - path: /users
        pathType: Prefix
        backend:
          service:
            name: user-service-svc
            port:
              number: 80
              
              
      - path: /departments
        pathType: Prefix
        backend:
          service:
            name: department-service-svc
            port:
              number: 80
---
# ingress.svc.domain.yaml
# https://kubernetes.io/docs/concepts/services-networking/ingress/
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: user.dcbapp.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: user-service-svc
            port:
              number: 80
  - host: department.dcbapp.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: department-service-svc
            port:
              number: 80
+++++++=
anji@anji:~/deployment$  kubectl get pod,deployment,pv,pvc,service,ing   -o wide 
NAME                                          READY   STATUS    RESTARTS   AGE    IP          NODE     NOMINATED NODE   READINESS GATES
pod/department-service-app-5498644f64-8kfdh   1/1     Running   0          102m   10.44.0.1   worker   <none>           <none>
pod/user-service-app-7657c9498d-tpc7j         1/1     Running   0          69m    10.44.0.2   worker   <none>           <none>

NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS               IMAGES                                     SELECTOR
deployment.apps/department-service-app   1/1     1            1           102m   department-service-app   dailycodebuffer/department-service:0.0.1   app=department-service-app
deployment.apps/user-service-app         1/1     1            1           69m    user-service-app         dailycodebuffer/user-service:0.0.1         app=user-service-app

NAME                             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/department-service-svc   ClusterIP   10.111.50.209   <none>        80/TCP    102m   app=department-service-app
service/kubernetes               ClusterIP   10.96.0.1       <none>        443/TCP   107m   <none>
service/user-service-svc         ClusterIP   10.98.131.57    <none>        80/TCP    69m    app=user-service-app

NAME                                   CLASS    HOSTS                                   ADDRESS   PORTS   AGE
ingress.networking.k8s.io/my-ingress   <none>   user.dcbapp.com,department.dcbapp.com             80      69m "


#########################################################################################3
https://docs.docker.com/engine/install/ubuntu/        success   success       


dell@dell:~$ minikube config set driver docker
â—  These changes will take effect upon a minikube delete and then a minikube start
dell@dell:~$ minikube start --driver=docker
ðŸ˜„  minikube v1.29.0 on Ubuntu 20.04 (kvm/amd64)
âœ¨  Using the docker driver based on user configuration

ðŸ’£  Exiting due to PROVIDER_DOCKER_NEWGRP: "docker version --format -:" exit status 1: permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.24/version": dial unix /var/run/docker.sock: connect: permission denied
ðŸ’¡  Suggestion: Add your user to the 'docker' group: 'sudo usermod -aG docker $USER && newgrp docker'
ðŸ“˜  Documentation: https://docs.docker.com/engine/install/linux-postinstall/

dell@dell:~$ sudo usermod -aG docker $USER && newgrp docker
dell@dell:~$ minikube start --driver=docker
ðŸ˜„  minikube v1.29.0 on Ubuntu 20.04 (kvm/amd64)
âœ¨  Using the docker driver based on user configuration
ðŸ“Œ  Using Docker driver with root privileges
ðŸ‘  Starting control plane node minikube in cluster minikube
ðŸšœ  Pulling base image ...
ðŸ’¾  Downloading Kubernetes v1.26.1 preload ...
    > preloaded-images-k8s-v18-v1...:  397.05 MiB / 397.05 MiB  100.00% 2.46 Mi
    > gcr.io/k8s-minikube/kicbase...:  407.19 MiB / 407.19 MiB  100.00% 1.80 Mi
ðŸ”¥  Creating docker container (CPUs=2, Memory=2200MB) ...
ðŸ³  Preparing Kubernetes v1.26.1 on Docker 20.10.23 ...
    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ðŸ”—  Configuring bridge CNI (Container Networking Interface) ...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ðŸ”Ž  Verifying Kubernetes components...
ðŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
ðŸ’¡  kubectl not found. If you need it, try: 'minikube kubectl -- get pods -A'
ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
dell@dell:~$ 

root@dell:~# docker images 
REPOSITORY                    TAG       IMAGE ID       CREATED      SIZE
gcr.io/k8s-minikube/kicbase   v0.0.37   01c0ce65fff7   9 days ago   1.15GB
root@dell:~# docker ps 
CONTAINER ID   IMAGE                                 COMMAND                  CREATED          STATUS          PORTS                                                                                                                                  NAMES
05c69cc7d256   gcr.io/k8s-minikube/kicbase:v0.0.37   "/usr/local/bin/entrâ€¦"   21 minutes ago   Up 21 minutes   127.0.0.1:32772->22/tcp, 127.0.0.1:32771->2376/tcp, 127.0.0.1:32770->5000/tcp, 127.0.0.1:32769->8443/tcp, 127.0.0.1:32768->32443/tcp   minikube
root@dell:~# docker ps -a 
CONTAINER ID   IMAGE                                 COMMAND                  CREATED          STATUS          PORTS                                                                                                                                  NAMES
05c69cc7d256   gcr.io/k8s-minikube/kicbase:v0.0.37   "/usr/local/bin/entrâ€¦"   21 minutes ago   Up 21 minutes   127.0.0.1:32772->22/tcp, 127.0.0.1:32771->2376/tcp, 127.0.0.1:32770->5000/tcp, 127.0.0.1:32769->8443/tcp, 127.0.0.1:32768->32443/tcp   minikube
root@dell:~# 

dell@dell:~$ minikube kubectl -- get pods -A
    > kubectl.sha256:  64 B / 64 B [-------------------------] 100.00% ? p/s 0s              ==   success  
    > kubectl:  45.80 MiB / 45.80 MiB [--------------] 100.00% 3.78 MiB p/s 12s
NAMESPACE     NAME                               READY   STATUS    RESTARTS      AGE
kube-system   coredns-787d4945fb-6gdcz           1/1     Running   0             22m
kube-system   etcd-minikube                      1/1     Running   0             23m
kube-system   kube-apiserver-minikube            1/1     Running   0             23m
kube-system   kube-controller-manager-minikube   1/1     Running   0             23m
kube-system   kube-proxy-mdv2p                   1/1     Running   0             22m
kube-system   kube-scheduler-minikube            1/1     Running   0             23m
kube-system   storage-provisioner                1/1     Running   1 (22m ago)   23m

dell@dell:~$ alias kubectl="minikube kubectl --"
dell@dell:~$ kubectl get node 
NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   28m   v1.26.1

dell@dell:~$ minikube dashboard
ðŸ”Œ  Enabling dashboard ...
    â–ª Using image docker.io/kubernetesui/dashboard:v2.7.0
    â–ª Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
ðŸ’¡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


ðŸ¤”  Verifying dashboard health ...
ðŸš€  Launching proxy ...
ðŸ¤”  Verifying proxy health ...
ðŸŽ‰  Opening http://127.0.0.1:46109/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser...


http://127.0.0.1:46109/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/#/pod?namespace=default

=========++++++++++++++++++++++++++++

dell@dell:~$ kubectl get pods -n kube-system
NAME                               READY   STATUS    RESTARTS      AGE
coredns-787d4945fb-6gdcz           1/1     Running   0             48m
etcd-minikube                      1/1     Running   0             48m
kube-apiserver-minikube            1/1     Running   0             48m
kube-controller-manager-minikube   1/1     Running   0             48m
kube-proxy-mdv2p                   1/1     Running   0             48m
kube-scheduler-minikube            1/1     Running   0             48m
storage-provisioner                1/1     Running   1 (47m ago)   48m
dell@dell:~$ 


apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: dcbapp.com
    http:
      paths:

      - path: /users
        pathType: Prefix
        backend:
          service:
            name: user-service-svc
            port:
              number: 80
              
              
      - path: /departments
        pathType: Prefix
        backend:
          service:
            name: department-service-svc
            port:
              number: 80

dell@dell:~$ kubectl apply -f ingres.yaml 
ingress.networking.k8s.io/my-ingress created

dell@dell:~$ kubectl get ingress
NAME         CLASS    HOSTS        ADDRESS   PORTS   AGE
my-ingress   <none>   dcbapp.com             80      66s

root@dell:~# nano /etc/hosts
root@dell:~# 
192.168.122.247      dcbapp.com      "
###############################################33
https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/

dell@dell:~$ minikube addons enable ingress
ðŸ’¡  ingress is an addon maintained by Kubernetes. For any concerns contact minikube on GitHub.
You can view the list of minikube maintainers at: https://github.com/kubernetes/minikube/blob/master/OWNERS
    â–ª Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343
    â–ª Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343
    â–ª Using image registry.k8s.io/ingress-nginx/controller:v1.5.1
ðŸ”Ž  Verifying ingress addon...
ðŸŒŸ  The 'ingress' addon is enabled "

dell@dell:~$ kubectl get pods -n ingress-nginx
NAME                                       READY   STATUS      RESTARTS   AGE
ingress-nginx-admission-create-fnghb       0/1     Completed   0          70s
ingress-nginx-admission-patch-d7xzb        0/1     Completed   0          70s
ingress-nginx-controller-77669ff58-8g94z   0/1     Running     0          70s

dell@dell:~$ kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0
error: failed to create deployment: deployments.apps "web" already exists

dell@dell:~$ kubectl expose deployment web --type=NodePort --port=8080
Error from server (AlreadyExists): services "web" already exists

dell@dell:~$ kubectl get service web
NAME   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
web    ClusterIP   10.109.179.5   <none>        8080/TCP   57m

dell@dell:~$ minikube service web --url
ðŸ˜¿  service default/web has no node port
dell@dell:~$ 

https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/      katakoda   "


dell@dell:~$ kubectl edit service web
service/web edited
dell@dell:~$ minikube service web --url
http://192.168.49.2:32107
dell@dell:~$ 

dell@dell:~$ curl http://192.168.49.2:32107/
Hello, world!
Version: 1.0.0
Hostname: web-68487bc957-z7xb7
dell@dell:~$ 

dell@dell:~$ kubectl get pod,service,deployment,ing 
NAME                       READY   STATUS    RESTARTS   AGE
pod/web-68487bc957-5cj8g   1/1     Running   0          75m
pod/web-68487bc957-z7xb7   1/1     Running   0          37m

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP          122m
service/web          NodePort    10.109.179.5   <none>        8080:32107/TCP   73m

NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/web   2/2     2            2           75m

NAME                                                                CLASS           HOSTS                          ADDRESS        PORTS     AGE
ingress.networking.k8s.io/example-ingress                           nginx           hello-world.info               192.168.49.2   80        3m49s
ingress.networking.k8s.io/ingress-wildcard-host                     <none>          foo.bar.com,*.foo.com          192.168.49.2   80        25m
ingress.networking.k8s.io/minimal-ingress                           nginx-example   *                                             80        30m
ingress.networking.k8s.io/my-ingress                                <none>          dcbapp.com                     192.168.49.2   80        62m
ingress.networking.k8s.io/name-virtual-host-ingress                 <none>          foo.bar.com,bar.foo.com        192.168.49.2   80        22m
ingress.networking.k8s.io/name-virtual-host-ingress-no-third-host   <none>          first.bar.com,second.bar.com   192.168.49.2   80        21m
ingress.networking.k8s.io/tls-example-ingress                       <none>          https-example.foo.com          192.168.49.2   80, 443   20m
dell@dell:~$ 

dell@dell:~$ kubectl get ingress
NAME                                      CLASS           HOSTS                          ADDRESS        PORTS     AGE
example-ingress                           nginx           hello-world.info               192.168.49.2   80        4m52s

root@dell:~# cat  /etc/hosts
127.0.0.1	localhost
127.0.1.1	dell
example-ingress    192.168.49.2 

----
dell@dell:~$ curl hello-world.info/v2
Hello, world!
Version: 2.0.0
Hostname: web2-6459878f46-ssqjb
dell@dell:~$ curl hello-world.info
Hello, world!
Version: 1.0.0
Hostname: web-68487bc957-z7xb7
dell@dell:~$ 
############################################################################################################################3"
https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-ingress-guide-nginx-example.html
https://thenewstack.io/kubernetes-ingress-for-beginners/

apiVersion: apps/v1
kind: Deployment
metadata: 
   name: web1
spec: 
  replicas: 1
  selector: 
    matchLabels: 
      app: web1
  template: 
    metadata: 
      labels:
        app: web1
    spec: 
      containers:
         - name: web1
           image: gcr.io/google-samples/hello-app:1.0  
           ports: 
             - containerPort: 8080

---
apiVersion: v1
kind: Service
metadata: 
  name: sv1
spec: 
  selector: 
    app: web1
  ports: 
   - port: 8080
  type: NodePort
  
----------
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: web2
spec: 
  replicas: 1
  selector: 
    matchLabels: 
       app: web2
  template: 
    metadata: 
       labels: 
         app: web2
    spec: 
      containers:
        - name: web2
          image: gcr.io/google-samples/hello-app:2.0
          ports: 
            - containerPort: 8080

---
apiVersion: v1
kind: Service
metadata: 
  name: sv2
spec: 
  selector:
    app: web2  
  ports: 
    - port: 8080
  type: NodePort
---------
#Create an Ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata: 
   name: ing
   annotations:
       nginx.ingress.kubernetes.io/rewrite-target: /$1
spec: 
  rules:
  - host: anji.com
    http: 
      paths: 
        - path: /
          pathType: Prefix
          backend:
            service: 
               name: sv1
               port:
                 number: 8080
        - path: /v2
          pathType: Prefix
          backend: 
             service: 
               name:  sv2
               port: 
                 number: 8080            
####################################################################################################################"
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#cronjob-v1-batch
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#joblist-v1-batch
jobs  and  crone jobs 
================
apiVersion: batch/v1
kind: CronJob
metadata: 
   name: myjob
spec:    
  schedule: "*/1 * * * *"
  jobTemplate: 
    spec: 
      template:
        spec:
          containers:
          - name: myjob
            image: nginx
            imagePullPolicy: IfNotPresent
            command:
             - /bin/sh
             - -c
             - date; echo helo from kubernetes cluster 
          restartPolicy: OnFailure

anji@anji:~/deployment$ kubectl apply -f  job.yaml 
cronjob.batch/myjob created
anji@anji:~/deployment$ 

anji@anji:~$  kubectl get cronjob,po
NAME                  SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/myjob   */1 * * * *   False     1        3s              13s

anji@anji:~$ kubectl get cronjob,pod
NAME                  SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/myjob   */1 * * * *   False     0        23s             33s

NAME                       READY   STATUS      RESTARTS   AGE
pod/myjob-27928237-"zkf9w "  0/1     Completed   0          23s "


anji@anji:~$ kubectl get cronjob,pod -o wide 
NAME                  SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE   CONTAINERS   IMAGES   SELECTOR
cronjob.batch/myjob   */1 * * * *   False     0        7s              77s   myjob        nginx    <none>

NAME                       READY   STATUS      RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/myjob-27928237-zkf9w   0/1     Completed   0          67s   10.44.0.1   worker   <none>           <none>
pod/myjob-27928238-28mxn   0/1     Completed   0          7s    10.44.0.1   worker   <none>           <none> "
anji@anji:~$ 


anji@anji:~$ kubectl get cronjob,pod -o wide 
NAME                  SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE     CONTAINERS   IMAGES   SELECTOR
cronjob.batch/myjob   */1 * * * *   False     0        8s              4m18s   myjob        nginx    <none>

NAME                       READY   STATUS      RESTARTS   AGE    IP          NODE     NOMINATED NODE   READINESS GATES
pod/myjob-27928239-v742n   0/1     Completed   0          2m8s   10.44.0.1   worker   <none>           <none>
pod/myjob-27928240-ndxsl   0/1     Completed   0          68s    10.44.0.1   worker   <none>           <none>
pod/myjob-27928241-89ds6   0/1     Completed   0          8s     10.44.0.1   worker   <none>           <none> "
anji@anji:~$  

anji@anji:~$ kubectl get cronjob,pod -o wide 
NAME                  SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE     CONTAINERS   IMAGES   SELECTOR
cronjob.batch/myjob   */1 * * * *   False     1        0s              5m10s   myjob        nginx    <none>

NAME                       READY   STATUS              RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/myjob-27928239-v742n   0/1     Completed           0          3m    10.44.0.1   worker   <none>           <none>
pod/myjob-27928240-ndxsl   0/1     Completed           0          2m    10.44.0.1   worker   <none>           <none>
pod/myjob-27928241-89ds6   0/1     Completed           0          60s   10.44.0.1   worker   <none>           <none>
pod/myjob-27928242-"hjb56   "0/1     ContainerCreating   0          0s    <none>      worker   <none>           <none>

anji@anji:~$ kubectl get cronjob,pod -o wide 
NAME                  SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE     CONTAINERS   IMAGES   SELECTOR
cronjob.batch/myjob   */1 * * * *   False     1        4s              5m14s   myjob        nginx    <none>

NAME                       READY   STATUS      RESTARTS   AGE    IP          NODE     NOMINATED NODE   READINESS GATES
pod/myjob-27928239-v742n   0/1     Completed   0          3m4s   10.44.0.1   worker   <none>           <none>
pod/myjob-27928240-ndxsl   0/1     Completed   0          2m4s   10.44.0.1   worker   <none>           <none>
pod/myjob-27928241-89ds6   0/1     Completed   0          64s    10.44.0.1   worker   <none>           <none>
pod/myjob-27928242-hjb56   0/1     Completed   0          4s     10.44.0.1   worker   <none>           <none> "
anji@anji:~$ kubectl get cronjob,pod -o wide 
NAME                  SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE     CONTAINERS   IMAGES   SELECTOR
cronjob.batch/myjob   */1 * * * *   False     0        5s              5m15s   myjob        nginx    <none>

NAME                       READY   STATUS      RESTARTS   AGE    IP          NODE     NOMINATED NODE   READINESS GATES
pod/myjob-27928240-ndxsl   0/1     Completed   0          2m5s   10.44.0.1   worker   <none>           <none>
pod/myjob-27928241-89ds6   0/1     Completed   0          65s    10.44.0.1   worker   <none>           <none>
pod/myjob-27928242-hjb56   0/1     Completed   0          5s     10.44.0.1   worker   <none>           <none>

anji@anji:~$ kubectl get cronjob,pod -o wide 
NAME                  SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE     CONTAINERS   IMAGES   SELECTOR
cronjob.batch/myjob   */1 * * * *   False     1        1s              6m11s   myjob        nginx    <none>

NAME                       READY   STATUS      RESTARTS   AGE    IP          NODE     NOMINATED NODE   READINESS GATES
pod/myjob-27928240-ndxsl   0/1     Completed   0          3m1s   10.44.0.1   worker   <none>           <none>
pod/myjob-27928241-89ds6   0/1     Completed   0          2m1s   10.44.0.1   worker   <none>           <none>
pod/myjob-27928242-hjb56   0/1     Completed   0          61s    10.44.0.1   worker   <none>           <none>
pod/myjob-27928243-fsw6j   0/1     Completed   0          1s     10.44.0.1   worker   <none>           <none>

anji@anji:~$ kubectl get cronjob,pod -o wide 
NAME                  SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE     CONTAINERS   IMAGES   SELECTOR
cronjob.batch/myjob   */1 * * * *   False     0        5s              6m15s   myjob        nginx    <none>

NAME                       READY   STATUS      RESTARTS   AGE    IP          NODE     NOMINATED NODE   READINESS GATES
pod/myjob-27928241-89ds6   0/1     Completed   0          2m5s   10.44.0.1   worker   <none>           <none>
pod/myjob-27928242-hjb56   0/1     Completed   0          65s    10.44.0.1   worker   <none>           <none>
pod/myjob-27928243-fsw6j   0/1     Completed   0          5s     10.44.0.1   worker   <none>           <none> "
anji@anji:~/deployment$ kubectl get job  -o wide
NAME             COMPLETIONS   DURATION   AGE     CONTAINERS   IMAGES   SELECTOR
myjob-27928248   1/1           4s         2m56s   myjob        nginx    controller-uid=7ee097be-9162-498d-9788-69eced29a6f7
myjob-27928249   1/1           4s         116s    myjob        nginx    controller-uid=387e7329-c774-4834-aea6-53b6211061bf
myjob-27928250   1/1           4s         56s     myjob        nginx    controller-uid=49f9c311-48c6-4a01-968c-00a2fb2dec98

anji@anji:~/deployment$ kubectl  delete  job  --all
job.batch "myjob-27928252" deleted
job.batch "myjob-27928253" deleted
job.batch "myjob-27928254" deleted   "


anji@anji:~$ kubectl get cronjob,pod -o wide 
NAME                  SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE   CONTAINERS   IMAGES   SELECTOR
cronjob.batch/myjob   */1 * * * *   False     0        49s             17m   myjob        nginx    <none>
anji@anji:~$ 


anji@anji:~$ kubectl get cronjob,pod -o wide 
NAME                  SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE   CONTAINERS   IMAGES   SELECTOR
cronjob.batch/myjob   */1 * * * *   False     0        20s             18m   myjob        nginx    <none>   "

NAME                       READY   STATUS      RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/myjob-27928255-hg6rk   0/1     Completed   0          20s   10.44.0.1   worker   <none>           <none>
anji@anji:~$ 

anji@anji:~/deployment$ kubectl get pod
NAME                   READY   STATUS      RESTARTS   AGE
myjob-27928255-hg6rk   0/1     Completed   0          56s
anji@anji:~/deployment$ kubectl get pod
NAME                   READY   STATUS      RESTARTS   AGE
myjob-27928255-hg6rk   0/1     Completed   0          65s
myjob-27928256-99hgc   0/1     Completed   0          5s

anji@anji:~$ kubectl get cronjob,pod -o wide  "
NAME                  SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE   CONTAINERS   IMAGES   SELECTOR
cronjob.batch/myjob   */1 * * * *   False     1        1s              20m   myjob        nginx    <none>

NAME                       READY   STATUS      RESTARTS   AGE    IP          NODE     NOMINATED NODE   READINESS GATES
pod/myjob-27928255-hg6rk   0/1     Completed   0          2m1s   10.44.0.1   worker   <none>           <none>
pod/myjob-27928256-99hgc   0/1     Completed   0          61s    10.44.0.1   worker   <none>           <none>
pod/myjob-27928257-ttc66   0/1     Completed   0          1s     10.44.0.1   worker   <none>           <none>


anji@anji:~/deployment$ kubectl delete -f job.yaml 
cronjob.batch "myjob" deleted
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++====
https://foxutech.medium.com/jobs-and-cronjobs-in-kubernetes-e8ca108f8c8e
---
apiVersion: batch/v1
kind: Job
metadata:
  name: example-job
spec:
  template:
    metadata:
      name: example-job
    spec:
      containers:
        -
          args:
            - "-Mbignum=bpi"
            - "-wle"
            - "print bpi(2000)"
          command:
            - perl
          image: perl
          name: pi
      restartPolicy: Never

---
---
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello
spec:
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            -
              args:
                - /bin/sh
                - "-c"
                - "date; echo Hello FoxuTech, from the your AKS cluster"
              image: busybox
              name: hello
          restartPolicy: OnFailure
  schedule: "*/1 * * * *"
---
apiVersion: batch/v1beta1
kind: CronJob
metadata:
   name: test-job
spec:
   schedule: "*/5 * * * *" 
   concurrencyPolicy: Forbid
   successfulJobsHistoryLimit: 5
   failedJobsHistoryLimit: 5
   jobTemplate:
     spec:
       template:
         spec:
           containers:
           - name: hello
             image: busybox
             command : ["echo", "Hello Kubernetes Job"]
           restartPolicy: OnFailure
====++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://k8s-examples.container-solutions.com/examples/Job/Job.html
https://docs.openshift.com/container-platform/4.9/nodes/jobs/nodes-nodes-jobs.html
https://jamesdefabia.github.io/docs/user-guide/jobs/
https://kubernetes.io/docs/concepts/workloads/controllers/job/     ok from 


apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl:5.34.0
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
  backoffLimit: 4

================
anji@anji:~$  kubectl get pod,job,cronjob -o wide 
NAME           READY   STATUS      RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx      1/1     Running     0          25m     10.44.0.1   worker   <none>           <none>
pod/pi-kw78q   0/1     "Completed "  0          2m50s   10.44.0.2   worker   <none>           <none>        "  == see  completed task"

NAME           COMPLETIONS   DURATION   AGE     CONTAINERS   IMAGES        SELECTOR
job.batch/pi   1/1           111s       2m50s   pi           perl:5.34.0   controller-uid=6bb6d49c-74b3-4774-b10f-d1b82d8e19c4

anji@anji:~/deployment$ kubectl logs pi-kw78q  
3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446
095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485
6692346034861045432664821339360726024914127372458700660631558817488152092096282925409171536436789259036001133053054882046652138414
695194151160943305727036575959195309218611738193261179310511854807446237996274956735188575272489122793818301194912983367336244065664
308602139494639522473719070217986094370277053921717629317675238467481846766940513200056812714526356082778577134275778960917363717872
1468440901224953430146549585371050792279689258923542019956112129021960864034418159813629774771309960518707211349999998372978049951059
731732816096318595024459455346908302642522308253344685035261931188171010003137838752886587533208381420617177669147303598253490428755
4687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019
5203530185296899577362259941389124972177528347913151557485724245415069595082953311686172785588907509838175463746493931925506040092770
167113900984882401285836160356370766010471018194295559619894676783744944825537977472684710404753464620804668425906949129331367702898
9152104752162056966024058038150193511253382430035587640247496473263914199272604269922796782354781636009341721641219924586315030286182
974555706749838505494588586926995690927210797509302955321165344987202755960236480665499119881834797753566369807426542527862551818417574
6728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189
8356948556209921922218427255025425688767179049460165346680498862723279178608578438382796797668145410095388378636095068006422512520511739
29848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
++++++++++++++++++++++++++++++++++++++++++++++ "

apiVersion: batch/v1
kind: Job
metadata:
  name: pi-with-timeout
spec:
  backoffLimit: 5
  activeDeadlineSeconds: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl:5.34.0
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never

anji@anji:~/deployment$ kubectl get pod,jobs -o wide  
NAME                        READY   STATUS      RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/pi-with-timeout-vp7wx   0/1     Completed   0          59s   10.44.0.1   worker   <none>           <none>

NAME                        COMPLETIONS   DURATION   AGE   CONTAINERS   IMAGES        SELECTOR
job.batch/pi-with-timeout   1/1           7s         59s   pi           perl:5.34.0   controller-uid=ca9dd7f7-ff24-44dc-a046-4a44e8a5961e

anji@anji:~/deployment$ kubectl  logs  pi-with-timeout-vp7wx  
3.14159265358979323846264338327950288419716939937510582097494459230781640628620899862803482534211706798214808651328230
66470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867
831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925
9036001133053054882046652138414695194151160943305727036575959195309218611738193261179310511854807446237996274956735188575
272489122793818301194912983367336244065664308602139494639522473719070217986094370277053921717629317675238467481846766940513
200056812714526356082778577134275778960917363717872146844090122495343014654958537105079227968925892354201995611212902196086
4034418159813629774771309960518707211349999998372978049951059731732816096318595024459455346908302642522308253344685035261931
18817101000313783875288658753320838142061717766914730359825349042875546873115956286388235378759375195778185778053217122680661
30019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791
31515574857242454150695950829533116861727855889075098381754637464939319255060400927701671139009848824012858361603563707660104
710181942955596198946767837449448255379774726847104047534646208046684259069491293313677028989152104752162056966024058038150193
511253382430035587640247496473263914199272604269922796782354781636009341721641219924586315030286182974555706749838505494588586
92699569092721079750930295532116534498720275596023648066549911988183479775356636980742654252786255181841757467289097777279380008
1647060016145249192173217214772350141441973568548161361157352552133475741849468438523323907394143334547762416862518983569485562
09921922218427255025425688767179049460165346680498862723279178608578438382796797668145410095388378636095068006422512520511739298
48960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
anji@anji:~/deployment$ 
+++++++++++++++++++++++++++++++++"

apiVersion: batch/v1
kind: Job
metadata: 
  name:  myjob
spec: 
  template: 
     metadata: 
       labels:
          app: myjob
     spec: 
       restartPolicy: Never
       containers: 
          - name: busybox
            image: busybox
            args: 
              - sleep
              - "6"

root@anji:~# kubectl get pod,job,cj  -o wide 
NAME              READY   STATUS              RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
pod/myjob-q8sh7   0/1     ContainerCreating   0          0s    <none>   worker   <none>           <none>

NAME              COMPLETIONS   DURATION   AGE   CONTAINERS   IMAGES    SELECTOR
job.batch/myjob   0/1           0s         0s    busybox      busybox   controller-uid=382416db-f9c5-46dd-a66d-e4f9b23247a1
---"

root@anji:~# kubectl get pod,job,cj  -o wide 
NAME              READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/myjob-q8sh7   1/1     Running   0          3s    10.44.0.1   worker   <none>           <none>

NAME              COMPLETIONS   DURATION   AGE   CONTAINERS   IMAGES    SELECTOR
---"

root@anji:~# kubectl get pod,job,cj  -o wide 
NAME              READY   STATUS      RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/myjob-q8sh7   0/1     Completed   0          5s    10.44.0.1   worker   <none>           <none>

NAME              COMPLETIONS   DURATION   AGE   CONTAINERS   IMAGES    SELECTOR
job.batch/myjob   0/1           5s         5s    busybox      busybox   controller-uid=382416db-f9c5-46dd-a66d-e4f9b23247a1
------++++++++
                  "jobs:
1. track the number of completions (completions)
   * default is 1
2. run jobs in parallel (parallelism)
3. number of retries (backofflimit)
4. max running time (activeDeadlineSeconds)

apiVersion: batch/v1
kind: CronJob
metadata: 
  name: 3minitscron
spec: 
  schedule: "*/1 * * * *"  
  jobTemplate:
    spec: 
      template: 
         metadata: 
           labels: 
              app: 3minitscron
         spec: 
           restartPolicy: Never
           containers: 
             - name: sleep3
               image: busybox
               args:
                 - sleep
                 - "10" 
                      
root@anji:~# kubectl get cj,pod,job 
NAME                        SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/3minitscron   */1 * * * *   False     0        <none>          0s

root@anji:~# kubectl get cj,pod,job 
NAME                        SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/3minitscron   */1 * * * *   False     1        4s              28s

NAME                             READY   STATUS    RESTARTS   AGE
pod/3minitscron-27929173-j48vn   1/1     Running   0          4s

NAME                             COMPLETIONS   DURATION   AGE
job.batch/3minitscron-27929173   0/1           4s         4s

root@anji:~# kubectl get cj,pod,job 
NAME                        SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/3minitscron   */1 * * * *   False     1        5s              29s

NAME                             READY   STATUS    RESTARTS   AGE
pod/3minitscron-27929173-j48vn   1/1     Running   0          5s

NAME                             COMPLETIONS   DURATION   AGE
job.batch/3minitscron-27929173   0/1           5s         5s
r

NAME                             READY   STATUS    RESTARTS   AGE
pod/3minitscron-27929173-j48vn   1/1     Running   0          8s

NAME                             COMPLETIONS   DURATION   AGE
job.batch/3minitscron-27929173   0/1           8s         8s


root@anji:~# kubectl get cj,pod,job 
NAME                        SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/3minitscron   */1 * * * *   False     1        3s              2m27s

NAME                             READY   STATUS      RESTARTS   AGE
pod/3minitscron-27929173-j48vn   0/1     Completed   0          2m3s
pod/3minitscron-27929174-qlsdz   0/1     Completed   0          63s
pod/3minitscron-27929175-xvrnc   1/1     Running     0          3s

NAME                             COMPLETIONS   DURATION   AGE
job.batch/3minitscron-27929173   1/1           16s        2m3s
job.batch/3minitscron-27929174   1/1           16s        63s
job.batch/3minitscron-27929175   0/1           3s         3s



root@anji:~# kubectl get cj,pod,job 
NAME                        SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/3minitscron   */1 * * * *   False     1        3s              5m27s

NAME                             READY   STATUS              RESTARTS   AGE
pod/3minitscron-27929175-xvrnc   0/1     Completed           0          3m3s
pod/3minitscron-27929176-jvf4j   0/1     Completed           0          2m3s
pod/3minitscron-27929177-zmlqp   0/1     Completed           0          63s
pod/3minitscron-27929178-zmh8j   0/1     ContainerCreating   0          3s

NAME                             COMPLETIONS   DURATION   AGE
job.batch/3minitscron-27929175   1/1           16s        3m3s
job.batch/3minitscron-27929176   1/1           15s        2m3s
job.batch/3minitscron-27929177   1/1           15s        63s
job.batch/3minitscron-27929178   0/1           3s         3s

root@anji:~# kubectl describe cj
Name:                          3minitscron
Namespace:                     default
Labels:                        <none>
Annotations:                   <none>
Schedule:                      */1 * * * *
Concurrency Policy:            Allow
Suspend:                       False
Successful Job History Limit:  3
Failed Job History Limit:      1
Starting Deadline Seconds:     <unset>
Selector:                      <unset>
Parallelism:                   <unset>
Completions:                   <unset>
Pod Template:
  Labels:  app=3minitscron
  Containers:
   sleep3:
    Image:      busybox
    Port:       <none>
    Host Port:  <none>
    Args:
      sleep
      10
    Environment:     <none>
    Mounts:          <none>
  Volumes:           <none>
Last Schedule Time:  Tue, 07 Feb 2023 12:04:00 +0530
Active Jobs:         <none>
Events:
  Type    Reason            Age                 From                Message
  ----    ------            ----                ----                -------
  Normal  SuccessfulCreate  21m                 cronjob-controller  Created job 3minitscron-27929173
  Normal  SawCompletedJob   21m                 cronjob-controller  Saw completed job: 3minitscron-27929173, status: Complete
  Normal  SuccessfulCreate  20m                 cronjob-controller  Created job 3minitscron-27929174
  Normal  SawCompletedJob   20m                 cronjob-controller  Saw completed job: 3minitscron-27929174, status: Complete
  Normal  SuccessfulCreate  19m                 cronjob-controller  Created job 3minitscron-27929175
  Normal  SawCompletedJob   19m                 cronjob-controller  Saw completed job: 3minitscron-27929175, status: Complete
  Normal  SuccessfulCreate  18m                 cronjob-controller  Created job 3minitscron-27929176
  Normal  SawCompletedJob   18m                 cronjob-controller  Saw completed job: 3minitscron-27929176, status: Complete
  Normal  SuccessfulDelete  18m                 cronjob-controller  Deleted job 3minitscron-27929173
  Normal  SuccessfulCreate  17m                 cronjob-controller  Created job 3minitscron-27929177
  Normal  SawCompletedJob   17m                 cronjob-controller  Saw completed job: 3minitscron-27929177, status: Complete
  Normal  SuccessfulDelete  17m                 cronjob-controller  Deleted job 3minitscron-27929174
  Normal  SuccessfulCreate  16m                 cronjob-controller  Created job 3minitscron-27929178
  Normal  SawCompletedJob   16m                 cronjob-controller  Saw completed job: 3minitscron-27929178, status: Complete
  Normal  SuccessfulDelete  16m                 cronjob-controller  Deleted job 3minitscron-27929175
  Normal  SuccessfulCreate  15m                 cronjob-controller  Created job 3minitscron-27929179
  Normal  SawCompletedJob   15m                 cronjob-controller  Saw completed job: 3minitscron-27929179, status: Complete
  Normal  SuccessfulDelete  15m                 cronjob-controller  Deleted job 3minitscron-27929176
  Normal  SuccessfulCreate  14m                 cronjob-controller  Created job 3minitscron-27929180
  Normal  SawCompletedJob   14m                 cronjob-controller  Saw completed job: 3minitscron-27929180, status: Complete
  Normal  SuccessfulDelete  14m                 cronjob-controller  Deleted job 3minitscron-27929177
  Normal  SuccessfulCreate  13m                 cronjob-controller  Created job 3minitscron-27929181
  Normal  SawCompletedJob   13m                 cronjob-controller  Saw completed job: 3minitscron-27929181, status: Complete
  Normal  SuccessfulDelete  13m                 cronjob-controller  Deleted job 3minitscron-27929178
  Normal  SuccessfulCreate  12m                 cronjob-controller  (combined from similar events): Created job 3minitscron-27929182
  Normal  SawCompletedJob   84s (x12 over 12m)  cronjob-controller  (combined from similar events): Saw completed job: 3minitscron-27929193, status: Complete
root@anji:~# kubectl get job
NAME                   COMPLETIONS   DURATION   AGE
3minitscron-27929192   1/1           16s        2m55s
3minitscron-27929193   1/1           16s        115s
3minitscron-27929194   1/1           16s        55s
root@anji:~# kubectl get cj
NAME          SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
3minitscron   */1 * * * *   False     1        6s              22m
root@anji:~# "
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++=
cat <<EOF  | kubectl apply -f - 
apiVersion: batch/v1
kind: Job
metadata: 
   name: 3sleep
spec: 
  completions: 3
  template: 
    metadata: 
      labels: 
        app: 3sleep
    spec: 
      restartPolicy: Never
      containers: 
         - name: 3sleep
           image: busybox
           args:
             - sleep
             - "30" 
EOF

root@anji:~# kubectl get job,cj,pod 
NAME               COMPLETIONS   DURATION   AGE
job.batch/3sleep   "0/3   "        0s         0s

NAME               READY   STATUS              RESTARTS   AGE
pod/3sleep-dgkh6   0/1     ContainerCreating   0          0s

root@anji:~# kubectl get job,cj,pod 
NAME               COMPLETIONS   DURATION   AGE
job.batch/3sleep   "1/3  "         37s        37s

NAME               READY   STATUS              RESTARTS   AGE
pod/3sleep-dgkh6   0/1     Completed           0          37s
pod/3sleep-gq7fj   0/1     ContainerCreating   0          1s


root@anji:~# kubectl get job,cj,pod 
NAME               COMPLETIONS   DURATION   AGE
job.batch/3sleep   1/3           64s        64s

NAME               READY   STATUS      RESTARTS   AGE
pod/3sleep-dgkh6   0/1     Completed   0          64s
pod/3sleep-gq7fj   1/1     Running     0          28s

root@anji:~# kubectl get job,cj,pod 
NAME               COMPLETIONS   DURATION   AGE
job.batch/3sleep   "2/3  "         73s        73s

NAME               READY   STATUS              RESTARTS   AGE
pod/3sleep-5h948   0/1     ContainerCreating   0          1s
pod/3sleep-dgkh6   0/1     Completed           0          73s
pod/3sleep-gq7fj   0/1     Completed           0          37s
root@anji:~# kubectl get job,cj,pod 
NAME               COMPLETIONS   DURATION   AGE
job.batch/3sleep  " 3/3   "        109s       8m6s

NAME               READY   STATUS      RESTARTS   AGE
pod/3sleep-5h948   0/1     Completed   0          6m54s
pod/3sleep-dgkh6   0/1     Completed   0          8m6s
pod/3sleep-gq7fj   0/1     Completed   0          7m30s
root@anji:~# 

+++"
root@anji:~# kubectl describe job
Name:             3sleep
Namespace:        default
Selector:         controller-uid=94c17e7c-8158-4f3d-9e21-7e2db46bc6bb
Labels:           app=3sleep
                  controller-uid=94c17e7c-8158-4f3d-9e21-7e2db46bc6bb
                  job-name=3sleep
Annotations:      batch.kubernetes.io/job-tracking: 
Parallelism:      1                     =============== 1 only 
Completions:      3                  ======================   3 only 
Completion Mode:  NonIndexed
Start Time:       Tue, 07 Feb 2023 12:53:36 +0530
Completed At:     Tue, 07 Feb 2023 12:55:25 +0530
Duration:         109s
Pods Statuses:    0 Active (0 Ready) / 3 Succeeded / 0 Failed
Pod Template:
  Labels:  app=3sleep
           controller-uid=94c17e7c-8158-4f3d-9e21-7e2db46bc6bb
           job-name=3sleep
  Containers:
   3sleep:
    Image:      busybox
    Port:       <none>
    Host Port:  <none>
    Args:
      sleep
      30
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age    From            Message
  ----    ------            ----   ----            -------
  Normal  SuccessfulCreate  11m    job-controller  Created pod: 3sleep-dgkh6
  Normal  SuccessfulCreate  10m    job-controller  Created pod: 3sleep-gq7fj
  Normal  SuccessfulCreate  9m48s  job-controller  Created pod: 3sleep-5h948
  Normal  Completed         9m11s  job-controller  Job completed
root@anji:~# 
++++++++++++++++++++++++++++++++++++++++"
cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata: 
   name: plsm3
spec: 
  completions: 4
  parallelism: 2
  template:
     metadata: 
       labels: 
          app: plsm3
     spec: 
       restartPolicy: Never
       containers: 
         - name: plsm3
           image: busybox
           args: 
             - sleep
             - "60"
EOF

root@anji:~# kubectl get pod,job,cj 
NAME              READY   STATUS              RESTARTS   AGE
pod/plsm3-8pgjx   "0/1 "    ContainerCreating   0          2s
pod/plsm3-m2kgp   0/1     ContainerCreating   0          2s

NAME              COMPLETIONS   DURATION   AGE
job.batch/plsm3   0/4           2s         2s

root@anji:~# kubectl get pod,job,cj 
NAME              READY   STATUS    RESTARTS   AGE
pod/plsm3-8pgjx  " 1/1  "   Running   0          5s
pod/plsm3-m2kgp   1/1     Running   0          5s

NAME              COMPLETIONS   DURATION   AGE
job.batch/plsm3   0/4           5s         5s

root@anji:~# kubectl get pod,job,cj 
NAME              READY   STATUS              RESTARTS   AGE
pod/plsm3-8pgjx   0/1     Completed           0          66s
pod/plsm3-9b9qv   0/1     ContainerCreating   0          0s
pod/plsm3-m2kgp   0/1     Completed           0          66s

NAME              COMPLETIONS   DURATION   AGE
job.batch/plsm3  " 1/4  "         66s        66s
root@anji:~# kubectl get pod,job,cj 
NAME              READY   STATUS              RESTARTS   AGE
pod/plsm3-8pgjx   0/1     Completed           0          67s
pod/plsm3-9b9qv   0/1     ContainerCreating   0          1s
pod/plsm3-m2kgp   0/1     Completed           0          67s

NAME              COMPLETIONS   DURATION   AGE
job.batch/plsm3   1/4           67s        67s

root@anji:~# kubectl get pod,job,cj 
NAME              READY   STATUS              RESTARTS   AGE
pod/plsm3-5dcc9   0/1     ContainerCreating   0          0s
pod/plsm3-8pgjx   0/1     Completed           0          68s
pod/plsm3-9b9qv   0/1     ContainerCreating   0          2s
pod/plsm3-m2kgp   0/1     Completed           0          68s

NAME              COMPLETIONS   DURATION   AGE
job.batch/plsm3  "  2/4 "         68s        68s
root@anji:~# kubectl get pod,job,cj 
NAME              READY   STATUS      RESTARTS   AGE
pod/plsm3-5dcc9  " 1/1  "   Running     0          5s
pod/plsm3-8pgjx   0/1     Completed   0          73s
pod/plsm3-9b9qv "  1/1 "    Running     0          7s
pod/plsm3-m2kgp   0/1     Completed   0          73s

NAME              COMPLETIONS   DURATION   AGE
job.batch/plsm3  " 2/4  "         73s        73s

root@anji:~# kubectl get pod,job,cj 
NAME              READY   STATUS      RESTARTS   AGE
pod/plsm3-5dcc9  " 0/1  "   Completed   0          65s
pod/plsm3-8pgjx   "0/1   "  Completed   0          2m13s
pod/plsm3-9b9qv   "0/1  "   Completed   0          67s
pod/plsm3-m2kgp   "0/1  "   Completed   0          2m13s

NAME              COMPLETIONS   DURATION   AGE
job.batch/plsm3   "3/4 "          2m13s      2m13s

root@anji:~# kubectl get pod,job,cj 
NAME              READY   STATUS      RESTARTS   AGE
pod/plsm3-5dcc9   0/1     Completed   0          67s
pod/plsm3-8pgjx   0/1     Completed   0          2m15s
pod/plsm3-9b9qv   0/1     Completed   0          69s
pod/plsm3-m2kgp   0/1     Completed   0          2m15s

NAME              COMPLETIONS   DURATION   AGE
job.batch/plsm3  " 4/4  "         2m14s      2m15s "

root@anji:~# kubectl describe  job
Name:             plsm3
Namespace:        default
Selector:         controller-uid=c748bff6-587b-485f-bd7c-9641d07a2a7d
Labels:           app=plsm3
                  controller-uid=c748bff6-587b-485f-bd7c-9641d07a2a7d
                  job-name=plsm3
Annotations:      batch.kubernetes.io/job-tracking: 
Parallelism:    "  2   "                                 ==========2 
Completions:     '" 4       "'                             ============= 4 
Completion Mode:  NonIndexed
Start Time:       Tue, 07 Feb 2023 13:25:07 +0530
Completed At:     Tue, 07 Feb 2023 13:27:21 +0530
Duration:         2m14s
Pods Statuses:    0 Active (0 Ready) / 4 Succeeded / 0 Failed
Pod Template:
  Labels:  app=plsm3
           controller-uid=c748bff6-587b-485f-bd7c-9641d07a2a7d
           job-name=plsm3
  Containers:
   plsm3:
    Image:      busybox
    Port:       <none>
    Host Port:  <none>
    Args:
      sleep
      60
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age    From            Message
  ----    ------            ----   ----            -------
  Normal  SuccessfulCreate  9m36s  job-controller  Created pod: plsm3-8pgjx
  Normal  SuccessfulCreate  9m36s  job-controller  Created pod: plsm3-m2kgp
  Normal  SuccessfulCreate  8m30s  job-controller  Created pod: plsm3-9b9qv
  Normal  SuccessfulCreate  8m28s  job-controller  Created pod: plsm3-5dcc9
  Normal  Completed         7m22s  job-controller " Job completed"
root@anji:~# 
+++++++++++++++++++++++++++++++++++++++++++"

cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata: 
  name: ads
spec: 
  completions: 3
  activeDeadlineSeconds: 19
  template: 
    metadata: 
      labels: 
        app: ads
    spec: 
      restartPolicy:  Never
      containers: 
        - name: ads
          image: busybox
          args:
            - sleep
            - "60"
EOF
====
root@anji:~# kubectl get pod,job,cj 
NAME            READY   STATUS              RESTARTS   AGE
pod/ads-8sb8q  " 0/1 "    ContainerCreating   0          0s

NAME            COMPLETIONS   DURATION   AGE
job.batch/ads   0/3           0s         0s

root@anji:~# kubectl get pod,job,cj 
NAME            READY   STATUS    RESTARTS   AGE
pod/ads-8sb8q   "1/1  "  " Running "  0          4s

NAME            COMPLETIONS   DURATION   AGE
job.batch/ads   0/3           4s         4s

root@anji:~# kubectl get pod,job,cj 
NAME            READY   STATUS        RESTARTS   AGE
pod/ads-8sb8q   1/1     "Terminating "  0          25s

NAME            COMPLETIONS   DURATION   AGE
job.batch/ads   0/3           25s        25s

root@anji:~# kubectl get pod,job,cj 
NAME            COMPLETIONS   DURATION   AGE
job.batch/ads   0/3           51s        51s
++
root@anji:~# kubectl describe job
Name:                     ads
Namespace:                default
Selector:                 controller-uid=1c92b728-6802-4b52-a56e-64fd1f41ce22
Labels:                   app=ads
                          controller-uid=1c92b728-6802-4b52-a56e-64fd1f41ce22
                          job-name=ads
Annotations:              batch.kubernetes.io/job-tracking: 
Parallelism:            "  1   "              ===== " only one"
Completions:            "'  3       '"   ===========" only 3 "
Completion Mode:          NonIndexed
Start Time:               Tue, 07 Feb 2023 14:08:21 +0530
Active Deadline Seconds:  19s
Pods Statuses:            0 Active (1 Ready) / 0 Succeeded / 1 Failed
Pod Template:
  Labels:  app=ads
           controller-uid=1c92b728-6802-4b52-a56e-64fd1f41ce22
           job-name=ads
  Containers:
   ads:
    Image:      busybox
    Port:       <none>
    Host Port:  <none>
    Args:
      sleep
   "   60"
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type     Reason            Age    From            Message
  ----     ------            ----   ----            -------
  Normal   SuccessfulCreate  5m48s  job-controller  Created pod: ads-8sb8q
  Normal   SuccessfulDelete  5m29s  job-controller  Deleted pod: ads-8sb8q
  Warning  DeadlineExceeded  5m29s  job-controller  Job was active longer than specified deadline
++++++=
cat << EOF | kubectl apply -f - 
apiVersion: batch/v1
kind: CronJob
metadata: 
   name: cj
spec: 
   schedule: "*/1 * * * *"
   jobTemplate:
      spec: 
        template:
          metadata:
            labels:
              app: cj
          spec:
            restartPolicy: Never
            containers: 
               - name: cj
                 image: busybox
                 args:
                   - sleep
                   - "10"
EOF 

root@anji:~# kubectl get pod,job,cj 
NAME               SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/cj   */1 * * * *   False    " 0   "     <none>          1s

root@anji:~# kubectl get pod,job,cj 
NAME                    READY   STATUS    RESTARTS   AGE
pod/cj-27929337-fsq97   1/1     "Running "  0          6s

NAME                    COMPLETIONS   DURATION   AGE
job.batch/cj-27929337   0/1           6s         6s

NAME               SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/cj   */1 * * * *   False     "1  "      6s              12s



root@anji:~# kubectl get pod,job,cj 
NAME                    READY   STATUS      RESTARTS   AGE
pod/cj-27929337-fsq97   0/1    " Completed "  0          19s

NAME                    COMPLETIONS   DURATION   AGE
job.batch/cj-27929337   "1/1   "        16s        19s

NAME               SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/cj   */1 * * * *   False     0        19s             25s

root@anji:~# kubectl get pod,job,cj 
NAME                    READY   STATUS      RESTARTS   AGE
pod/cj-27929357-x6j7b   0/1     Completed   0          2m54s
pod/cj-27929358-mb5sx   0/1     Completed   0          114s
pod/cj-27929359-q5csd   0/1     Completed   0          54s

NAME                    COMPLETIONS   DURATION   AGE
job.batch/cj-27929357  " 1/1           15s        2m54s
job.batch/cj-27929358   1/1           16s        114s
job.batch/cj-27929359   1/1  "         16s        54s

NAME               SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/cj   */1 * * * *   False     0        54s             23m
root@anji:~# 

root@anji:~# kubectl describe job 
Name:             cj-27929375
Namespace:        default
Selector:         controller-uid=cbfd9354-66f9-44c4-a30a-846ca95e1dbc
Labels:           app=cj
                  controller-uid=cbfd9354-66f9-44c4-a30a-846ca95e1dbc
                  job-name=cj-27929375
Annotations:      batch.kubernetes.io/job-tracking: 
Controlled By:    CronJob/cj
Parallelism:      1               =============="only one "
Completions:      1               ================"only one"
Completion Mode:  NonIndexed
Start Time:       Tue, 07 Feb 2023 15:05:00 +0530
Completed At:     Tue, 07 Feb 2023 15:05:16 +0530
Duration:         16s
Pods Statuses:    0 Active (0 Ready) / 1 Succeeded / 0 Failed
Pod Template:
  Labels:  app=cj
           controller-uid=cbfd9354-66f9-44c4-a30a-846ca95e1dbc
           job-name=cj-27929375
  Containers:
   cj:
    Image:      busybox
    Port:       <none>
    Host Port:  <none>
    Args:
      sleep
      10
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age    From            Message
  ----    ------            ----   ----            -------
  Normal  SuccessfulCreate  2m41s  job-controller  Created pod: cj-27929375-2wpmp
  Normal  Completed         2m25s  job-controller  Job completed

#####################################################################################33"
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
cat << EOF | kubectl apply -f - 
apiVersion: batch/v1
kind: Job
metadata: 
   name: jb
spec: 
   template:
     metadata: 
       labels: 
         app: jb
     spec: 
       restartPolicy: Never
       containers: 
          - name: jb
            image: busybox
            command: ["echo", "helo welcome to india"  ] 
EOF

root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS              RESTARTS   AGE
pod/jb-zknd2   0/1     "ContainerCreating"   0          0s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   0/1           0s         0s

root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS      RESTARTS   AGE
pod/jb-zknd2   0/1     "Completed "  0          5s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   0/1           5s         5s

root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS      RESTARTS   AGE
pod/jb-zknd2   0/1     Completed   0          8s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb  " 1/1    "       6s         8s

root@anji:~# kubectl describe job 
Name:             jb
Namespace:        default
Selector:         controller-uid=95a1da16-f9d5-45dc-9203-619be322d422
Labels:           app=jb
                  controller-uid=95a1da16-f9d5-45dc-9203-619be322d422
                  job-name=jb
Annotations:      batch.kubernetes.io/job-tracking: 
Parallelism:      1
Completions:      1
Completion Mode:  NonIndexed
Start Time:       Tue, 07 Feb 2023 15:53:08 +0530
Completed At:     Tue, 07 Feb 2023 15:53:14 +0530
Duration:         6s
Pods Statuses:    0 Active (0 Ready) / 1 Succeeded / 0 Failed
Pod Template:
  Labels:  app=jb
           controller-uid=95a1da16-f9d5-45dc-9203-619be322d422
           job-name=jb
  Containers:
   jb:
    Image:      busybox
    Port:       <none>
    Host Port:  <none>
    Command:
      echo
      helo welcome to india
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age    From            Message
  ----    ------            ----   ----            -------
  Normal  SuccessfulCreate  4m4s   job-controller  Created pod: jb-zknd2
  Normal  Completed         3m58s  job-controller  Job completed
root@anji:~# 
++++++++++++++++++++++++++++++++++++++++++"

cat << EOF | kubectl apply -f - 
apiVersion: batch/v1
kind: Job
metadata: 
   name: jb
spec: 
  template: 
    metadata: 
      labels: 
         app: jb
    spec: 
      containers:  
         - name: jb
           image: busybox
           command: ["sleep" , "45"]
      restartPolicy: Never
EOF

root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS              RESTARTS   AGE
pod/jb-wqhvm   "0/1"    " ContainerCreating "  0          0s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   0/1           0s         0s


root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS    RESTARTS   AGE
pod/jb-wqhvm   "1/1 "   " Running  " 0          6s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   0/1           6s         6s


root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS      RESTARTS   AGE
pod/jb-wqhvm   0/1    " Completed"   0          52s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   "1/1    "       51s        52s

nji@anji:~/deployment$ kubectl describe job 
Name:             jb
Namespace:        default
Selector:         controller-uid=b85c5d67-868b-4d9c-9f55-263bceaabb53
Labels:           app=jb
                  controller-uid=b85c5d67-868b-4d9c-9f55-263bceaabb53
                  job-name=jb
Annotations:      batch.kubernetes.io/job-tracking: 
Parallelism:      1
Completions:      1
Completion Mode:  NonIndexed
Start Time:       Tue, 07 Feb 2023 16:17:01 +0530
Completed At:     Tue, 07 Feb 2023 16:17:52 +0530
Duration:         51s
Pods Statuses:    0 Active (0 Ready) / 1 Succeeded / 0 Failed
Pod Template:
  Labels:  app=jb
           controller-uid=b85c5d67-868b-4d9c-9f55-263bceaabb53
           job-name=jb
  Containers:
   jb:
    Image:      busybox
    Port:       <none>
    Host Port:  <none>
    Command:
      sleep
      45
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  75s   job-controller  Created pod: jb-wqhvm
  Normal  Completed         24s   job-controller  Job completed
++++++++++++++++++++++++++++++++++++++++
cat << EOF | kubectl apply -f - 
apiVersion: batch/v1
kind: Job
metadata: 
  name: jb
spec: 
  completions: 2             |||||||||||||  this is importent u menction 2 only times 
  template: 
     metadata: 
        labels: 
          app: jb
     spec: 
       containers: 
         - name: jb
           image: busybox
           command: ["sleep", "60"]
       restartPolicy: Never
EOF

root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS              RESTARTS   AGE
pod/jb-8dx5k   "0/1  "   ContainerCreating   0          0s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb "  0/2   "        0s         0s


root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS    RESTARTS   AGE
pod/jb-8dx5k   "1/1 "    Running   0          2s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   0/2           3s         3s


root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS      RESTARTS   AGE
pod/jb-8dx5k   0/1     Completed   0          65s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   0/2           65s        65s

root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS              RESTARTS   AGE
pod/jb-8dx5k   0/1     Completed           0          66s
pod/jb-pdwvd   0/1     "ContainerCreating "  0          0s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   "1/2 "          66s        66s

root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS              RESTARTS   AGE
pod/jb-8dx5k   0/1     Completed           0          68s
pod/jb-pdwvd   0/1     ContainerCreating   0          2s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   1/2           68s        68s

root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS      RESTARTS   AGE
pod/jb-8dx5k   0/1     Completed   0          69s
pod/jb-pdwvd   1/1    " Running  "   0          3s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   "1/2    "       69s        69s

root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS      RESTARTS   AGE
pod/jb-8dx5k   0/1     Completed   0          7m15s
pod/jb-pdwvd   0/1     Completed   0          6m9s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb  " 2/2   "        2m12s      7m15s

+++++++++++++++++++++++++++++++++++++++++++++=
root@anji:~# kubectl describe job 
Parallelism:      1
Completions:      2                   =====  this is menction 
Completion Mode:  NonIndexed
"Start Time:       Tue, 07 Feb 2023 17:19:46 +0530"
"Completed At":     Tue, 07 Feb 2023 17:21:58 +0530
"Duration:         2m12s"
    Command:
      sleep
      "60"
 Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  15m   job-controller  Created pod: "jb-8dx5k"
  Normal  SuccessfulCreate  14m   job-controller  Created pod: jb-p"dwvd"
  Normal " Completed  "       13m   job-controller  Job completed
+++++++++++++++++++++++++++++++++++++
cat << EOF | kubectl apply -f - 
apiVersion: batch/v1
kind: Job
metadata: 
  name: jb
spec: 
  completions: 2
  parallelism: 2
  template: 
    metadata: 
      labels: 
        app: jb
    spec: 
      containers: 
       - name: jb
         image: busybox
         command: ["echo", "HELO HOW  ARE  YOU"]
      restartPolicy: Never
EOF

root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS              RESTARTS   AGE
pod/jb-tg5nv   0/1     ContainerCreating   0          1s
pod/jb-vf7b9   0/1     ContainerCreating   0          1s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   0/2           1s         1s

root@anji:~# kubectl describe job 

Parallelism:     " 2"   =====  /// /
Completions:     " 2"      = ==== ///  
Completion Mode:  NonIndexed
"Start Time":       Tue, 07 Feb 2023 "18:07:"04 +0530
"Completed At":     Tue, 07 Feb 2023 "18:07":11 +0530
Duration:         7s
    Command:
      echo
      HELO HOW  ARE  YOU
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age    From            Message
  ----    ------            ----   ----            -------
  Normal  SuccessfulCreate  5m21s  job-controller  Created pod: jb-vf7b9
  Normal  SuccessfulCreate  5m21s  job-controller  Created pod: jb-tg5nv
  Normal  Completed         5m14s  job-controller  Job completed
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
cat << EOF | kubectl apply -f - 
apiVersion: batch/v1
kind: Job
metadata: 
  name: jb
spec: 
  template:
    metadata: 
       labels: 
         app: jb
    spec: 
      restartPolicy: Never
      containers:
           - name: bj
             image: busybox
             command: ["ls", "/anjireddy"] 
EOF
root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS              RESTARTS   AGE
pod/jb-tl4sn   0/1     "ContainerCreating"   0          1s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   0/1           1s         1s

root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS              RESTARTS   AGE
pod/jb-f5b44   0/1     ContainerCreating   0          2s
pod/jb-tl4sn   0/1    " Error "              0          7s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   0/1           7s         7s

root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS   RESTARTS   AGE
pod/jb-f5b44   0/1     "Error  "  0          4s
pod/jb-tl4sn   0/1     "Error  "  0          9s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   0/1           9s         9s

root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS   RESTARTS   AGE
pod/jb-f5b44   0/1     Error    0          4m10s
pod/jb-hsbhf   0/1     Error    0          3m31s
pod/jb-jcsz5   0/1     Error    0          81s
pod/jb-t7hfw   0/1     Error    0          2m46s
pod/jb-tl4sn   0/1     Error    0          4m15s
pod/jb-vksqq   0/1     Error    0          3m55s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   0/1           4m15s      4m15s

root@anji:~# kubectl describe job
Parallelism:      1
Completions:      1
  Containers:
   bj:
   " Image:      busybox"
    Command:
     " ls
      /anjireddy"
Events:
  Type    Reason            Age    From            Message
  ----    ------            ----   ----            -------
  Normal  SuccessfulCreate  3m23s  job-controller  Created pod: jb-tl4sn
  Normal  SuccessfulCreate  3m18s  job-controller  Created pod: jb-f5b44
  Normal  SuccessfulCreate  3m3s   job-controller  Created pod: jb-vksqq
  Normal  SuccessfulCreate  2m39s  job-controller  Created pod: jb-hsbhf
  Normal  SuccessfulCreate  114s   job-controller  Created pod: jb-t7hfw
  Normal  SuccessfulCreate  29s    job-controller  Created pod: jb-jcsz5
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
cat << EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata: 
   name: jb
spec: 
  backoffLimit: 2
  template: 
    metadata: 
      labels: 
        app: jb
    spec: 
      restartPolicy: Never
      containers: 
         - name: jb
           image: busybox
           command: ["ls", "/reddy"]
EOF
root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS              RESTARTS   AGE
pod/jb-dmpp5   0/1     ContainerCreating   0          0s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   0/1           0s         0s

root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS   RESTARTS   AGE
pod/jb-dmpp5   0/1     Error    0          5s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   0/1           5s         5s

root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS              RESTARTS   AGE
pod/jb-9m6vz   0/1     ContainerCreating   0          2s
pod/jb-dmpp5   0/1     Error               0          8s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   0/1           8s         8s

root@anji:~# kubectl get pod,job,cj 
NAME           READY   STATUS   RESTARTS   AGE
pod/jb-9m6vz   0/1     Error    0          3s
pod/jb-dmpp5   0/1     Error    0          9s

NAME           COMPLETIONS   DURATION   AGE
job.batch/jb   0/1           9s         9s

anji@anji:~/deployment$ kubectl describe job 
Parallelism:      1
Completions:      1
Completion Mode:  NonIndexed
Start Time:       Tue, 07 Feb 2023 19:11:54 +0530
Pods Statuses:    0 Active (0 Ready) / 0 Succeeded / "3 Failed"
Events:
  Type     Reason                Age    From            Message
  ----     ------                ----   ----            -------
  Normal   SuccessfulCreate      2m40s  job-controller  Created pod: jb-dmpp5
  Normal   SuccessfulCreate      2m34s  job-controller  Created pod: jb-9m6vz
  Normal   SuccessfulCreate      2m19s  job-controller  Created pod: jb-npsqj
  "Warning " "BackoffLimitExceeded " 114s   job-controller  "Job has reached the specified backoff limit"  
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
cat << EOF | kubectl apply -f - 
apiVersion: batch/v1
kind: Job
metadata: 
  name: ads
spec: 
  activeDeadlineSeconds: 13
  template: 
    metadata: 
      labels: 
        app: ads
    spec: 
      restartPolicy: Never
      containers: 
         - name: ads
           image: busybox
           command: ["sleep", "35"]
EOF

root@anji:~# kubectl get pod,job,cj 
NAME            READY   STATUS              RESTARTS   AGE
pod/ads-2wp2z   0/1    " ContainerCreating"   0          2s

NAME            COMPLETIONS   DURATION   AGE
job.batch/ads   0/1           2s         2s
root@anji:~# kubectl get pod,job,cj 
NAME            READY   STATUS    RESTARTS   AGE
pod/ads-2wp2z   1/1     "Running "  0        "  4s"

NAME            COMPLETIONS   DURATION   AGE
job.batch/ads   0/1           4s         4s

root@anji:~# kubectl get pod,job,cj 
NAME            READY   STATUS    RESTARTS   AGE
pod/ads-2wp2z   1/1     "Running  " 0         " 11s"

NAME            COMPLETIONS   DURATION   AGE
job.batch/ads   0/1           11s        11s

root@anji:~# kubectl get pod,job,cj 
NAME            READY   STATUS        RESTARTS   AGE
pod/ads-2wp2z   1/1    " Terminating "  0         " 13s"

NAME            COMPLETIONS   DURATION   AGE
job.batch/ads   0/1           13s        13s

root@anji:~# kubectl get pod,job,cj 
NAME            COMPLETIONS   DURATION   AGE
job.batch/ads   0/1           2m45s      2m45s

anji@anji:~/deployment$ kubectl  describe job 
"Active Deadline Seconds:  13s"
Pods Statuses:            0 Active (1 Ready) / 0 Succeeded /" 1 Failed"
    Command:
      sleep
      35
Events:
  Type     Reason            Age    From            Message
  ----     ------            ----   ----            -------
  Normal   SuccessfulCreate  4m6s   job-controller  Created pod: ads-2wp2z
  Normal   SuccessfulDelete  3m53s  job-controller  Deleted pod: ads-2wp2z
  "Warning " D"eadlineExceeded " 3m53s  job-controller  "Job was active longer than specified deadline"

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
cat << EOF | kubectl apply -f - 
apiVersion: batch/v1
kind: CronJob
metadata: 
  name: cj
spec: 
  schedule: "*/1 * * * *"
  jobTemplate: 
    spec: 
      template: 
         metadata: 
           labels: 
             app: cj
         spec: 
           restartPolicy: Never
           containers: 
             - name: cj
               image: busybox
               command: ["echo", "hiiiiigelo"]
EOF
root@anji:~# kubectl get pod,job,cj 
NAME               SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/cj   */1 * * * *   False     0        <none>          0s

root@anji:~# kubectl get pod,job,cj 
NAME                    READY   STATUS      RESTARTS   AGE
pod/cj-27929689-fl6hm   0/1     Completed   0          3m4s
pod/cj-27929690-8qkxt   0/1     Completed   0          2m4s
pod/cj-27929691-25dv9   0/1     Completed   0          64s
pod/cj-27929692-vscbb   0/1     Completed   0          4s

NAME                    COMPLETIONS   DURATION   AGE
job.batch/cj-27929689 "  1/1  "         6s         3m4s
job.batch/cj-27929690   1/1           6s         2m4s
job.batch/cj-27929691   1/1           6s         64s
job.batch/cj-27929692 "  0/1 "          4s         4s

NAME               SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/cj   */1 * * * *   False     1        4s              4m38s

root@anji:~# kubectl describe cj
Schedule:                      */1 * * * *
Concurrency Policy:           " Allow"
Suspend:                       "False"
Successful Job History Limit: " 3 "       
Failed Job History Limit:     " 1"
    Command:
      echo
      hiiiiigelo
Last Schedule Time:  Tue, 07 Feb 2023 20:33:00 +0530
Active Jobs:         <none>
Events:
  Type    Reason            Age                    From                Message
  ----    ------            ----                   ----                -------
  Normal  SuccessfulCreate  15m                    cronjob-controller  Created job cj-27929688
  Normal  SawCompletedJob   15m                    cronjob-controller  Saw completed job: cj-27929688, status: Complete
  Normal  SuccessfulCreate  14m                    cronjob-controller  Created job cj-27929689
  Normal  SawCompletedJob   14m                    cronjob-controller  Saw completed job: cj-27929689, status: Complete
  Normal  SuccessfulCreate  13m                    cronjob-controller  Created job cj-27929690
  Normal  SawCompletedJob   13m                    cronjob-controller  Saw completed job: cj-27929690, status: Complete
  Normal  SuccessfulCreate  12m                    cronjob-controller  Created job cj-27929691
  Normal  SawCompletedJob   12m (x2 over 12m)      cronjob-controller  Saw completed job: cj-27929691, status: Complete
  Normal  SuccessfulDelete  12m                    cronjob-controller  Deleted job cj-27929688
  Normal  SuccessfulCreate  11m                    cronjob-controller  Created job cj-27929692
  Normal  SawCompletedJob   11m                    cronjob-controller  Saw completed job: cj-27929692, status: Complete
  Normal  SuccessfulDelete  11m                    cronjob-controller  Deleted job cj-27929689
  Normal  SuccessfulCreate  10m                    cronjob-controller  Created job cj-27929693
  Normal  SawCompletedJob   10m                    cronjob-controller  Saw completed job: cj-27929693, status: Complete
  Normal  SuccessfulDelete  10m                    cronjob-controller  Deleted job cj-27929690
  Normal  SuccessfulCreate  9m57s                  cronjob-controller  Created job cj-27929694
  Normal  SawCompletedJob   9m51s                  cronjob-controller  Saw completed job: cj-27929694, status: Complete
  Normal  SuccessfulDelete  9m51s                  cronjob-controller  Deleted job cj-27929691
  Normal  SuccessfulCreate  8m57s                  cronjob-controller  Created job cj-27929695
  Normal  SawCompletedJob   8m51s                  cronjob-controller  Saw completed job: cj-27929695, status: Complete
  Normal  SuccessfulDelete  8m51s                  cronjob-controller  Deleted job cj-27929692
  Normal  SuccessfulCreate  7m57s                  cronjob-controller  Created job cj-27929696
  Normal  SawCompletedJob   7m52s (x2 over 7m52s)  cronjob-controller  Saw completed job: cj-27929696, status: Complete
  Normal  SuccessfulDelete  7m52s                  cronjob-controller  Deleted job cj-27929693
  Normal  SawCompletedJob   51s (x7 over 6m51s)    cronjob-controller  (combined from similar events): Saw completed job: cj-27929703, status: Complete

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++=
cat << EOF | kubectl apply -f -
apiVersion: batch/v1
kind: CronJob
metadata: 
  name: cj
spec: 
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 2
  schedule: "*/1 * * * * "
  jobTemplate: 
     spec: 
       template: 
          metadata: 
            labels: 
              app: cj
          spec: 
            restartPolicy: Never
            containers:
            - name: cj
              image: busybox      
              command: ["echo", "helo how r u "]     
EOF

root@anji:~# kubectl get pod,job,cj 
NAME               SCHEDULE       SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/cj   */1 * * * *    False     0        <none>          1s

root@anji:~# kubectl get pod,job,cj 
NAME                    READY   STATUS              RESTARTS   AGE
pod/cj-27929730-vdljv   0/1     "ContainerCreating "  0          2s

NAME                    COMPLETIONS   DURATION   AGE
job.batch/cj-27929730   0/1           2s         2s

NAME               SCHEDULE       SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/cj   */1 * * * *    False     1        2s              45s

root@anji:~# kubectl get pod,job,cj 
NAME                    READY   STATUS      RESTARTS   AGE
pod/cj-27929730-vdljv   0/1     "Completed "  0          4s

NAME                    COMPLETIONS   DURATION   AGE
job.batch/cj-27929730   0/1           4s         4s

NAME               SCHEDULE       SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/cj   */1 * * * *    False     1        4s              47s

root@anji:~# kubectl get pod,job,cj 
NAME                    READY   STATUS              RESTARTS   AGE
pod/cj-27929730-vdljv   0/1     Completed           0          62s
pod/cj-27929731-bgcd2   0/1     "ContainerCreating "  0          2s

NAME                    COMPLETIONS   DURATION   AGE
job.batch/cj-27929730   1/1           6s         62s
job.batch/cj-27929731   0/1           2s         2s

NAME               SCHEDULE       SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/cj   */1 * * * *    False     1        2s              105s

NAME                    READY   STATUS      RESTARTS   AGE
pod/cj-27929730-vdljv   0/1     Completed   0          65s
pod/cj-27929731-bgcd2   0/1     Completed   0          5s

NAME                    COMPLETIONS   DURATION   AGE
job.batch/cj-27929730  " 1/1  "         6s         65s
job.batch/cj-27929731   0/1           5s         5s

NAME               SCHEDULE       SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/cj   */1 * * * *    False     1        5s              108s

root@anji:~# kubectl get pod,job,cj 
NAME                    READY   STATUS      RESTARTS   AGE
pod/cj-27929733-pgsgp   0/1     Completed   0          4m56s
pod/cj-27929734-vqknr   0/1     Completed   0          3m56s
pod/cj-27929735-c47dk   0/1     Completed   0          2m56s
pod/cj-27929736-2nbp9   0/1     Completed   0          116s
pod/cj-27929737-jjj7c   0/1     Completed   0          56s

NAME                    COMPLETIONS   DURATION   AGE
job.batch/cj-27929733   1/1           6s         4m56s
job.batch/cj-27929734   1/1           6s         3m56s
job.batch/cj-27929735   1/1           6s         2m56s
job.batch/cj-27929736   1/1           5s         116s
job.batch/cj-27929737   1/1           6s         56s

NAME               SCHEDULE       SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/cj   */1 * * * *    False     0        56s             8m39s

anji@anji:~/deployment$ kubectl describe cj
Schedule:                      */1 * * * * 
Concurrency Policy:            Allow
Suspend:                       False
Successful Job History Limit:  5
Failed Job History Limit:      2
Events:
  Type    Reason            Age    From                Message
  ----    ------            ----   ----                -------
  Normal  SuccessfulCreate  2m26s  cronjob-controller  Created job cj-27929730
  Normal  SawCompletedJob   2m20s  cronjob-controller  Saw completed job: cj-27929730, status: Complete
  Normal  SuccessfulCreate  86s    cronjob-controller  Created job cj-27929731
  Normal  SawCompletedJob   80s    cronjob-controller  Saw completed job: cj-27929731, status: Complete
  Normal  SuccessfulCreate  26s    cronjob-controller  Created job cj-27929732
  Normal  SawCompletedJob   21s    cronjob-controller  Saw completed job: cj-27929732, status: Complete
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
cat << EOF | kubectl apply -f -
apiVersion: batch/v1
kind: CronJob
metadata: 
  name: cj
spec: 
  successfulJobsHistoryLimit: 0
  failedJobsHistoryLimit: 0
  schedule: "*/1 * * * * "
  jobTemplate: 
     spec: 
       template: 
          metadata: 
            labels: 
              app: cj
          spec: 
            restartPolicy: Never
            containers:
            - name: cj
              image: busybox      
              command: ["echo", "helo how r u "]     
EOF
root@anji:~# kubectl get pod,job,cj 
NAME               SCHEDULE       SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/cj   */1 * * * *    False     "0    "    <none>          1s

root@anji:~# kubectl get pod,job,cj 
NAME                    READY   STATUS      RESTARTS   AGE
pod/cj-27929749-s225z   0/1     "Completed  " 0          3s

NAME                    COMPLETIONS   DURATION   AGE
job.batch/cj-27929749   0/1           3s         3s

NAME               SCHEDULE       SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/cj   */1 * * * *    False    " 1   "     3s              8s

root@anji:~# kubectl get pod,job,cj 
NAME                    READY   STATUS      RESTARTS   AGE
pod/cj-27929749-s225z   0/1   "  Completed  " 0          4s

NAME                    COMPLETIONS   DURATION   AGE
job.batch/cj-27929749   0/1           4s         4s

NAME               SCHEDULE       SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/cj   */1 * * * *    False     1        4s              9s

anji@anji:~/deployment$ kubectl describe cj
Schedule:                      */1 * * * * 
Successful Job History Limit:  0
Failed Job History Limit:      0
Last Schedule Time:  Tue, 07 Feb 2023 21:19:00 +0530
Active Jobs:         <none>
Events:
  Type    Reason            Age   From                Message
  ----    ------            ----  ----                -------
  Normal  SuccessfulCreate  45s   cronjob-controller  Created job cj-27929749
  Normal  SawCompletedJob   40s   cronjob-controller  Saw completed job: cj-27929749, "status: Complete"
  Normal  SuccessfulDelete  40s   cronjob-controller " Deleted job cj-27929749"
########################################################################################################################3"
Continuous Blue-Green Deployments With Kubernetes
Intro to Deployment Strategies: Blue-Green, Canary, and More
Blue-Green, Canary, and Other Kubernetes Deployment Strategies
https://codefresh.io/learn/software-deployment/kubernetes-rolloing-deployment-a-practical-guide/


apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginx
spec: 
  replicas: 6
  selector: 
    matchLabels: 
      app: nginx
  strategy: 
    type: RollingUpdate
    rollingUpdate:
       maxSurge: 1
       maxUnavailable: 0
  template: 
    metadata: 
       labels: 
         app: nginx
    spec: 
       containers: 
        - name: nginx
          image: nginx:1.15
          ports: 
            - containerPort: 80
            

anji@anji:~/deployment$ kubectl get pod 
NAME                    READY   STATUS    RESTARTS   AGE
nginx-788b6d85f-b8rrp   1/1     Running   0          19s
nginx-788b6d85f-fbzbq   1/1     Running   0          4m55s
nginx-788b6d85f-fsrlb   1/1     Running   0          4m55s
nginx-788b6d85f-j4gpp   1/1     Running   0          4m55s
nginx-788b6d85f-lblht   1/1     Running   0          19s
nginx-788b6d85f-r29wd   1/1     Running   0          19s

apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginx
spec: 
  replicas: 3
  selector: 
    matchLabels: 
      app: nginx
  strategy: 
    type: RollingUpdate
    rollingUpdate:
       maxSurge: 1
       maxUnavailable: 0
  template: 
    metadata: 
       labels: 
         app: nginx
    spec: 
       containers: 
        - name: nginx
          image: nginx:1.14
          ports: 
            - containerPort: 80

anji@anji:~/deployment$ kubectl get pod
NAME                     READY   STATUS              RESTARTS   AGE
nginx-7668b5b665-k8h5s   1/1     Running             0          8m9s
nginx-7668b5b665-nnrzh   1/1     Running             0          7m44s
nginx-7668b5b665-qhwnx   1/1     Running             0          7m46s
nginx-7f5b678dcb-"lnh74  " 0/1     "ContainerCreating "  0          1s

anji@anji:~/deployment$ kubectl get pod
NAME                     READY   STATUS              RESTARTS   AGE
nginx-7668b5b665-qhwnx   1/1     Running             0          8m6s
nginx-7f5b678dcb-654kl   1/1     Running             0          2s
nginx-7f5b678dcb-lnh74   1/1     Running             0          21s
nginx-7f5b678dcb-tmn4q   0/1     ContainerCreating   0          1s
anji@anji:~/deployment$ kubectl get pod
NAME                     READY   STATUS        RESTARTS   AGE
nginx-7668b5b665-qhwnx   1/1     "Terminating "  0          8m7s
nginx-7f5b678dcb-654kl   1/1     Running       0          3s
nginx-7f5b678dcb-lnh74   1/1     Running       0          22s
nginx-7f5b678dcb-tmn4q   1/1     Running       0          2s
anji@anji:~/deployment$ kubectl get pod
NAME                     READY   STATUS    RESTARTS   AGE
nginx-7f5b678dcb-654kl   1/1     Running   0          3s
nginx-7f5b678dcb-lnh74   1/1     Running   0          22s
nginx-7f5b678dcb-tmn4q   1/1     Running   0          2s

anji@anji:~/deployment$ kubectl get deployment  -o wide 
NAME    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
nginx   3/3     3            3           50m   nginx        nginx:1.14   app=nginx

+++++++++++++++++++"
anji@anji:~/deployment$ kubectl rollout history deployment.apps/nginx
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1         <none>
2         <none>
3         <none>
4         <none>

anji@anji:~/deployment$ kubectl rollout undo deployment.apps/nginx --to-revision=2
deployment.apps/nginx rolled back

anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-7f5b678dcb-654kl   1/1     Running   0          41m   10.44.0.5   worker   <none>           <none>
pod/nginx-7f5b678dcb-lnh74   1/1     Running   0          42m   10.44.0.4   worker   <none>           <none>
pod/nginx-7f5b678dcb-tmn4q   1/1     Running   0          41m   10.44.0.2   worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   3/3    " 3   "         3           63m   nginx       " nginx:1.14 "  app=nginx

anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS              RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-68d9b6c666-pjp8c   0/1     "ContainerCreating "  0          0s    <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-654kl   1/1     Running             0          46m   10.44.0.5   worker   <none>           <none>
pod/nginx-7f5b678dcb-lnh74   1/1     Running             0          46m   10.44.0.4   worker   <none>           <none>
pod/nginx-7f5b678dcb-tmn4q   1/1     Running             0          46m   10.44.0.2   worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   3/3    " 1   "         3           68m   nginx        "nginx:1.16   "app=nginx

anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS              RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-68d9b6c666-pjp8c   1/1     Running             0          2s    10.44.0.1   worker   <none>           <none>
pod/nginx-68d9b6c666-q6sfc   0/1     "ContainerCreating "  0          1s    <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-654kl   1/1    " Terminating    "     0          46m   10.44.0.5   worker   <none>           <none>
pod/nginx-7f5b678dcb-lnh74   1/1     Running             0          46m   10.44.0.4   worker   <none>           <none>
pod/nginx-7f5b678dcb-tmn4q   1/1     Running             0          46m   10.44.0.2   worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   3/3  "   2    "       " 3   "        68m   nginx        nginx:1.16   app=nginx
anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS        RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-68d9b6c666-pjp8c   1/1     Running       0          2s    10.44.0.1   worker   <none>           <none>
pod/nginx-68d9b6c666-q6sfc   1/1     Running       0          1s    10.44.0.3   worker   <none>           <none>
pod/nginx-7f5b678dcb-654kl   0/1    " Terminating  " 0          46m   10.44.0.5   worker   <none>           <none>
pod/nginx-7f5b678dcb-lnh74   1/1     Running       0          46m   10.44.0.4   worker   <none>           <none>
pod/nginx-7f5b678dcb-tmn4q   1/1     Running       0          46m   10.44.0.2   worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   "4/3  "   2          "  4    "       68m   nginx      "  nginx:1.16 "  app=nginx
anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS              RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-68d9b6c666-kqmjx   0/1    " ContainerCreating  " 0          1s    <none>      worker   <none>           <none>
pod/nginx-68d9b6c666-pjp8c   1/1     Running             0          3s    10.44.0.1   worker   <none>           <none>
pod/nginx-68d9b6c666-q6sfc   1/1     Running             0          2s    10.44.0.3   worker   <none>           <none>
pod/nginx-7f5b678dcb-lnh74   1/1     Running             0          46m   10.44.0.4   worker   <none>           <none>
pod/nginx-7f5b678dcb-tmn4q   1/1  "   Terminating   "      0          46m   10.44.0.2   worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   3/3     3          '  3    '       68m   nginx        nginx:1.16   app=nginx
anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS        RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-68d9b6c666-kqmjx   1/1     Running       0          2s    10.44.0.2   worker   <none>           <none>
pod/nginx-68d9b6c666-pjp8c   1/1     Running       0          4s    10.44.0.1   worker   <none>           <none>
pod/nginx-68d9b6c666-q6sfc   1/1     Running       0          3s    10.44.0.3   worker   <none>           <none>
pod/nginx-7f5b678dcb-lnh74   1/1     Terminating   0          46m   10.44.0.4   worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   3/3     3            3           68m   nginx        nginx:1.16   app=nginx

anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS        RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-68d9b6c666-kqmjx   1/1     Running       0          2s    10.44.0.2   worker   <none>           <none>
pod/nginx-68d9b6c666-pjp8c   1/1     Running       0          4s    10.44.0.1   worker   <none>           <none>
pod/nginx-68d9b6c666-q6sfc   1/1     Running       0          3s    10.44.0.3   worker   <none>           <none>
pod/nginx-7f5b678dcb-lnh74   0/1     Terminating   0          46m   10.44.0.4   worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   3/3     3            3           68m   nginx        nginx:1.16   app=nginx

anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-68d9b6c666-kqmjx   1/1     Running   0          3s    10.44.0.2   worker   <none>           <none>
pod/nginx-68d9b6c666-pjp8c   1/1     Running   0          5s    10.44.0.1   worker   <none>           <none>
pod/nginx-68d9b6c666-q6sfc   1/1     Running   0          4s    10.44.0.3   worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   3/3     3            3           68m   nginx        "nginx:1.16 "  app=nginx

==================================================================

apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginx
spec: 
  replicas: 6
  selector: 
    matchLabels: 
      app: nginx
  strategy: 
    type: RollingUpdate
    rollingUpdate:
       maxSurge: 2           ====================//// 
       maxUnavailable: 2    ======================////see  look 
  template: 
    metadata: 
       labels: 
         app: nginx
    spec: 
       containers: 
        - name: nginx
          image: nginx:1.14
          ports: 
            - containerPort: 80

anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS    RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-7d6785dbdc-49xpz   1/1     Running   0          2m34s   10.44.0.1   worker   <none>           <none>
pod/nginx-7d6785dbdc-4sz2p   1/1     Running   0          10m     10.44.0.4   worker   <none>           <none>
pod/nginx-7d6785dbdc-c8r8x   1/1     Running   0          9m59s   10.44.0.2   worker   <none>           <none>
pod/nginx-7d6785dbdc-cmwfm   1/1     Running   0          2m34s   10.44.0.5   worker   <none>           <none>
pod/nginx-7d6785dbdc-f67tx   1/1     Running   0          2m34s   10.44.0.6   worker   <none>           <none>
pod/nginx-7d6785dbdc-krzd4   1/1     Running   0          10m     10.44.0.3   worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   6/6    " 6  "          6           99m   nginx        "nginx:1.20 "  app=nginx
anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS              RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-7d6785dbdc-49xpz   1/1   "  Terminating   "      0          2m43s   10.44.0.1   worker   <none>           <none>
pod/nginx-7d6785dbdc-4sz2p   1/1     Running             0          10m     10.44.0.4   worker   <none>           <none>
pod/nginx-7d6785dbdc-c8r8x   1/1     Running             0          10m     10.44.0.2   worker   <none>           <none>
pod/nginx-7d6785dbdc-cmwfm   1/1    " Terminating    "     0          2m43s   10.44.0.5   worker   <none>           <none>
pod/nginx-7d6785dbdc-f67tx   1/1     Running             0          2m43s   10.44.0.6   worker   <none>           <none>
pod/nginx-7d6785dbdc-krzd4   1/1     Running             0          10m     10.44.0.3   worker   <none>           <none>
pod/nginx-7f5b678dcb-dhdm5   0/1     ContainerCreating   0          1s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-hm5gs   0/1     ContainerCreating   0          1s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-mmp8z   0/1     ContainerCreating   0          1s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-ppdcg   0/1     ContainerCreating   0          1s      <none>      worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   4/6     "4  "         " 4      "     99m   nginx        "nginx:1.14 "  app=nginx
anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS              RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES
"pod/nginx-7d6785dbdc-49xpz   1/1     Terminating  "       0          2m43s   10.44.0.1   worker   <none>           <none>
pod/nginx-7d6785dbdc-4sz2p   1/1     Running             0          10m     10.44.0.4   worker   <none>           <none>
pod/nginx-7d6785dbdc-c8r8x   1/1     Running             0          10m     10.44.0.2   worker   <none>           <none>
"pod/nginx-7d6785dbdc-cmwfm   1/1     Terminating  "       0          2m43s   10.44.0.5   worker   <none>           <none>
pod/nginx-7d6785dbdc-f67tx   1/1     Running             0          2m43s   10.44.0.6   worker   <none>           <none>
pod/nginx-7d6785dbdc-krzd4   1/1     Running             0          10m     10.44.0.3   worker   <none>           <none>
pod/nginx-7f5b678dcb-dhdm5   0/1     ContainerCreating   0          1s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-hm5gs   0/1     ContainerCreating   0          1s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-mmp8z   0/1     ContainerCreating   0          1s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-ppdcg   0/1     ContainerCreating   0          1s      <none>      worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx  " 4/6  "   4            4           99m   nginx        nginx:1.14   app=nginx
anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS              RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES
"pod/nginx-7d6785dbdc-49xpz   1/1     Terminating   "      0          2m44s   10.44.0.1   worker   <none>           <none>
pod/nginx-7d6785dbdc-4sz2p   1/1     Running             0          10m     10.44.0.4   worker   <none>           <none>
pod/nginx-7d6785dbdc-c8r8x   1/1     Running             0          10m     10.44.0.2   worker   <none>           <none>
pod/nginx-7d6785dbdc-f67tx   1/1     Running             0          2m44s   10.44.0.6   worker   <none>           <none>
pod/nginx-7d6785dbdc-krzd4   1/1     Running             0          10m     10.44.0.3   worker   <none>           <none>
pod/nginx-7f5b678dcb-dhdm5   0/1     ContainerCreating   0          2s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-hm5gs   0/1     ContainerCreating   0          2s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-mmp8z   0/1     ContainerCreating   0          2s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-ppdcg   0/1     ContainerCreating   0          2s      <none>      worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   4/6     4            4           99m   nginx        nginx:1.14   app=nginx
anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS              RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-7d6785dbdc-4sz2p   1/1     Running             0          10m     10.44.0.4   worker   <none>           <none>
pod/nginx-7d6785dbdc-c8r8x   1/1     Running             0          10m     10.44.0.2   worker   <none>           <none>
"pod/nginx-7d6785dbdc-f67tx   1/1     Terminating  "       0          2m45s   10.44.0.6   worker   <none>           <none>
pod/nginx-7d6785dbdc-krzd4   1/1     Running             0          10m     10.44.0.3   worker   <none>           <none>
pod/nginx-7f5b678dcb-dhdm5   1/1     Running             0          3s      10.44.0.1   worker   <none>           <none>
pod/nginx-7f5b678dcb-hm5gs   0/1     ContainerCreating   0          3s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-mmp8z   0/1     ContainerCreating   0          3s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-ppdcg   0/1     ContainerCreating   0          3s      <none>      worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   5/6     4            5           99m   nginx        nginx:1.14   app=nginx
anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS              RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-7d6785dbdc-4sz2p   1/1     Running             0          10m     10.44.0.4   worker   <none>           <none>
pod/nginx-7d6785dbdc-c8r8x   1/1     Running             0          10m     10.44.0.2   worker   <none>           <none>
pod/nginx-7d6785dbdc-f67tx   1/1     Terminating         0          2m46s   10.44.0.6   worker   <none>           <none>
pod/nginx-7d6785dbdc-krzd4   1/1     Running             0          10m     10.44.0.3   worker   <none>           <none>
pod/nginx-7f5b678dcb-dhdm5   1/1     Running             0          4s      10.44.0.1   worker   <none>           <none>
pod/nginx-7f5b678dcb-hm5gs   0/1     ContainerCreating   0          4s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-mmp8z   0/1     ContainerCreating   0          4s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-ppdcg   0/1     ContainerCreating   0          4s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-zmtqn   0/1     Pending             0          1s      <none>      worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   4/6     5            4           99m   nginx        nginx:1.14   app=nginx
anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS              RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES
"pod/nginx-7d6785dbdc-4sz2p   1/1     Terminating "        0          10m     10.44.0.4   worker   <none>           <none>
pod/nginx-7d6785dbdc-c8r8x   1/1     Running             0          10m     10.44.0.2   worker   <none>           <none>
"pod/nginx-7d6785dbdc-f67tx   1/1     Terminating  "       0          2m46s   10.44.0.6   worker   <none>           <none>
pod/nginx-7d6785dbdc-krzd4   1/1     Running             0          10m     10.44.0.3   worker   <none>           <none>
pod/nginx-7f5b678dcb-dgtr9   0/1     Pending             0          0s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-dhdm5   1/1     Running             0          4s      10.44.0.1   worker   <none>           <none>
pod/nginx-7f5b678dcb-hm5gs   0/1     ContainerCreating   0          4s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-mmp8z   1/1     Running             0          4s      10.44.0.7   worker   <none>           <none>
pod/nginx-7f5b678dcb-ppdcg   0/1     ContainerCreating   0          4s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-zmtqn   0/1     ContainerCreating   0          1s      <none>      worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   4/6     6            4           99m   nginx        nginx:1.14   app=nginx
anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS              RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-7d6785dbdc-4sz2p   1/1     Terminating         0          10m     10.44.0.4   worker   <none>           <none>
pod/nginx-7d6785dbdc-c8r8x   1/1     Terminating         0          10m     10.44.0.2   worker   <none>           <none>
pod/nginx-7d6785dbdc-f67tx   1/1     Terminating         0          2m47s   10.44.0.6   worker   <none>           <none>
pod/nginx-7d6785dbdc-krzd4   1/1     Terminating         0          10m     10.44.0.3   worker   <none>           <none>
pod/nginx-7f5b678dcb-dgtr9   0/1     Pending             0          1s      <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-dhdm5   1/1     Running             0          5s      10.44.0.1   worker   <none>           <none>
pod/nginx-7f5b678dcb-hm5gs   1/1     Running             0          5s      10.44.0.5   worker   <none>           <none>
pod/nginx-7f5b678dcb-mmp8z   1/1     Running             0          5s      10.44.0.7   worker   <none>           <none>
pod/nginx-7f5b678dcb-ppdcg   1/1     Running             0          5s      10.44.0.8   worker   <none>           <none>
pod/nginx-7f5b678dcb-zmtqn   0/1     ContainerCreating   0          2s      <none>      worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   4/6     6            4           99m   nginx        nginx:1.14   app=nginx
anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS              RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-7d6785dbdc-4sz2p   1/1     Terminating         0          10m   10.44.0.4   worker   <none>           <none>
pod/nginx-7d6785dbdc-c8r8x   1/1     Terminating         0          10m   10.44.0.2   worker   <none>           <none>
pod/nginx-7d6785dbdc-krzd4   1/1     Terminating         0          10m   10.44.0.3   worker   <none>           <none>
pod/nginx-7f5b678dcb-dgtr9   0/1     Pending             0          2s    <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-dhdm5   1/1     Running             0          6s    10.44.0.1   worker   <none>           <none>
pod/nginx-7f5b678dcb-hm5gs   1/1     Running             0          6s    10.44.0.5   worker   <none>           <none>
pod/nginx-7f5b678dcb-mmp8z   1/1     Running             0          6s    10.44.0.7   worker   <none>           <none>
pod/nginx-7f5b678dcb-ppdcg   1/1     Running             0          6s    10.44.0.8   worker   <none>           <none>
pod/nginx-7f5b678dcb-zmtqn   0/1     ContainerCreating   0          3s    <none>      worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   4/6     6            4           99m   nginx        nginx:1.14   app=nginx
anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS              RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-7d6785dbdc-c8r8x   1/1     Terminating         0          10m   10.44.0.2   worker   <none>           <none>
pod/nginx-7d6785dbdc-krzd4   1/1     Terminating         0          10m   10.44.0.3   worker   <none>           <none>
pod/nginx-7f5b678dcb-dgtr9   0/1     ContainerCreating   0          4s    <none>      worker   <none>           <none>
pod/nginx-7f5b678dcb-dhdm5   1/1     Running             0          8s    10.44.0.1   worker   <none>           <none>
pod/nginx-7f5b678dcb-hm5gs   1/1     Running             0          8s    10.44.0.5   worker   <none>           <none>
pod/nginx-7f5b678dcb-mmp8z   1/1     Running             0          8s    10.44.0.7   worker   <none>           <none>
pod/nginx-7f5b678dcb-ppdcg   1/1     Running             0          8s    10.44.0.8   worker   <none>           <none>
pod/nginx-7f5b678dcb-zmtqn   1/1     Running             0          5s    10.44.0.6   worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   5/6     6            5           99m   nginx        nginx:1.14   app=nginx
anji@anji:~/deployment$ kubectl get pod,deployment -o wide 
NAME                         READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-7f5b678dcb-dgtr9   1/1     Running   0          6s    10.44.0.4   worker   <none>           <none>
pod/nginx-7f5b678dcb-dhdm5   1/1     Running   0          10s   10.44.0.1   worker   <none>           <none>
pod/nginx-7f5b678dcb-hm5gs   1/1     Running   0          10s   10.44.0.5   worker   <none>           <none>
pod/nginx-7f5b678dcb-mmp8z   1/1     Running   0          10s   10.44.0.7   worker   <none>           <none>
pod/nginx-7f5b678dcb-ppdcg   1/1     Running   0          10s   10.44.0.8   worker   <none>           <none>
pod/nginx-7f5b678dcb-zmtqn   1/1     Running   0          7s    10.44.0.6   worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   6/6     6            6           99m   nginx      "  nginx:1.14  " app=nginx
===========================================================

apiVersion: v1
kind: ReplicationController
metadata: 
   name: nginx1

spec: 
  replicas: 3
  selector: 
    app: anji   
    cat: dog 
    a: b
    c: g

  template: 
     metadata:
       labels: 
        app: anji
        cat: dog
        a: b
        c: g
     spec:
       containers: 
         - name: nginx
           image: nginx:1.12

anji@anji:~$ kubectl get rc,pod -o wide --show-labels
NAME                           DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR                   LABELS
replicationcontroller/nginx1 '  3  '      " 3    "     3       26m   nginx       " nginx:1.12 "  a=b,app=anji,c=g,cat=dog   a=b,app=anji,c=g,cat=dog

NAME               READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES   LABELS
pod/nginx1-28m7m   1/1     Running   0          26m   10.44.0.3   worker   <none>           <none>            a=b,app=anji,c=g,cat=dog
pod/nginx1-bbt5h   1/1     Running   0          26m   10.44.0.1   worker   <none>           <none>            a=b,app=anji,c=g,cat=dog
pod/nginx1-mqw8x   1/1     Running   0          26m   10.44.0.2   worker   <none>           <none>            a=b,app=anji,c=g,cat=dog

anji@anji:~$ kubectl describe rc nginx1
Name:         nginx1
Namespace:    default
Selector:     a=b,app=anji,c=g,cat=dog
Labels:       a=b
              app=anji
              c=g
              cat=dog
Annotations:  <none>
"Replicas:     3 current / 3 desired"
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  a=b
           app=anji
           c=g
           cat=dog
  Containers:
   nginx:
    Image:        nginx:1.12

=====================           after  after  =============****-
||  replicationcontroller was support by rolling updates   ||
||  replicaset was  "not "supported by rolling updates     ||
||   deployment is  best  way to supported rolling updates ||
///////////////////////////////////////////////////////////


anji@anji:~$ kubectl delete rc  --cascade=false  nginx1
warning: --cascade=false is deprecated (boolean value) and can be replaced with --cascade=orphan.
replicationcontroller "nginx1" deleted
anji@anji:~$ 

anji@anji:~$ kubectl get nodes -o wide 
NAME     STATUS   ROLES           AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
master   Ready    control-plane   13d   v1.26.1   192.168.122.31    <none>        Ubuntu 20.04.5 LTS   5.15.0-58-generic   containerd://1.6.16
worker   Ready    <none>          13d   v1.26.1   192.168.122.226   <none>        Ubuntu 20.04.5 LTS   5.15.0-58-generic   containerd://1.6.16
anji@anji:~$ 

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++=
apiVersion: apps/v1
kind: ReplicaSet
metadata: 
   name: test
spec: 
  replicas: 3
  selector: 
    matchExpressions: 
       - key: team
         operator: In
         values: 
           - test
           - dev
           - prod
       - key: team
         operator: NotIn
         values: 
           - team
  template: 
     metadata:
        labels: 
          team: dev
     spec:
       containers: 
         - name: rs
           image: nginx:1.2

anji@anji:~$ kubectl get pod,rs  -o wide 
NAME             READY   STATUS    RESTARTS   AGE    IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx        1/1     Running   0          114s   10.44.0.4   worker   <none>           <none>
pod/test-556l2   1/1     Running   0          25s    10.44.0.2   worker   <none>           <none>
pod/test-skzll   1/1     Running   0          25s    10.44.0.1   worker   <none>           <none>
pod/test-ssldp   1/1     Running   0          25s    10.44.0.3   worker   <none>           <none>

NAME                   DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR
replicaset.apps/test   3         3         3       25s   rs           nginx:1.16  " team in (dev,prod,test),team notin (team)"
anji@anji:~$ kubectl describe  replicaset.apps/test
Name:         test
Namespace:    default
"Selector:     team in (dev,prod,test),team notin (team)"
Labels:       <none>
Annotations:  <none>
"Replicas:     3 current / 3 desired"
Pods Status:  "3 Running" / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels: " team=dev"
  Containers:
   rs:
    Image:        nginx:1.16
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  30s   replicaset-controller  Created pod: test-ssldp
  Normal  SuccessfulCreate  30s   replicaset-controller  Created pod: test-skzll
  Normal  SuccessfulCreate  30s   replicaset-controller  Created pod: test-556l2
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec: 
  replicas: 3
  selector: 
    matchLabels: 
       app: nginx
  template: 
    metadata: 
       labels:
          app: nginx
    spec:
      containers: 
        - name: nginx
          image: nginx:1.7  
          ports:
            - containerPort: 80     "replicaset on top of deployment used to  overcome the feature rolling update "

 anji@anji:~/deployment$ kubectl get deployment,pod,rs -o wide 
NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES      SELECTOR
"deployment.apps/nginx "  3/3     3            3           11m   nginx        nginx:1.7   app=nginx

NAME                         READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-6d77c7955d-49q5q   1/1     Running   0          11m   10.44.0.1   worker   <none>           <none>
pod/nginx-6d77c7955d-x7f6f   1/1     Running   0          11m   10.44.0.2   worker   <none>           <none>
pod/nginx-6d77c7955d-xzjkd   1/1     Running   0          11m   10.44.0.3   worker   <none>           <none>

NAME                               DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES      SELECTOR
"replicaset.apps/nginx-6d77c7955d "  3         3         3       11m   nginx        nginx:1.7   app=nginx,pod-template-hash=6d77c7955d
-----
anji@anji:~/deployment$ kubectl describe deployment nginx  

"Annotations:            deployment.kubernetes.io/revision: 1"
Selector:               app=nginx
Replicas:              " 3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:          " RollingUpdate""
MinReadySeconds:        0
"RollingUpdateStrategy:  25% max unavailable, 25% max surge"
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.7
   
"NewReplicaSet:   nginx-6d77c7955d (3/3 replicas created)"
"OldReplicaSets:  <none>"
"NewReplicaSet:   nginx-6d77c7955d (3/3 replicas created)"
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  34m   "deployment-controller "" Scaled up replica set nginx-6d77c7955d to 3"

-----"
anji@anji:~/deployment$ kubectl get all -o wide 
NAME                         READY   STATUS    RESTARTS   AGE    IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-6d77c7955d-49q5q   1/1     Running   0          159m   10.44.0.1   worker   <none>           <none>
pod/nginx-6d77c7955d-x7f6f   1/1     Running   0          159m   10.44.0.2   worker   <none>           <none>
pod/nginx-6d77c7955d-xzjkd   1/1     Running   0          159m   10.44.0.3   worker   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   7d1h   <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES      SELECTOR
deployment.apps/nginx   3/3     3            3           159m   nginx        nginx:1.7   app=nginx

NAME                               DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES      SELECTOR
replicaset.apps/nginx-6d77c7955d   3         3         3       159m   nginx        nginx:1.7   app=nginx,pod-template-hash=6d77c7955d

anji@anji:~/deployment$ kubectl rollout history deployment.apps/nginx 
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1         <none>

-----
anji@anji:~/deployment$ kubectl apply -f a.yaml --record=true        "see look"
Flag --record has been "deprecated, "--record will be removed in the future

deployment.apps/nginx created

anji@anji:~$ kubectl get all -o wide 
NAME                         READY   STATUS    RESTARTS   AGE    IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-6d77c7955d-6tv77   1/1     Running   0          2m2s   10.44.0.2   worker   <none>           <none>
pod/nginx-6d77c7955d-lvt4b   1/1     Running   0          2m2s   10.44.0.3   worker   <none>           <none>
pod/nginx-6d77c7955d-rdzpz   1/1     Running   0          2m2s   10.44.0.1   worker   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   7d1h   <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES      SELECTOR
deployment.apps/nginx   3/3     3            3           2m3s   nginx        nginx:1.7   app=nginx

NAME                               DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES      SELECTOR
replicaset.apps/nginx-6d77c7955d   3         3         3       2m2s   nginx        nginx:1.7   app=nginx,pod-template-hash=6d77c7955d

anji@anji:~$ kubectl rollout history deployment.apps/nginx  
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1         "kubectl apply --filename=a.yaml --record=true"    super nice "

anji@anji:~/deployment$ kubectl apply -f depstable.yaml  --record=true
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/flask-html configured
service/flask-html configured
anji@anji:~/deployment$ kubectl roleout history deployment.apps/flask-html 
error: unknown command "roleout" for "kubectl"

Did you mean this?
	rollout
anji@anji:~/deployment$ kubectl rollout history deployment.apps/flask-html 
deployment.apps/flask-html 
REVISION  CHANGE-CAUSE
8         <none>
9         <none>
10        kubectl apply --filename=depstable.yaml --record=true "


+++++++++++++
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec: 
  replicas: 3
  selector: 
    matchLabels: 
       app: nginx
  template: 
    metadata: 
       labels:
          app: nginx
    spec:
      containers: 
        - name: nginx
          image: nginx:1.9        "  look see "
          ports:
            - containerPort: 80
--"
anji@anji:~$ kubectl describe deployment

Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        "nginx:1.7"
OldReplicaSets:  "<none>"
NewReplicaSet:   nginx-6d77c7955d (3/3 replicas created) 
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  10m   deployment-controller  Scaled up replica set nginx-6d77c7955d to 3 "
---
nji@anji:~$ kubectl describe deployment

Replicas:               "3 desired "| "1 updated" |" 4 total "| "3 available "| "1 unavailable"
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:       " nginx:1.9"
OldReplicaSets:  nginx-6d77c7955d "(3/3 "replicas created)   
NewReplicaSet:   nginx-79f86456d4 "(1/1 "replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  11m   deployment-controller  Scaled up replica set" nginx-6d77c7955d to 3"
  Normal  ScalingReplicaSet  0s    deployment-controller  Scaled up replica set "nginx-79f86456d4 to 1"
---  "


anji@anji:~$ kubectl describe deployment
Replicas:              " 3 desired "| "3 updated "| "3 total" | "3 available "| "0 unavailable"
    Image:       " nginx:1.9"
OldReplicaSets:  "<none>"
NewReplicaSet:   "nginx-79f86456d4 (3/3 replicas created)"
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  11m   deployment-controller  Scaled up replica set "nginx-6d77c7955d to 3"
  Normal  ScalingReplicaSet  33s   deployment-controller  Scaled up replica set nginx-79f86456d4 to 1
  Normal  ScalingReplicaSet  7s    deployment-controller  Scaled down replica set nginx-6d77c7955d to 2 from 3
  Normal  ScalingReplicaSet  7s    deployment-controller  Scaled up replica set "nginx-79f86456d4 to 2 from 1"
  Normal  ScalingReplicaSet  5s    deployment-controller  Scaled down replica set "nginx-6d77c7955d to 1 from 2"
  Normal  ScalingReplicaSet  5s    deployment-controller  Scaled up replica set nginx-79f86456d4 to 3 from 2
  Normal  ScalingReplicaSet  4s    deployment-controller  Scaled down replica set "nginx-6d77c7955d to 0 from 1"
---"
anji@anji:~$ kubectl describe deployment
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:     "   nginx:1.9"   success 
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   nginx-79f86456d4 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  11m   deployment-controller  Scaled up replica set nginx-6d77c7955d to 3
  Normal  ScalingReplicaSet  35s   deployment-controller  Scaled up replica set nginx-79f86456d4 to 1
  Normal  ScalingReplicaSet  9s    deployment-controller  Scaled down replica set nginx-6d77c7955d to 2 from 3
  Normal  ScalingReplicaSet  9s    deployment-controller  Scaled up replica set nginx-79f86456d4 to 2 from 1
  Normal  ScalingReplicaSet  7s    deployment-controller  Scaled down replica set nginx-6d77c7955d to 1 from 2
  Normal  ScalingReplicaSet  7s    deployment-controller  Scaled up replica set nginx-79f86456d4 to 3 from 2
  Normal  ScalingReplicaSet  6s    deployment-controller  Scaled down replica set nginx-6d77c7955d to 0 from 1
----"
anji@anji:~$ kubectl get all -o wide 
NAME                         READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-79f86456d4-22lg8   1/1     Running   0          15m   10.44.0.1   worker   <none>           <none>
pod/nginx-79f86456d4-5kqt7   1/1     Running   0          15m   10.44.0.5   worker   <none>           <none>
pod/nginx-79f86456d4-p992x   1/1     Running   0          15m   10.44.0.4   worker   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES      SELECTOR
deployment.apps/nginx   3/3     3            3           27m   nginx        nginx:1.9   app=nginx

NAME                               DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES      SELECTOR
replicaset.apps/nginx-6d77c7955d   0         0         0       27m   nginx       " nginx:1.7 "  app=nginx,pod-template-hash=6d77c7955d
replicaset.apps/nginx-"79f86456d4 "  3         3         3       15m   nginx       " nginx:1.9  " app=nginx,pod-template-hash="79f86456d4" "

anji@anji:~$ kubectl rollout history deployment.apps/nginx 
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=a.yaml --record=true
2         <none>
++++++++++++++++++++++++++++++++"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec: 
  replicas: 3
  selector: 
    matchLabels: 
       app: nginx
  template: 
    metadata: 
       labels:
          app: nginx
    spec:
      containers: 
        - name: nginx
          image: nginx:1.22          "update "
          ports:
            - containerPort: 80
                      
------
anji@anji:~$ kubectl describe deployment 
Replicas:               3 desired |" 1 updated "| "4 total "| "3 available "| "1 unavailable"
    Image:        "nginx:1.22"
OldReplicaSets:  "nginx-79f86456d4 (3/3 replicas created)"
NewReplicaSet:   "nginx-8cf4bf97 (1/1 replicas created)"
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  35m   deployment-controller  Scaled up replica set" nginx-6d77c7955d to 3"
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled up replica set nginx-79f86456d4 to 1
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled down replica set' nginx-6d77c7955d to 2 from 3'
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled up replica set "nginx-79f86456d4 to 2 from 1"
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled down replica set "nginx-6d77c7955d to 1 from 2"
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled up replica set nginx-79f86456d4 to 3 from 2
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled down replica set" nginx-6d77c7955d to 0 from 1"
  Normal  ScalingReplicaSet  1s    deployment-controller  Scaled up replica set "nginx-8cf4bf97 to 1"  
=== "
anji@anji:~$ kubectl describe deployment 
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.22
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   "nginx-8cf4bf97 (3/3 replicas created)"
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  35m   deployment-controller  Scaled up replica set nginx-6d77c7955d to 3
  Normal  ScalingReplicaSet  24m   deployment-controller  Scaled up replica set nginx-79f86456d4 to 1
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled down replica set nginx-6d77c7955d to 2 from 3
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled up replica set nginx-79f86456d4 to 2 from 1
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled down replica set nginx-6d77c7955d to 1 from 2
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled up replica set nginx-79f86456d4 to 3 from 2
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled down replica set nginx-6d77c7955d to 0 from 1
  Normal  ScalingReplicaSet  15s   deployment-controller  Scaled up replica set nginx-8cf4bf97 to 1
  Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-79f86456d4 to 2 from 3
  Normal  ScalingReplicaSet  14s   deployment-controller  Scaled up replica set nginx-8cf4bf97 to 2 from 1
  Normal  ScalingReplicaSet  13s   deployment-controller  Scaled down replica set nginx-79f86456d4 to 1 from 2
  Normal  ScalingReplicaSet  13s   deployment-controller  Scaled up replica set nginx-8cf4bf97 to 3 from 2
  Normal  ScalingReplicaSet  12s   deployment-controller  Scaled down replica set nginx-79f86456d4 to 0 from 1
+++++++++++++++++++++++++++++++++++++"
anji@anji:~$ kubectl get all -o wide 
NAME                       READY   STATUS    RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-8cf4bf97-4lh66   1/1     Running   0          8m23s   10.44.0.1   worker   <none>           <none>
pod/nginx-8cf4bf97-9fsb2   1/1     Running   0          8m25s   10.44.0.2   worker   <none>           <none>
pod/nginx-8cf4bf97-nl2r7   1/1     Running   0          8m24s   10.44.0.3   worker   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   7d2h   <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   3/3     3            3           43m   nginx        "nginx:1.22  " app=nginx

NAME                               DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES       SELECTOR
replicaset.apps/nginx-6d77c7955d   0         0         0       43m     nginx        "nginx:1.7"    app=nginx,pod-template-hash=6d77c7955d
replicaset.apps/nginx-79f86456d4   0         0         0       32m     nginx        "nginx:1.9   " app=nginx,pod-template-hash=79f86456d4
replicaset.apps/nginx-8cf4bf97     3         3         3       8m25s   nginx        "nginx:1.22  " app=nginx,pod-template-hash=8cf4bf97
----"
deployment.apps/nginx           \\\\\\\\\\\\\\\\\\//////////////  importent  look see  rolleback
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=a.yaml --record=true
2         <none>
3         <none>
++++++++++++
anji@anji:~$ kubectl rollout undo deployment.apps/nginx --to-revision=1
deployment.apps/nginx "rolled back"

anji@anji:~$ kubectl rollout history    deployment.apps/nginx  
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
2         <none>
3         <none>
4    "     kubectl apply --filename=a.yaml --record=true"

anji@anji:~$ kubectl get all -o wide 
NAME                         READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-6d77c7955d-8svph   1/1     Running   0          38s   10.44.0.2   worker   <none>           <none>
pod/nginx-6d77c7955d-t2hrk   1/1     Running   0          39s   10.44.0.1   worker   <none>           <none>
pod/nginx-6d77c7955d-xwnsm   1/1     Running   0          40s   10.44.0.4   worker   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   7d2h   <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES      SELECTOR
deployment.apps/nginx   3/3     3            3           53m   nginx        "nginx:1.7   "app=nginx

NAME                               DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR
replicaset.apps/nginx-6d77c7955d   3         3         3       53m   nginx        nginx:1.7    app=nginx,pod-template-hash=6d77c7955d
replicaset.apps/nginx-79f86456d4   0         0         0       42m   nginx        nginx:1.9    app=nginx,pod-template-hash=79f86456d4
replicaset.apps/nginx-8cf4bf97     0         0         0       18m   nginx        nginx:1.22   app=nginx,pod-template-hash=8cf4bf97  "

anji@anji:~$ kubectl rollout history deployment.apps/nginx  --revision=3        \\\\\\\\\\\\\\"\3333"
deployment.apps/nginx with revision #3
Pod Template:
  Labels:	app=nginx
	pod-template-hash=8cf4bf97
  Containers:
   nginx:
    Image:	"nginx:1.22"
    Port:	80/TCP
    Host Port:	0/TCP
    Environment:	<none>
    Mounts:	<none>
  Volumes:	<none>

anji@anji:~$ kubectl rollout history deployment.apps/nginx  --revision=4                 \\\\\\\\\\\\\\\"\\444"
deployment.apps/nginx with revision #4
Pod Template:
  Labels:	app=nginx
	pod-template-hash=6d77c7955d
  Annotations:	kubernetes.io/change-cause: kubectl apply --filename=a.yaml --record=true
  Containers:
   nginx:
    Image:	"nginx:1.7"
    Port:	80/TCP
    Host Port:	0/TCP
    Environment:	<none>
    Mounts:	<none>
  Volumes:	<none>

anji@anji:~$ kubectl rollout history deployment.apps/nginx  --revision=2           \\\\\\\\\\\\\"\22"
deployment.apps/nginx with revision #2"
Pod Template:
  Labels:	app=nginx
	pod-template-hash=79f86456d4
  Containers:
   nginx:
    Image:	"nginx:1.9"
    Port:	80/TCP
    Host Port:	0/TCP
    Environment:	<none>
    Mounts:	<none>
  Volumes:	<none>

++++++++++++++++++"   importent           " "
anji@anji:~$ kubectl rollout" pause "deployment.apps/nginx  
deployment.apps/nginx" paused"

anji@anji:~$ kubectl rollout undo deployment.apps/nginx --to-revision=4
error: you" cannot rollback a paused" deployment; resume it first with 'kubectl rollout resume' and try again
|||||||||||||||||||||||||||||||||| "
anji@anji:~$ kubectl rollout resume  deployment.apps/nginx
deployment.apps/nginx resumed

anji@anji:~$ kubectl scale deployment.apps/nginx --replicas=10
deployment.apps/nginx scaled

anji@anji:~$ kubectl scale deployment.apps/nginx --replicas=1
deployment.apps/nginx scaled
anji@anji:~$ 

anji@anji:~$ kubectl scale deployment.apps/nginx --replicas=0
deployment.apps/nginx scaled
+++++++++++         =========== \\\\\\\ /  OR   OR   OR   ====== }{}||| |\[]]{}\\\\///  second method 

anji@anji:~$ kubectl get all -o wide 
NAME                         READY   STATUS    RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-6d77c7955d-7vw9m   1/1     Running   0          2m56s   10.44.0.3   worker   <none>           <none>
pod/nginx-6d77c7955d-pmsk8   1/1     Running   0          2m56s   10.44.0.1   worker   <none>           <none>
pod/nginx-6d77c7955d-xdjww   1/1     Running   0          2m56s   10.44.0.2   worker   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   7d3h   <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES      SELECTOR
deployment.apps/nginx   3/3     3            3           94m   nginx        "nginx:1.7 "  app=nginx

NAME                               DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR
replicaset.apps/nginx-6d77c7955d   3         3         3       94m   nginx        nginx:1.7    app=nginx,pod-template-hash=6d77c7955d
replicaset.apps/nginx-79f86456d4   0         0         0       83m   nginx        nginx:1.9    app=nginx,pod-template-hash=79f86456d4
replicaset.apps/nginx-8cf4bf97     0         0         0       59m   nginx        nginx:1.22   app=nginx,pod-template-hash=8cf4bf97

anji@anji:~$ kubectl rollout history deployment.apps/nginx 
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
2         <none>
3         <none>
4         kubectl apply --filename=a.yaml --record=true

anji@anji:~$ kubectl set image deployment.apps/nginx nginx=nginx:1.22 --record=true      === \\\/ /importent 
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/nginx image updated

anji@anji:~$ kubectl get all -o wide 
NAME                       READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/nginx-8cf4bf97-hfcng   1/1     Running   0          4s    10.44.0.1   worker   <none>           <none>
pod/nginx-8cf4bf97-rtgqk   1/1     Running   0          3s    10.44.0.2   worker   <none>           <none>
pod/nginx-8cf4bf97-vszbw   1/1     Running   0          5s    10.44.0.4   worker   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   7d3h   <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES       SELECTOR
deployment.apps/nginx   3/3     3            3           96m   nginx        "nginx:1.22 "  app=nginx

NAME                               DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR
replicaset.apps/nginx-6d77c7955d   0         0         0       96m   nginx        nginx:1.7    app=nginx,pod-template-hash=6d77c7955d
replicaset.apps/nginx-79f86456d4   0         0         0       85m   nginx        nginx:1.9    app=nginx,pod-template-hash=79f86456d4
replicaset.apps/nginx-8cf4bf97     3         3         3       61m   nginx        nginx:1.22   app=nginx,pod-template-hash=8cf4bf97

anji@anji:~$ kubectl rollout history deployment.apps/nginx 
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
2         <none>
4         kubectl apply --filename=a.yaml --record=true
5      "   kubectl set image deployment.apps/nginx nginx=nginx:1.22 --record=true  "
#####################################################################################################################3"
https://stackoverflow.com/questions/54602224/how-to-view-the-permissions-roles-associated-with-a-specific-service-account-in
https://www.strongdm.com/blog/kubernetes-rbac-role-based-access-control
root@master:~/red# kubectl get rolebindings,clusterrolebindings --all-namespaces -o wide
NAMESPACE     NAME                                                                                      ROLE                                                  AGE   USERS                                                   GROUPS                                                          SERVICEACCOUNTS
kube-public   rolebinding.rbac.authorization.k8s.io/kubeadm:bootstrap-signer-clusterinfo                Role/kubeadm:bootstrap-signer-clusterinfo             16d   system:anonymous                                                                                                        
kube-public   rolebinding.rbac.authorization.k8s.io/system:controller:bootstrap-signer                  Role/system:controller:bootstrap-signer               16d                                                                                                                           kube-system/bootstrap-signer
kube-system   rolebinding.rbac.authorization.k8s.io/kube-proxy                                          Role/kube-proxy                                       16d                                                           system:bootstrappers:kubeadm:default-node-token                 
kube-system   rolebinding.rbac.authorization.k8s.io/kubeadm:kubelet-config                              Role/kubeadm:kubelet-config                           16d                                                           system:nodes, system:bootstrappers:kubeadm:default-node-token   
kube-system   rolebinding.rbac.authorization.k8s.io/kubeadm:nodes-kubeadm-config                        Role/kubeadm:nodes-kubeadm-config                     16d                                                           system:bootstrappers:kubeadm:default-node-token, system:nodes   
kube-system   rolebinding.rbac.authorization.k8s.io/system::extension-apiserver-authentication-reader   Role/extension-apiserver-authentication-reader        16d   system:kube-controller-manager, system:kube-scheduler                                                                   
kube-system   rolebinding.rbac.authorization.k8s.io/system::leader-locking-kube-controller-manager      Role/system::leader-locking-kube-controller-manager   16d   system:kube-controller-manager                                                                                          kube-system/kube-controller-manager
kube-system   rolebinding.rbac.authorization.k8s.io/system::leader-locking-kube-scheduler               Role/system::leader-locking-kube-scheduler            16d   system:kube-scheduler                                                                                                   kube-system/kube-scheduler
kube-system   rolebinding.rbac.authorization.k8s.io/system:controller:bootstrap-signer                  Role/system:controller:bootstrap-signer               16d                                                                                                                           kube-system/bootstrap-signer
kube-system   rolebinding.rbac.authorization.k8s.io/system:controller:cloud-provider                    Role/system:controller:cloud-provider                 16d                                                                                                                           kube-system/cloud-provider
kube-system   rolebinding.rbac.authorization.k8s.io/system:controller:token-cleaner                     Role/system:controller:token-cleaner                  16d                                                                                                                           kube-system/token-cleaner
kube-system   rolebinding.rbac.authorization.k8s.io/weave-net                                           Role/weave-net                                        16d                                                                                                                           kube-system/weave-net

NAMESPACE   NAME                                                                                                ROLE                                                                               AGE     USERS                            GROUPS                                            SERVICEACCOUNTS
            clusterrolebinding.rbac.authorization.k8s.io/cluster-admin                                          ClusterRole/cluster-admin                                                          16d                                      system:masters                                    
           " clusterrolebinding.rbac.authorization.k8s.io/cpu1"                                                   ClusterRole/admin                                                                  5h59m   "cpu  "                                                                              
          "  clusterrolebinding.rbac.authorization.k8s.io/kick1   "                                               ClusterRole/cluster-admin                                                          6h39m "  kick   "                                                                            
           " clusterrolebinding.rbac.authorization.k8s.io/kk  "                                                   ClusterRole/cluster-admin                                                          3h56m  " k7   "                                                                              
            clusterrolebinding.rbac.authorization.k8s.io/kubeadm:get-nodes                                      ClusterRole/kubeadm:get-nodes                                                      16d                                      system:bootstrappers:kubeadm:default-node-token   
            clusterrolebinding.rbac.authorization.k8s.io/kubeadm:kubelet-bootstrap                              ClusterRole/system:node-bootstrapper                                               16d                                      system:bootstrappers:kubeadm:default-node-token   
            clusterrolebinding.rbac.authorization.k8s.io/kubeadm:node-autoapprove-bootstrap                     ClusterRole/system:certificates.k8s.io:certificatesigningrequests:nodeclient       16d                                      system:bootstrappers:kubeadm:default-node-token   
            clusterrolebinding.rbac.authorization.k8s.io/kubeadm:node-autoapprove-certificate-rotation          ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   16d                                      system:nodes                                      
            clusterrolebinding.rbac.authorization.k8s.io/kubeadm:node-proxier                                   ClusterRole/system:node-proxier                                                    16d                                                                                        kube-system/kube-proxy
           " clusterrolebinding.rbac.authorization.k8s.io/man21 "                                                 ClusterRole/cluster-admin                                                          134m  "  man1    "                                                                           
           " clusterrolebinding.rbac.authorization.k8s.io/pen    "                                                ClusterRole/cluster-admin                                                          4h37m "  pen    "                                                                            
            clusterrolebinding.rbac.authorization.k8s.io/system:basic-user                                      ClusterRole/system:basic-user                                                      16d                                      system:authenticated                              
            clusterrolebinding.rbac.authorization.k8s.io/system:controller:attachdetach-controller              ClusterRole/system:controller:attachdetach-controller                              16d                                                                                        kube-system/attachdetach-controller
                                                                    16d                                                                                        kube-system/weave-net
######################################################################################################"
root@master:~# kubectl get clusterrolebinding  -o wide 
NAME                                                   ROLE                                                                               AGE   USERS                            GROUPS                                            SERVICEACCOUNTS
cluster-admin                                          ClusterRole/cluster-admin                                                          16d                                    system:masters                                    
"kick1   "                                               ClusterRole/"cluster-admin  "                                                        11h   kick                                                                               
kubeadm:get-nodes                                      ClusterRole/kubeadm:get-nodes                                                      16d                                    system:bootstrappers:kubeadm:default-node-token   
kubeadm:kubelet-bootstrap                              ClusterRole/system:node-bootstrapper                                               16d                                    system:bootstrappers:kubeadm:default-node-token   
kubeadm:node-autoapprove-bootstrap                     ClusterRole/system:certificates.k8s.io:certificatesigningrequests:nodeclient       16d                                    system:bootstrappers:kubeadm:default-node-token   
kubeadm:node-autoapprove-certificate-rotation          ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   16d                                    system:nodes                                      
kubeadm:node-proxier                                   ClusterRole/system:node-proxier                                                    16d                                                                                      kube-system/kube-proxy
system:basic-user                                      ClusterRole/system:basic-user                                                      16d                                    system:authenticated                              

root@master:~# kubectl describe clusterrole cluster-admin  
Name:         cluster-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  ----- :"
  *.*        []                 []              [*]
             [*]                []              [*]

+++++++++++++++++++++++++ ""

root@master:~# kubectl describe clusterrole view   
Name:         view
Labels:       kubernetes.io/bootstrapping=rbac-defaults
              rbac.authorization.k8s.io/aggregate-to-edit=true
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                    Non-Resource URLs  Resource Names " Verbs"
  ---------                                    -----------------  --------------  -----
  bindings                                     []                 []              [get list watch]
  configmaps                                   []                 []              [get list watch]
  endpoints                                    []                 []              [get list watch]
  events                                       []                 []              [get list watch]
  limitranges                                  []                 []              [get list watch]
  namespaces/status                            []                 []              [get list watch]
  namespaces                                   []                 []              [get list watch]
  persistentvolumeclaims/status                []                 []              [get list watch]
  persistentvolumeclaims                       []                 []              [get list watch]
  pods/log                                     []                 []              [get list watch]
  pods/status                                  []                 []              [get list watch]
  pods                                         []                 []              [get list watch]
  replicationcontrollers/scale                 []                 []              [get list watch]
  replicationcontrollers/status                []                 []              [get list watch]
  replicationcontrollers                       []                 []              [get list watch]
  resourcequotas/status                        []                 []              [get list watch]
  resourcequotas                               []                 []              [get list watch]
  serviceaccounts                              []                 []              [get list watch]
  services/status                              []                 []              [get list watch]
  services                                     []                 []              [get list watch]
  controllerrevisions.apps                     []                 []              [get list watch]
  daemonsets.apps/status                       []                 []              [get list watch]
  daemonsets.apps                              []                 []              [get list watch]
  deployments.apps/scale                       []                 []              [get list watch]
  deployments.apps/status                      []                 []              [get list watch]
  deployments.apps                             []                 []              [get list watch]
  replicasets.apps/scale                       []                 []              [get list watch]
  replicasets.apps/status                      []                 []              [get list watch]
  replicasets.apps                             []                 []              [get list watch]
  statefulsets.apps/scale                      []                 []              [get list watch]
  statefulsets.apps/status                     []                 []              [get list watch]
  statefulsets.apps                            []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling/status  []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling         []                 []              [get list watch]
  cronjobs.batch/status                        []                 []              [get list watch]
  cronjobs.batch                               []                 []              [get list watch]
  jobs.batch/status                            []                 []              [get list watch]
  jobs.batch                                   []                 []              [get list watch]
  endpointslices.discovery.k8s.io              []                 []              [get list watch]
  daemonsets.extensions/status                 []                 []              [get list watch]
  daemonsets.extensions                        []                 []              [get list watch]
  deployments.extensions/scale                 []                 []              [get list watch]
  deployments.extensions/status                []                 []              [get list watch]
  deployments.extensions                       []                 []              [get list watch]
  ingresses.extensions/status                  []                 []              [get list watch]
  ingresses.extensions                         []                 []              [get list watch]
  networkpolicies.extensions                   []                 []              [get list watch]
  replicasets.extensions/scale                 []                 []              [get list watch]
  replicasets.extensions/status                []                 []              [get list watch]
  replicasets.extensions                       []                 []              [get list watch]
  replicationcontrollers.extensions/scale      []                 []              [get list watch]
  ingresses.networking.k8s.io/status           []                 []              [get list watch]
  ingresses.networking.k8s.io                  []                 []              [get list watch]
  networkpolicies.networking.k8s.io            []                 []              [get list watch]
  poddisruptionbudgets.policy/status           []                 []              [get list watch]
  poddisruptionbudgets.policy                  []                 []              [get list watch] "
++++++++++++++++++++++++++++++++++++==
root@master:~# kubectl describe clusterrole admin 
Name:         admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                       Non-Resource URLs  Resource Names  Verbs
  ---------                                       -----------------  --------------  -----
  leases.coordination.k8s.io                      []                 []              "[create delete deletecollection get list patch update watch]"
  rolebindings.rbac.authorization.k8s.io          []                 []              [create delete deletecollection get list patch update watch]
  roles.rbac.authorization.k8s.io                 []                 []              [create delete deletecollection get list patch update watch]
  configmaps                                      []                 []              [create delete deletecollection patch update get list watch]
  events                                          []                 []              [create delete deletecollection patch update get list watch]
  persistentvolumeclaims                          []                 []              [create delete deletecollection patch update get list watch]
  "pods                                            []                 []              [create delete deletecollection patch update get list watch]"
  replicationcontrollers/scale                    []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers                          []                 []              [create delete deletecollection patch update get list watch]
  services                                        []                 []              [create delete deletecollection patch update get list watch]
  daemonsets.apps                                 []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps/scale                          []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps                                []                 []              [create delete deletecollection patch update get list watch]
  replicasets.apps/scale                          []                 []              [create delete deletecollection patch update get list watch]
  replicasets.apps                                []                 []              [create delete deletecollection patch update get list watch]
  statefulsets.apps/scale                         []                 []              [create delete deletecollection patch update get list watch]
  statefulsets.apps                               []                 []              [create delete deletecollection patch update get list watch]
  horizontalpodautoscalers.autoscaling            []                 []              [create delete deletecollection patch update get list watch]
  cronjobs.batch                                  []                 []              [create delete deletecollection patch update get list watch]
  jobs.batch                                      []                 []              [create delete deletecollection patch update get list watch]
  daemonsets.extensions                           []                 []              [create delete deletecollection patch update get list watch]
  deployments.extensions/scale                    []                 []              [create delete deletecollection patch update get list watch]
  deployments.extensions                          []                 []              [create delete deletecollection patch update get list watch]
  ingresses.extensions                            []                 []              [create delete deletecollection patch update get list watch]
  networkpolicies.extensions                      []                 []              [create delete deletecollection patch update get list watch]
  replicasets.extensions/scale                    []                 []              [create delete deletecollection patch update get list watch]
  replicasets.extensions                          []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers.extensions/scale         []                 []              [create delete deletecollection patch update get list watch]
  ingresses.networking.k8s.io                     []                 []              [create delete deletecollection patch update get list watch]
  networkpolicies.networking.k8s.io               []                 []              [create delete deletecollection patch update get list watch]
  poddisruptionbudgets.policy                     []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps/rollback                       []                 []              [create delete deletecollection patch update]
  deployments.extensions/rollback                 []                 []              [create delete deletecollection patch update]
  pods/eviction                                   []                 []              [create]
  serviceaccounts/token                           []                 []              [create]
  localsubjectaccessreviews.authorization.k8s.io  []                 []              [create]
 " pods/attach                                     []                 []              [get list watch create delete deletecollection patch update]"
  pods/exec                                       []                 []              [get list watch create delete deletecollection patch update]
  pods/portforward                                []                 []              [get list watch create delete deletecollection patch update]
  pods/proxy                                      []                 []              [get list watch create delete deletecollection patch update]
  secrets                                         []                 []              [get list watch create delete deletecollection patch update]
  services/proxy                                  []                 []              [get list watch create delete deletecollection patch update]
  bindings                                        []                 []              [get list watch]
  endpoints                                       []                 []              [get list watch] "
++++++++++++++++++==
root@master:~# kubectl create namespace kick 
namespace/kick created
root@master:~# kubectl get ns 
NAME              STATUS   AGE
argocd            Active   14d
default           Active   16d
kick              Active   10s
kube-node-lease   Active   16d
kube-public       Active   16d
kube-system       Active   16d
++++++++++++++++++++++"
root@master:~# kubectl create clusterrolebinding  kick1 --clusterrole=view  --user=kick  -n kick 
clusterrolebinding.rbac.authorization.k8s.io/kick1 created

kick@master:~$ kubectl get all  -n kick 
No resources found in kick namespace.

kick@master:~$ kubectl get all 
Error from server (Forbidden): pods is forbidden: User "kick" cannot list resource "pods" in API group "" in the namespace "default"
Error from server (Forbidden): replicationcontrollers is forbidden: User "kick" cannot list resource "replicationcontrollers" in API group "" in the namespace "default"
Error from server (Forbidden): services is forbidden: User "kick" cannot list resource "services" in API group "" in the namespace "default"
Error from server (Forbidden): daemonsets.apps is forbidden: User "kick" cannot list resource "daemonsets" in API group "apps" in the namespace "default"
Error from server (Forbidden): deployments.apps is forbidden: User "kick" cannot list resource "deployments" in API group "apps" in the namespace "default"
Error from server (Forbidden): replicasets.apps is forbidden: User "kick" cannot list resource "replicasets" in API group "apps" in the namespace "default"
Error from server (Forbidden): statefulsets.apps is forbidden: User "kick" cannot list resource "statefulsets" in API group "apps" in the namespace "default"
Error from server (Forbidden): horizontalpodautoscalers.autoscaling is forbidden: User "kick" cannot list resource "horizontalpodautoscalers" in API group "autoscaling" in the namespace "default"
Error from server (Forbidden): cronjobs.batch is forbidden: User "kick" cannot list resource "cronjobs" in API group "batch" in the namespace "default"
Error from server (Forbidden): jobs.batch is forbidden: User "kick" cannot list resource "jobs" in API group "batch" in the namespace "default"
kick@master:~$ 

================================="
root@master:~# kubectl create clusterrolebinding kick   --clusterrole=view  --user=kick -n kick
clusterrolebinding.rbac.authorization.k8s.io/kick created
root@master:~# 


kick@master:~$ kubectl create deployment nginx --image=nginx -n kick 
error: failed to create deployment: deployments.apps is forbidden: User "kick" cannot create resource "deployments" in API group "apps" in the namespace "kick"
kick@master:~$ kubectl create deployment nginx --image=nginx 
error: failed to create deployment: deployments.apps is forbidden: User "kick" cannot create resource "deployments" in API group "apps" in the namespace "default"
kick@master:~$ 

kick@master:~$ kubectl edit deployment nginx  -n hp
error: deployments.apps "nginx" could not be patched: deployments.apps "nginx" is forbidden: User "kick" cannot patch resource "deployments" in API group "apps" in the namespace "hp"
You can run `kubectl replace -f /tmp/kubectl-edit-4136326731.yaml` to try this update again.
kick@master:~$ 
################################################################################################################################"
What is a Kubernetes StatefulSet?
https://www.bluematador.com/blog/an-introduction-to-kubernetes-statefulset
Kubernetes StatefulSet - Examples & Best Practices
https://loft.sh/blog/kubernetes-statefulset-examples-and-best-practices/


apiVersion: apps/v1
kind: StatefulSet
metadata: 
   name: mysql
spec: 
  selector: 
    matchLabels:
      app: mysql
  serviceName: "mysql"
  replicas: 3
  template:
    metadata: 
      labels: 
        app: mysql
    spec: 
      terminationGracePeriodSeconds: 10
      containers: 
      - name: mysql
        image: mysql
        ports:
          - containerPort: 3306
        volumeMounts:
           - name: mysql
             mountPath: /var/lib/mysql
        env:
          - name: MYSQL_ROOT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: secret
                key: password

      volumes:
        - name: mysql
          persistentVolumeClaim:
            claimName: pvc          
---
apiVersion: v1
kind: Secret
metadata: 
  name: secret
type:  opaque
data: 
    password: YW5qaXJlZGR5Cg==
---
apiVersion: v1
kind: PersistentVolume
metadata: 
  name: pv
spec: 
   storageClassName: ssd
   capacity:
      storage: 5Gi
   accessModes: 
     - ReadWriteOnce
   hostPath:
     path: "/opt/statefull"

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc
spec: 
  storageClassName: ssd
  accessModes:
     - ReadWriteOnce
  resources:
     requests:
       storage: 3Gi

---
apiVersion: v1
kind: Service
metadata: 
  name: mysql
spec:
  selector:
    app: mysql
  ports:
    - port: 3306
  clusterIP: None
+++++++++++++++++++"
anji@anji:~$ kubectl get pod,deployment,pv,pvc,service,secret,statefulset  -o wide
NAME          READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
pod/mysql-0   0/1   "  Pending "  0          1s    <none>   <none>   <none>           <none>

NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS   REASON   AGE   VOLUMEMODE
persistentvolume/pv   5Gi        RWO            Retain           Bound    default/pvc   ssd                     1s    Filesystem

NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE
persistentvolumeclaim/pvc   Bound    pv       5Gi        RWO            ssd            1s    Filesystem

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE   SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP    9d    <none>
service/mysql        ClusterIP   None         <none>        3306/TCP   1s    app=mysql

NAME            TYPE     DATA   AGE
secret/secret   opaque   1      1s

NAME                     READY   AGE   CONTAINERS   IMAGES
statefulset.apps/mysql  " 0/3 "    1s    mysql        mysql "

anji@anji:~$ kubectl get pod,deployment,pv,pvc,service,secret,statefulset  -o wide
NAME          READY   STATUS              RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
pod/mysql-0   0/1     "ContainerCreating "  0          2s    <none>   worker   <none>           <none>

NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS   REASON   AGE   VOLUMEMODE
persistentvolume/pv   5Gi        RWO            Retain           Bound    default/pvc   ssd                     2s    Filesystem

NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE
persistentvolumeclaim/pvc   Bound    pv       5Gi        RWO            ssd            2s    Filesystem

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE   SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP    9d    <none>
service/mysql        ClusterIP   None         <none>        3306/TCP   2s    app=mysql

NAME            TYPE     DATA   AGE
secret/secret   opaque   1      2s

NAME                     READY   AGE   CONTAINERS   IMAGES
statefulset.apps/mysql   0/3     2s    mysql        mysql

anji@anji:~$ kubectl get pod,deployment,pv,pvc,service,secret,statefulset  -o wide
NAME          READY   STATUS              RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
pod/mysql-0   0/1     ContainerCreating   0          4s    <none>   worker   <none>           <none>

NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS   REASON   AGE   VOLUMEMODE
persistentvolume/pv   5Gi        RWO            Retain           Bound    default/pvc   ssd                     4s    Filesystem

NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE
persistentvolumeclaim/pvc   Bound    pv       5Gi        RWO            ssd            4s    Filesystem

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE   SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP    9d    <none>
service/mysql        ClusterIP   None         <none>        3306/TCP   4s    app=mysql

NAME            TYPE     DATA   AGE
secret/secret   opaque   1      4s

NAME                     READY   AGE   CONTAINERS   IMAGES
statefulset.apps/mysql   0/3     4s    mysql        mysql"

anji@anji:~$ kubectl get pod,deployment,pv,pvc,service,secret,statefulset  -o wide
NAME          READY   STATUS              RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/mysql-0   1/1     "Running  "           0          7s    10.44.0.1   worker   <none>           <none>
pod/mysql-1   0/1     "ContainerCreating  " 0          3s    <none>      worker   <none>           <none>

NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS   REASON   AGE   VOLUMEMODE
persistentvolume/pv   5Gi        RWO            Retain           Bound    default/pvc   ssd                     7s    Filesystem

NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE
persistentvolumeclaim/pvc   Bound    pv       5Gi        RWO            ssd            7s    Filesystem

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE   SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP    9d    <none>
service/mysql        ClusterIP   None         <none>        3306/TCP   7s    app=mysql

NAME            TYPE     DATA   AGE
secret/secret   opaque   1      7s

NAME                     READY   AGE   CONTAINERS   IMAGES
statefulset.apps/mysql  " 1/3  "   7s    mysql        mysql"

anji@anji:~$ kubectl get pod,deployment,pv,pvc,service,secret,statefulset  -o wide
NAME          READY   STATUS              RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/mysql-0   1/1     "Running "            0          9s    10.44.0.1   worker   <none>           <none>
pod/mysql-1   1/1     "Running "            0          5s    10.44.0.2   worker   <none>           <none>
pod/mysql-2   0/1     ContainerCreating   0          1s    <none>      worker   <none>           <none>

NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS   REASON   AGE   VOLUMEMODE
persistentvolume/pv   5Gi        RWO            Retain           Bound    default/pvc   ssd                     9s    Filesystem

NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE
persistentvolumeclaim/pvc   Bound    pv       5Gi        RWO            ssd            9s    Filesystem

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE   SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP    9d    <none>
service/mysql        ClusterIP   None         <none>        3306/TCP   9s    app=mysql

NAME            TYPE     DATA   AGE
secret/secret   opaque   1      9s

NAME                     READY   AGE   CONTAINERS   IMAGES
statefulset.apps/mysql "  2/3     9s    mysql        mysql"
"
anji@anji:~$ kubectl get pod,deployment,pv,pvc,service,secret,statefulset  -o wide
NAME          READY   STATUS              RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/mysql-0   1/1     Running             0          10s   10.44.0.1   worker   <none>           <none>
pod/mysql-1   1/1     Running             0          6s    10.44.0.2   worker   <none>           <none>
pod/mysql-2   0/1     "ContainerCreating  " 0          2s    <none>      worker   <none>           <none>

NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS   REASON   AGE   VOLUMEMODE
persistentvolume/pv   5Gi        RWO            Retain           Bound    default/pvc   ssd                     10s   Filesystem

NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE
persistentvolumeclaim/pvc   Bound    pv       5Gi        RWO            ssd            10s   Filesystem

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE   SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP    9d    <none>
service/mysql        ClusterIP   None         <none>        3306/TCP   10s   app=mysql

NAME            TYPE     DATA   AGE
secret/secret   opaque   1      10s

NAME                     READY   AGE   CONTAINERS   IMAGES
statefulset.apps/mysql  " 2/3    " 10s   mysql        mysql
"

anji@anji:~$ kubectl get pod,deployment,pv,pvc,service,secret,statefulset  -o wide
NAME          READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/mysql-0   1/1     "Running  " 0          13s   10.44.0.1   worker   <none>           <none>
pod/mysql-1   1/1     "Running "  0          9s    10.44.0.2   worker   <none>           <none>
pod/mysql-2   1/1     "Running"   0          5s    10.44.0.3   worker   <none>           <none>

NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS   REASON   AGE   VOLUMEMODE
persistentvolume/pv   5Gi        RWO            Retain           Bound    default/pvc   ssd                     13s   Filesystem

NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE
persistentvolumeclaim/pvc   Bound    pv       5Gi        RWO            ssd            13s   Filesystem

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE   SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP    9d    <none>
service/mysql        ClusterIP   None         <none>        3306/TCP   13s   app=mysql

NAME            TYPE     DATA   AGE
secret/secret   opaque   1      13s

NAME                     READY   AGE   CONTAINERS   IMAGES
statefulset.apps/mysql  " 3/3   "  13s   mysql        mysql

+++++++++++++++++++++++++"
anji@anji:~$ kubectl describe statefulset 
Name:               mysql
Selector:           app=mysql
Replicas:          " 3 desired | 1 total"
Update Strategy:    "RollingUpdate"
  Partition:        0
Pods Status:        0 Running / "1 Waiting /" 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=mysql
  Containers:
   mysql:
    Image:      mysql
    Port:       3306/TCP
    Host Port:  0/TCP
    Environment:
      MYSQL_ROOT_PASSWORD: " <set to the key 'password' in secret 'secret'>  Optional: false"
    Mounts:
      /var/lib/mysql from mysql (rw)
  Volumes:
   mysql:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  pvc
    ReadOnly:   false
Volume Claims:  <none>
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  0s    statefulset-controller  create Pod "mysql-0 "in "StatefulSet mysql successful""


anji@anji:~$ kubectl describe statefulset 
Replicas:        "   3 desired | 2 total"
Update Strategy:    RollingUpdate
  Partition:        0
Pods Status:        "1 Running "/ "1 Waiting" / 0 Succeeded / 0 Failed
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  18s   statefulset-controller  create Pod "mysql-0" in StatefulSet mysql successful
  Normal  SuccessfulCreate  2s    statefulset-controller  create Pod "mysql-1 "in StatefulSet mysql successful"

anji@anji:~$ kubectl describe statefulset 

Replicas:         "  3 desired | 3 total"
Update Strategy:    RollingUpdate
  Partition:        0
Pods Status:        "2 Running "/ "1 Waiting" / 0 Succeeded / 0 Failed
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  20s   statefulset-controller  create Pod "mysql-0 "in StatefulSet mysql successful
  Normal  SuccessfulCreate  4s    statefulset-controller  create Pod" mysql-1 "in StatefulSet mysql successful
  Normal  SuccessfulCreate  1s    statefulset-controller  create Pod "mysql-2" in StatefulSet mysql successful"

anji@anji:~$ kubectl describe statefulset 
Replicas:         "  3 desired | 3 total"
Update Strategy:    RollingUpdate
  Partition:        0
Pods Status:        "3 Running "/ 0 Waiting / 0 Succeeded / 0 Failed
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  24s   statefulset-controller  create Pod" mysql-0 "in StatefulSet mysql successful
  Normal  SuccessfulCreate  8s    statefulset-controller  create Pod "mysql-1 "in StatefulSet mysql successful
  Normal  SuccessfulCreate  5s    statefulset-controller  create Pod "mysql-2 "in StatefulSet mysql successful
++++++++++++++++++++++++"
https://www.youtube.com/watch?v=nUm4Y5uEG7k&t=6s
https://github.com/anjilinux/project-StatefulSets?organization=anjilinux&organization=anjilinux
anji@anji:~$ kubectl run temp --rm -it --image=kunchalavikram/sampleflask:v1 -- sh
If you don't see a command prompt, try pressing enter.
/app # pwd
/app
/app # ls
app.py
/app # cat app.py 
import socket
from flask import Flask

def getIP():
	hostname = socket.gethostname()
	s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
	s.connect(("8.8.8.8", 80))
	ip = s.getsockname()[0]
	print(ip)
	s.close()
	return hostname,ip
	
app = Flask(__name__)
@app.route("/")
def hello():
    hostname,ip = getIP()
    out = "Hello from host: " + hostname + " with host ip: " + ip
    return out
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int("5000"), debug=True)

+++++++++++++++++++++++++++++++++++++++++="
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: flask
spec: 
  replicas: 3
  selector: 
    matchLabels:
       app: flask
  template:
    metadata:
      labels:
        app: flask
    spec:
      containers: 
         - name: flask
           image: kunchalavikram/sampleflask:v1
           ports:
             - containerPort: 5000
---
apiVersion: v1
kind: Service
metadata: 
  name: headless
spec:
   clusterIP: None
   selector: 
     app: flask
   ports:
     - port: 80
       targetPort: 5000
       protocol: TCP
---
apiVersion: v1
kind: Service
metadata: 
   name: normal
spec:
  selector:
    app: flask
  ports:
    - port: 80
      targetPort: 5000
      protocol: TCP

anji@anji:~/deployment$ kubectl get all -o wide 
NAME                        READY   STATUS    RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES
pod/flask-554ccc869-4mcvf   1/1     Running   0          7m18s   10.44.0.1   worker   <none>           <none>
pod/flask-554ccc869-l6jb6   1/1     Running   0          7m18s   10.44.0.3   worker   <none>           <none>
pod/flask-554ccc869-ldn9z   1/1     Running   0          7m18s   10.44.0.2   worker   <none>           <none>

NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE     SELECTOR
service/headless    " ClusterIP   None    "        <none>        80/TCP    7m18s   app=flask
service/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   8m3s    <none>
service/"normal       ClusterIP   10.97.127.227"   <none>        80/TCP    7m18s   app=flask

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES                          SELECTOR
deployment.apps/flask  ' 3/3   '  3            3           7m18s   flask        kunchalavikram/sampleflask:v1   app=flask

NAME                              DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES                          SELECTOR
replicaset.apps/flask-554ccc869  " 3  "       3        " 3   "    7m18s   flask        kunchalavikram/sampleflask:v1   app=flask,pod-template-hash=554ccc869
"
anji@anji:~/deployment$ kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm
If you don't see a command prompt, try pressing enter.
/ # nslookup normal
Server:    10.96.0.10
Address 1: 10.96.0.10
nslookup: can't resolve 'normal'
/ # 
/ # nslookup headless
Server:    10.96.0.10
Address 1: 10.96.0.10
nslookup: can't resolve 'headless'

anji@anji:~/deployment$ kubectl run -i --tty --image nginx:alpine test-pod --restart=Never --rm -- sh
If you don't see a command prompt, try pressing enter.
/ # curl flask:80
curl: (6) Could not resolve host: flask"
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
apiVersion: v1
kind: Service
metadata:
  name: nginx-headless
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-normal
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  selector:
    app: nginx
---    
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx-headless"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      volumes:
      - name: shared-volume
        emptyDir: {}
      initContainers:
      - name: busybox
        image: busybox
        volumeMounts:
        - name: shared-volume
          mountPath: /nginx-data
        command: ["/bin/sh", "-c"]
        args: ["echo Hello from container $HOSTNAME > /nginx-data/index.html"]
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: shared-volume
          mountPath: /usr/share/nginx/html
        - name: www
          mountPath: /usr/share/nginx
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Mi
"root@anji:~# kubectl get all -o wide --show-labels
NAME        READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES   LABELS
pod/web-0   0/1     Pending   0          14m   <none>   <none>   <none>           <none>            app=nginx,controller-revision-hash=web-7bc9487745,statefulset.kubernetes.io/pod-name=web-0

NAME                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE   SELECTOR    LABELS
service/kubernetes       ClusterIP   10.96.0.1        <none>        443/TCP   14m   <none>      component=apiserver,provider=kubernetes
service/nginx-headless   ClusterIP   None             <none>        80/TCP    14m   app=nginx   app=nginx
service/nginx-normal     ClusterIP   10.104.251.149   <none>        80/TCP    14m   app=nginx   app=nginx

NAME                   READY   AGE   CONTAINERS   IMAGES                      LABELS
"statefulset".apps/web "  0/2   "  14m   nginx        k8s.gcr.io/nginx-slim:0.8   <none>
################################################################################################" success 
eks with terraform  eks  cluster create with terraform with eks cluster 

https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role_policy_attachment
https://github.com/antonputra/tutorials/tree/main/lessons/038
https://www.youtube.com/watch?v=oYHZ3EPR094&t=440s
https://docs.aws.amazon.com/cdk/api/v1/python/aws_cdk.aws_eks/NodegroupAmiType.html


#
# EKS Cluster Resources
#  * IAM Role to allow EKS service to manage other AWS services
#  * EC2 Security Group to allow networking traffic with EKS cluster
#  * EKS Cluster
#

resource "aws_iam_role" "demo-cluster" {
  name = "terraform-eks-demo-cluster"

  assume_role_policy = <<POLICY
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "eks.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
POLICY
}

resource "aws_iam_role_policy_attachment" "demo-cluster-AmazonEKSClusterPolicy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
  role       = aws_iam_role.demo-cluster.name
}

resource "aws_iam_role_policy_attachment" "demo-cluster-AmazonEKSVPCResourceController" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSVPCResourceController"
  role       = aws_iam_role.demo-cluster.name
}

resource "aws_security_group" "demo-cluster" {
  name        = "terraform-eks-demo-cluster"
  description = "Cluster communication with worker nodes"
  vpc_id      = aws_vpc.demo.id

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "terraform-eks-demo"
  }
}

resource "aws_security_group_rule" "demo-cluster-ingress-workstation-https" {
  cidr_blocks       = [local.workstation-external-cidr]
  description       = "Allow workstation to communicate with the cluster API Server"
  from_port         = 443
  protocol          = "tcp"
  security_group_id = aws_security_group.demo-cluster.id
  to_port           = 443
  type              = "ingress"
}

resource "aws_eks_cluster" "demo" {
  name     = var.cluster-name
  role_arn = aws_iam_role.demo-cluster.arn
  version =  "1.24"

  vpc_config {
    security_group_ids = [aws_security_group.demo-cluster.id]
    subnet_ids         = aws_subnet.demo[*].id
  }

  depends_on = [
    aws_iam_role_policy_attachment.demo-cluster-AmazonEKSClusterPolicy,
    aws_iam_role_policy_attachment.demo-cluster-AmazonEKSVPCResourceController,
  ]
}

###############################################


#
# EKS Worker Nodes Resources
#  * IAM role allowing Kubernetes actions to access other AWS services
#  * EKS Node Group to launch worker nodes
#

resource "aws_iam_role" "demo-node" {
  name = "terraform-eks-demo-node"

  assume_role_policy = <<POLICY
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
POLICY
}

resource "aws_iam_role_policy_attachment" "demo-node-AmazonEKSWorkerNodePolicy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
  role       = aws_iam_role.demo-node.name
}

resource "aws_iam_role_policy_attachment" "demo-node-AmazonEKS_CNI_Policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
  role       = aws_iam_role.demo-node.name
}

resource "aws_iam_role_policy_attachment" "demo-node-AmazonEC2ContainerRegistryReadOnly" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
  role       = aws_iam_role.demo-node.name
}

resource "aws_eks_node_group" "demo" {
  cluster_name    = aws_eks_cluster.demo.name
  node_group_name = "demo"
  node_role_arn   = aws_iam_role.demo-node.arn
  subnet_ids      = aws_subnet.demo[*].id
  disk_size       = 8
  instance_types = [ "t2.micro" ]

  scaling_config {
    desired_size = 1
    max_size     = 1
    min_size     = 1
  }

  depends_on = [
    aws_iam_role_policy_attachment.demo-node-AmazonEKSWorkerNodePolicy,
    aws_iam_role_policy_attachment.demo-node-AmazonEKS_CNI_Policy,
    aws_iam_role_policy_attachment.demo-node-AmazonEC2ContainerRegistryReadOnly,
  ]
}


#################################################


#
# Outputs
#

locals {
  config_map_aws_auth = <<CONFIGMAPAWSAUTH


apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
    - rolearn: ${aws_iam_role.demo-node.arn}
      username: system:node:{{EC2PrivateDNSName}}
      groups:
        - system:bootstrappers
        - system:nodes
CONFIGMAPAWSAUTH

  kubeconfig = <<KUBECONFIG


apiVersion: v1
clusters:
- cluster:
    server: ${aws_eks_cluster.demo.endpoint}
    certificate-authority-data: ${aws_eks_cluster.demo.certificate_authority[0].data}
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: aws
  name: aws
current-context: aws
kind: Config
preferences: {}
users:
- name: aws
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      command: aws-iam-authenticator
      args:
        - "token"
        - "-i"
        - "${var.cluster-name}"
KUBECONFIG
}

output "config_map_aws_auth" {
  value = local.config_map_aws_auth
}

output "kubeconfig" {
  value = local.kubeconfig
}

#############################


terraform {
  required_version = ">= 0.12"
}

provider "aws" {
  region = var.aws_region
}

data "aws_availability_zones" "available" {}

# Not required: currently used in conjunction with using
# icanhazip.com to determine local workstation external IP
# to open EC2 Security Group access to the Kubernetes cluster.
# See workstation-external-ip.tf for additional information.
provider "http" {}
#####################################


variable "aws_region" {
  default = "us-east-1"
}

variable "cluster-name" {
  default = "anjireddy"
  type    = string
}


################################


#
# VPC Resources
#  * VPC
#  * Subnets
#  * Internet Gateway
#  * Route Table
#

resource "aws_vpc" "demo" {
  cidr_block = "10.0.0.0/16"

  tags = tomap({
    "Name"                                      = "terraform-eks-demo-node",
    "kubernetes.io/cluster/${var.cluster-name}" = "shared",
  })
}

resource "aws_subnet" "demo" {
  count = 2

  availability_zone       = data.aws_availability_zones.available.names[count.index]
  cidr_block              = "10.0.${count.index}.0/24"
  map_public_ip_on_launch = true
  vpc_id                  = aws_vpc.demo.id

  tags = tomap({
    "Name"                                      = "terraform-eks-demo-node",
    "kubernetes.io/cluster/${var.cluster-name}" = "shared",
  })
}

resource "aws_internet_gateway" "demo" {
  vpc_id = aws_vpc.demo.id

  tags = {
    Name = "terraform-eks-demo"
  }
}

resource "aws_route_table" "demo" {
  vpc_id = aws_vpc.demo.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.demo.id
  }
}

resource "aws_route_table_association" "demo" {
  count = 2

  subnet_id      = aws_subnet.demo[count.index].id
  route_table_id = aws_route_table.demo.id
}
#####################


#
# Workstation External IP
#
# This configuration is not required and is
# only provided as an example to easily fetch
# the external IP of your local workstation to
# configure inbound EC2 Security Group access
# to the Kubernetes cluster.
#

data "http" "workstation-external-ip" {
  url = "http://ipv4.icanhazip.com"
}

# Override with variable or hardcoded value if necessary
locals {
  workstation-external-cidr = "${chomp(data.http.workstation-external-ip.response_body)}/32"
}
OUTPUT : \[\[[[[[[[\[\\[\[[\\[[]]]]]]]]]]]]]]\\\/\///\\/\/\/////\/

hi@DESKTOP-LQI2IA5 MINGW64 ~/Music/New folder (2)
$ terraform apply --auto-approve
data.http.workstation-external-ip: Reading...
data.http.workstation-external-ip: Read complete after 1s [id=http://ipv4.icanhazip.com]
data.aws_availability_zones.available: Reading...
data.aws_availability_zones.available: Read complete after 1s [id=us-east-1]

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # aws_eks_cluster.demo will be created
  + resource "aws_eks_cluster" "demo" {
      + arn                   = (known after apply)
      + certificate_authority = (known after apply)
      + cluster_id            = (known after apply)
      + created_at            = (known after apply)
      + endpoint              = (known after apply)
      + id                    = (known after apply)
      + identity              = (known after apply)
      + name                  = "anjireddy"
      + platform_version      = (known after apply)
      + role_arn              = (known after apply)
      + status                = (known after apply)
      + tags_all              = (known after apply)
      + version               = "1.24"

      + kubernetes_network_config {
          + ip_family         = (known after apply)
          + service_ipv4_cidr = (known after apply)
          + service_ipv6_cidr = (known after apply)
        }

      + vpc_config {
          + cluster_security_group_id = (known after apply)
          + endpoint_private_access   = false
          + endpoint_public_access    = true
          + public_access_cidrs       = (known after apply)
          + security_group_ids        = (known after apply)
          + subnet_ids                = (known after apply)
          + vpc_id                    = (known after apply)
        }
    }

  # aws_eks_node_group.demo will be created
  + resource "aws_eks_node_group" "demo" {
      + ami_type               = (known after apply)
      + arn                    = (known after apply)
      + capacity_type          = (known after apply)
      + cluster_name           = "anjireddy"
      + disk_size              = 8
      + id                     = (known after apply)
      + instance_types         = [
          + "t2.micro",
        ]
      + node_group_name        = "demo"
      + node_group_name_prefix = (known after apply)
      + node_role_arn          = (known after apply)
      + release_version        = (known after apply)
      + resources              = (known after apply)
      + status                 = (known after apply)
      + subnet_ids             = (known after apply)
      + tags_all               = (known after apply)
      + version                = (known after apply)

      + scaling_config {
          + desired_size = 1
          + max_size     = 1
          + min_size     = 1
        }

      + update_config {
          + max_unavailable            = (known after apply)
          + max_unavailable_percentage = (known after apply)
        }
    }

  # aws_iam_role.demo-cluster will be created
  + resource "aws_iam_role" "demo-cluster" {
      + arn                   = (known after apply)
      + assume_role_policy    = jsonencode(
            {
              + Statement = [
                  + {
                      + Action    = "sts:AssumeRole"
                      + Effect    = "Allow"
                      + Principal = {
                          + Service = "eks.amazonaws.com"
                        }
                    },
                ]
              + Version   = "2012-10-17"
            }
        )
      + create_date           = (known after apply)
      + force_detach_policies = false
      + id                    = (known after apply)
      + managed_policy_arns   = (known after apply)
      + max_session_duration  = 3600
      + name                  = "terraform-eks-demo-cluster"
      + name_prefix           = (known after apply)
      + path                  = "/"
      + tags_all              = (known after apply)
      + unique_id             = (known after apply)

      + inline_policy {
          + name   = (known after apply)
          + policy = (known after apply)
        }
    }

  # aws_iam_role.demo-node will be created
  + resource "aws_iam_role" "demo-node" {
      + arn                   = (known after apply)
      + assume_role_policy    = jsonencode(
            {
              + Statement = [
                  + {
                      + Action    = "sts:AssumeRole"
                      + Effect    = "Allow"
                      + Principal = {
                          + Service = "ec2.amazonaws.com"
                        }
                    },
                ]
              + Version   = "2012-10-17"
            }
        )
      + create_date           = (known after apply)
      + force_detach_policies = false
      + id                    = (known after apply)
      + managed_policy_arns   = (known after apply)
      + max_session_duration  = 3600
      + name                  = "terraform-eks-demo-node"
      + name_prefix           = (known after apply)
      + path                  = "/"
      + tags_all              = (known after apply)
      + unique_id             = (known after apply)

      + inline_policy {
          + name   = (known after apply)
          + policy = (known after apply)
        }
    }

  # aws_iam_role_policy_attachment.demo-cluster-AmazonEKSClusterPolicy will be created
  + resource "aws_iam_role_policy_attachment" "demo-cluster-AmazonEKSClusterPolicy" {
      + id         = (known after apply)
      + policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
      + role       = "terraform-eks-demo-cluster"
    }

  # aws_iam_role_policy_attachment.demo-cluster-AmazonEKSVPCResourceController will be created
  + resource "aws_iam_role_policy_attachment" "demo-cluster-AmazonEKSVPCResourceController" {
      + id         = (known after apply)
      + policy_arn = "arn:aws:iam::aws:policy/AmazonEKSVPCResourceController"
      + role       = "terraform-eks-demo-cluster"
    }

  # aws_iam_role_policy_attachment.demo-node-AmazonEC2ContainerRegistryReadOnly will be created
  + resource "aws_iam_role_policy_attachment" "demo-node-AmazonEC2ContainerRegistryReadOnly" {
      + id         = (known after apply)
      + policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
      + role       = "terraform-eks-demo-node"
    }

  # aws_iam_role_policy_attachment.demo-node-AmazonEKSWorkerNodePolicy will be created
  + resource "aws_iam_role_policy_attachment" "demo-node-AmazonEKSWorkerNodePolicy" {
      + id         = (known after apply)
      + policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
      + role       = "terraform-eks-demo-node"
    }

  # aws_iam_role_policy_attachment.demo-node-AmazonEKS_CNI_Policy will be created
  + resource "aws_iam_role_policy_attachment" "demo-node-AmazonEKS_CNI_Policy" {
      + id         = (known after apply)
      + policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
      + role       = "terraform-eks-demo-node"
    }

  # aws_internet_gateway.demo will be created
  + resource "aws_internet_gateway" "demo" {
      + arn      = (known after apply)
      + id       = (known after apply)
      + owner_id = (known after apply)
      + tags     = {
          + "Name" = "terraform-eks-demo"
        }
      + tags_all = {
          + "Name" = "terraform-eks-demo"
        }
      + vpc_id   = (known after apply)
    }

  # aws_route_table.demo will be created
  + resource "aws_route_table" "demo" {
      + arn              = (known after apply)
      + id               = (known after apply)
      + owner_id         = (known after apply)
      + propagating_vgws = (known after apply)
      + route            = [
          + {
              + carrier_gateway_id         = ""
              + cidr_block                 = "0.0.0.0/0"
              + core_network_arn           = ""
              + destination_prefix_list_id = ""
              + egress_only_gateway_id     = ""
              + gateway_id                 = (known after apply)
              + instance_id                = ""
              + ipv6_cidr_block            = ""
              + local_gateway_id           = ""
              + nat_gateway_id             = ""
              + network_interface_id       = ""
              + transit_gateway_id         = ""
              + vpc_endpoint_id            = ""
              + vpc_peering_connection_id  = ""
            },
        ]
      + tags_all         = (known after apply)
      + vpc_id           = (known after apply)
    }

  # aws_route_table_association.demo[0] will be created
  + resource "aws_route_table_association" "demo" {
      + id             = (known after apply)
      + route_table_id = (known after apply)
      + subnet_id      = (known after apply)
    }

  # aws_route_table_association.demo[1] will be created
  + resource "aws_route_table_association" "demo" {
      + id             = (known after apply)
      + route_table_id = (known after apply)
      + subnet_id      = (known after apply)
    }

  # aws_security_group.demo-cluster will be created
  + resource "aws_security_group" "demo-cluster" {
      + arn                    = (known after apply)
      + description            = "Cluster communication with worker nodes"
      + egress                 = [
          + {
              + cidr_blocks      = [
                  + "0.0.0.0/0",
                ]
              + description      = ""
              + from_port        = 0
              + ipv6_cidr_blocks = []
              + prefix_list_ids  = []
              + protocol         = "-1"
              + security_groups  = []
              + self             = false
              + to_port          = 0
            },
        ]
      + id                     = (known after apply)
      + ingress                = (known after apply)
      + name                   = "terraform-eks-demo-cluster"
      + name_prefix            = (known after apply)
      + owner_id               = (known after apply)
      + revoke_rules_on_delete = false
      + tags                   = {
          + "Name" = "terraform-eks-demo"
        }
      + tags_all               = {
          + "Name" = "terraform-eks-demo"
        }
      + vpc_id                 = (known after apply)
    }

  # aws_security_group_rule.demo-cluster-ingress-workstation-https will be created
  + resource "aws_security_group_rule" "demo-cluster-ingress-workstation-https" {
      + cidr_blocks              = [
          + "49.37.155.242/32",
        ]
      + description              = "Allow workstation to communicate with the cluster API Server"
      + from_port                = 443
      + id                       = (known after apply)
      + protocol                 = "tcp"
      + security_group_id        = (known after apply)
      + security_group_rule_id   = (known after apply)
      + self                     = false
      + source_security_group_id = (known after apply)
      + to_port                  = 443
      + type                     = "ingress"
    }

  # aws_subnet.demo[0] will be created
  + resource "aws_subnet" "demo" {
      + arn                                            = (known after apply)
      + assign_ipv6_address_on_creation                = false
      + availability_zone                              = "us-east-1a"
      + availability_zone_id                           = (known after apply)
      + cidr_block                                     = "10.0.0.0/24"
      + enable_dns64                                   = false
      + enable_resource_name_dns_a_record_on_launch    = false
      + enable_resource_name_dns_aaaa_record_on_launch = false
      + id                                             = (known after apply)
      + ipv6_cidr_block_association_id                 = (known after apply)
      + ipv6_native                                    = false
      + map_public_ip_on_launch                        = true
      + owner_id                                       = (known after apply)
      + private_dns_hostname_type_on_launch            = (known after apply)
      + tags                                           = {
          + "Name"                            = "terraform-eks-demo-node"
          + "kubernetes.io/cluster/anjireddy" = "shared"
        }
      + tags_all                                       = {
          + "Name"                            = "terraform-eks-demo-node"
          + "kubernetes.io/cluster/anjireddy" = "shared"
        }
      + vpc_id                                         = (known after apply)
    }

  # aws_subnet.demo[1] will be created
  + resource "aws_subnet" "demo" {
      + arn                                            = (known after apply)
      + assign_ipv6_address_on_creation                = false
      + availability_zone                              = "us-east-1b"
      + availability_zone_id                           = (known after apply)
      + cidr_block                                     = "10.0.1.0/24"
      + enable_dns64                                   = false
      + enable_resource_name_dns_a_record_on_launch    = false
      + enable_resource_name_dns_aaaa_record_on_launch = false
      + id                                             = (known after apply)
      + ipv6_cidr_block_association_id                 = (known after apply)
      + ipv6_native                                    = false
      + map_public_ip_on_launch                        = true
      + owner_id                                       = (known after apply)
      + private_dns_hostname_type_on_launch            = (known after apply)
      + tags                                           = {
          + "Name"                            = "terraform-eks-demo-node"
          + "kubernetes.io/cluster/anjireddy" = "shared"
        }
      + tags_all                                       = {
          + "Name"                            = "terraform-eks-demo-node"
          + "kubernetes.io/cluster/anjireddy" = "shared"
        }
      + vpc_id                                         = (known after apply)
    }

  # aws_vpc.demo will be created
  + resource "aws_vpc" "demo" {
      + arn                                  = (known after apply)
      + cidr_block                           = "10.0.0.0/16"
      + default_network_acl_id               = (known after apply)
      + default_route_table_id               = (known after apply)
      + default_security_group_id            = (known after apply)
      + dhcp_options_id                      = (known after apply)
      + enable_classiclink                   = (known after apply)
      + enable_classiclink_dns_support       = (known after apply)
      + enable_dns_hostnames                 = (known after apply)
      + enable_dns_support                   = true
      + enable_network_address_usage_metrics = (known after apply)
      + id                                   = (known after apply)
      + instance_tenancy                     = "default"
      + ipv6_association_id                  = (known after apply)
      + ipv6_cidr_block                      = (known after apply)
      + ipv6_cidr_block_network_border_group = (known after apply)
      + main_route_table_id                  = (known after apply)
      + owner_id                             = (known after apply)
      + tags                                 = {
          + "Name"                            = "terraform-eks-demo-node"
          + "kubernetes.io/cluster/anjireddy" = "shared"
        }
      + tags_all                             = {
          + "Name"                            = "terraform-eks-demo-node"
          + "kubernetes.io/cluster/anjireddy" = "shared"
        }
    }

Plan: 18 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + config_map_aws_auth = (known after apply)
  + kubeconfig          = (known after apply)
aws_iam_role.demo-node: Creating...
aws_iam_role.demo-cluster: Creating...
aws_vpc.demo: Creating...
aws_iam_role.demo-node: Creation complete after 2s [id=terraform-eks-demo-node]
aws_iam_role_policy_attachment.demo-node-AmazonEC2ContainerRegistryReadOnly: Creating...
aws_iam_role_policy_attachment.demo-node-AmazonEKS_CNI_Policy: Creating...
aws_iam_role_policy_attachment.demo-node-AmazonEKSWorkerNodePolicy: Creating...
aws_iam_role.demo-cluster: Creation complete after 2s [id=terraform-eks-demo-cluster]
aws_iam_role_policy_attachment.demo-cluster-AmazonEKSVPCResourceController: Creating...
aws_iam_role_policy_attachment.demo-cluster-AmazonEKSClusterPolicy: Creating...
aws_iam_role_policy_attachment.demo-node-AmazonEC2ContainerRegistryReadOnly: Creation complete after 1s [id=terraform-eks-demo-node-20230216114528645000000001]
aws_iam_role_policy_attachment.demo-node-AmazonEKS_CNI_Policy: Creation complete after 1s [id=terraform-eks-demo-node-20230216114528680700000002]
aws_iam_role_policy_attachment.demo-node-AmazonEKSWorkerNodePolicy: Creation complete after 1s [id=terraform-eks-demo-node-20230216114528964500000003]
aws_iam_role_policy_attachment.demo-cluster-AmazonEKSVPCResourceController: Creation complete after 1s [id=terraform-eks-demo-cluster-20230216114529002700000004]
aws_iam_role_policy_attachment.demo-cluster-AmazonEKSClusterPolicy: Creation complete after 2s [id=terraform-eks-demo-cluster-20230216114529300100000005]
aws_vpc.demo: Creation complete after 9s [id=vpc-0828d8ea725d0fcd2]
aws_internet_gateway.demo: Creating...
aws_subnet.demo[0]: Creating...
aws_subnet.demo[1]: Creating...
aws_security_group.demo-cluster: Creating...
aws_internet_gateway.demo: Creation complete after 3s [id=igw-0a935fdf0314b30ec]
aws_route_table.demo: Creating...
aws_security_group.demo-cluster: Creation complete after 5s [id=sg-0372db9abdd674af6]
aws_security_group_rule.demo-cluster-ingress-workstation-https: Creating...
aws_route_table.demo: Creation complete after 3s [id=rtb-04006529b03ed3dea]
aws_security_group_rule.demo-cluster-ingress-workstation-https: Creation complete after 2s [id=sgrule-3942381807]
aws_subnet.demo[1]: Still creating... [10s elapsed]
aws_subnet.demo[0]: Still creating... [10s elapsed]
aws_subnet.demo[1]: Creation complete after 13s [id=subnet-08d936281be1e2f99]
aws_subnet.demo[0]: Creation complete after 14s [id=subnet-0dbf08bfce27d5c6e]
aws_route_table_association.demo[1]: Creating...
aws_route_table_association.demo[0]: Creating...
aws_eks_cluster.demo: Creating...
aws_route_table_association.demo[1]: Creation complete after 3s [id=rtbassoc-0cb52ef1894736390]
aws_route_table_association.demo[0]: Creation complete after 3s [id=rtbassoc-0d36a9d01b9bcd707]
aws_eks_cluster.demo: Still creating... [10s elapsed]
aws_eks_cluster.demo: Still creating... [10m11s elapsed]
aws_eks_cluster.demo: Creation complete after 10m19s [id=anjireddy]
aws_eks_node_group.demo: Creating...
aws_eks_node_group.demo: Still creating... [10s elapsed]

aws_eks_node_group.demo: Still creating... [2m30s elapsed]
aws_eks_node_group.demo: Creation complete after 2m31s [id=anjireddy:demo]

Apply complete! Resources: 18 added, 0 changed, 0 destroyed.

Outputs:
config_map_aws_auth = <<EOT
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
    - rolearn: arn:aws:iam::130593926195:role/terraform-eks-demo-node
      username: system:node:{{EC2PrivateDNSName}}
      groups:
        - system:bootstrappers
        - system:nodes

EOT
kubeconfig = <<EOT

apiVersion: v1
clusters:
- cluster:
    server: https://CE9900B58EC5C1E8AFBDE04EB92AA8B9.gr7.us-east-1.eks.amazonaws.com
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1ESXhOakV4TlRJ
d09Wb1hEVE16TURJeE16RXhOVEl3T1Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTCtmCkRaeitZeHIvQ1kxMnNMN2srN25RY2Z1ZTNuVXpPYTVRVU92Z1Z2cWFz
NXVtemFadDk4WDN0NlNLM2h5Y1drM1cKTkdiMGM2TENibGs0cEJpYnM2YXprWkVPL3FTZ25uY2Q0Q0JDek1OWTFvV0FYZGNNUCs3TTIxVGcxdmUzZGZVbApDZE9TaTJnRjFzVnFxcnhCYzFBMFBmUG5CUmFGZW9oM1hnaUh0QldnK1RWU0p0cTM3dStJMHZx
NmVBUHFaSWlmCm5VZW9YNmMrN2JWVEhKQUJKZ0dueFFsTElpU2NsRFJscjV5OXZMVjNBVmdHTWtBNFF4YjhqWWMzdW5OUHhmTzAKQkFjWVVzdDBqS1F4bEp0cU1GT0c1ZFZwWTJkQjBPSldPeWp5SFF1L0UxczBOUFJZeVZSdE1TSXNmMldvbGJBQgpmaGdr
aGtuYXhrZDRpVmNHSmlNQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZLdEJ0ajFFaTZ2ZTN0cERBNEJFK1pqODE1UVFNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpY
TXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBQno3TEZXdDBsc2czWnBDaHRFbwpXeGlYeFpDOG4weVN2ZnZ3L3VzbXViLzRYY3QxdkgwTGUyZUd3dkJKNzFQUzZTa3RXWlBwT01OVkVKYytaKzY3CjAwMWl2MFRDR2lCOHFlQU1yZUJMMXJGWlhuTVZQb1Zr
V1NYZlp3TkkxYWQzZ3FSbHJ0Qzc3WDRtUkFyNktRb1QKakwySU5vMUVJd2JtVGh3S2dBUklob2lsQ0NKUzd3cy9LOEZsamZMR0U0SjZwRHZmeVhRUDdjQWQxeUMrL2ViRwpPNFJNeVFBbUkyZjc0cEhiOGtLUG8wVHoxTGtCdjVHWVQ4aXVISE9TUTBkY1VK
VENBYTVxRUJ5cWVkL2NBK2JnCnhqa1ozMTdyUTVjUVhHT2J0Ly9pTU1SbjhtNW1YeTdTMDNlUG9lVFZ5dUk3V1RraHQyRXN3WTYvZlc2UDU5TFgKL1FnPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: aws
  name: aws
current-context: aws
kind: Config
preferences: {}
users:
- name: aws
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      command: aws-iam-authenticator
      args:
        - "token"
        - "-i"
        - "anjireddy"

EOT
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@


$ aws eks update-kubeconfig --name a    --region us-east-1
 aws eks update-kubeconfig --name test   --region us-east-1

An error occurred (ResourceNotFoundException) when calling the DescribeCluster o
peration: No cluster found for name: clustername.


https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/

curl.exe -LO "https://dl.k8s.io/release/v1.26.0/bin/windows/amd64/kubectl.exe"

hi@DESKTOP-LQI2IA5 MINGW64 ~/Desktop
$ kubectl version --client

WARNING: This version information is deprecated and will be replaced with the output from kubectl ve
rsion --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.4", GitCommit:"872a965c6c6526c
aa949f0c6ac028ef7aff3fb78", GitTreeState:"clean", BuildDate:"2022-11-09T13:36:36Z", GoVersion:"go1.1
9.3", Compiler:"gc", Platform:"windows/amd64"}
Kustomize Version: v4.5.7

AmazonEKSWorkerNodePolicy
AmazonEKS_CNI_Policy
AmazonEC2ContainerRegistryReadOnly

hi@DESKTOP-LQI2IA5 MINGW64 ~/Desktop
$ aws sts get-caller-identity
{
    "UserId": "AIDAR42ABFQZ7HRDVAYNS",
    "Account": "130593926195",
    "Arn": "arn:aws:iam::130593926195:user/anji"
}


apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec: 
  replicas: 3
  selector: 
    matchLabels: 
       app: nginx
  template: 
    metadata: 
       labels:
          app: nginx
    spec:
      containers: 
        - name: nginx
          image: ashokit/javawebapp     
          ports:
            - containerPort: 8080
---
apiversion: v1
kind: Service
metadata:
name: javawebappsvc
spec:
   type: LoadBalancer
   selector:
      app: nginx
   ports:
    - port: 80
      targetPort: 8080


Cluster status is CREATING
PS C:\Users\hi>  aws eks update-kubeconfig --name   test    --region us-east-1
Updated context arn:aws:eks:us-east-1:130593926195:cluster/test in C:\Users\hi\.kube\config
PS C:\Users\hi>  kubectl get all --all-namespaces  -o wide   --show-labels
NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE     IP       NODE     NOMINATED NODE   READINESS GATES   LABELS
kube-system   pod/coredns-79989457d9-b9jlw   0/1     Pending   0          4m10s   <none>   <none>   <none>           <none>            eks.amazonaws.com/component=coredns,k8s-app=kube-dns,pod-template-hash=79989457d9
kube-system   pod/coredns-79989457d9-llm9w   0/1     Pending   0          4m10s   <none>   <none>   <none>           <none>            eks.amazonaws.com/component=coredns,k8s-app=kube-dns,pod-template-hash=79989457d9

NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE     SELECTOR           LABELS
default       service/kubernetes   ClusterIP   10.100.0.1    <none>        443/TCP         4m28s   <none>             component=apiserver,provider=kubernetes
kube-system   service/kube-dns     ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   4m21s   k8s-app=kube-dns   eks.amazonaws.com/component=kube-dns,k8s-app=kube-dns,kubernetes.io/cluster-service=true,kubernetes.io/name=CoreDNS

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE     CONTAINERS   IMAGES                                                                                   SELECTOR             LABELS
kube-system   daemonset.apps/aws-node     0         0         0       0            0           <none>          4m22s   aws-node     602401143452.dkr.ecr.us-east-1.amazonaws.com/amazon-k8s-cni:v1.11.4-eksbuild.1           k8s-app=aws-node     app.kubernetes.io/instance=aws-vpc-cni,app.kubernetes.io/name=aws-node,app.kubernetes.io/version=v1.11.4,k8s-app=aws-node
kube-system   daemonset.apps/kube-proxy   0         0         0       0            0           <none>          4m22s   kube-proxy   602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/kube-proxy:v1.24.7-minimal-eksbuild.2   k8s-app=kube-proxy   eks.amazonaws.com/component=kube-proxy,k8s-app=kube-proxy

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES                                                                       SELECTOR                                               LABELS
kube-system   deployment.apps/coredns   0/2     2            0           4m22s   coredns      602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/coredns:v1.8.7-eksbuild.3   eks.amazonaws.com/component=coredns,k8s-app=kube-dns   eks.amazonaws.com/component=coredns,k8s-app=kube-dns,kubernetes.io/name=CoreDNS

NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES                                                                       SELECTOR                                                                            LABELS
kube-system   replicaset.apps/coredns-79989457d9   2         2         0       4m12s   coredns      602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/coredns:v1.8.7-eksbuild.3   eks.amazonaws.com/component=coredns,k8s-app=kube-dns,pod-template-hash=79989457d9   eks.amazonaws.com/component=coredns,k8s-app=kube-dns,pod-template-hash=79989457d9
PS C:\Users\hi> kubectl get svc  -o wide
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   4m36s   <none>
PS C:\Users\hi>


PS C:\Users\hi> kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://DBE771C9B0F50FCDDF3E4191A8F2D499.gr7.us-east-1.eks.amazonaws.com
  name: arn:aws:eks:us-east-1:130593926195:cluster/a
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://074A61D33F22CF319760D897F94F1D7D.gr7.us-east-1.eks.amazonaws.com
  name: arn:aws:eks:us-east-1:130593926195:cluster/anji
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://5161A6BA1FA241A6000CDA4F634AF4E9.gr7.us-east-1.eks.amazonaws.com
  name: arn:aws:eks:us-east-1:130593926195:cluster/test
contexts:
- context:
    cluster: arn:aws:eks:us-east-1:130593926195:cluster/a
    user: arn:aws:eks:us-east-1:130593926195:cluster/a
  name: arn:aws:eks:us-east-1:130593926195:cluster/a
- context:
    cluster: arn:aws:eks:us-east-1:130593926195:cluster/anji
    user: arn:aws:eks:us-east-1:130593926195:cluster/anji
  name: arn:aws:eks:us-east-1:130593926195:cluster/anji
- context:
    cluster: arn:aws:eks:us-east-1:130593926195:cluster/test
    user: arn:aws:eks:us-east-1:130593926195:cluster/test
  name: arn:aws:eks:us-east-1:130593926195:cluster/test
current-context: arn:aws:eks:us-east-1:130593926195:cluster/test
kind: Config
preferences: {}
users:
- name: arn:aws:eks:us-east-1:130593926195:cluster/a
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - --region
      - us-east-1
      - eks
      - get-token
      - --cluster-name
      - a
      command: aws
      env: null
      interactiveMode: IfAvailable
      provideClusterInfo: false
- name: arn:aws:eks:us-east-1:130593926195:cluster/anji
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - --region
      - us-east-1
      - eks
      - get-token
      - --cluster-name
      - anji
      command: aws
      env: null
      interactiveMode: IfAvailable
      provideClusterInfo: false
- name: arn:aws:eks:us-east-1:130593926195:cluster/test
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - --region
      - us-east-1
      - eks
      - get-token
      - --cluster-name
      - test
      command: aws
      env: null
      interactiveMode: IfAvailable
      provideClusterInfo: false
PS C:\Users\hi>
PS C:\Users\hi> kubectl get pods --all-namespaces
NAMESPACE     NAME                       READY   STATUS    RESTARTS   AGE
kube-system   coredns-79989457d9-b9jlw   0/1     Pending   0          7m29s
kube-system   coredns-79989457d9-llm9w   0/1     Pending   0          7m29s
PS C:\Users\hi> kubectl get nodes -o wide
No resources found
PS C:\Users\hi> kubectl get nodes
No resources found
PS C:\Users\hi> kubectl get nodes -o wide
No resources found
PS C:\Users\hi>

Issue type
Description
	
Affected resources
Ec2SubnetInvalidConfiguration	One or more Amazon EC2 Subnets of [subnet-08b82a2496ed438fa, subnet-0d881b737ebcefa06, subnet-0a9e613cf4722ffe8] for node group node does not automatically assign public IP addresses to instances launched into it. If you want your instances to be assigned a public IP address, then you need to enable auto-assign public IP address for the subnet. See IP addressing in VPC guide: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-ip-addressing.html#subnet-public-ip

Cluster status is CREATING
PS C:\Users\hi> aws eks update-kubeconfig --name   test    --region us-east-1
Updated context arn:aws:eks:us-east-1:130593926195:cluster/test in C:\Users\hi\.kube\config
PS C:\Users\hi>  kubectl get all --all-namespaces  -o wide   --show-labels
NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE     IP       NODE     NOMINATED NODE   READINESS GATES   LABELS
kube-system   pod/coredns-79989457d9-d92qk   0/1     Pending   0          4m53s   <none>   <none>   <none>           <none>            eks.amazonaws.com/component=coredns,k8s-app=kube-dns,pod-template-hash=79989457d9
kube-system   pod/coredns-79989457d9-mdmcg   0/1     Pending   0          4m53s   <none>   <none>   <none>           <none>            eks.amazonaws.com/component=coredns,k8s-app=kube-dns,pod-template-hash=79989457d9

NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE     SELECTOR           LABELS
default       service/kubernetes   ClusterIP   10.100.0.1    <none>        443/TCP         5m12s   <none>             component=apiserver,provider=kubernetes
kube-system   service/kube-dns     ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   5m5s    k8s-app=kube-dns   eks.amazonaws.com/component=kube-dns,k8s-app=kube-dns,kubernetes.io/cluster-service=true,kubernetes.io/name=CoreDNS

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE    CONTAINERS   IMAGES                                                                                   SELECTOR             LABELS
kube-system   daemonset.apps/aws-node     0         0         0       0            0           <none>          5m5s   aws-node     602401143452.dkr.ecr.us-east-1.amazonaws.com/amazon-k8s-cni:v1.11.4-eksbuild.1           k8s-app=aws-node     app.kubernetes.io/instance=aws-vpc-cni,app.kubernetes.io/name=aws-node,app.kubernetes.io/version=v1.11.4,k8s-app=aws-node
kube-system   daemonset.apps/kube-proxy   0         0         0       0            0           <none>          5m5s   kube-proxy   602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/kube-proxy:v1.24.7-minimal-eksbuild.2   k8s-app=kube-proxy   eks.amazonaws.com/component=kube-proxy,k8s-app=kube-proxy

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES                                                                       SELECTOR                                               LABELS
kube-system   deployment.apps/coredns   0/2     2            0           5m5s   coredns      602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/coredns:v1.8.7-eksbuild.3   eks.amazonaws.com/component=coredns,k8s-app=kube-dns   eks.amazonaws.com/component=coredns,k8s-app=kube-dns,kubernetes.io/name=CoreDNS

NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES                                                                       SELECTOR                                                                            LABELS
kube-system   replicaset.apps/coredns-79989457d9   2         2         0       4m55s   coredns      602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/coredns:v1.8.7-eksbuild.3   eks.amazonaws.com/component=coredns,k8s-app=kube-dns,pod-template-hash=79989457d9   eks.amazonaws.com/component=coredns,k8s-app=kube-dns,pod-template-hash=79989457d9
PS C:\Users\hi> kubectl get svc  -o wide
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   5m20s   <none>
PS C:\Users\hi> kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://DBE771C9B0F50FCDDF3E4191A8F2D499.gr7.us-east-1.eks.amazonaws.com
  name: arn:aws:eks:us-east-1:130593926195:cluster/a
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://074A61D33F22CF319760D897F94F1D7D.gr7.us-east-1.eks.amazonaws.com
  name: arn:aws:eks:us-east-1:130593926195:cluster/anji
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://C89B0E9BC84BAF86898D8C37BA5F78EC.sk1.us-east-1.eks.amazonaws.com
  name: arn:aws:eks:us-east-1:130593926195:cluster/test
contexts:
- context:
    cluster: arn:aws:eks:us-east-1:130593926195:cluster/a
    user: arn:aws:eks:us-east-1:130593926195:cluster/a
  name: arn:aws:eks:us-east-1:130593926195:cluster/a
- context:
    cluster: arn:aws:eks:us-east-1:130593926195:cluster/anji
    user: arn:aws:eks:us-east-1:130593926195:cluster/anji
  name: arn:aws:eks:us-east-1:130593926195:cluster/anji
- context:
    cluster: arn:aws:eks:us-east-1:130593926195:cluster/test
    user: arn:aws:eks:us-east-1:130593926195:cluster/test
  name: arn:aws:eks:us-east-1:130593926195:cluster/test
current-context: arn:aws:eks:us-east-1:130593926195:cluster/test
kind: Config
preferences: {}
users:
- name: arn:aws:eks:us-east-1:130593926195:cluster/a
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - --region
      - us-east-1
      - eks
      - get-token
      - --cluster-name
      - a
      command: aws
      env: null
      interactiveMode: IfAvailable
      provideClusterInfo: false
- name: arn:aws:eks:us-east-1:130593926195:cluster/anji
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - --region
      - us-east-1
      - eks
      - get-token
      - --cluster-name
      - anji
      command: aws
      env: null
      interactiveMode: IfAvailable
      provideClusterInfo: false
- name: arn:aws:eks:us-east-1:130593926195:cluster/test
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - --region
      - us-east-1
      - eks
      - get-token
      - --cluster-name
      - test
      command: aws
      env: null
      interactiveMode: IfAvailable
      provideClusterInfo: false
PS C:\Users\hi>
PS C:\Users\hi> kubectl get pods --all-namespaces
NAMESPACE     NAME                       READY   STATUS    RESTARTS   AGE
kube-system   coredns-79989457d9-d92qk   0/1     Pending   0          5m21s
kube-system   coredns-79989457d9-mdmcg   0/1     Pending   0          5m21s
PS C:\Users\hi>  kubectl get nodes -o wide
No resources found
PS C:\Users\hi>  kubectl get nodes -o wide
NAME                           STATUS   ROLES    AGE   VERSION                INTERNAL-IP    EXTERNAL-IP     OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME
ip-192-168-3-26.ec2.internal   Ready    <none>   57s   v1.24.10-eks-48e63af   192.168.3.26   54.175.26.147   Amazon Linux 2   5.10.165-143.735.amzn2.x86_64   containerd://1.6.6
PS C:\Users\hi>  kubectl get nodes -o wide
NAME                           STATUS   ROLES    AGE     VERSION                INTERNAL-IP    EXTERNAL-IP     OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME
ip-192-168-3-26.ec2.internal   Ready    <none>   4m37s   v1.24.10-eks-48e63af   192.168.3.26   54.175.26.147   Amazon Linux 2   5.10.165-143.735.amzn2.x86_64   containerd://1.6.6
PS C:\Users\hi> kubectl get pods --all-namespaces
NAMESPACE     NAME                       READY   STATUS    RESTARTS   AGE
default       nginx                      0/1     Pending   0          7m6s
default       nginx-688bc96c4-gqprr      0/1     Pending   0          7m57s
default       nginx-688bc96c4-vxmhc      0/1     Pending   0          7m57s
default       nginx-688bc96c4-z4wjh      0/1     Pending   0          7m3s
kube-system   aws-node-mvrgx             1/1     Running   0          7m3s
kube-system   coredns-79989457d9-s4k4m   0/1     Pending   0          2m22s
kube-system   coredns-79989457d9-w9p5q   0/1     Pending   0          2m22s
kube-system   kube-proxy-ftd7t           1/1     Running   0          9m3s
PS C:\Users\hi>  kubectl delete all --all
pod "nginx" deleted
pod "nginx-688bc96c4-gqprr" deleted
pod "nginx-688bc96c4-vxmhc" deleted
pod "nginx-688bc96c4-z4wjh" deleted
service "kubernetes" deleted
deployment.apps "nginx" deleted
PS C:\Users\hi>



hi@DESKTOP-LQI2IA5 MINGW64 ~/Desktop/New folder (2)
$  kubectl get all --all-namespaces  -o wide
NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE     IP              NODE                           NOMINATED NOD
E   READINESS GATES
default       pod/nginx                      0/1     Pending   0          87s     <none>          <none>                         <none>
    <none>
default       pod/nginx-688bc96c4-gqprr      0/1     Pending   0          2m18s   <none>          <none>                         <none>
    <none>
default       pod/nginx-688bc96c4-vxmhc      0/1     Pending   0          2m18s   <none>          <none>                         <none>
    <none>
default       pod/nginx-688bc96c4-z4wjh      0/1     Pending   0          84s     <none>          <none>                         <none>
    <none>
kube-system   pod/aws-node-mvrgx             1/1     Running   0          84s     192.168.3.26    ip-192-168-3-26.ec2.internal   <none>
    <none>
kube-system   pod/coredns-79989457d9-d92qk   1/1     Running   0          9m38s   192.168.3.245   ip-192-168-3-26.ec2.internal   <none>
    <none>
kube-system   pod/coredns-79989457d9-mdmcg   1/1     Running   0          9m38s   192.168.3.211   ip-192-168-3-26.ec2.internal   <none>
    <none>
kube-system   pod/kube-proxy-ftd7t           1/1     Running   0          3m24s   192.168.3.26    ip-192-168-3-26.ec2.internal   <none>
    <none>

NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE     SELECTOR
default       service/kubernetes   ClusterIP   10.100.0.1    <none>        443/TCP         9m58s   <none>
kube-system   service/kube-dns     ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   9m51s   k8s-app=kube-dns

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE     CONTAINERS   IMAGES
                                                                               SELECTOR
kube-system   daemonset.apps/aws-node     1         1         1       1            1           <none>          9m51s   aws-node     6024011434
52.dkr.ecr.us-east-1.amazonaws.com/amazon-k8s-cni:v1.11.4-eksbuild.1           k8s-app=aws-node
kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           <none>          9m51s   kube-proxy   6024011434
52.dkr.ecr.us-east-1.amazonaws.com/eks/kube-proxy:v1.24.7-minimal-eksbuild.2   k8s-app=kube-proxy

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES
                             SELECTOR
default       deployment.apps/nginx     0/3     3            0           2m19s   nginx        ashokit/javawebapp
                             app=nginx
kube-system   deployment.apps/coredns   2/2     2            2           9m51s   coredns      602401143452.dkr.ecr.us-east-1.amazonaws.com/eks
/coredns:v1.8.7-eksbuild.3   eks.amazonaws.com/component=coredns,k8s-app=kube-dns

NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES
                                   SELECTOR
default       replicaset.apps/nginx-688bc96c4      3         3         0       2m19s   nginx        ashokit/javawebapp
                                   app=nginx,pod-template-hash=688bc96c4
kube-system   replicaset.apps/coredns-79989457d9   2         2         2       9m40s   coredns      602401143452.dkr.ecr.us-east-1.amazonaws.c
om/eks/coredns:v1.8.7-eksbuild.3   eks.amazonaws.com/component=coredns,k8s-app=kube-dns,pod-template-hash=79989457d9

hi@DESKTOP-LQI2IA5 MINGW64 ~/Desktop/New folder (2)
$ kubectl get pod -o wide
NAME                    READY   STATUS    RESTARTS   AGE     IP       NODE     NOMINATED NODE   READINESS GATES
nginx                   0/1     Pending   0          2m49s   <none>   <none>   <none>           <none>
nginx-688bc96c4-gqprr   0/1     Pending   0          3m40s   <none>   <none>   <none>           <none>
nginx-688bc96c4-vxmhc   0/1     Pending   0          3m40s   <none>   <none>   <none>           <none>
nginx-688bc96c4-z4wjh   0/1     Pending   0          2m46s   <none>   <none>   <none>           <none>

hi@DESKTOP-LQI2IA5 MINGW64 ~/Desktop/New folder (2)
$ kubectl get pod -o wide
NAME                    READY   STATUS    RESTARTS   AGE     IP       NODE     NOMINATED NODE   READINESS GATES
nginx                   0/1     Pending   0          3m3s    <none>   <none>   <none>           <none>
nginx-688bc96c4-gqprr   0/1     Pending   0          3m54s   <none>   <none>   <none>           <none>
nginx-688bc96c4-vxmhc   0/1     Pending   0          3m54s   <none>   <none>   <none>           <none>
nginx-688bc96c4-z4wjh   0/1     Pending   0          3m      <none>   <none>   <none>           <none>

hi@DESKTOP-LQI2IA5 MINGW64 ~/Desktop/New folder (2)
$ kubectl desscribe pod nginx-688bc96c4-gqprr
error: unknown command "desscribe" for "kubectl"

Did you mean this?
        describe

hi@DESKTOP-LQI2IA5 MINGW64 ~/Desktop/New folder (2)
$ kubectl describe pod nginx-688bc96c4-gqprr
Name:             nginx-688bc96c4-gqprr
Namespace:        default
Priority:         0
Service Account:  default
Node:             <none>
Labels:           app=nginx
                  pod-template-hash=688bc96c4
Annotations:      kubernetes.io/psp: eks.privileged
Status:           Pending
IP:
IPs:              <none>
Controlled By:    ReplicaSet/nginx-688bc96c4
Containers:
  nginx:
    Image:        ashokit/javawebapp
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-42rt8 (ro)
Conditions:
  Type           Status
  PodScheduled   False
Volumes:
  kube-api-access-42rt8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                    From               Message
  ----     ------            ----                   ----               -------
  Warning  FailedScheduling  3m36s (x3 over 4m32s)  default-scheduler  0/1 nodes are available: 1 Too many pods. preemption: 0/1 nodes are available: 1 No preemption victims found f
or incoming pod.

hi@DESKTOP-LQI2IA5 MINGW64 ~/Desktop/New folder (2)
$



https://docs.aws.amazon.com/eks/latest/userguide/connecting-cluster.html
aws eks register-cluster \
     --name my-first-registered-cluster \
     --connector-config roleArn=arn:aws:iam::111122223333:role/AmazonEKSConnectorAgentRole,provider="OTHER" \
     --region us-east-1
https://www.ecloudcontrol.com/how-to-create-amazon-eks-using-aws-management-console-and-aws-cli/
https://dev.to/aws-builders/the-essential-guide-to-create-a-kubernetes-cluster-using-aws-ekscli-5f89

https://awscli.amazonaws.com/v2/documentation/api/2.0.34/reference/eks/create-cluster.html

aws eks create-cluster --name prod \
--role-arn arn:aws:iam::130593926195:role/prod-eks-role \
--resources-vpc-config subnetIds=subnet-08b82a2496ed438fa,subnet-0a9e613cf4722ffe8,subnet-0d881b737ebcefa06,securityGroupIds=sg-06b518ac61dfd97b1

arn:aws:iam::130593926195:role/prod-eks-role


PS C:\Users\hi> aws eks create-cluster --name prod --role-arn arn:aws:iam::130593926195:role/prod-eks-role  --resources-vpc-config subnetIds=subnet-08b82a2496ed438fa,subnet-0a9e613cf4722ffe8,subnet-0d881b737ebcefa06,securityGroupIds=sg-06b518ac61dfd97b1
{
    "cluster": {
        "name": "prod",
        "arn": "arn:aws:eks:us-east-1:130593926195:cluster/prod",
        "createdAt": "2023-02-23T16:16:00.712000+05:30",
        "version": "1.24",
        "roleArn": "arn:aws:iam::130593926195:role/prod-eks-role",
        "resourcesVpcConfig": {
            "subnetIds": [
                "subnet-08b82a2496ed438fa",
                "subnet-0a9e613cf4722ffe8",
                "subnet-0d881b737ebcefa06"
            ],
            "securityGroupIds": [
                "sg-06b518ac61dfd97b1"
            ],
            "vpcId": "vpc-0af7f272779d9b4af",
            "endpointPublicAccess": true,
            "endpointPrivateAccess": false,
            "publicAccessCidrs": [
                "0.0.0.0/0"
            ]
        },
        "kubernetesNetworkConfig": {
            "serviceIpv4Cidr": "10.100.0.0/16",
            "ipFamily": "ipv4"
        },
        "logging": {
            "clusterLogging": [
                {
                    "types": [
                        "api",
                        "audit",
                        "authenticator",
                        "controllerManager",
                        "scheduler"
                    ],
                    "enabled": false
                }
            ]
        },
        "status": "CREATING",
        "certificateAuthority": {},
        "platformVersion": "eks.4",
        "tags": {}
    }
}

aws iam create-role \
  --role-name myAmazonEKSClusterRole \
  --assume-role-policy-document file://"eks-cluster-role-trust-policy.json"

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
PS C:\Users\hi>  kubectl get all --all-namespaces  -o wide
NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE     IP              NODE                           NOMINATED NODE   READINESS GATES
kube-system   pod/aws-node-gtkln             1/1     Running   0          57s     192.168.2.69    ip-192-168-2-69.ec2.internal   <none>           <none>
kube-system   pod/coredns-79989457d9-m9q2k   1/1     Running   0          7m13s   192.168.2.228   ip-192-168-2-69.ec2.internal   <none>           <none>
kube-system   pod/coredns-79989457d9-rbf5h   1/1     Running   0          7m13s   192.168.2.139   ip-192-168-2-69.ec2.internal   <none>           <none>
kube-system   pod/kube-proxy-ghwwq           1/1     Running   0          57s     192.168.2.69    ip-192-168-2-69.ec2.internal   <none>           <none>

NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE     SELECTOR
default       service/kubernetes   ClusterIP   10.100.0.1    <none>        443/TCP         7m24s   <none>
kube-system   service/kube-dns     ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   7m18s   k8s-app=kube-dns

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE     CONTAINERS   IMAGES                                                                                   SELECTOR
kube-system   daemonset.apps/aws-node     1         1         1       1            1           <none>          7m18s   aws-node     602401143452.dkr.ecr.us-east-1.amazonaws.com/amazon-k8s-cni:v1.11.4-eksbuild.1           k8s-app=aws-node
kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           <none>          7m18s   kube-proxy   602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/kube-proxy:v1.24.7-minimal-eksbuild.2   k8s-app=kube-proxy

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES                                                                       SELECTOR
kube-system   deployment.apps/coredns   2/2     2            2           7m18s   coredns      602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/coredns:v1.8.7-eksbuild.3   eks.amazonaws.com/component=coredns,k8s-app=kube-dns

NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES                                                                       SELECTOR
kube-system   replicaset.apps/coredns-79989457d9   2         2         2       7m14s   coredns      602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/coredns:v1.8.7-eksbuild.3   eks.amazonaws.com/component=coredns,k8s-app=kube-dns,pod-template-hash=79989457d9
PS C:\Users\hi>
PS C:\Users\hi>  kubectl get all --all-namespaces  -o wide
NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE     IP              NODE                           NOMINATED NODE   READINESS GATES
default       pod/nginx-688bc96c4-bfxt4      1/1     Running   0          89s     192.168.2.120   ip-192-168-2-69.ec2.internal   <none>           <none>
kube-system   pod/aws-node-sn4bn             1/1     Running   0          2m3s    192.168.2.69    ip-192-168-2-69.ec2.internal   <none>           <none>
kube-system   pod/coredns-79989457d9-m9q2k   1/1     Running   0          9m35s   192.168.2.228   ip-192-168-2-69.ec2.internal   <none>           <none>
kube-system   pod/coredns-79989457d9-rbf5h   1/1     Running   0          9m35s   192.168.2.139   ip-192-168-2-69.ec2.internal   <none>           <none>
kube-system   pod/kube-proxy-ghwwq           1/1     Running   0          3m19s   192.168.2.69    ip-192-168-2-69.ec2.internal   <none>           <none>

NAMESPACE     NAME                    TYPE           CLUSTER-IP      EXTERNAL-IP                                                               PORT(S)         AGE     SELECTOR
default       service/javawebappsvc   LoadBalancer   10.100.71.150   a4fe1304d251649b59f96ba6110915e2-1371758696.us-east-1.elb.amazonaws.com   80:32393/TCP    35s     app=nginx
default       service/kubernetes      ClusterIP      10.100.0.1      <none>                                                                    443/TCP         9m45s   <none>
kube-system   service/kube-dns        ClusterIP      10.100.0.10     <none>                                                                    53/UDP,53/TCP   9m39s   k8s-app=kube-dns

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE     CONTAINERS   IMAGES                                                                                   SELECTOR
kube-system   daemonset.apps/aws-node     1         1         1       1            1           <none>          9m39s   aws-node     602401143452.dkr.ecr.us-east-1.amazonaws.com/amazon-k8s-cni:v1.11.4-eksbuild.1           k8s-app=aws-node
kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           <none>          9m39s   kube-proxy   602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/kube-proxy:v1.24.7-minimal-eksbuild.2   k8s-app=kube-proxy

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES                                                                       SELECTOR
default       deployment.apps/nginx     1/1     1            1           90s     nginx        ashokit/javawebapp                                                           app=nginx
kube-system   deployment.apps/coredns   2/2     2            2           9m40s   coredns      602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/coredns:v1.8.7-eksbuild.3   eks.amazonaws.com/component=coredns,k8s-app=kube-dns

NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES                                                                       SELECTOR
default       replicaset.apps/nginx-688bc96c4      1         1         1       90s     nginx        ashokit/javawebapp                                                           app=nginx,pod-template-hash=688bc96c4
kube-system   replicaset.apps/coredns-79989457d9   2         2         2       9m36s   coredns      602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/coredns:v1.8.7-eksbuild.3   eks.amazonaws.com/component=coredns,k8s-app=kube-dns,pod-template-hash=79989457d9
PS C:\Users\hi>
PS C:\Users\hi> kubectl get svc
NAME            TYPE           CLUSTER-IP      EXTERNAL-IP                                                               PORT(S)        AGE
javawebappsvc   LoadBalancer   10.100.71.150   a4fe1304d251649b59f96ba6110915e2-1371758696.us-east-1.elb.amazonaws.com   80:32393/TCP   47s
kubernetes      ClusterIP      10.100.0.1      <none>                                                                    443/TCP        9m57s
PS C:\Users\hi> kubectl  get deployment
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
nginx   1/1     1            1           3m31s
PS C:\Users\hi> kubectl  get svc  -o wide
NAME            TYPE           CLUSTER-IP      EXTERNAL-IP                                                               PORT(S)        AGE     SELECTOR
javawebappsvc   LoadBalancer   10.100.71.150   a4fe1304d251649b59f96ba6110915e2-1371758696.us-east-1.elb.amazonaws.com   80:32393/TCP   2m48s   app=nginx
kubernetes      ClusterIP      10.100.0.1      <none>                                                                    443/TCP        11m     <none>
PS C:\Users\hi>
PS C:\Users\hi> kubectl  get nodes
NAME                           STATUS   ROLES    AGE     VERSION
ip-192-168-2-69.ec2.internal   Ready    <none>   5m58s   v1.24.10-eks-48e63af
PS C:\Users\hi> kubectl  get nodes  -o wide
NAME                           STATUS   ROLES    AGE    VERSION                INTERNAL-IP    EXTERNAL-IP     OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME
ip-192-168-2-69.ec2.internal   Ready    <none>   6m3s   v1.24.10-eks-48e63af   192.168.2.69   44.211.48.244   Amazon Linux 2   5.10.165-143.735.amzn2.x86_64   containerd://1.6.6
PS C:\Users\hi>
PS C:\Users\hi> kubectl edit deployment
deployment.apps/nginx edited
PS C:\Users\hi> kubectl  get svc  -o wide
NAME            TYPE           CLUSTER-IP      EXTERNAL-IP                                                               PORT(S)        AGE    SELECTOR
javawebappsvc   LoadBalancer   10.100.71.150   a4fe1304d251649b59f96ba6110915e2-1371758696.us-east-1.elb.amazonaws.com   80:32393/TCP   4m6s   app=nginx
kubernetes      ClusterIP      10.100.0.1      <none>                                                                    443/TCP        13m    <none>
PS C:\Users\hi> kubectl get pod -o wide
NAME                    READY   STATUS    RESTARTS   AGE     IP              NODE                           NOMINATED NODE   READINESS GATES
nginx-688bc96c4-4tkbp   1/1     Running   0          20s     192.168.2.104   ip-192-168-2-69.ec2.internal   <none>           <none>
nginx-688bc96c4-bfxt4   1/1     Running   0          5m11s   192.168.2.120   ip-192-168-2-69.ec2.internal   <none>           <none>
nginx-688bc96c4-nk67c   1/1     Running   0          20s     192.168.2.92    ip-192-168-2-69.ec2.internal   <none>           <none>
PS C:\Users\hi>
+++++++++++++++"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec: 
  replicas: 1
  selector: 
    matchLabels: 
       app: nginx
  template: 
    metadata: 
       labels:
          app: nginx
    spec:
      containers: 
        - name: nginx
          image: ashokit/javawebapp     
          ports:
            - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
spec:
   type: LoadBalancer
   selector:
      app: nginx
   ports:
    - port: 80
      targetPort: 8080

""
 C:\Users\hi> kubectl get all  --all-namespaces  "
NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE
default       pod/nginx                      1/1     Running   0          3m41s
default       pod/nginx-688bc96c4-gwwtn      1/1     Running   0          36s
kube-system   pod/aws-node-5qspw             1/1     Running   0          2m53s
kube-system   pod/coredns-79989457d9-854lc   1/1     Running   0          11m
kube-system   pod/coredns-79989457d9-c99bw   1/1     Running   0          11m
kube-system   pod/kube-proxy-bqds7           1/1     Running   0          5m12s

NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE
default       service/kubernetes   ClusterIP   10.100.0.1    <none>        443/TCP         11m
kube-system   service/kube-dns     ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   11m

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
kube-system   daemonset.apps/aws-node     1         1         1       1            1           <none>          11m
kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           <none>          11m

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
default       deployment.apps/nginx     1/1     1            1           37s
kube-system   deployment.apps/coredns   2/2     2            2           11m

NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE
default       replicaset.apps/nginx-688bc96c4      1         1         1       37s
kube-system   replicaset.apps/coredns-79989457d9   2         2         2       11m

PS C:\Users\hi> kubectl get pod -o wide   "
NAME                    READY   STATUS    RESTARTS   AGE    IP             NODE                            NOMINATED NODE   READINESS GATES
nginx                   1/1     Running   0          4m5s   192.168.3.78   ip-192-168-3-220.ec2.internal   <none>           <none>
nginx-688bc96c4-gwwtn   1/1     Running   0          60s    192.168.3.68   ip-192-168-3-220.ec2.internal   <none>           <none>



PS C:\Users\hi> kubectl describe deployment
Name:                   nginx
Namespace:              default
CreationTimestamp:      Fri, 24 Feb 2023 11:08:59 +0530
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=nginx
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        ashokit/javawebapp
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   nginx-688bc96c4 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  4m7s  deployment-controller  Scaled up replica set nginx-688bc96c4 to 1  "

PS C:\Users\hi> kubectl describe pod nginx-688bc96c4-gwwtn
Name:             nginx-688bc96c4-gwwtn
Namespace:        default
Priority:         0
Service Account:  default
Node:             ip-192-168-3-220.ec2.internal/192.168.3.220
Start Time:       Fri, 24 Feb 2023 11:08:59 +0530
Labels:           app=nginx
                  pod-template-hash=688bc96c4
Annotations:      kubernetes.io/psp: eks.privileged
Status:           Running
IP:               192.168.3.68
IPs:
  IP:           192.168.3.68
Controlled By:  ReplicaSet/nginx-688bc96c4
Containers:
  nginx:
    Container ID:   containerd://b4550d1ee35f39330f70b3e0fe27ec42d9c67c1ffa7620b6cfbb167ff8bd480a
    Image:          ashokit/javawebapp
    Image ID:       docker.io/ashokit/javawebapp@sha256:dc6c6b8ece0ce5a770814bf0b0ecbc68d0277cf6c472f3996dc9d3db90407fe9
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 24 Feb 2023 11:09:07 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2xkvs (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-2xkvs:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  4m36s  default-scheduler  Successfully assigned default/nginx-688bc96c4-gwwtn to ip-192-168-3-220.ec2.internal
  Normal  Pulling    4m35s  kubelet            Pulling image "ashokit/javawebapp"
  Normal  Pulled     4m28s  kubelet            Successfully pulled image "ashokit/javawebapp" in 6.878384761s
  Normal  Created    4m28s  kubelet            Created container nginx
  Normal  Started    4m28s  kubelet            Started container nginx  :"
  

PS C:\Users\hi> kubectl get all
NAME                        READY   STATUS    RESTARTS   AGE
pod/nginx                   1/1     Running   0          8m46s
pod/nginx-688bc96c4-gwwtn   1/1     Running   0          5m41s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   17m

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx   1/1     1            1           5m42s

NAME                              DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-688bc96c4   1         1         1       5m42s"

PS C:\Users\hi> kubectl exec -it nginx-688bc96c4-gwwtn -- /bin/bash
root@nginx-688bc96c4-gwwtn:/usr/local/tomcat#
root@nginx-688bc96c4-gwwtn:/usr/local/tomcat#
root@nginx-688bc96c4-gwwtn:/usr/local/tomcat# ls
LICENSE  NOTICE  RELEASE-NOTES  RUNNING.txt  bin  conf  lib  logs  temp  webapps  work"
##########################################################################################################
+++++++======= PYTHON- FLASK - INGRESS = =========+++++++++++++++++""
https://www.youtube.com/watch?v=UfwWmjHnfg0&t=445s

https://github.com/anjilinux/project-flask-docker-file-?organization=anjilinux&organization=anjilinux

https://github.com/anjilinux/project-eks-python-flask-load-balancer-?organization=anjilinux&organization=anjilinux

K8s EKS + Flask + LoadBalancer = Public Cloud Rest API   "
=====
from flask import Flask,jsonify,request
from time import time
app = Flask(__name__)
@app.route('/', methods = ['GET'])
def ReturnJSON():
    if(request.method == 'GET'):
        data = {
        "message" : 'Hello World',
        "timestamp" : int(time()),
        }

    return jsonify(data)
if __name__ == '__main__':
    app.run(host='0.0.0.0')

######################"
FROM python:3.6
COPY . /app
WORKDIR /app
RUN pip install -r requirements.txt
EXPOSE  5000
ENTRYPOINT ["python"]
CMD ["app.py"]
++++++++++++++++++++++
requirements.txt   =  flask
++++++++++++++++++++++++++++"

apiVersion: apps/v1
kind: Deployment
metadata: 
   name: flask
   labels: 
     app: flask
spec: 
  replicas: 1
  selector:
     matchLabels: 
        app: flask
  template: 
     metadata: 
        labels:
           app: flask
     spec:
        containers:
           - name: flask
             image: docker.io/smugford/anotherflask:init
             ports: 
               - containerPort: 5000
---
apiVersion: v1
kind: Service
metadata: 
  name: flask
spec:
  selector:
    app: flask
  ports:
    - port: 5000
      targetPort: 5000
  type: LoadBalancer         
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: flask
spec:
  rules:
    - host: flask.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: flask
                port: 
                  number: 80
++++++++++++++++"

PS C:\Users\hi>  aws eks update-kubeconfig --name   aaa    --region us-east-1
Added new context arn:aws:eks:us-east-1:473319165751:cluster/aaa to C:\Users\hi\.kube\config

PS C:\Users\hi>  kubectl get all --all-namespaces  -o wide
NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE     IP       NODE     NOMINATED NODE   READINESS GATES
kube-system   pod/coredns-7975d6fb9b-7r8pg   0/1     Pending   0          3m45s   <none>   <none>   <none>           <none>
kube-system   pod/coredns-7975d6fb9b-vxng9   0/1     Pending   0          3m45s   <none>   <none>   <none>           <none>

NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE     SELECTOR
default       service/kubernetes   ClusterIP   172.20.0.1    <none>        443/TCP         3m57s   <none>
kube-system   service/kube-dns     ClusterIP   172.20.0.10   <none>        53/UDP,53/TCP   3m50s   k8s-app=kube-dns

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE     CONTAINERS   IMAGES                                                                                   SELECTOR
kube-system   daemonset.apps/aws-node     0         0         0       0            0           <none>          3m51s   aws-node     602401143452.dkr.ecr.us-east-1.amazonaws.com/amazon-k8s-cni:v1.12.2-eksbuild.1           k8s-app=aws-node
kube-system   daemonset.apps/kube-proxy   0         0         0       0            0           <none>          3m51s   kube-proxy   602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/kube-proxy:v1.25.6-minimal-eksbuild.1   k8s-app=kube-proxy

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES                                                                       SELECTOR
kube-system   deployment.apps/coredns   0/2     2            0           3m51s   coredns      602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/coredns:v1.9.3-eksbuild.2   eks.amazonaws.com/component=coredns,k8s-app=kube-dns

NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES                                                                       SELECTOR
kube-system   replicaset.apps/coredns-7975d6fb9b   2         2         0       3m47s   coredns  "    602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/coredns:v1.9.3-eksbuild.2   eks.amazonaws.com/component=coredns,k8s-app=kube-dns,pod-template-hash=7975d6fb9b

 kubectl get all --all-namespaces  -o wide
NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE     IP            NODE                          NOMINATED NODE   READINESS GATES
kube-system   pod/aws-node-mz445             1/1     Running   0          47s     10.0.15.175   ip-10-0-15-175.ec2.internal   <none>           <none>
kube-system   pod/coredns-7975d6fb9b-7r8pg   1/1     Running   0          7m22s   10.0.15.29    ip-10-0-15-175.ec2.internal   <none>           <none>
kube-system   pod/coredns-7975d6fb9b-vxng9   1/1     Running   0          7m22s   10.0.15.216   ip-10-0-15-175.ec2.internal   <none>           <none>
kube-system   pod/kube-proxy-b9cjg           1/1     Running   0          47s     10.0.15.175   ip-10-0-15-175.ec2.internal   <none>           <none>

NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE     SELECTOR
default       service/kubernetes   ClusterIP   172.20.0.1    <none>        443/TCP         7m35s   <none>
kube-system   service/kube-dns     ClusterIP   172.20.0.10   <none>        53/UDP,53/TCP   7m28s   k8s-app=kube-dns

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE     CONTAINERS   IMAGES                                                                                   SELECTOR
kube-system   daemonset.apps/aws-node     1         1         1       1            1           <none>          7m28s   aws-node     602401143452.dkr.ecr.us-east-1.amazonaws.com/amazon-k8s-cni:v1.12.2-eksbuild.1           k8s-app=aws-node
kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           <none>          7m28s   kube-proxy   602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/kube-proxy:v1.25.6-minimal-eksbuild.1   k8s-app=kube-proxy

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES                                                                       SELECTOR
kube-system   deployment.apps/coredns   2/2     2            2           7m28s   coredns      602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/coredns:v1.9.3-eksbuild.2   eks.amazonaws.com/component=coredns,k8s-app=kube-dns

NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES                                                                       SELECTOR
kube-system   replicaset.apps/coredns-7975d6fb9b   2         2         2       7m24s   coredns      602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/coredns:v1.9.3-eksbuild.2   eks.amazonaws.com/component=coredns,k8s-app=kube-dns,pod-template-hash=7975d6fb9b
"
PS C:\Users\hi>  kubectl get all --all-namespaces  -o wide
NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE     IP            NODE                          NOMINATED NODE   READINESS GATES
default       "pod/flask-6f4c5ddcc4-dwf65"     1/1     Running   0          46s    " 10.0.15.83"    ip-10-0-15-175.ec2.internal   <none>           <none>
kube-system   pod/aws-node-mz445             1/1     Running   0          105s    10.0.15.175   ip-10-0-15-175.ec2.internal   <none>           <none>
kube-system   pod/coredns-7975d6fb9b-7r8pg   1/1     Running   0          8m20s   10.0.15.29    ip-10-0-15-175.ec2.internal   <none>           <none>
kube-system   pod/coredns-7975d6fb9b-vxng9   1/1     Running   0          8m20s   10.0.15.216   ip-10-0-15-175.ec2.internal   <none>           <none>
kube-system   pod/kube-proxy-b9cjg           1/1     Running   0          105s    10.0.15.175   ip-10-0-15-175.ec2.internal   <none>           <none>

NAMESPACE     NAME                 TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)          AGE     SELECTOR
default       service/flask      "  LoadBalancer  " 172.20.146.240  " a35f11019984741bf969a51442417a35-403333476.us-east-1.elb.amazonaws.com"   "5000:30574/TCP"   47s     app=flask
default       service/kubernetes   ClusterIP      172.20.0.1       <none>                                                                   443/TCP          8m33s   <none>
kube-system   service/kube-dns     ClusterIP      172.20.0.10      <none>                                                                   53/UDP,53/TCP    8m26s   k8s-app=kube-dns

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE     CONTAINERS   IMAGES                                                                                   SELECTOR
kube-system   daemonset.apps/aws-node     1         1         1       1            1           <none>          8m26s   aws-node     602401143452.dkr.ecr.us-east-1.amazonaws.com/amazon-k8s-cni:v1.12.2-eksbuild.1           k8s-app=aws-node
kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           <none>          8m26s   kube-proxy   602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/kube-proxy:v1.25.6-minimal-eksbuild.1   k8s-app=kube-proxy

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES                                                                       SELECTOR
default      " deployment.apps/flask  "  " 1/1 "    1            1           47s     flask       " docker.io/smugford/anotherflask:init "                                        app=flask
kube-system   deployment.apps/coredns   2/2     2            2           8m26s   coredns      602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/coredns:v1.9.3-eksbuild.2   eks.amazonaws.com/component=coredns,k8s-app=kube-dns

NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES                                                                       SELECTOR
default       replicaset.apps/flask-6f4c5ddcc4     1         1         1       47s     flask        docker.io/smugford/anotherflask:init                                         app=flask,pod-template-hash=6f4c5ddcc4
kube-system   replicaset.apps/coredns-7975d6fb9b   2         2         2       8m22s   coredns      602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/coredns:v1.9.3-eksbuild.2   eks.amazonaws.com/component=coredns,k8s-app=kube-dns,pod-template-hash=7975d6fb9b
"
PS C:\Users\hi> kubectl get pod -o wide
NAME                     READY   STATUS    RESTARTS   AGE   IP           NODE                          NOMINATED NODE   READINESS GATES
flask-6f4c5ddcc4-dwf65   1/1    " Running "  0          67s   "10.0.15.83"   ip-10-0-15-175.ec2.internal   <none>           <none>
"
PS C:\Users\hi> kubectl get svc
NAME         TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)          AGE
flask       " LoadBalancer  " 172.20.146.240   "a35f11019984741bf969a51442417a35-403333476.us-east-1.elb.amazonaws.com "  5000:30574/TCP   77s
kubernetes   ClusterIP      172.20.0.1       <none>                                                                   443/TCP          9m3s
"
PS C:\Users\hi> kubectl describe "service "flask
Name:                     flask
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=flask
Type:                     LoadBalancer
IP Family Policy:         SingleStack
IP Families:              IPv4
"IP:                       172.20.146.240"
IPs:                      172.20.146.240
"LoadBalancer Ingress:"     "a35f11019984741bf969a51442417a35-403333476.us-east-1.elb.amazonaws.com"
Port:                     <unset>  5000/TCP
"TargetPort:   "            5000/TCP
"NodePort: "                <unset>  30574/TCP
Endpoints:                "10.0.15.83:5000"
Session Affinity:         None
External Traffic Policy:  Cluster
Events:
  Type    Reason                Age   From                Message
  ----    ------                ----  ----                -------
  Normal  EnsuringLoadBalancer  4m5s  service-controller  Ensuring load balancer
  Normal  EnsuredLoadBalancer   4m2s  service-controller  Ensured load balancer"

PS C:\Users\hi> kubectl describe pod flask
Name:             flask-6f4c5ddcc4-dwf65
Namespace:        default
Priority:         0
Service Account:  default
Node:             ip-10-0-15-175.ec2.internal/10.0.15.175
Start Time:       Sat, 25 Feb 2023 13:50:46 +0530
Labels:           app=flask
                  pod-template-hash=6f4c5ddcc4
Annotations:      <none>
Status:           Running
IP:              " 10.0.15.83"
IPs:
  IP:           10.0.15.83
Controlled By: {" ReplicaSet/flask-6f4c5ddcc4"}
Containers:
  flask:
    Container ID:   containerd://4ea588f28fdac8de08a01a38eb10ea1517e38ff944197f485479616ff188f72f
    Image:          docker.io/smugford/anotherflask:init
    Image ID:       docker.io/smugford/anotherflask@sha256:5ed030ee7fd2da3c553f11a1f04c44ced83f610ef2c14c46b4a7451da1516679
    Port:           5000/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Sat, 25 Feb 2023 13:51:05 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ftk7r (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-ftk7r:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  4m20s  default-scheduler  Successfully assigned default/flask-6f4c5ddcc4-dwf65 to ip-10-0-15-175.ec2.internal
  Normal  Pulling    4m19s  kubelet            Pulling image "docker.io/smugford/anotherflask:init"
  Normal  Pulled     4m1s   kubelet            Successfully pulled image "docker.io/smugford/anotherflask:init" in 17.731329159s (17.731349351s including waiting)
  Normal  Created    4m1s   kubelet            Created container flask
  Normal  Started    4m1s   kubelet            Started container flask"

PS C:\Users\hi> kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://2C2F2A628D9EA09D2CA972EB4E66FCE5.gr7.us-east-1.eks.amazonaws.com
  name: arn:aws:eks:us-east-1:130593926195:cluster/aaa
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://5751C70A20C0CABB43EA7AFC8A99C8FB.gr7.us-east-1.eks.amazonaws.com
  name: arn:aws:eks:us-east-1:473319165751:cluster/aaa
contexts:
- context:
    cluster: arn:aws:eks:us-east-1:130593926195:cluster/aaa
    user: arn:aws:eks:us-east-1:130593926195:cluster/aaa
  name: arn:aws:eks:us-east-1:130593926195:cluster/aaa
- context:
    cluster: arn:aws:eks:us-east-1:473319165751:cluster/aaa
    user: arn:aws:eks:us-east-1:473319165751:cluster/aaa
  name: arn:aws:eks:us-east-1:473319165751:cluster/aaa
current-context: arn:aws:eks:us-east-1:473319165751:cluster/aaa
kind: Config
preferences: {}
users:
- name: arn:aws:eks:us-east-1:130593926195:cluster/aaa
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - --region
      - us-east-1
      - eks
      - get-token
      - --cluster-name
      - aaa
      command: aws
      env: null
      interactiveMode: IfAvailable
      provideClusterInfo: false
- name: arn:aws:eks:us-east-1:473319165751:cluster/aaa
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - --region
      - us-east-1
      - eks
      - get-token
      - --cluster-name
      - aaa
      command: aws
      env: null
      interactiveMode: IfAvailable
      provideClusterInfo: false"

PS C:\Users\hi> kubectl get pods --all-namespaces
NAMESPACE     NAME                       READY   STATUS    RESTARTS   AGE
default       flask-6f4c5ddcc4-dwf65     1/1     Running   0          4m49s
kube-system   aws-node-9px8j             1/1     Running   0          3m55s
kube-system   coredns-7975d6fb9b-7r8pg   1/1     Running   0          12m
kube-system   coredns-7975d6fb9b-vxng9   1/1     Running   0          12m
kube-system   kube-proxy-b9cjg           1/1     Running   0          5m48s"

PS C:\Users\hi>  kubectl get "nodes" -o wide
NAME                          STATUS   ROLES    AGE     VERSION               INTERNAL-IP   EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME
ip-10-0-15-175.ec2.internal   Ready    <none>   5m58s   v1.25.6-eks-48e63af  " 10.0.15.175 "  "3.88.38.207"   Amazon Linux 2   5.10.165-143.735.amzn2.x86_64  " containerd://1.6.6"

hi@DESKTOP-LQI2IA5 $:  curl a35f11019984741bf969a51442417a35-403333476.us-east-1.elb.amazonaws.com:5000       "
   {"message":"Hello World","timestamp":1677313688}    ##  SUCCESS 
###############################################################################################3
  python  FLASK WITH HTML WEB APP IMAGE FROM DOCKER HUB  AND NODE PORT AND CONTAINER POD IP SUCCESS URL 

FROM python:3.8.6-slim

LABEL NAME=my-flask-app-image
LABEL VERSION=stable

ENV VERSION="STABLE"

WORKDIR /app

RUN pip install --upgrade pip
RUN pip install pipfile-requirements
COPY requirements.txt  .
RUN pip install -r requirements.txt

EXPOSE  5000
ENTRYPOINT ["python"]
CMD ["main.py"]

# COPY Pipfile* /app/
# RUN pipfile2req > requirements.txt \
#     && pip install -r requirements.txt

 COPY . /app/
# RUN chmod +x /app/scripts/*

# ENTRYPOINT [ "/app/scripts/run_app_local.sh" ]
++++++++++++++++++"
import os

from flask import Flask, render_template

app = Flask(__name__, template_folder='templates')
env = os.environ.get('VERSION', '--')


@app.route('/')
def root():
    return render_template(
        'index.html',
        message=f'You accessed the {env} version of the app.',
    )


if  __name__ == "__main__":
    app.run(host='0.0.0.0',debug=True)
++++++++++++++++++++=
REQUREMENTS  flask
++++++++++++++++++++++++++++++++"
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: flask-html
   labels:
      name: flask-html
spec:
  replicas: 4
  revisionHistoryLimit: 2
  minReadySeconds: 5
  progressDeadlineSeconds: 60
  selector: 
    matchLabels:
       app: flask-html
  strategy: 
     type: RollingUpdate
     rollingUpdate: 
        maxSurge: 2
        maxUnavailable: 1
  template:
     metadata:
        labels:
          app: flask-html         
     spec: 
       containers:
         - name: flask-html
           image: anjireddy3993/flask-html:1.0
           ports:
             - containerPort: 5000
---
apiVersion: v1
kind: Service
metadata: 
  name: flask-html
spec:
  selector: 
    app: flask-html
  ports:
    - port: 80
      targetPort: 5000
  type: NodePort        
+++++++++++++"

anji@anji:~$ kubectl get pod,pv,pvc,deployment,svc -o wide   "
NAME                              READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/flask-html-5b99f4f448-c2k2c   1/1     Running   0          74m  " 10.44.0.1 "  worker   <none>           <none>
pod/flask-html-5b99f4f448-f75sd   1/1     Running   0          22m   10.44.0.4   worker   <none>           <none>
pod/flask-html-5b99f4f448-frz2x   1/1     Running   0          74m   10.44.0.3   worker   <none>           <none>
pod/flask-html-5b99f4f448-wcfdf   1/1     Running   0          74m   10.44.0.2   worker   <none>           <none>

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                         SELECTOR
deployment.apps/flask-html   4/4     4            4           74m   flask-html   anjireddy3993/flask-html:1.0   app=flask-html

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE     SELECTOR
service/flask-html   NodePort   " 10.101.193.6  " <none>        80:30951/TCP   7m35s   app=flask-html
service/kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP        60m     <none>
service/nginx       " NodePort   " "10.97.88.78  "  <none>        80:30307/TCP   11m     app=flask-html

anji@anji:~$ kubectl describe service flask-html   "
Name:                     flask-html
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=flask-html
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                     "  10.101.193.6"
IPs:                      10.101.193.6
Port:                     <unset> " 80/TCP"
TargetPort:               5000/TCP
NodePort:                 <unset>  30951/TCP
Endpoints:                10.44.0.1:5000,10.44.0.2:5000,10.44.0.3:5000 + 1 more...
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
anji@anji:~$ 
+++++++++++++++++"
anji@anji:~$ kubectl describe deployment 
Name:                   flask-html
Namespace:              default
CreationTimestamp:      Mon, 27 Feb 2023 15:00:40 +0530
Labels:                 name=flask-html
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=flask-html
Replicas:               4 desired | 4 updated | 4 total | 4 available | 0 unavailable
StrategyType:           "RollingUpdate"
MinReadySeconds:        5
RollingUpdateStrategy:  1 max unavailable, 2 max surge
Pod Template:
  Labels:  app=flask-html
  Containers:
   flask-html:
    Image:        anjireddy3993/flask-html:1.0
    Port:         5000/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   flask-html-5b99f4f448 (4/4 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  29m   deployment-controller  Scaled up replica set flask-html-5b99f4f448 to 4 from 3"
###########################################################################################33
     mysql  MYSQL  DATABASE COMMANDS 
https://www.tecmint.com/fix-mysql-error-1819-hy000/


mysql> create user â€˜tecmintâ€™@â€™localhostâ€™ IDENTIFIED BY â€˜mypasswordâ€™;

       create user â€˜anjiâ€™@â€™localhostâ€™ IDENTIFIED BY â€˜adminâ€™;

mysql>  SHOW VARIABLES LIKE 'validate_password%';
+--------------------------------------+-------+
| Variable_name                        | Value |
+--------------------------------------+-------+
| validate_password.check_user_name    | ON    |
| validate_password.dictionary_file    |       |
| validate_password.length             | 8     |
| validate_password.mixed_case_count   | 1     |
| validate_password.number_count       | 1     |
| validate_password.policy             | LOW   |
| validate_password.special_char_count | 1     |
+--------------------------------------+-------+
7 rows in set (0.00 sec)

mysql> select plugin_name, plugin_status from information_schema.plugins where plugin_name like 'validate%';
Empty set (0.00 sec)

mysql> install plugin validate_password soname 'validate_password.so';
Query OK, 0 rows affected, 1 warning (0.01 sec)

mysql> select plugin_name, plugin_status from information_schema.plugins where plugin_name like 'validate%';
+-------------------+---------------+
| plugin_name       | plugin_status |
+-------------------+---------------+
| validate_password | ACTIVE        |
+-------------------+---------------+
1 row in set (0.00 sec)

mysql> SET GLOBAL validate_password_policy=LOW;
Query OK, 0 rows affected (0.00 sec)

mysql> SET GLOBAL validate_password_policy=0;
Query OK, 0 rows affected (0.00 sec)

mysql> SHOW VARIABLES LIKE 'validate_password%';
+--------------------------------------+-------+
| Variable_name                        | Value |
+--------------------------------------+-------+
| validate_password.check_user_name    | ON    |
| validate_password.dictionary_file    |       |
| validate_password.length             | 8     |
| validate_password.mixed_case_count   | 1     |
| validate_password.number_count       | 1     |
| validate_password.policy             | LOW   |
| validate_password.special_char_count | 1     |
| validate_password_check_user_name    | ON    |
| validate_password_dictionary_file    |       |
| validate_password_length             | 8     |
| validate_password_mixed_case_count   | 1     |
| validate_password_number_count       | 1     |
| validate_password_policy             | LOW   |
| validate_password_special_char_count | 1     |
+--------------------------------------+-------+
14 rows in set (0.00 sec)

mysql> create user â€˜tecmintâ€™@â€™localhostâ€™ IDENTIFIED BY â€˜mypasswordâ€™;
create user â€˜anjiâ€™@â€™localhostâ€™ IDENTIFIED BY â€˜adminab1â€™;


mysql> SET GLOBAL validate_password.length = 5;
Query OK, 0 rows affected (0.00 sec)

mysql> SET GLOBAL validate_password.number_count = 0;
Query OK, 0 rows affected (0.00 sec)

mysql> SET GLOBAL validate_password.policy=LOW;
Query OK, 0 rows affected (0.00 sec)

mysql> SHOW VARIABLES LIKE 'validate_password%';
+--------------------------------------+-------+
| Variable_name                        | Value |
+--------------------------------------+-------+
| validate_password.check_user_name    | ON    |
| validate_password.dictionary_file    |       |
| validate_password.length             | 5     |
| validate_password.mixed_case_count   | 1     |
| validate_password.number_count       | 0     |
| validate_password.policy             | LOW   |
| validate_password.special_char_count | 1     |
| validate_password_check_user_name    | ON    |
| validate_password_dictionary_file    |       |
| validate_password_length             | 8     |
| validate_password_mixed_case_count   | 1     |
| validate_password_number_count       | 1     |
| validate_password_policy             | LOW   |
| validate_password_special_char_count | 1     |
+--------------------------------------+-------+
14 rows in set (0.01 sec)
https://stackoverflow.com/questions/43094726/your-password-does-not-satisfy-the-current-policy-requirements



SET GLOBAL validate_password.length = 5;
SET GLOBAL validate_password.number_count = 0;
SET GLOBAL     validate_password.mixed_case_count = 0;

SET GLOBAL validate_password.policy=LOW;

set global validate_password.special_char_count = 0;

SHOW VARIABLES LIKE 'validate_password%';

mysql> SHOW VARIABLES LIKE 'validate_password%';
+--------------------------------------+-------+
| Variable_name                        | Value |
+--------------------------------------+-------+
| validate_password.check_user_name    | ON    |
| validate_password.dictionary_file    |       |
| validate_password.length             | 5     |
| validate_password.mixed_case_count   | 0     |
| validate_password.number_count       | 0     |
| validate_password.policy             | LOW   |
| validate_password.special_char_count | 0     |
| validate_password_check_user_name    | ON    |
| validate_password_dictionary_file    |       |
| validate_password_length             | 8     |
| validate_password_mixed_case_count   | 1     |
| validate_password_number_count       | 1     |
| validate_password_policy             | LOW   |
| validate_password_special_char_count | 1     |
+--------------------------------------+-------+
14 rows in set (0.00 sec)



If you trying to set a blank password. Then running the following query in MySQL client:

SET GLOBAL validate_password.check_user_name = No;
SET GLOBAL validate_password.dictionary_file = '';
SET GLOBAL validate_password.length = 0;
SET GLOBAL validate_password.mixed_case_count = 0;
SET GLOBAL validate_password.number_count = 0;
SET GLOBAL validate_password.policy = LOW;
SET GLOBAL validate_password.special_char_count = 0;


####################################################################333


ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'password';
ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'admin';

mysql> ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'admin';
ERROR 1819 (HY000): Your password does not satisfy the current policy requirements


In MySQL, there is 3 levels password validation policy that checks the password strength. Different levels of password validation policy are:

    LOW: This rule allows users to create a weak password and that is a password of 8 or fewer characters.
    MEDIUM: This rule allows users to create passwords of 8 or fewer characters with mixed cases and special characters.
    STRONG: This rule allows users to create strong passwords with the combination of the dictionary files.


mysql>  SHOW VARIABLES LIKE 'validate_password%';
+--------------------------------------+--------+
| Variable_name                        | Value  |
+--------------------------------------+--------+
| validate_password.check_user_name    | ON     |
| validate_password.dictionary_file    |        |
| validate_password.length             | 8      |
| validate_password.mixed_case_count   | 1      |
| validate_password.number_count       | 1      |
| validate_password.policy             | MEDIUM |
| validate_password.special_char_count | 1      |
| validate_password_check_user_name    | ON     |
| validate_password_dictionary_file    |        |
| validate_password_length             | 8      |
| validate_password_mixed_case_count   | 1      |
| validate_password_number_count       | 1      |
| validate_password_policy             | MEDIUM |
| validate_password_special_char_count | 1      |
+--------------------------------------+--------+
14 rows in set (0.00 sec)
https://itsubuntu.com/fix-error-1819-hy000-your-password-does-not-satisfy/


mysql> select plugin_name, plugin_status from information_schema.plugins where plugin_name like 'validate%';
+-------------------+---------------+
| plugin_name       | plugin_status |
+-------------------+---------------+
| validate_password | ACTIVE        |
+-------------------+---------------+
1 row in set (0.00 sec)


mysql> install plugin validate_password soname 'validate_password.so';
ERROR 1125 (HY000): Function 'validate_password' already exists
mysql> 

mysql> select plugin_name, plugin_status from information_schema.plugins where plugin_name like 'validate%';
+-------------------+---------------+
| plugin_name       | plugin_status |
+-------------------+---------------+
| validate_password | ACTIVE        |
+-------------------+---------------+
1 row in set (0.00 sec)

mysql> set global validate_password_policy= LOW;
mysql> SHOW VARIABLES LIKE 'validate_password%';
+--------------------------------------+--------+
| Variable_name                        | Value  |
+--------------------------------------+--------+
| validate_password.check_user_name    | ON     |
| validate_password.dictionary_file    |        |
| validate_password.length             | 8      |
| validate_password.mixed_case_count   | 1      |
| validate_password.number_count       | 1      |
| validate_password.policy             | MEDIUM |
| validate_password.special_char_count | 1      |
| validate_password_check_user_name    | ON     |
| validate_password_dictionary_file    |        |
| validate_password_length             | 8      |
| validate_password_mixed_case_count   | 1      |
| validate_password_number_count       | 1      |
| validate_password_policy             | LOW    |
| validate_password_special_char_count | 1      |
+--------------------------------------+--------+
14 rows in set (0.00 sec)

mysql> set global validate_password_policy=0;
Query OK, 0 rows affected (0.00 sec)

mysql> show variables like 'validate_password%';
+--------------------------------------+--------+
| Variable_name                        | Value  |
+--------------------------------------+--------+
| validate_password.check_user_name    | ON     |
| validate_password.dictionary_file    |        |
| validate_password.length             | 8      |
| validate_password.mixed_case_count   | 1      |
| validate_password.number_count       | 1      |
| validate_password.policy             | MEDIUM |
| validate_password.special_char_count | 1      |
| validate_password_check_user_name    | ON     |
| validate_password_dictionary_file    |        |
| validate_password_length             | 8      |
| validate_password_mixed_case_count   | 1      |
| validate_password_number_count       | 1      |
| validate_password_policy             | LOW    |
| validate_password_special_char_count | 1      |
+--------------------------------------+--------+
14 rows in set (0.00 sec)


create user â€˜anjiâ€™@â€™localhostâ€™ IDENTIFIED BY â€˜adminâ€™;

create user â€˜anjiâ€™@â€™localhostâ€™ IDENTIFIED BY â€˜anji@123â€™;

https://dev.mysql.com/doc/refman/5.7/en/validate-password-options-variables.html

https://www.digitalocean.com/community/questions/sudo-mysql-create-db-and-user-sql-syntax-error-1064-42000-at-line-1

CREATE DATABASE anji; 

mysql> CREATE DATABASE anji; 
Query OK, 1 row affected (0.01 sec)

CREATE USER 'anji'@'localhost' IDENTIFIED WITH mysql_native_password BY 'admin@123'; 

 SET GLOBAL     validate_password_mixed_case_count = 0;

https://hoststud.com/resources/resolved-mysql-error-your-password-does-not-satisfy-the-current-policy-requirements.464/
mysql> UNINSTALL PLUGIN validate_password;
Query OK, 0 rows affected, 1 warning (0.02 sec)

mysql>  show variables like 'validate_password%';
+--------------------------------------+--------+
| Variable_name                        | Value  |
+--------------------------------------+--------+
| validate_password.check_user_name    | ON     |
| validate_password.dictionary_file    |        |
| validate_password.length             | 8      |
| validate_password.mixed_case_count   | 1      |
| validate_password.number_count       | 1      |
| validate_password.policy             | MEDIUM |
| validate_password.special_char_count | 1      |
+--------------------------------------+--------+
7 rows in set (0.00 sec)

##########################################################
https://micheleberardi.medium.com/error-1819-hy000-your-password-does-not-satisfy-the-current-policy-requirements-102043b67039

create user 'demo'@'localhost' identified by 'admin';

mysql> SHOW VARIABLES LIKE 'validate_password%';
+--------------------------------------+--------+
| Variable_name                        | Value  |
+--------------------------------------+--------+
| validate_password.check_user_name    | ON     |
| validate_password.dictionary_file    |        |
| validate_password.length             | 8      |
| validate_password.mixed_case_count   | 1      |
| validate_password.number_count       | 1      |
| validate_password.policy             | MEDIUM |
| validate_password.special_char_count | 1      |
+--------------------------------------+--------+
7 rows in set (0.00 sec)

SET GLOBAL validate_password_length = 5;
SET GLOBAL validate_password_number_count = 0;


root@anji:~# mysql -h localhost -u root -p
Enter password:      ## just  enter ok 

mysql> uninstall plugin validate_password;
ERROR 1305 (42000): PLUGIN validate_password does not exist
mysql> 

mysql>  SET GLOBAL validate_password_policy=LOW;
ERROR 1193 (HY000): Unknown system variable 'validate_password_policy'
mysql> 

=====
ERROR 1193 (HY000): Unknown system variable 'validate_password_length'

https://stackoverflow.com/questions/55237257/mysql-validate-password-policy-unknown-system-variable


mysql>  select plugin_name, plugin_status from information_schema.plugins where plugin_name like 'validate%'; 
Empty set (0.00 sec)


mysql> install plugin validate_password soname 'validate_password.so';
Query OK, 0 rows affected, 1 warning (0.01 sec)

mysql> select plugin_name, plugin_status from information_schema.plugins where plugin_name like 'validate%';
+-------------------+---------------+
| plugin_name       | plugin_status |
+-------------------+---------------+
| validate_password | ACTIVE        |
+-------------------+---------------+
1 row in set (0.00 sec)

mysql> SHOW VARIABLES LIKE 'validate_password%';
+--------------------------------------+--------+
| Variable_name                        | Value  |
+--------------------------------------+--------+
| validate_password.check_user_name    | ON     |
| validate_password.dictionary_file    |        |
| validate_password.length             | 8      |
| validate_password.mixed_case_count   | 1      |
| validate_password.number_count       | 1      |
| validate_password.policy             | MEDIUM |
| validate_password.special_char_count | 1      |
| validate_password_check_user_name    | ON     |
| validate_password_dictionary_file    |        |
| validate_password_length             | 8      |
| validate_password_mixed_case_count   | 1      |
| validate_password_number_count       | 1      |
| validate_password_policy             | MEDIUM |
| validate_password_special_char_count | 1      |
+--------------------------------------+--------+
14 rows in set (0.00 sec)


mysql> SHOW VARIABLES LIKE 'validate_password%';
+--------------------------------------+--------+
| Variable_name                        | Value  |
+--------------------------------------+--------+
| validate_password.check_user_name    | ON     |
| validate_password.dictionary_file    |        |
| validate_password.length             | 8      |
| validate_password.mixed_case_count   | 1      |
| validate_password.number_count       | 1      |
| validate_password.policy             | LOW    |
| validate_password.special_char_count | 1      |
| validate_password_check_user_name    | ON     |
| validate_password_dictionary_file    |        |
| validate_password_length             | 8      |
| validate_password_mixed_case_count   | 1      |
| validate_password_number_count       | 1      |
| validate_password_policy             | MEDIUM |
| validate_password_special_char_count | 1      |
+--------------------------------------+--------+
14 rows in set (0.00 sec)
-------
https://www.digitalocean.com/community/tutorials/how-to-install-mysql-on-ubuntu-20-04

mysql> ALTER USER 'root'@'localhost' IDENTIFIED WITH auth_socket;
Query OK, 0 rows affected (0.00 sec)


mysql> CREATE USER 'username'@'host' IDENTIFIED WITH authentication_plugin BY 'password';
ERROR 1524 (HY000): Plugin 'authentication_plugin' is not loaded
mysql> CREATE USER 'sammy'@'localhost' IDENTIFIED WITH mysql_native_password BY 'password';
Query OK, 0 rows affected (0.01 sec)


mysql> ALTER USER 'sammy'@'localhost' IDENTIFIED WITH mysql_native_password BY 'password';
Query OK, 0 rows affected (0.00 sec)


mysql> GRANT PRIVILEGE ON database.table TO 'username'@'host';
ERROR 3619 (HY000): Illegal privilege level specified for table
mysql> 

mysql> GRANT CREATE, ALTER, DROP, INSERT, UPDATE, INDEX, DELETE, SELECT, REFERENCES, RELOAD on *.* TO 'sammy'@'localhost' WITH GRANT OPTION;
Query OK, 0 rows affected (0.00 sec)

mysql> 

mysql> GRANT ALL PRIVILEGES ON *.* TO 'sammy'@'localhost' WITH GRANT OPTION;
Query OK, 0 rows affected (0.00 sec)

mysql> 

mysql> FLUSH PRIVILEGES;
Query OK, 0 rows affected (0.00 sec)


root@anji:~# mysql -u sammy -p
Enter password:   password          ==== success

anji@anji:~$ sudo mysqladmin -p -u sammy version
[sudo] password for anji: 
Enter password:   password
mysqladmin  Ver 8.0.32-0ubuntu0.20.04.2 for Linux on x86_64 ((Ubuntu))
Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Server version		8.0.32-0ubuntu0.20.04.2
Protocol version	10
Connection		Localhost via UNIX socket
UNIX socket		/var/run/mysqld/mysqld.sock
Uptime:			2 hours 11 min 12 sec

Threads: 4  Questions: 68  Slow queries: 0  Opens: 220  Flush tables: 3  Open tables: 139  Queries per second avg: 0.008 "

CREATE USER 'new_user'@'localhost' IDENTIFIED BY 'new_user_password';
CREATE USER 'india'@'localhost' IDENTIFIED BY 'indihhhhhhha';

create user 'cpu'@'localhost' identified  by 'asdfghjk';

mysql> create user 'cpu'@'localhost' identified  by 'asdfghjk';
Query OK, 0 rows affected (0.01 sec)

create user 'ssd'@localhost identified  by 'qwertyuio';

mysql> create user 'ssd'@localhost identified  by 'qwertyuio';
Query OK, 0 rows affected (0.00 sec)


create user 'ram'@localhost identified  by 'qwertyuiolakdlasjdajgfhpiafufsibdfsvibdfsvibdsijvbdsijvbdsb';

mysql> create user 'ram'@localhost identified  by 'qwertyuiolakdlasjdajgfhpiafufsibdfsvibdfsvibdsijvbdsijvbdsb';
Query OK, 0 rows affected (0.01 sec)

mysql> 

create user  'anjireddy'@localhost  identified  by 'anjireddy' ;

mysql> create user  'anjireddy'@localhost  identified  by 'anjireddy' ;
Query OK, 0 rows affected (0.00 sec)

anji@anji:~$ mysql -u anjireddy -p 
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 26
Server version: 8.0.32-0ubuntu0.20.04.2 (Ubuntu)

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> 

create user 'anji'@'localhost' identified by 'anjianji'

anji@anji:~$ mysql  -u anji -p 
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 30
Server version: 8.0.32-0ubuntu0.20.04.2 (Ubuntu)

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> 


==========================
https://www.strongdm.com/blog/mysql-create-user-manage-access-privileges-how-to

anji@anji:~$ mysql  -u anji -p 
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 30
Server version: 8.0.32-0ubuntu0.20.04.2 (Ubuntu)

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

CREATE DATABASE IF NOT EXISTS anji;

mysql> CREATE DATABASE  anji;
ERROR 1044 (42000): Access denied for user 'anji'@'localhost' to database 'anji'
mysql> 

mysql> grant all privileges on *.* to 'anji'@'localhost';

mysql> grant all privileges on `database_name`.`table_name` to 'anji'@'hostname';


mysql> grant all privileges on *.* to 'anji'@'localhost';
Query OK, 0 rows affected (0.01 sec)

mysql> create database foodball;
Query OK, 1 row affected (0.00 sec)

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| anji               |
| foodball           |
| information_schema |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
6 rows in set (0.00 sec)

mysql> use mysql
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> 


SELECT host,user,Grant_priv,Super_priv FROM mysql.user;

mysql> SELECT host,user,Grant_priv,Super_priv FROM mysql.user;
+-----------+------------------+------------+------------+
| host      | user             | Grant_priv | Super_priv |
+-----------+------------------+------------+------------+
| localhost | anji             | N          | Y          |
| localhost | anjireddy        | N          | N          |
| localhost | cpu              | N          | N          |
| localhost | debian-sys-maint | Y          | Y          |
| localhost | india            | N          | N          |
| localhost | mysql.infoschema | N          | N          |
| localhost | mysql.session    | N          | Y          |
| localhost | mysql.sys        | N          | N          |
| localhost | ram              | N          | N          |
| localhost | root             | Y          | Y          |
| localhost | sammy            | Y          | Y          |
| localhost | ssd              | N          | N          |
+-----------+------------------+------------+------------+
12 rows in set (0.00 sec)

 UPDATE mysql.user SET Grant_priv=â€˜Yâ€˜, Super_priv=â€˜Yâ€˜ ;

 mysql>  FLUSH PRIVILEGES;
Query OK, 0 rows affected (0.00 sec)

mysql> select user ();
+----------------+
| user ()        |
+----------------+
| root@localhost |
+----------------+
1 row in set (0.00 sec)

mysql> 

mysql> select user ();
+----------------+
| user ()        |
+----------------+
| anji@localhost |
+----------------+
1 row in set (0.00 sec)

mysql> select current_user ();
+-----------------+
| current_user () |
+-----------------+
| root@localhost  |
+-----------------+
1 row in set (0.00 sec)

mysql> select current_user ();
+-----------------+
| current_user () |
+-----------------+
| anji@localhost  |
+-----------------+
1 row in set (0.00 sec)



mysql> show grants;
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Grants for root@localhost                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, RELOAD, SHUTDOWN, PROCESS, FILE, REFERENCES, INDEX, ALTER, SHOW DATABASES, SUPER, CREATE TEMPORARY TABLES, LOCK TABLES, EXECUTE, REPLICATION SLAVE, REPLICATION CLIENT, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, CREATE USER, EVENT, TRIGGER, CREATE TABLESPACE, CREATE ROLE, DROP ROLE ON *.* TO `root`@`localhost` WITH GRANT OPTION                                                                                                                                                                                                                                                                                                                                                                 |
| GRANT APPLICATION_PASSWORD_ADMIN,AUDIT_ABORT_EXEMPT,AUDIT_ADMIN,AUTHENTICATION_POLICY_ADMIN,BACKUP_ADMIN,BINLOG_ADMIN,BINLOG_ENCRYPTION_ADMIN,CLONE_ADMIN,CONNECTION_ADMIN,ENCRYPTION_KEY_ADMIN,FIREWALL_EXEMPT,FLUSH_OPTIMIZER_COSTS,FLUSH_STATUS,FLUSH_TABLES,FLUSH_USER_RESOURCES,GROUP_REPLICATION_ADMIN,GROUP_REPLICATION_STREAM,INNODB_REDO_LOG_ARCHIVE,INNODB_REDO_LOG_ENABLE,PASSWORDLESS_USER_ADMIN,PERSIST_RO_VARIABLES_ADMIN,REPLICATION_APPLIER,REPLICATION_SLAVE_ADMIN,RESOURCE_GROUP_ADMIN,RESOURCE_GROUP_USER,ROLE_ADMIN,SENSITIVE_VARIABLES_OBSERVER,SERVICE_CONNECTION_ADMIN,SESSION_VARIABLES_ADMIN,SET_USER_ID,SHOW_ROUTINE,SYSTEM_USER,SYSTEM_VARIABLES_ADMIN,TABLE_ENCRYPTION_ADMIN,XA_RECOVER_ADMIN ON *.* TO `root`@`localhost` WITH GRANT OPTION |
| GRANT PROXY ON ``@`` TO `root`@`localhost` WITH GRANT OPTION                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
3 rows in set (0.00 sec)


mysql> show grants;
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Grants for anji@localhost                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, RELOAD, SHUTDOWN, PROCESS, FILE, REFERENCES, INDEX, ALTER, SHOW DATABASES, SUPER, CREATE TEMPORARY TABLES, LOCK TABLES, EXECUTE, REPLICATION SLAVE, REPLICATION CLIENT, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, CREATE USER, EVENT, TRIGGER, CREATE TABLESPACE, CREATE ROLE, DROP ROLE ON *.* TO `anji`@`localhost`                                                                                                                                                                                                                                                                                                                                                                 |
| GRANT APPLICATION_PASSWORD_ADMIN,AUDIT_ABORT_EXEMPT,AUDIT_ADMIN,AUTHENTICATION_POLICY_ADMIN,BACKUP_ADMIN,BINLOG_ADMIN,BINLOG_ENCRYPTION_ADMIN,CLONE_ADMIN,CONNECTION_ADMIN,ENCRYPTION_KEY_ADMIN,FIREWALL_EXEMPT,FLUSH_OPTIMIZER_COSTS,FLUSH_STATUS,FLUSH_TABLES,FLUSH_USER_RESOURCES,GROUP_REPLICATION_ADMIN,GROUP_REPLICATION_STREAM,INNODB_REDO_LOG_ARCHIVE,INNODB_REDO_LOG_ENABLE,PASSWORDLESS_USER_ADMIN,PERSIST_RO_VARIABLES_ADMIN,REPLICATION_APPLIER,REPLICATION_SLAVE_ADMIN,RESOURCE_GROUP_ADMIN,RESOURCE_GROUP_USER,ROLE_ADMIN,SENSITIVE_VARIABLES_OBSERVER,SERVICE_CONNECTION_ADMIN,SESSION_VARIABLES_ADMIN,SET_USER_ID,SHOW_ROUTINE,SYSTEM_USER,SYSTEM_VARIABLES_ADMIN,TABLE_ENCRYPTION_ADMIN,XA_RECOVER_ADMIN ON *.* TO `anji`@`localhost` |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
2 rows in set (0.00 sec)

RENAME USER 'root'@'localhost' TO 'apache'@'127.0.0.1';


mysql>  SELECT * FROM mysql.user;
+-----------+------------------+-------------+-------------+-------------+-------------+-------------+-----------+-------------+---------------+--------------+-----------+------------+-----------------+------------+------------+--------------+------------+-----------------------+------------------+--------------+-----------------+------------------+------------------+----------------+---------------------+--------------------+------------------+------------+--------------+------------------------+----------+------------------------+--------------------------+----------------------------+---------------+-------------+-----------------+----------------------+-----------------------+------------------------------------------------------------------------+------------------+-----------------------+-------------------+----------------+------------------+----------------+------------------------+---------------------+--------------------------+-----------------+
| Host      | User             | Select_priv | Insert_priv | Update_priv | Delete_priv | Create_priv | Drop_priv | Reload_priv | Shutdown_priv | Process_priv | File_priv | Grant_priv | References_priv | Index_priv | Alter_priv | Show_db_priv | Super_priv | Create_tmp_table_priv | Lock_tables_priv | Execute_priv | Repl_slave_priv | Repl_client_priv | Create_view_priv | Show_view_priv | Create_routine_priv | Alter_routine_priv | Create_user_priv | Event_priv | Trigger_priv | Create_tablespace_priv | ssl_type | ssl_cipher             | x509_issuer              | x509_subject               | max_questions | max_updates | max_connections | max_user_connections | plugin                | authentication_string                                                  | password_expired | password_last_changed | password_lifetime | account_locked | Create_role_priv | Drop_role_priv | Password_reuse_history | Password_reuse_time | Password_require_current | User_attributes |
+-----------+------------------+-------------+-------------+-------------+-------------+-------------+-----------+-------------+---------------+--------------+-----------+------------+-----------------+------------+------------+--------------+------------+-----------------------+------------------+--------------+-----------------+------------------+------------------+----------------+---------------------+--------------------+------------------+------------+--------------+------------------------+----------+------------------------+--------------------------+----------------------------+---------------+-------------+-----------------+----------------------+-----------------------+------------------------------------------------------------------------+------------------+-----------------------+-------------------+----------------+------------------+----------------+------------------------+---------------------+--------------------------+-----------------+
| localhost | anji             | Y           | Y           | Y           | Y           | Y           | Y         | Y           | Y             | Y            | Y         | N          | Y               | Y          | Y          | Y            | Y          | Y                     | Y                | Y            | Y               | Y                | Y                | Y              | Y                   | Y                  | Y                | Y          | Y            | Y                      |          | 0x                     | 0x                       | 0x                         |             0 |           0 |               0 |                    0 | caching_sha2_password | $A$005$MmXv_xkE7iP\*y1;HKeHQ1k42mMmaW0zGwMKxk.z7IUFVySNWaBZSQgXtm3 | N                | 2023-03-01 17:22:24   |              NULL | N              | Y                | Y              |                   NULL |                NULL | NULL                     | NULL            |
| localhost | anjireddy        | N           | N           | N           | N           | N           | N         | N           | N             | N            | N         | N          | N               | N          | N          | N            | N          | N                     | N                | N            | N               | N                | N                | N              | N                   | N                  | N     7Tz20roDfx2vH33Rdt1URqNBY30DyBVjpprlPd.lCO8Sn9 | N                | 2023-03-01 17:18:21   |              NULL | N              | N                | N              |                   NULL |                NULL | NULL                     | NULL            |005$;ufT;3wbN0A
| localhost | cpu              | N           | N           | N           | N           | N           | N         | N           | N             | N            | N         | N          | N               | N          | N          | N            | N          | N                     | N                | N            | N               | N                | N                | N              | N                   | N                  | N                | N          | N            | N                      |          | 0x                     | 0x                       | 0x                         |             0 |           0 |               0 |                    0 | caching_sha2_password | $A$005$-P
                                                                                                                                                                                                                                                                      .(
                                                                                                                                                                                                                                                                        5;lEU6uYo/D.hOa.DceuDtPQoCc2PIDcHsd/0jNKbxXqe4PBJElJjF5 | N                | 2023-03-01 17:03:27   |              NULL | N              | N                | N              |                   NULL |                NULL | NULL                     | NULL            |
| localhost | debian-sys-maint | Y           | Y           | Y           | Y           | Y           | Y         | Y           | Y             | Y            | Y         | Y          | Y               | Y          | Y          | Y            | Y          | Y                     | Y                | Y            | Y               | Y                | Y                | Y              | Y                   | Y                  | Y                | Y          | Y            | Y                      |          | 0x                     | 0x                       | 0x                         |             0 |           0 |               0 |                    0 | caching_sha2_password | $A$005$)Jk{+TXhn&1}.uSk6sOZc1FkXvvDESrLBWoxDDoFo/0/7sXeqjWCQ4tZhm/ | N                | 2023-02-26 18:31:20   |              NULL | N              | Y                | Y              |                   NULL |                NULL | NULL                     | NULL            |
| localhost | india            | N           | N           | N           | N           | N           | N         | N           | N             | N            | N         | N          | N               | N          | N          | N            | N          | N                     | N                | N            | N               | N                | N                | N              | N                   | N                  | N                | N          | N            | N                      |          | 0x                     | 0x                       | 0x                         |             0 |           0 |               0 |                    0 | caching_sha2_password | $A$005$m&[,et4pTPfH[	narHbhfaMO8qlzkLkibvZsgCTA4bdWOi6ZHq7hF9cI. | N                | 2023-03-01 16:59:14   |              NULL | N              | N                | N              |                   NULL |                NULL | NULL                     | NULL            |
| localhost | mysql.infoschema | Y           | N           | N           | N           | N           | N         | N           | N             | N            | N         | N          | N               | N          | N          | N            | N          | N                     | N                | N            | N               | N                | N                | N              | N                   | N                  | N                | N          | N            | N                      |          | 0x                     | 0x                       | 0x                         |             0 |           0 |               0 |                    0 | caching_sha2_password | $A$005$THISISACOMBINATIONOFINVALIDSALTANDPASSWORDTHATMUSTNEVERBRBEUSED | N                | 2023-02-26 18:31:19   |              NULL | Y              | N                | N              |                   NULL |                NULL | NULL                     | NULL            |
| localhost | mysql.session    | N           | N           | N           | N           | N           | N         | N           | Y             | N            | N         | N          | N               | N          | N          | N            | Y          | N                     | N                | N            | N               | N                | N                | N              | N                   | N                  | N                | N          | N            | N                      |          | 0x                     | 0x                       | 0x                         |             0 |           0 |               0 |                    0 | caching_sha2_password | $A$005$THISISACOMBINATIONOFINVALIDSALTANDPASSWORDTHATMUSTNEVERBRBEUSED | N                | 2023-02-26 18:31:19   |              NULL | Y              | N                | N              |                   NULL |                NULL | NULL                     | NULL            |
| localhost | mysql.sys        | N           | N           | N           | N           | N           | N         | N           | N             | N            | N         | N          | N               | N          | N          | N            | N          | N                     | N                | N            | N               | N                | N                | N              | N                   | N                  | N                | N          | N            | N                      |          | 0x                     | 0x                       | 0x                         |             0 |           0 |               0 |                    0 | caching_sha2_password | $A$005$THISISACOMBINATIONOFINVALIDSALTANDPASSWORDTHATMUSTNEVERBRBEUSED | N                | 2023-02-26 18:31:19   |              NULL | Y              | N                | N              |                   NULL |                NULL | NULL                     | NULL            |
| localhost | ram              | N           | N           | N           | N           | N           | N         | N           | N             | N            | N         | N          | N               | N          | N          | N            | N          | N                     | N                | N            | N               | N                | N                | N              | N                   | N                  | N                | N          | N            | N                      |          | 0x                     | 0x                       | 0x                         |             0 |           0 |               0 |                    0 | caching_sha2_password | $A$005$=[8?!\2+WBG`5Eh1wum0Kbe7.Bzc6iEj6gbwsP.gNbQoHG3IAwFIBR6.CZtC | N                | 2023-03-01 17:15:10   |              NULL | N              | N                | N              |                   NULL |                NULL | NULL                     | NULL            |
| localhost | root             | Y           | Y           | Y           | Y           | Y           | Y         | Y           | Y             | Y            | Y         | Y          | Y               | Y          | Y          | Y            | Y          | Y                     | Y                | Y            | Y               | Y                | Y                | Y              | Y                   | Y                  | Y                | Y          | Y            | Y                      |          | 0x                     | 0x                       | 0x                         |             0 |           0 |               0 |                    0 | auth_socket           |                                                                        | N                | 2023-02-26 18:31:19   |              NULL | N              | Y                | Y              |                   NULL |                NULL | NULL                     | NULL            |
| localhost | sammy            | Y           | Y           | Y           | Y           | Y           | Y         | Y           | Y             | Y            | Y         | Y          | Y               | Y          | Y          | Y            | Y          | Y                     | Y                | Y            | Y               | Y                | Y                | Y              | Y                   | Y                  | Y                | Y          | Y            | Y                      |          | 0x                     | 0x                       | 0x                         |             0 |           0 |               0 |                    0 | mysql_native_password | *2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19                              | N                | 2023-03-01 16:23:35   |              NULL | N              | Y                | Y              |                   NULL |                NULL | NULL                     | NULL            |
| localhost | ssd              | N           | N           | N           | N           | N           | N         | N           | N             | N            | N         | N          | N               | N          | N          | N            | N          | N                     | N                | N            | N               | N                | N                | N              | N                   | N                  | N     CJ&_ImyYRfN84pvrBhhszUQAQx6cmF1pHAf0tvOq56BmrD35 | N                | 2023-03-01 17:11:14   |              NULL | N              | N                | N              |                   NULL |                NULL | NULL                     | NULL            |5$K+Le.Ui~@iIE
+-----------+------------------+-------------+-------------+-------------+-------------+-------------+-----------+-------------+---------------+--------------+-----------+------------+-----------------+------------+------------+--------------+------------+-----------------------+------------------+--------------+-----------------+------------------+------------------+----------------+---------------------+--------------------+------------------+------------+--------------+------------------------+----------+------------------------+--------------------------+----------------------------+---------------+-------------+-----------------+----------------------+-----------------------+------------------------------------------------------------------------+------------------+-----------------------+-------------------+----------------+------------------+----------------+------------------------+---------------------+--------------------------+-----------------+
12 rows in set (0.00 sec)

mysql> 

mysql>  SELECT * FROM mysql.user;
ERROR 1142 (42000): SELECT command denied to user 'anji'@'localhost' for table 'user'
mysql> 

Some common privileges include:

    `ALL PRIVILEGES`: The user is granted all privileges except GRANT OPTION and PROXY.
    `ALTER`: The user can change the structure of a table or database.
    `CREATE`: The user can create new databases and tables.
    `DELETE`: The user can delete rows in a table.
    `INSERT`: The user can add rows to a table.
    `SELECT`: The user can read rows from a table.
    `UPDATE`: The user can update rows in a table.

 GRANT ALL ON *.* TO 'anji'@'localhost' WITH GRANT OPTION;

mysql>  GRANT ALL ON *.* TO 'anji'@'localhost' WITH GRANT OPTION;
Query OK, 0 rows affected (0.00 sec) 

mysql> SELECT host,user,Grant_priv,Super_priv FROM mysql.user;
+-----------+------------------+------------+------------+
| host      | user             | Grant_priv | Super_priv |
+-----------+------------------+------------+------------+
"| localhost | anji             | Y          | Y    "      |
| localhost | anjireddy        | N          | N          |
| localhost | cpu              | N          | N          |
| localhost | debian-sys-maint | Y          | Y          |
| localhost | india            | N          | N          |
| localhost | mysql.infoschema | N          | N          |
| localhost | mysql.session    | N          | Y          |
| localhost | mysql.sys        | N          | N          |
| localhost | ram              | N          | N          |
| localhost | root             | Y          | Y          |
| localhost | sammy            | Y          | Y          |
| localhost | ssd              | N          | N          |
+-----------+------------------+------------+------------+
12 rows in set (0.00 sec)

REVOKE SELECT, INSERT ON strongdm.* FROM â€˜local_userâ€™@â€™localhostâ€™;

mysql>  SELECT version();
+-------------------------+
| version()               |
+-------------------------+
| 8.0.32-0ubuntu0.20.04.2 |
+-------------------------+
1 row in set (0.00 sec)

mysql> ALTER USER 'local_user'@'localhost' IDENTIFIED BY 'new_password';

For older versions of MySQL, use this command instead:
mysql>SET PASSWORD FOR 'local_user'@'localhost' = PASSWORD('new_password');

Delete MySQL users

To delete a MySQL user, use the DROP command:
DROP USER 'local_user'@'localhost';
 SHOW GRANTS FOR 'anji'@'localhost';

mysql>  SHOW GRANTS FOR 'anji'@'localhost';
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Grants for anji@localhost                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, RELOAD, SHUTDOWN, PROCESS, FILE, REFERENCES, INDEX, ALTER, SHOW DATABASES, SUPER, CREATE TEMPORARY TABLES, LOCK TABLES, EXECUTE, REPLICATION SLAVE, REPLICATION CLIENT, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, CREATE USER, EVENT, TRIGGER, CREATE TABLESPACE, CREATE ROLE, DROP ROLE ON *.* TO `anji`@`localhost` WITH GRANT OPTION                                                                                                                                                                                                                                                                                                                                                                 |
| GRANT APPLICATION_PASSWORD_ADMIN,AUDIT_ABORT_EXEMPT,AUDIT_ADMIN,AUTHENTICATION_POLICY_ADMIN,BACKUP_ADMIN,BINLOG_ADMIN,BINLOG_ENCRYPTION_ADMIN,CLONE_ADMIN,CONNECTION_ADMIN,ENCRYPTION_KEY_ADMIN,FIREWALL_EXEMPT,FLUSH_OPTIMIZER_COSTS,FLUSH_STATUS,FLUSH_TABLES,FLUSH_USER_RESOURCES,GROUP_REPLICATION_ADMIN,GROUP_REPLICATION_STREAM,INNODB_REDO_LOG_ARCHIVE,INNODB_REDO_LOG_ENABLE,PASSWORDLESS_USER_ADMIN,PERSIST_RO_VARIABLES_ADMIN,REPLICATION_APPLIER,REPLICATION_SLAVE_ADMIN,RESOURCE_GROUP_ADMIN,RESOURCE_GROUP_USER,ROLE_ADMIN,SENSITIVE_VARIABLES_OBSERVER,SERVICE_CONNECTION_ADMIN,SESSION_VARIABLES_ADMIN,SET_USER_ID,SHOW_ROUTINE,SYSTEM_USER,SYSTEM_VARIABLES_ADMIN,TABLE_ENCRYPTION_ADMIN,XA_RECOVER_ADMIN ON *.* TO `anji`@`localhost` WITH GRANT OPTION |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
2 rows in set (0.00 sec)

SELECT CONCAT('SHOW GRANTS FOR''',user,'''@''',host,''';') FROM mysql.user WHERE user=â€™local_userâ€™;


++++++++++++++++++++++

CREATE DATABASE cricket CHARACTER SET utf8 COLLATE utf8_bin;

CREATE USER 'myuser'@'%' IDENTIFIED BY PASSWORD '*HASH';

GRANT ALL ON cricket.* TO 'anji'@'%';

GRANT ALL ON cricket TO 'anji'@'%';
GRANT CREATE ON mydb TO 'myuser'@'%';
FLUSH PRIVILEGES;


mysql> CREATE DATABASE cricket CHARACTER SET utf8 COLLATE utf8_bin;
Query OK, 1 row affected, 2 warnings (0.01 sec)

mysql> 

GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;
============================ ========\\/\\\\\\\\\\\\\\\\\\\\/\//\/
from flask import Flask, jsonify
import mysql.connector

app = Flask(__name__)

# Connect to the MySQL database
db = mysql.connector.connect(
    #host="mysql",
    host="127.0.0.1",
    user="anji",
    password="anjianji",
    database="anji"
)

@app.route("/")
def hello():
    cursor = db.cursor()
    cursor.execute("SELECT * FROM users")
    rows = cursor.fetchall()
    return jsonify(rows)

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0')
=========
OUT PUT :: == 

anji@anji:~/depmysqlpython$ python3 app.py 
 * Serving Flask app 'app'
 * Debug mode: on
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.29.54:5000
Press CTRL+C to quit
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 729-672-875
192.168.29.54 - - [01/Mar/2023 20:06:29] "GET / HTTP/1.1" 500 -
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/mysql/connector/connection_cext.py", line 608, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: Table 'anji.users' doesn't exist

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/anji/.local/lib/python3.8/site-packages/flask/app.py", line 2551, in __call__
    return self.wsgi_app(environ, start_response)
  File "/usr/local/lib/python3.8/dist-packages/mysql/connector/connection_cext.py", line 616, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.ProgrammingError: 1146 (42S02): Table 'anji.users' doesn't exist
192.168.29.54 - - [01/Mar/2023 20:06:29] "GET /?__debugger__=yes&cmd=resource&f=style.css HTTP/1.1" 304 -
=============
https://stackoverflow.com/questions/4005409/error-1046-no-database-selected-how-to-resolve
https://www.w3schools.com/mysql/mysql_create_table.asp

CREATE TABLE workers (
    PersonID int,
    LastName varchar(255),
    FirstName varchar(255),
    Address varchar(255),
    City varchar(255)
);

mysql> CREATE TABLE workers (
    ->     PersonID int,
    ->     LastName varchar(255),
    ->     FirstName varchar(255),
    ->     Address varchar(255),
    ->     City varchar(255)
    -> );
ERROR 1046 (3D000): No database selected
You need to tell MySQL which database to use:  
USE database_name;
use anji;

mysql> use anji;
Database changed

mysql> CREATE TABLE workers (
    ->     PersonID int,
    ->     LastName varchar(255),
    ->     FirstName varchar(255),
    ->     Address varchar(255),
    ->     City varchar(255)
    -> );
Query OK, 0 rows affected (0.02 sec)

CREATE TABLE `anji`.`workers` (
 _id int not null,
 LastName varchar(255) NOT NULL,
 FirstName varchar(255),
 Age int,
 PRIMARY KEY (_id)
);
============================================
https://phoenixnap.com/kb/how-to-create-a-table-in-mysql

CREATE TABLE movies(title VARCHAR(50) NOT NULL,genre VARCHAR(30) NOT NULL,director VARCHAR(60) NOT NULL,release_year INT NOT NULL,PRIMARY KEY(title));

mysql> CREATE TABLE movies(title VARCHAR(50) NOT NULL,genre VARCHAR(30) NOT NULL,director VARCHAR(60) NOT NULL,release_year INT NOT NULL,PRIMARY KEY(title));
Query OK, 0 rows affected (0.03 sec)

mysql> DESCRIBE movies;
+--------------+-------------+------+-----+---------+-------+
| Field        | Type        | Null | Key | Default | Extra |
+--------------+-------------+------+-----+---------+-------+
| title        | varchar(50) | NO   | PRI | NULL    |       |
| genre        | varchar(30) | NO   |     | NULL    |       |
| director     | varchar(60) | NO   |     | NULL    |       |
| release_year | int         | NO   |     | NULL    |       |
+--------------+-------------+------+-----+---------+-------+
4 rows in set (0.01 sec)
The terminal prints out information about the table:

    Field â€“ Indicates column name.
    Type â€“ Specifies data type for the column (varchar for characters, int for numbers).
    Null â€“ Indicates whether the column can remain with null values.
    Key â€“ Displays the primary column.
    Default â€“ Displays the columnâ€™s default value.
    Extra â€“ Indicates additional information about the columns.

mysql> INSERT INTO movies VALUE ("Joker", "psychological thriller", "Todd Phillips", 2019);
Query OK, 1 row affected (0.00 sec)

mysql> select * from movies;
+-------+------------------------+---------------+--------------+
| title | genre                  | director      | release_year |
+-------+------------------------+---------------+--------------+
| Joker | psychological thriller | Todd Phillips |         2019 |
+-------+------------------------+---------------+--------------+
1 row in set (0.00 sec)


mysql> create database movies1;
Query OK, 1 row affected (0.01 sec)

mysql> use  movies1;
Database changed
mysql> CREATE TABLE movies1(title VARCHAR(50) NOT NULL,genre VARCHAR(30) NOT NULL,director VARCHAR(60) NOT NULL,release_year INT NOT NULL,PRIMARY KEY(title));
Query OK, 0 rows affected (0.01 sec)

mysql> INSERT INTO movies1 VALUE ("Joker", "psychological thriller", "Todd Phillips", 2019);
Query OK, 1 row affected (0.01 sec)


mysql> SHOW DATABASES;
+--------------------+
| Database           |
+--------------------+
| anji               |
| cricket            |
| foodball           |
| information_schema |
| movies1            |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
8 rows in set (0.00 sec)

mysql> USE movies1;
Database changed
mysql> SELECT * FROM movies1;
+-------+------------------------+---------------+--------------+
| title | genre                  | director      | release_year |
+-------+------------------------+---------------+--------------+
| Joker | psychological thriller | Todd Phillips |         2019 |
+-------+------------------------+---------------+--------------+
1 row in set (0.00 sec)

root@anji:~# mysql -u root -p <  anji.sql

mysql> select title from movies1;
+-------+
| title |
+-------+
| Joker |
+-------+
1 row in set (0.00 sec) "

############################################################################
             MYSQL DATABASE WITH FLASK PYTHON FLASK WITH MYSQL DATA BASE    === SUCCESS 

from flask import Flask, jsonify
import mysql.connector

app = Flask(__name__)

# Connect to the MySQL database
db = mysql.connector.connect(
    #host="mysql",
    host="127.0.0.1",
    user="anji",
    password="anjianji",
    database="movies1"
)

@app.route("/")
def hello():
    cursor = db.cursor()
    cursor.execute(" SELECT * FROM movies1")
    rows = cursor.fetchall()
    return jsonify(rows)

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0')

+++++++++++++++++++++++++++++++++++++++++++++++++++++++
output = 
root@anji:~# curl http://192.168.29.54:5000/
[
  [
    "Joker",
    "psychological thriller",
    "Todd Phillips",
    2019
  ]
]
root@anji:~# 
++++++++++++++++++++++++++++++++++++++++++

###########################
Not able to install package flask-mysqldb
https://askubuntu.com/questions/1159764/not-able-to-install-package-flask-mysqldb
How to install Flask-mysqldb for Python?
https://stackoverflow.com/questions/58957474/how-to-install-flask-mysqldb-for-python



(venv) anji@anji:~/tempfol$ pip install flask_mysqldb
Collecting flask_mysqldb
  Using cached Flask-MySQLdb-1.0.1.tar.gz (4.3 kB)
  Preparing metadata (setup.py) ... done
Requirement already satisfied: Flask>=0.12.4 in ./venv/lib/python3.8/site-packages (from flask_mysqldb) (2.2.3)
Collecting mysqlclient>=1.3.7
  Using cached mysqlclient-2.1.1.tar.gz (88 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error
  
  Ã— python setup.py egg_info did not run successfully.
  â”‚ exit code: 1
  â•°â”€> [16 lines of output]
      /bin/sh: 1: mysql_config: not found
      /bin/sh: 1: mariadb_config: not found
      /bin/sh: 1: mysql_config: not found
      Traceback (most recent call last):
        File "<string>", line 2, in <module>
        File "<pip-setuptools-caller>", line 34, in <module>
        File "/tmp/pip-install-7gy22r7w/mysqlclient_3c1c3584a2b54b3cb0200fe94c363a50/setup.py", line 15, in <module>
          metadata, options = get_config()
        File "/tmp/pip-install-7gy22r7w/mysqlclient_3c1c3584a2b54b3cb0200fe94c363a50/setup_posix.py", line 70, in get_config
          libs = mysql_config("libs")
        File "/tmp/pip-install-7gy22r7w/mysqlclient_3c1c3584a2b54b3cb0200fe94c363a50/setup_posix.py", line 31, in mysql_config
          raise OSError("{} not found".format(_mysql_config_path))
      OSError: mysql_config not found
      mysql_config --version
      mariadb_config --version
      mysql_config --libs
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

Ã— Encountered error while generating package metadata.
â•°â”€> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.

OUT-PUT=
https://github.com/alexferl/flask-mysqldb/issues/13

rm -rf .* /root/tmp

  
sudo apt-get install mysql-server
sudo apt-get install libmysqlclient-dev
pip3 install flask-mysqldb
######################################################################################################33
anji@anji:~/tempfol$ python3 server.py 
Traceback (most recent call last):
  File "server.py", line 3, in <module>
    from flask_mysqldb import MYSQL
ImportError: cannot import name 'MYSQL' from 'flask_mysqldb' (/usr/local/lib/python3.8/dist-packages/flask_mysqldb/__init__.py)
anji@anji:~/tempfol$  pip install MySQL-python

+++++++++++++++++++++++++++++++++++++++
https://phoenixnap.com/kb/mysql-server-through-socket-var-run-mysqld-mysqld-sock-2

root@anji:/tmp# sudo systemctl status mysql

â— mysql.service - MySQL Community Server
     Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor pres>
     Active: "failed "(Result: exit-code) since Mon 2023-03-06 09:24:11 IST; 8>
    Process: 1669 ExecStartPre=/usr/share/mysql/mysql-systemd-start pre (cod>
    Process: 1677 ExecStart=/usr/sbin/mysqld (code=exited, status=1/FAILURE)
   Main PID: 1677 (code=exited, status=1/FAILURE)
     Status: "Server shutdown complete"
      Error: 13 (Permission denied)

root@anji:/tmp# systemctl start mysql 
Job for mysql.service failed because the control process exited with error code.
See "systemctl status mysql.service" and "journalctl -xe" for details.

root@anji:/tmp# sudo systemctl enable mysql
Synchronizing state of mysql.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable mysql

root@anji:/tmp# sudo systemctl start mysql
Job for mysql.service failed because the control process exited with error code.
See "systemctl status mysql.service" and "journalctl -xe" for details.

root@anji:/tmp# sudo find / -type s
        ===  not found ===  values
 
 root@anji:/tmp#  journalctl -xe

Mar 06 18:25:15 anji rtkit-daemon[1212]: Supervising 4 threads of 3 processe>
Mar 06 18:25:16 anji rtkit-daemon[1212]: Supervising 4 threads of 3 processe>
Mar 06 18:25:16 anji rtkit-daemon[1212]: Supervising 4 threads of 3 processe>
Mar 06 18:25:16 anji rtkit-daemon[1212]: Supervising 4 threads of 3 processe>
Mar 06 18:25:16 anji rtkit-daemon[1212]: Supervising 4 threads of 3 processe>

SOLUTION :    "https://www.tutsmake.com/failed-to-start-mysql-community-server/"

 Step 2 â€“ Reinstall MySQL
    Step 3 â€“ Restart MySQL Server

Step 1 â€“ Login to Your SSH Server

First of all, you need to login into your ssh server with username and password.
Step 2 â€“ Reinstall MySQL

Then execute the following command into your terminal to â€œpurgeâ€ every file related to MySQL and reinstall mysql-server:
:
sudo apt-get purge mysql-server mysql-client mysql-common
sudo apt-get install mysql-server 
sudo systemctl restart mysql

root@anji:/tmp# mysql
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 8
Server version: 8.0.32-0ubuntu0.20.04.2 (Ubuntu)

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

##############################3333
anji@anji:~/tempfol$ python3 -m venv venv 
anji@anji:~/tempfol$ ls
venv
anji@anji:~/tempfol$ nano server.py
anji@anji:~/tempfol$ source ./venv/bin/activate
(venv) anji@anji:~/tempfol$ ls
venv
(venv) anji@anji:~/tempfol$ 

(venv) anji@anji:~/tempfol$ env | grep VIRTUAL
VIRTUAL_ENV=/home/anji/tempfol/venv
(venv) anji@anji:~/tempfol$ 

(venv) anji@anji:~/tempfol$ pip3 install PYlint

(venv) anji@anji:~/tempfol$ pip install jedi 

anji@anji:~/tempfol$ python3 -m pip install --upgrade pi

(venv) anji@anji:~/tempfol$ pip install Pyjwt

(venv) anji@anji:~/tempfol$ pip install flask 
pip3 install flask-mysqldb

nv) anji@anji:~/tempfol$ export MYSQL_HOST=localhost

(venv) anji@anji:~/tempfol$ export MYSQL_HOST=localhost
(venv) anji@anji:~/tempfol$ python3 server.py 
localhost
(venv) anji@anji:~/tempfol$ 

================================================
| import jwt, datetime, os
from  flask import Flask
from flask_mysqldb import MySQL
from flask import Flask, request, send_file, jsonify, render_template, url_for, redirect
server = Flask(__name__)
mysql = MySQL(server)
# config
server.config["MYSQL_HOST"] = os.environ.get("MYSQL_HOST")
print(server.config["MYSQL_HOST"]) 
================================================

(venv) anji@anji:~/tempfol$ python3 server.py 
localhost


CREATE USER 'sammy'@'localhost' IDENTIFIED BY 'anji123';

#####################3333

root@anji:/home/anji/tempfol# mysql -uroot < init.sql 
ERROR 1064 (42000) at line 9: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'AUTO_INCRIMENT PRIMARY KEY,
    email VARCHAR(255) NOT NULL,
    password VARCHA' at line 2
root@anji:/home/anji/tempfol# 

root@anji:/home/anji/tempfol# mysql -uroot < init.sql 
ERROR 1396 (HY000) at line 1: Operation CREATE USER failed for 'auth_user'@'localhost'
root@anji:/home/anji/tempfol# 

root@anji:/home/anji/tempfol# mysql -Uroot -e "DROP DATABASE auth"

root@anji:/home/anji/tempfol# mysql -Uroot -e "DROP USER auth_user@localhost"

root@anji:/home/anji/tempfol# mysql -Uroot < init.sql 

(venv) anji@anji:~/tempfol$ mysql -Uroot
ERROR 1045 (28000): Access denied for user 'anji'@'localhost' (using password: NO)

mysql> CREATE USER 'root'@'localhost' IDENTIFIED BY 'anji1234';
ERROR 1396 (HY000): Operation CREATE USER failed for 'root'@'localhost'

MySQL/MariaDB command prompt: 
ALTER USER 'root'@'localhost' IDENTIFIED BY 'anji1234'; 
flush privileges; exit; 

mysql> ALTER USER 'root'@'localhost' IDENTIFIED BY 'anji1234'; 
Query OK, 0 rows affected (0.00 sec)

(venv) anji@anji:~/tempfol$ sudo mysql -Uroot

Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 14
Server version: 8.0.32-0ubuntu0.20.04.2 (Ubuntu)
Copyright (c) 2000, 2023, Oracle and/or its affiliates.
Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| anji               |
| auth               |
| cricket            |
| foodball           |
| information_schema |
| movies1            |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
9 rows in set (0.01 sec)

mysql> use auth
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A
Database changed

mysql> show tables;
+----------------+
| Tables_in_auth |
+----------------+
| user           |
+----------------+
1 row in set (0.00 sec)

mysql> describe user;
+----------+--------------+------+-----+---------+----------------+
| Field    | Type         | Null | Key | Default | Extra          |
+----------+--------------+------+-----+---------+----------------+
| id       | int          | NO   | PRI | NULL    | auto_increment |
| email    | varchar(255) | NO   |     | NULL    |                |
| password | varchar(255) | NO   |     | NULL    |                |
+----------+--------------+------+-----+---------+----------------+
3 rows in set (0.01 sec)

mysql> select * from user;
+----+---------------+----------+
| id | email         | password |
+----+---------------+----------+
|  1 | asd@gmail.com | anji123  |
+----+---------------+----------+
1 row in set (0.00 sec)

#################################################333
anji@anji:~/tempfol$ pip3 freeze > requirements.txt 
anji@anji:~/tempfol$ cat requirements.txt 
appdirs==1.4.3
apturl==0.5.2
attrs==21.2.0
autopep8==1.5.7
bcrypt==3.1.7
blinker==1.4
Brlapi==0.7.0
cached-property==1.5.1
certifi==2019.11.28
chardet==3.0.4
click==8.0.1
colorama==0.4.3
command-not-found==0.3
cryptography==2.8
cupshelpers==1.0
DateTime==5.0
dbus-python==1.2.16

 => [5/6] RUN pip install --no-cache-dir -r requirements.txt                     6.9s
 => [6/6] COPY . /app                                                            0.5s
 => exporting to image                                                           0.4s 
 => => exporting layers                                                          0.4s 
 => => writing image sha256:ab73255277c654c672a1c11e9b5ddebd686df05b04e869b72e8  0.0s 
 => => naming to docker.io/library/pypy                                          0.0s 
anji@anji:~/tempfol$ docker tag ab73255277c654c672a1c11e9b5ddebd686df05b04e869b72e8  anjireddy3993/flask-mysql-k8-jwt:latest
anji@anji:~/tempfol$ docker push  anjireddy3993/flask-mysql-k8-jwt:latest
The push refers to repository [docker.io/anjireddy3993/flask-mysql-k8-jwt]
c1e349c1cf7a: Pushed 
729808c49a8a: Pushed 

++++++++++++++++++++
root@auth-7597f4847d-lkkvp:/app# ls
Dockerfile  errors.tf  init.sql  manifests  requirements.txt  server.py  venv
root@auth-7597f4847d-lkkvp:/app# env
KUBERNETES_SERVICE_PORT_HTTPS=443
MYSQL_PORT=3306
AUTH_SERVICE_PORT_80_TCP_ADDR=10.103.155.0
KUBERNETES_SERVICE_PORT=443
AUTH_SERVICE_PORT_80_TCP_PROTO=tcp
HOSTNAME=auth-7597f4847d-lkkvp
PYTHON_VERSION=3.10.10
MYSQL_DB=auth
AUTH_SERVICE_PORT=tcp://10.103.155.0:80
PWD=/app
PYTHON_SETUPTOOLS_VERSION=65.5.1
MYSQL_PASSWORD=Auth123
MYSQL_USER=auth_user
HOME=/root
LANG=C.UTF-8
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
AUTH_SERVICE_SERVICE_PORT=80
AUTH_SERVICE_SERVICE_HOST=10.103.155.0
GPG_KEY=A035C8C19219BA821ECEA86B64E628F8D684696D
MYSQL_HOST=host.minikube.internal
TERM=xterm
AUTH_SERVICE_PORT_80_TCP_PORT=80
SHLVL=1
KUBERNETES_PORT_443_TCP_PROTO=tcp
PYTHON_PIP_VERSION=22.3.1
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
PYTHON_GET_PIP_SHA256=394be00f13fa1b9aaa47e911bdb59a09c3b2986472130f30aa0bfaf7f3980637
AUTH_SERVICE_PORT_80_TCP=tcp://10.103.155.0:80
KUBERNETES_SERVICE_HOST=10.96.0.1
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP_PORT=443
PYTHON_GET_PIP_URL=https://github.com/pypa/get-pip/raw/d5cb0afaf23b8520f1bbcfed521017b4a95f5c01/public/get-pip.py
PATH=/usr/local/bin:/u  "

root@auth-7597f4847d-lkkvp:/app# env | grep  MYSQL
MYSQL_PORT=3306
MYSQL_DB=auth
MYSQL_PASSWORD=Auth123
MYSQL_USER=auth_user
MYSQL_HOST=host.minikube.internal
root@auth-7597f4847d-lkkvp:/app# "

++++++++++++++++++++++
solution=:

sudo apt-get purge mysql-server mysql-client mysql-common
sudo apt-get install mysql-server 
sudo systemctl restart mysql

ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2)
https://phoenixnap.com/kb/mysql-server-through-socket-var-run-mysqld-mysqld-sock-2

root@anji:/tmp# sudo systemctl status mysql

â— mysql.service - MySQL Community Server
     Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor pres>
     Active: "failed "(Result: exit-code) since Mon 2023-03-06 09:24:11 IST; 8>
    Process: 1669 ExecStartPre=/usr/share/mysql/mysql-systemd-start pre (cod>
    Process: 1677 ExecStart=/usr/sbin/mysqld (code=exited, status=1/FAILURE)
   Main PID: 1677 (code=exited, status=1/FAILURE)
     Status: "Server shutdown complete"
      Error: 13 (Permission denied)

root@anji:/tmp# systemctl start mysql 
Job for mysql.service failed because the control process exited with error code.
See "systemctl status mysql.service" and "journalctl -xe" for details.

root@anji:/tmp# sudo systemctl enable mysql
Synchronizing state of mysql.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable mysql

root@anji:/tmp# sudo systemctl start mysql
Job for mysql.service failed because the control process exited with error code.
See "systemctl status mysql.service" and "journalctl -xe" for details.

root@anji:/tmp# sudo find / -type s
        ===  not found ===  values
 
 root@anji:/tmp#  journalctl -xe

Mar 06 18:25:15 anji rtkit-daemon[1212]: Supervising 4 threads of 3 processe>
Mar 06 18:25:16 anji rtkit-daemon[1212]: Supervising 4 threads of 3 processe>
Mar 06 18:25:16 anji rtkit-daemon[1212]: Supervising 4 threads of 3 processe>
Mar 06 18:25:16 anji rtkit-daemon[1212]: Supervising 4 threads of 3 processe>
Mar 06 18:25:16 anji rtkit-daemon[1212]: Supervising 4 threads of 3 processe>

SOLUTION :    "https://www.tutsmake.com/failed-to-start-mysql-community-server/"

 Step 2 â€“ Reinstall MySQL
    Step 3 â€“ Restart MySQL Server

Step 1 â€“ Login to Your SSH Server

First of all, you need to login into your ssh server with username and password.
Step 2 â€“ Reinstall MySQL

Then execute the following command into your terminal to â€œpurgeâ€ every file related to MySQL and reinstall mysql-server:

sudo apt-get purge mysql-server mysql-client mysql-common
sudo apt-get install mysql-server 
sudo systemctl restart mysql

root@anji:/tmp# mysql
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 8
Server version: 8.0.32-0ubuntu0.20.04.2 (Ubuntu)

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement"
#################################################################


Not able to install package flask-mysqldb 
[FIXED] Error while trying to install mysqldb #13


note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

Ã— Encountered error while generating package metadata.
â•°â”€> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
bin/sh: 1: mysql_config: not found
      /bin/sh: 1: mariadb_config: not found
      /bin/sh: 1: mysql_config: not found
      Traceback (most recent call last):
        File "<string>", line 2, in <module>
        File "<pip-setuptools-caller>", line 34, in <module>
        File "/tmp/pip-install-7gy22r7w/mysqlclient_3c1c3584a2b54b3cb0200fe94c363a50/setup.py", line 15, in <module>
###########################
Not able to install package flask-mysqldb
https://askubuntu.com/questions/1159764/not-able-to-install-package-flask-mysqldb
How to install Flask-mysqldb for Python?
https://stackoverflow.com/questions/58957474/how-to-install-flask-mysqldb-for-python

(venv) anji@anji:~/tempfol$ pip install flask_mysqldb
Collecting flask_mysqldb
  Using cached Flask-MySQLdb-1.0.1.tar.gz (4.3 kB)
  Preparing metadata (setup.py) ... done
Requirement already satisfied: Flask>=0.12.4 in ./venv/lib/python3.8/site-packages (from flask_mysqldb) (2.2.3)
Collecting mysqlclient>=1.3.7
  Using cached mysqlclient-2.1.1.tar.gz (88 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error
  
  Ã— python setup.py egg_info did not run successfully.
  â”‚ exit code: 1
  â•°â”€> [16 lines of output]
      /bin/sh: 1: mysql_config: not found
      /bin/sh: 1: mariadb_config: not found
      /bin/sh: 1: mysql_config: not found
      Traceback (most recent call last):
        File "<string>", line 2, in <module>
        File "<pip-setuptools-caller>", line 34, in <module>
        File "/tmp/pip-install-7gy22r7w/mysqlclient_3c1c3584a2b54b3cb0200fe94c363a50/setup.py", line 15, in <module>
          metadata, options = get_config()
        File "/tmp/pip-install-7gy22r7w/mysqlclient_3c1c3584a2b54b3cb0200fe94c363a50/setup_posix.py", line 70, in get_config
          libs = mysql_config("libs")
        File "/tmp/pip-install-7gy22r7w/mysqlclient_3c1c3584a2b54b3cb0200fe94c363a50/setup_posix.py", line 31, in mysql_config
          raise OSError("{} not found".format(_mysql_config_path))
      OSError: mysql_config not found
      mysql_config --version
      mariadb_config --version
      mysql_config --libs
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed
Ã— Encountered error while generating package metadata.
â•°â”€> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.

OUT-PUT=
https://github.com/alexferl/flask-mysqldb/issues/13

rm -rf .* /root/tmp
  
sudo apt-get install mysql-server
sudo apt-get install libmysqlclient-dev
pip3 install flask-mysqldb
#################################"
project success  SUCCESS  

apiVersion: apps/v1
kind: Deployment
metadata:
  name: auth
  labels:
    app: auth
spec:
  replicas: 2
  selector:
    matchLabels:
      app: auth
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 3
  template:
    metadata:
      labels:
        app: auth
    spec:
      containers:
        - name: auth
          image: anjireddy3993/flask-mysql-k8-jwt:1.0
          ports:
            - containerPort: 5000
          envFrom:
            - configMapRef:
                name: auth-configmap
            - secretRef:
                name: auth-secret
++++++++++++++++++++++++=
apiVersion: v1
kind: ConfigMap
metadata:
  name: auth-configmap
data:
  MYSQL_HOST: host.minikube.internal
  MYSQL_USER: auth_user
  MYSQL_DB: auth
  MYSQL_PORT: "3306"
++++++++++++++++++++++++++
apiVersion: v1
kind: Secret
metadata:
  name: auth-secret
stringData:
  MYSQL_PASSWORD: Auth123
  JWT_SECRET: sarcasm
type: Opaque
++++++++++++++++++++
apiVersion: v1
kind: Service
metadata: 
   name: auth-service
spec:
  selector:
     app: auth
  ports:
    - port: 80
      targetPort: 5000
      protocol: TCP
  type: NodePort
+++++++++++++++++++++++
import jwt, datetime, os
from flask import Flask, request
from flask_mysqldb import MySQL

server = Flask(__name__)
mysql = MySQL(server)

# config
server.config["MYSQL_HOST"] = os.environ.get("MYSQL_HOST")
server.config["MYSQL_USER"] = os.environ.get("MYSQL_USER")
server.config["MYSQL_PASSWORD"] = os.environ.get("MYSQL_PASSWORD")
server.config["MYSQL_DB"] = os.environ.get("MYSQL_DB")
server.config["MYSQL_PORT"] = os.environ.get("MYSQL_PORT")


@server.route("/login", methods=["POST"])
def login():
    auth = request.authorization
    if not auth:
        return "missing credentials", 401

    # check db for username and password
    cur = mysql.connection.cursor()
    res = cur.execute(
        "SELECT email, password FROM user WHERE email=%s", (auth.username,)
    )

    if res > 0:
        user_row = cur.fetchone()
        email = user_row[0]
        password = user_row[1]

        if auth.username != email or auth.password != password:
            return "invalid credentials", 401
        else:
            return createJWT(auth.username, os.environ.get("JWT_SECRET"), True)
    else:
        return "invalide credentials", 401


@server.route("/validate", methods=["POST"])
def validate():
    encoded_jwt = request.headers["Authorization"]

    if not encoded_jwt:
        return "missing credentials", 401

    encoded_jwt = encoded_jwt.split(" ")[1]

    try:
        decoded = jwt.decode(
            encoded_jwt, os.environ.get("JWT_SECRET"), algorithms=["HS256"]
        )
    except:
        return "not authorized", 403

    return decoded, 200


def createJWT(username, secret, authz):
    return jwt.encode(
        {
            "username": username,
            "exp": datetime.datetime.now(tz=datetime.timezone.utc)
            + datetime.timedelta(days=1),
            "iat": datetime.datetime.utcnow(),
            "admin": authz,
        },
        secret,
        algorithm="HS256",
    )


if __name__ == "__main__":
    server.run(host="0.0.0.0", port=5000)
+++++++++++++++++++++++++++++++++
FROM python:3.10-slim-bullseye

RUN apt-get update \
  && apt-get install -y --no-install-recommends --no-install-suggests \
  build-essential default-libmysqlclient-dev \
  && pip install --no-cache-dir --upgrade pip
WORKDIR /app
COPY ./requirements.txt /app
RUN pip install --no-cache-dir --requirement /app/requirements.txt
COPY . /app
EXPOSE 5000
CMD ["python3", "server.py"]
+++++++++++++++++++++++++++++
astroid==2.9.3
click==8.0.3
Flask==2.0.2
Flask-MySQLdb==0.2.0
isort==5.10.1
itsdangerous==2.0.1
jedi==0.18.1
Jinja2==3.0.3
lazy-object-proxy==1.7.1
MarkupSafe==2.0.1
mccabe==0.6.1
mysqlclient==2.1.0
parso==0.8.3
platformdirs==2.4.1
PyJWT==2.3.0
pylint==2.12.2
toml==0.10.2
Werkzeug==2.0.2
wrapt==1.13.3
+++++++++++++++++++++++++==
CREATE USER 'auth_user'@'localhost' IDENTIFIED BY 'Auth123';
CREATE DATABASE auth;
GRANT ALL PRIVILEGES ON auth.* TO 'auth_user'@'localhost';
USE auth;
CREATE TABLE user (
  id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,
  email VARCHAR(255) NOT NULL UNIQUE,
  password VARCHAR(255) NOT NULL
);
INSERT INTO user (email, password) VALUES ('georgio@email.com', 'Admin123');
++++++++++++++++++++++
anji@anji:~/tempfol$ python3 -m  venv venv 
anji@anji:~/tempfol$ ll
drwxrwxr-x  6 anji anji 4096 Mar  7 17:35 venv/
anji@anji:~/tempfol$ source ./venv/bin/activate
(venv) anji@anji:~/tempfol$ env | grep ENV 
VIRTUAL_ENV=/home/anji/tempfol/venv
(venv) anji@anji:~/tempfol$ 

(venv) anji@anji:~/tempfol$ pip3 freeze > requirements.txt 
(venv) anji@anji:~/tempfol$ ls
errors.tf  requirements.txt  venv
(venv) anji@anji:~/tempfol$ 

Jedi is a static analysis tool for Python that is typically used in IDEs/editors plugins. Jedi has a focus on autocompletion and goto functionality. Other features include refactoring, code search and finding references.

(venv) anji@anji:~/tempfol$ pip3 install jedi 

What is Pylint?
Pylint is a static code analyser for Python 2 or 3. The latest version supports Python 3.7.2 and above.
Pylint analyses your code without actually running it. It checks for errors, enforces a coding standard, looks for code smells, and can make suggestions about how the code could be refactored. Pylint can infer actual values from your code using its internal code representation (astroid). If your code is import logging as argparse, Pylint will know that argparse.error(...) is in fact a logging call and not an argparse call.

GridFS Example
This example shows how to use gridfs to store large binary objects (e.g. files) in MongoDB.

Introduction to Pika
Pika is a pure-Python implementation of the AMQP 0-9-1 protocol that tries to stay fairly independent of the underlying network support library.
pip3 install pony

Project description
About
Pony ORM is easy to use and powerful object-relational mapper for Python. Using Pony, developers can create and maintain database-oriented software applications faster and with less effort. One of the most interesting features of Pony is its ability to write queries to the database using generator expressions. Pony then analyzes the abstract syntax tree of a generator and translates it to its SQL equivalent.
This package provides Morepath integration for the Pony Object-Relational Mapper library.

This package binds the database session to the request so you can interact with the database in your App directly without using db_session.


https://docs.ponyorm.org/firststeps.html

Getting Started with PonyÂ¶
Installing

To install Pony, type the following command into the command prompt:

pip install pony

Pony can be installed on Python 2.7 or Python 3. If you are going to work with SQLite database, you donâ€™t need to install anything else. If you wish to use another database, you need to have the access to the database and have the corresponding database driver installed:



An ORM (Object Relational Mapper) is a piece/layer of software that helps map your code Objects to your database

What is an ORM?

Before we talk about what an Object-Relational-Mapper is, it might be better to talk about Object-Relational-Mapping as a concept first.

Unless youâ€™ve worked exclusively with NoSQL databases, youâ€™ve likely written your fair share of SQL queries. They usually look something like this:


Entity

    This attribute represents the base class which should be inherited by all entities which are mapped to the particular database.

    Example:

    db = Database()

    class Person(db.Entity):
        name = Required(str)
        age = Required(int)

https://docs.ponyorm.org/firststeps.html
Defining entities
Now, letâ€™s create two entities â€“ Person and Car. The entity Person has two attributes â€“ name and age, and Car has attributes make and model. In the Python interpreter, type the following code:

>>> class Person(db.Entity):
...     name = Required(str)
...     age = Required(int)
...     cars = Set('Car')
...
>>> class Car(db.Entity):
...     make = Required(str)
...     model = Required(str)
...     owner = Required(Person)
...
>>>

my_file = open("hello.txt", "r")
print(my_file.read())

# Output : 
# Hello world
# I hope you're doing well today
# This is a text file

F-strings provide a way to embed expressions inside string literals, using a minimal syntax. It should be noted that an f-string is really an expression evaluated at run time, not a constant value. In Python source code, an f-string is a literal string, prefixed with 'f', which contains expressions inside braces.

Method 4: Read a File Line by Line using for loop and list comprehension

A list comprehension consists of brackets containing the expression, which is executed for each element along with the for loop to iterate over each element. Here, we will read the text file and print the raw data including the new line character in another output we removed all the new line characters from the list.

Example

with open('myfile.txt') as f:
    lines = [line for line in f]
  
print(lines)
  
# removing the new line characters
with open('myfile.txt') as f:
    lines = [line.rstrip() for line in f]
  
print(lines)

Method 	Description
delete(url, args) 	Sends a DELETE request to the specified url
get(url, params, args) 	Sends a GET request to the specified url
head(url, args) 	Sends a HEAD request to the specified url
patch(url, data, args) 	Sends a PATCH request to the specified url
post(url, data, json, args) 	Sends a POST request to the specified url
put(url, data, args) 	Sends a PUT request to the specified url
request(method, url, args) 	Sends a request of the specified method to the specified url

>>> import requests
>>> r = requests.get('https://httpbin.org/basic-auth/user/pass', auth=('user', 'pass'))
>>> r.status_code
200
>>> r.headers['content-type']
'application/json; charset=utf8'
>>> r.encoding
'utf-8'
>>> r.text
'{"authenticated": true, ...'
>>> r.json()
{'authenticated': True, ...}

In Python, with statement is used in exception handling to make the code cleaner and much more readable. It simplifies the management of common resources like file streams. Observe the following code example on how the use of with statement makes code cleaner. 

Determines the type of the values of the dictionary.

    'dict' (default) : dict like {column -> {index -> value}}
    'list' : dict like {column -> [values]}
    'series' : dict like {column -> Series(values)}
    'split' : dict like {'index' -> [index], 'columns' -> [columns], 'data' -> [values]}
    'records' : list like [{column -> value}, â€¦ , {column -> value}]
    'index' : dict like {index -> {column -> value}}

Abbreviations are allowed. s indicates series and sp indicates split. 	str {'dict', 'list', 'series', 'split', 'records', 'index'} 	Required

Syntax: DataFrame.to_dict(orient=â€™dictâ€™, into=)

Parameters:
orient: String value, (â€˜dictâ€™, â€˜listâ€™, â€˜seriesâ€™, â€˜splitâ€™, â€˜recordsâ€™, â€˜indexâ€™) Defines which dtype to convert Columns(series into). For example, â€˜listâ€™ would return a dictionary of lists with Key=Column name and Value=List (Converted series).
into: class, can pass an actual class or instance. For example in case of defaultdict instance of class can be passed. Default value of this parameter is dict.



pandas.DataFrame.to_dict
DataFrame.to_dict(orient='dict', into=<class 'dict'>)[source]

Convert the DataFrame to a dictionary.

The type of the key-value pairs can be customized with the parameters (see below).

Parameters

    orientstr {â€˜dictâ€™, â€˜listâ€™, â€˜seriesâ€™, â€˜splitâ€™, â€˜tightâ€™, â€˜recordsâ€™, â€˜indexâ€™}

        Determines the type of the values of the dictionary.

            â€˜dictâ€™ (default) : dict like {column -> {index -> value}}

            â€˜listâ€™ : dict like {column -> [values]}

            â€˜seriesâ€™ : dict like {column -> Series(values)}

            â€˜splitâ€™ : dict like {â€˜indexâ€™ -> [index], â€˜columnsâ€™ -> [columns], â€˜dataâ€™ -> [values]}

            â€˜tightâ€™ : dict like {â€˜indexâ€™ -> [index], â€˜columnsâ€™ -> [columns], â€˜dataâ€™ -> [values], â€˜index_namesâ€™ -> [index.names], â€˜column_namesâ€™ -> [column.names]}

            â€˜recordsâ€™ : list like [{column -> value}, â€¦ , {column -> value}]

            â€˜indexâ€™ : dict like {index -> {column -> value}}

        Abbreviations are allowed. s indicates series and sp indicates split.


import requests

headers = {'Accept': 'application/json'}

r = requests.get('https://reqbin.com/echo/get/json', headers=headers)

print(f"Response: {r.json()}")

##TRY
The try block lets you test a block of code for errors. The except block lets you handle the error. The else block lets you execute code when there is no error.


try:
  print(x)
except NameError:
  print("Variable x is not defined")
except:
  print("Something else went wrong")
results = Variable x is not defined 

Is it a good practice to use try-except-else in Python?

What is the reason for the try-except-else to exist?

I do not like that kind of programming, as it is using exceptions to perform flow control. However, if it is included in the language, there must be a good reason for it, isn't it?

It is my understanding that exceptions are not errors, and that they should only be used for exceptional conditions (e.g. I try to write a file into disk and there is no more space, or maybe I do not have permission), and not for flow control. 

>>> print( 0 / 0)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ZeroDivisionError: integer division or modulo by zero

A try and except block is used for error handling in Python.

    Try: Helps to test the code. If the code inside the try block is error free it is executed. Otherwise the error gets caught and control goes to the except block.
    Except: Except displays the error message.
    Else: Executes if there is no error in the code in the try block.
    Finally: Executes independently of the try-except block results.

The Else and Finally block are optional but it is considered a good programming practice to include them.

Type of error messages

There are two types of error that can occur:

    Syntax Error/Parsing Error: When the Python parser is unable to understand a line of code.
    Exception: When the error is detected during execution, e.g.,ZeroDivisionError.

List of exception errors

The following are the list of exception error that arises:

    IOError: When a file canâ€™t be opened
    KeyboardInterrupt: When an unrequired key is pressed by the user
    ValueError :When a built-in function receives a wrong argument
    EOFError :When one of the built-in functions (input() or raw_input()) hits an end-of-file condition (EOF) without reading any data
    ImportError: When a module is not found

The try block lets you test a block of code for errors.

The except block lets you handle the error.

The else block lets you execute code when there is no error.

The finally block lets you execute code, regardless of the result of the try- and except blocks.

##SESSION:-
What does the Session do ?

In the most general sense, the Session establishes all conversations with the database and represents a â€œholding zoneâ€ for all the objects which youâ€™ve loaded or associated with it during its lifespan. It provides the interface where SELECT and other queries are made that will return and modify ORM-mapped objects. The ORM objects themselves are maintained inside the Session, inside a structure called the identity map - a data structure that maintains unique copies of each object, where â€œuniqueâ€ means â€œonly one object with a particular primary keyâ€.

scoped_session.

To use SQLAlchemy in a declarative way with your application, you just have to put the following code into your application module. Flask will automatically remove database sessions at the end of the request or when the application shuts down:

#####################################################################################
Learn Flask-RESTful APIs with this Project
https://www.youtube.com/watch?v=ow451CjBV-U
https://github.com/anjilinux/project-flask-restful-demo?organization=anjilinux&organization=anjilinux

from flask import Flask, request
from flask_restful import Resource, Api
from pony import orm

app = Flask(__name__)
api = Api(app)
db = orm.Database()


# database
class Game(db.Entity):
    game_id = orm.Required(str, unique=True)
    home_team = orm.Required(str)
    away_team = orm.Required(str)
    home_score = orm.Required(int)
    away_score = orm.Required(int)


db.bind(provider="sqlite", filename="database.sqlite", create_db=True)
db.generate_mapping(create_tables=True)


# Api

class GameList(Resource):
    def get(self):
        with orm.db_session:
            items = orm.select(p for p in Game)
            result = [i.to_dict() for i in items]

        return {"results": result }

    def post(self):
        new_game = request.json

        try:
            with orm.db_session:
                Game(
                    game_id=new_game["game_id"],
                    home_team=new_game["home_team"],
                    away_team=new_game["away_team"],
                    home_score=new_game["home_score"],
                    away_score=new_game["away_score"],
                )
                return {"game": new_game}
        except orm.TransactionIntegrityError as err:
            print(err)
            return {"error": "game id already exists"}


class GameDetail(Resource):
    def get(self, game_id):
        try:
            with orm.db_session:
                item = Game.get(game_id=game_id)

            return {"result": item.to_dict()}
        except:
            return {"error": "game does not exist"}


api.add_resource(GameList, "/")
api.add_resource(GameDetail, "/<string:game_id>")

if __name__ == '__main__':
    app.run(debug=True, port=5000)

+++++++++++++++++++++++++++++++++++++++++++++
import requests
import csv

with open("data1.csv") as f:
    r = csv.reader(f)
    data = [row for row in r if row]


def post_data(item):
    headers = {"Content-type": "application/json"}
    payload = {
        "game_id": item[0],
        "home_team": item[1],
        "away_team": item[2],
        "home_score": item[3],
        "away_score": item[4],
    }
    resp = requests.post("http://localhost:5000", headers=headers, json=payload)
    return resp.json()


for item in data:
    print(post_data(item))

#####################################################################33
########  MONGODB  \/\/\/\/\/  STATEFULSET 
https://github.com/anjilinux/kubernetes-install-in-ubuntu-20-Install-Kubernetes-Cluster-on-Ubuntu-20.04-/edit/master/yamls/mongodb/mongodb-statefulset.yaml
https://www.youtube.com/watch?v=W-lJX3_uE5I&t=878s
Kube 60.1 ] Running MongoDB Replicaset in Kubernetes | Part 1

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongo
spec:
  selector:
    matchLabels:
      app: mongo
  serviceName: "mongo"
  replicas: 3
  template:
    metadata:
      labels:
        app: mongo
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: mongo
        image: mongo
        command: 
        - mongod 
        - "--bind_ip_all"
        - "--replSet"
        - rs0
        ports:
        - containerPort: 27017
  #       volumeMounts:
  #       - name: mongo-volume
  #         mountPath: /tmp/mongo
  # volumeClaimTemplates:
  # - metadata:
  #     name: mongo-volume
  #   spec:
  #     accessModes: [ "ReadWriteOnce" ]
  #     resources:
  #       requests:
  #         storage: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: mongo
  labels:
    app: mongo
spec:
  ports:
  - name: mongo
    port: 27017
    targetPort: 27017
  clusterIP: None
  selector:
    app: mongo
+++++++++++++++++++++++++++++++++++++++++++++++++++++++==
anji@anji:~/tempfol$ kubectl get pod,sts,svc,rs,deploy  -o wide 
NAME          READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
pod/mongo-0   1/1     Running   0          21m   10.44.0.1   worker   <none>           <none>
pod/mongo-1   1/1     Running   0          21m   10.44.0.2   worker   <none>           <none>
pod/mongo-2   1/1     Running   0          21m   10.44.0.3   worker   <none>           <none>

NAME                     READY   AGE   CONTAINERS   IMAGES
statefulset.apps/mongo   3/3     21m   mongo        mongo

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)     AGE   SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP     35m   <none>
service/mongo        ClusterIP   None         <none>        27017/TCP   21m   app=mongo
+++++++++++++++++++++++++
anji@anji:~/Desktop$  kubectl describe pod mongo-0
Name:             mongo-0
Node:             worker/192.168.122.226
Status:           Running
IP:               10.44.0.1
Controlled By:  StatefulSet/mongo
Containers:
    Image:         mongo
    Port:          27017/TCP
    Command:
      mongod
      --bind_ip_all
      --replSet
      rs0
    State:          Running
      Started:      Thu, 09 Mar 2023 22:25:51 +0530
    Ready:          True
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  24m   default-scheduler  Successfully assigned default/mongo-0 to worker
  Normal  Pulling    24m   kubelet            Pulling image "mongo"
  Normal  Pulled     24m   kubelet            Successfully pulled image "mongo" in 1.744261374s (1.744269531s including waiting)
  Normal  Created    24m   kubelet            Created container mongo
  Normal  Started    24m   kubelet            Started container mongo
+++++++++++++++++++++++++++++
anji@anji:~$  kubectl describe statefulset 
Name:               mongo
Replicas:           3 desired | 3 total
Update Strategy:   " RollingUpdate"
  Partition:        0
Pods Status:        3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=mongo
  Containers:
   mongo:
    Image:      mongo
    Port:       27017/TCP
    Host Port:  0/TCP
    Command:
      mongod
      --bind_ip_all
      --replSet
      rs0
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  28m   statefulset-controller  create Pod mongo-0 in StatefulSet mongo successful
  Normal  SuccessfulCreate  28m   statefulset-controller  create Pod mongo-1 in StatefulSet mongo successful
  Normal  SuccessfulCreate  28m   statefulset-controller  create Pod mongo-2 in StatefulSet mongo successful
+++++++++++++++++++++++++
anji@anji:~/Desktop$ kubectl describe service mongo
Name:              mongo
Namespace:         default
Labels:            app=mongo
Annotations:       <none>
Selector:          app=mongo
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                None
IPs:               None
Port:              mongo  27017/TCP
TargetPort:        27017/TCP
Endpoints:         10.44.0.1:27017,10.44.0.2:27017,10.44.0.3:27017
Session Affinity:  None
Events:            <none>
+++++++++++++++++++++++++++++++++++
anji@anji:~/tempfol$ kubectl exec   mongo-0 -it   -- /bin/bash 
root@mongo-0:/# ls
bin  boot  data  dev  docker-entrypoint-initdb.d  etc  home  js-yaml.js  lib  lib32  lib64  libx32  media  mnt	opt  proc  root  run  sbin  srv  sys  tmp  usr	var

root@mongo-0:~# ls
WiredTiger	   WiredTigerHS.wt			 collection-2-5339359903256127194.wt  diagnostic.data		       index-5-5339359903256127194.wt  mongod.lock
WiredTiger.lock    _mdb_catalog.wt			 collection-4-5339359903256127194.wt  index-1-5339359903256127194.wt   index-7-5339359903256127194.wt  sizeStorer.wt
WiredTiger.turtle  collection-0-5339359903256127194.wt	 collection-6-5339359903256127194.wt  index-11-5339359903256127194.wt  index-9-5339359903256127194.wt  storage.bson
WiredTiger.wt	   collection-10-5339359903256127194.wt  collection-8-5339359903256127194.wt  index-3-5339359903256127194.wt   journal
+++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://devopscube.com/deploy-mongodb-kubernetes/
why using readiness probe liveness probe difference

For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs. The kubelet uses readiness probes to know when a container is ready to start accepting traffic

Does liveness probe start working only after the pod becomes ready ? In other words, I assume readiness probe job is complete once the POD is ready. After that livenessProbe takes care of health check. In this case, I can ignore the initialDelaySeconds for livenessProbe. If they are independent, what is the point of doing livenessProbe check when the pod itself is not ready! ?

    It is used to indicate if the application inside the Container has started.
    If a startup probe is provided, all other probes are disabled.
    Once the startup probe has succeeded once, the liveness probe takes over to provide a fast response to container deadlocks.
    In the given example, if the request fails, it will restart the container.
    If not provided the default state is Success.

    The deadlock is situation, when your container is not ready but liveness probe is performing and it exceed failure treshold , because of too short delay time. In this situation your container keeps restarting. To prevent that you should use startup probe and put your threshold high enough

    Startup probe use-cases

The example reason to use startup probe is:

Your application is starting for a long time. You can increase delays for readiness probe and liveness probe but you do not know when your container is ready because those probes are not performed for delay time.

So startup probe is used commonly with readines and liveness probes. It is performed until your container is ready(till your startup probe returns the Success status), so you do not need delays anymore.

Liveness Probes:

livenessProbe:
  httpGet:
   path: /healthz
   port: 8080
  initialDelaySeconds: 3
  periodSeconds: 3

    It is used to indicate if the container has started and is alive or not i.e. proof of being available.
    It could catch a deadlock, where an application is running, but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs.
    It doesnâ€™t wait for readiness probes to succeed. If you want to wait before executing a liveness probe you should use initialDelaySeconds or a startupProbe.
    In the given example, if the request fails, it will restart the container.
    If not provided the default state is Success.

Readiness Probes:

readinessProbe:
  httpGet:
   path: /healthz
   port: 8080
  initialDelaySeconds: 3
  periodSeconds: 3

    It is used to indicate if the container is ready to serve traffic or not i.e.proof of being ready to use.
    It checks dependencies like database connections or other services your container is depending on to fulfill its work.
    A Pod is considered ready when all of its containers are ready. When a Pod is not ready, it is removed from Service load balancers.
    Kubernetes relies on the readiness probes during rolling updates, it keeps the old container up and running until the new service declares that it is ready to take traffic.
    It runs on the container during its whole lifecycle.
    Incorrect implementation of it may result in an ever-growing number of processes in the container and resource starvation if this is left unchecked.
    In the given example, until the request returns Success, it wonâ€™t serve any traffic(by removing the Podâ€™s IP address from the endpoints of all Services that match the Pod).
    If not provided the default state is Success
+++++++++++++++
apiVersion: v1
kind: Pod
metadata: 
  name: nginx
spec: 
  containers: 
     - name: nginx
       image: nginx
       livenessProbe:
         httpGet:
           path: /
           port: 80
           httpHeaders:
         initialDelaySeconds: 3
         periodSeconds: 3
       readinessProbe:
          httpGet:
            path: /ready
            port: 80
          initialDelaySeconds: 3
          periodSeconds: 3
+++++++++++++
anji@anji:~/tempfol$ kubectl describe pod nginx 
Name:             nginx
Status:           Pending
IP:               
IPs:              <none>
Containers:
  nginx:
    Container ID:   
    Image:          nginx
    Image ID:       
    Port:           <none>
    Host Port:      <none>
    State:        "  Waiting"
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
   " Liveness: "      http-get http://:80/ delay=3s timeout=1s period=3s #success=1 #"failure=3"
  "  Readiness:  "    http-get http://:80/ready delay=3s timeout=1s period=3s #success=1 #failure=3
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  0s    default-scheduler  Successfully assigned default/nginx to worker "
+++++++++++++++++++++++++++++++++
anji@anji:~/tempfol$ kubectl describe pod nginx 
Name:             nginx
Status:           Pending
    Restart Count:  0
   " Liveness:  "     http-get http://:80/ delay=3s timeout=1s period=3s #success=1 #failure=3
    "Readiness: "     http-get http://:80/ready delay=3s timeout=1s period=3s #success=1 #failure=3
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  1s    default-scheduler  "Successfully assigned default/nginx to worker"
  Normal  Pulling    1s    kubelet            Pulling image "nginx
++++++++++++++++++++++++++++++++++++++++++++

anji@anji:~/tempfol$ kubectl describe pod nginx 
Name:             nginx
Status:           Pending
   " Liveness:   "    http-get http://:80/ delay=3s timeout=1s period=3s #success=1 #failure=3
   " Readiness:  "    http-get http://:80/ready delay=3s timeout=1s period=3s #success=1 #failure=3

  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  3s    default-scheduler  Successfully assigned default/nginx to worker
  Normal  Pulling    3s    kubelet           " Pulling image "nginx""
  Normal  Pulled     0s    kubelet           " Successfully pulled image "nginx" in 2.428306861s (2.428339667s including waiting)"
  Normal  Created    0s    kubelet            Created container nginx"
++++++++++++++++++++++++++++++++++++++++++++++++++++  
anji@anji:~/tempfol$ kubectl describe pod nginx 
Name:             nginx
Node:             worker/192.168.122.226
Status:          " Running"
    Liveness:      " http-get http://:80/ delay=3s timeout=1s period=3s #success=1 #failure=3"
    Readiness:  "    http-get http://:80/ready delay=3s timeout=1s period=3s #success=1 #failure=3"

  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  4s    default-scheduler  Successfully assigned default/nginx to worker
  Normal  Pulling    4s    kubelet            Pulling image "nginx"
  Normal  Pulled     1s    kubelet            Successfully pulled image "nginx" in 2.428306861s (2.428339667s including waiting)
  Normal  Created    1s    kubelet            Created container nginx
  Normal  Started    1s    kubelet          "  Started container nginx"

+++++++++++++++++++++++++++++++++++++++++++=

anji@anji:~/tempfol$ kubectl describe pod nginx 
Name:             nginx
Status:           Running
    Liveness:      " http-get http://:80/ delay=3s timeout=1s period=3s #success=1 #failure=3"
    Readiness:     " http-get http://:80/ready delay=3s timeout=1s period=3s #success=1 #failure=3"

Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  6s    default-scheduler  Successfully assigned default/nginx to worker
  Normal   Pulling    7s    kubelet            Pulling image "nginx"
  Normal   Pulled     4s    kubelet            Successfully pulled image "nginx" in 2.428306861s (2.428339667s including waiting)
  Normal   Created    4s    kubelet            Created container nginx
  Normal   Started    4s    kubelet         "   Started container nginx"
  Warning  Unhealthy  1s    kubelet           " Readiness probe failed: HTTP probe failed with statuscode: "404""
++++++++++++++++++++++++++++++++++++  
NAME    READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
nginx   "0/1 "    Running   0          13m   10.44.0.1   worker   <none>           <none>

###################################################################
"
apiVersion: v1
kind: Pod
metadata: 
  name: busybox
spec: 
   containers: 
     - name: busybox
       image: busybox
       args:
        - /bin/sh
        - -c 
        - touch /tmp/anjireddy; sleep 30; rm -f /tmp/anjireddy; sleep 600
       livenessProbe:
          exec:
            command:
            - cat
            - /tmp/anjireddy
          initialDelaySeconds: 5
          periodSeconds: 5
+++++++++++++++++++++++++++++++++++++++=                
NAME      READY   STATUS    RESTARTS      AGE   IP          NODE     NOMINATED NODE   READINESS GATES
busybox   "1/1 "    Running   11 (8s ago)   25m   10.44.0.1   worker   <none>           <none> "
++++++
anji@anji:~/tempfol$ kubectl describe pod busybox 
Name:             busybox
Status:          " Pending"
  "  Args:
      /bin/sh
      -c
      touch /tmp/anjireddy; sleep 30; rm -f /tmp/anjireddy; sleep 600"
    State:         " Waiting"
      "Reason:       ContainerCreating"
    Ready:          False
    Restart Count:  0
   " Liveness: "      exec [cat /tmp/anjireddy] delay=5s timeout=1s period=5s #"success=1 "#failure="3"
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pckl9 (ro)
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  0s    default-scheduler  Successfully assigned "default/busybox to worker"
  Normal  Pulling    0s    kubelet            Pulling image "busybox"
+++++++++++++++++++  

anji@anji:~/tempfol$ kubectl describe pod busybox 
Name:             busybox
Node:             worker/192.168.122.226
Status:          " Pending"
    /* Args:
      /bin/sh
      -c
      touch /tmp/anjireddy; sleep 30; rm -f /tmp/anjireddy; sleep 600 */
  "  Liveness:"       exec [cat /tmp/anjireddy] delay=5s timeout=1s period=5s #success=1 #failure=3
  ----    ------     ----  ----               -------
  Normal  Scheduled  4s    default-scheduler  Successfully assigned default/busybox to worker
  Normal  Pulling    4s    kubelet            Pulling image "busybox"
++++++++++++++++++++++++++  
anji@anji:~/tempfol$ kubectl describe pod busybox 
Name:             busybox
Node:             worker/192.168.122.226
Status:           "Running"
    /* Args:
      /bin/sh
      -c
      touch /tmp/anjireddy; sleep 30; rm -f /tmp/anjireddy; sleep 600 */
    State:          "Running"
   " Liveness:   "    exec [cat /tmp/anjireddy] delay=5s timeout=1s period=5s #success=1 #failure=3
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  6s    default-scheduler  Successfully assigned default/busybox to worker
  Normal  Pulling    6s    kubelet            Pulling image "busybox"
  Normal  Pulled     1s    kubelet           " Successfully pulled image "busybox" in 5.581105576s (5.581110905s including waiting)"
  Normal  Created    1s    kubelet            Created container busybox
  Normal  Started    1s    kubelet           " Started container busybox"
+++++++++++++++
anji@anji:~$ kubectl exec -it busybox -- /bin/sh 
/ # ls
bin    dev    etc    home   lib    lib64  proc   root   sys    tmp    usr    var
/ # ls -al
total 48
drwxr-xr-x    1 root     root          4096 Mar 10 09:55 .
drwxr-xr-x    1 root     root          4096 Mar 10 09:55 ..
drwxr-xr-x    2 root     root         12288 Mar  3 23:15 bin
drwxr-xr-x    5 root     root           360 Mar 10 09:55 dev
drwxr-xr-x    1 root     root          4096 Mar 10 09:55 etc
drwxr-xr-x    2 nobody   nobody        4096 Mar  3 23:15 home
drwxr-xr-x    2 root     root          4096 Mar  3 23:15 lib
lrwxrwxrwx    1 root     root             3 Mar  3 23:15 lib64 -> lib
dr-xr-xr-x  270 root     root             0 Mar 10 09:55 proc
drwx------    1 root     root          4096 Mar 10 09:55 root
dr-xr-xr-x   13 root     root             0 Mar 10 09:55 sys
drwxrwxrwt    1 root     root          4096 Mar 10 09:55 tmp
drwxr-xr-x    4 root     root          4096 Mar  3 23:15 usr
drwxr-xr-x    1 root     root          4096 Mar 10 09:55 var
/ # cd tmp/
/tmp # ls
anjireddy           #############################  
/tmp # ls -al
total 8
drwxrwxrwt    1 root     root          4096 Mar 10 09:56 .
drwxr-xr-x    1 root     root          4096 Mar 10 09:55 
############################################################
https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/

apiVersion: v1
kind: Pod
metadata: 
   name: live
spec: 
  containers: 
    - name: live
      image:  registry.k8s.io/liveness
      args:
       - /server
      livenessProbe:
         httpGet:
           path: /healthz
           port: 8080
           httpHeaders:
             - name: Custom-Header
               value: Awesome   
         initialDelaySeconds: 3
         periodSeconds: 3
++++++++++
anji@anji:~/tempfol$ kubectl describe pod live 
Name:             live
Status:           Pending
    Args:
      /server
    "State:          Waiting"
     " Reason: "      ContainerCreating
    Ready:          False
    Restart Count:  0
 "   Liveness:   "    http-get http://:8080/healthz delay=3s timeout=1s period=3s #success=1" #failure=3

Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  0s    default-scheduler  Successfully assigned default/live to worker
  Normal  Pulling    0s    kubelet           " Pulling image "registry.k8s.io/liveness"
+++++++++++++++++++++++
anji@anji:~/tempfol$ kubectl describe pod live 
"Status:           Running"
    Liveness:       http-get http://:8080/healthz delay=3s timeout=1s period=3s #success=1 #failure=3
    Type:                    Projected (a volume that contains injected data from multiple sources)
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  4s    default-scheduler  Successfully assigned default/live to worker
  Normal  Pulling    4s    kubelet            Pulling image "registry.k8s.io/liveness"
  Normal  Pulled     2s    kubelet          "  Successfully pulled image "registry.k8s.io/liveness" in 1.797503159s (1.797534227s including waiting)"
  Normal  Created    2s    kubelet            Created container live
  Normal  Started    2s    kubelet           " Started container live"
+++++++++++++++++++++++++"  
anji@anji:~/tempfol$ kubectl describe pod live 
    Args:
      /server
    State:          Running
    Liveness:       http-get http://:8080/healthz delay=3s timeout=1s period=3s #success=1 #failure=3
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  16s   default-scheduler  Successfully assigned default/live to worker
  Normal   Pulling    16s   kubelet            Pulling image "registry.k8s.io/liveness"
  Normal   Pulled     14s   kubelet            Successfully pulled image "registry.k8s.io/liveness" in 1.797503159s (1.797534227s including waiting)
  Normal   Created    14s   kubelet            Created container live
  Normal   Started    14s   kubelet            Started container live
  Warning  Unhealthy  1s    kubelet            "Liveness probe failed: HTTP probe failed with statuscode: 500"
++++++++++++++++++++
anji@anji:~/tempfol$ kubectl describe pod live 
    Args:
      /server
    State:          Running
    Liveness:       http-get http://:8080/healthz delay=3s timeout=1s period=3s #success=1 #failure=3
  Type     Reason     Age              From               Message
  ----     ------     ----             ----               -------
  Normal   Scheduled  20s              default-scheduler  Successfully assigned default/live to worker
  Normal   Pulling    20s              kubelet            Pulling image "registry.k8s.io/liveness"
  Normal   Pulled     18s              kubelet            Successfully pulled image "registry.k8s.io/liveness" in 1.797503159s (1.797534227s including waiting)
  Normal   Created    18s              kubelet            Created container live
  Normal   Started    18s              kubelet            Started container live
  Warning  Unhealthy  2s (x2 over 5s)  kubelet           " Liveness probe failed: HTTP probe failed with statuscode: 500"
+++++++++++++==  
anji@anji:~/tempfol$ kubectl describe pod live 

Events:
  Type     Reason     Age               From               Message
  ----     ------     ----              ----               -------
  Normal   Scheduled  22s               default-scheduler  Successfully assigned default/live to worker
  Normal   Pulled     20s               kubelet            Successfully pulled image "registry.k8s.io/liveness" in 1.797503159s (1.797534227s including waiting)
  Normal   Created    20s               kubelet            Created container live
  Normal   Started    20s               kubelet            Started container live
  Normal   Pulling    1s (x2 over 22s)  kubelet            Pulling image "registry.k8s.io/liveness"
  "Warning  Unhealthy " 1s (x3 over 7s)   kubelet         "   Liveness probe failed: HTTP probe failed with statuscode: 500"
  "Normal   Killing "   1s                kubelet            C"ontainer live failed liveness probe, will be restarted"
+++++++++++++++++++++++++++++++++"
anji@anji:~/tempfol$ kubectl describe pod live 
    Restart Count: " 1"
    Liveness:       http-get http://:8080/healthz delay=3s timeout=1s period=3s #success=1 #failure=3
  Type     Reason     Age               From               Message
  ----     ------     ----              ----               -------
  Normal   Scheduled  27s               default-scheduler  Successfully assigned default/live to worker
  Normal   Pulled     24s               kubelet            Successfully pulled image "registry.k8s.io/liveness" in 1.797503159s (1.797534227s including waiting)
  Normal   Pulling    5s (x2 over 26s)  kubelet            Pulling image "registry.k8s.io/liveness"
"  Warning  Unhealthy"  5s (x3 over 11s)  kubelet        "    Liveness probe failed: HTTP probe failed with statuscode: 500"
  Normal  " Killing  "  5s                kubelet        "    Container live failed liveness probe, will be restarted"
  Normal   "Created "   2s (x2 over 24s)  kubelet            Created container live
  Normal  " Started "   2s (x2 over 24s)  kubelet            Started container live
  Normal   "Pulled  "   2s                kubelet            Successfully pulled image "registry.k8s.io/liveness" in 3.258222617s (3.258230662s including waiting)
+++++++++++++++++++++++++++++++++++"
anji@anji:~/tempfol$ kubectl describe pod live 
    Last State:    " Terminated"
      Reason:      " Error"
      Exit Code:   " 2"
    Ready:          "True"
    Restart Count:  "1"
    Liveness:       http-get http://:8080/healthz delay=3s timeout=1s period=3s #success=1 #failure=3
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  37s                default-scheduler  Successfully assigned default/live to worker
  Normal   Pulled     35s                kubelet            Successfully pulled image "registry.k8s.io/liveness" in 1.797503159s (1.797534227s including waiting)
  Normal   Pulling    16s (x2 over 37s)  kubelet            Pulling image "registry.k8s.io/liveness"
  Normal   Killing    16s                kubelet            Container live failed liveness probe, will be restarted
  Normal   Created    13s (x2 over 35s)  kubelet            Created container live
  Normal   Started    13s (x2 over 35s)  kubelet            Started container live
  Normal   Pulled     13s                kubelet            Successfully pulled image "registry.k8s.io/liveness" in 3.258222617s (3.258230662s including waiting)
  Warning  "Unhealthy"  1s (x4 over 22s)   kubelet           " Liveness probe failed: HTTP probe failed with statuscode: 500"
++++++++++++++++++++="
anji@anji:~/tempfol$ kubectl describe pod live 

Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  44s                default-scheduler  Successfully assigned default/live to worker
  Normal   Pulled     41s                kubelet            Successfully pulled image "registry.k8s.io/liveness" in 1.797503159s (1.797534227s including waiting)
  Normal   Created    19s (x2 over 41s)  kubelet            Created container live
  Normal   Started    19s (x2 over 41s)  kubelet            Started container live
  Normal   Pulled     19s                kubelet            Successfully pulled image "registry.k8s.io/liveness" in 3.258222617s (3.258230662s including waiting)
  Normal   Pulling    1s (x3 over 43s)   kubelet            Pulling image "registry.k8s.io/liveness"
  Warning  Unhealthy  1s (x6 over 28s)   kubelet            Liveness probe failed: HTTP probe failed with statuscode: 500
  Normal  " Killing  "  1s (x2 over 22s)   kubelet            "Container live failed liveness probe, will be restarted"
++++++++++++++="  
anji@anji:~/tempfol$ kubectl describe pod live 

    Restart Count:  "2"
    Liveness:       http-get http://:8080/healthz delay=3s timeout=1s period=3s #success=1 #failure=3
  ----     ------     ----              ----               -------
  Normal   Scheduled  46s               default-scheduler  Successfully assigned default/live to worker
  Normal   Pulled     44s               kubelet            Successfully pulled image "registry.k8s.io/liveness" in 1.797503159s (1.797534227s including waiting)
  Normal   Pulled     22s               kubelet            Successfully pulled image "registry.k8s.io/liveness" in 3.258222617s (3.258230662s including waiting)
  Normal   Pulling    4s (x3 over 46s)  kubelet            Pulling image "registry.k8s.io/liveness"
  Warning " Unhealthy"  4s (x6 over 31s)  kubelet            Liveness probe failed: HTTP probe failed with statuscode: 500
  Normal  " Killing   " 4s (x2 over 25s)  kubelet            Container live failed liveness probe, will be restarted
  Normal  " Created   " 2s (x3 over 44s)  kubelet            Created container live
  Normal   "Started  "  2s (x3 over 44s)  kubelet            Started container live
  Normal  " Pulled   "  2s                kubelet            Successfully pulled image "registry.k8s.io/liveness" in 1.967659206s (1.967683925s including waiting)
+++++++++++++++++++++++++++++"
anji@anji:~/tempfol$ kubectl describe pod live 
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  65s                default-scheduler  Successfully assigned default/live to worker
  Normal   Pulled     62s                kubelet            Successfully pulled image "registry.k8s.io/liveness" in 1.797503159s (1.797534227s including waiting)
  Normal   Pulled     40s                kubelet            Successfully pulled image "registry.k8s.io/liveness" in 3.258222617s (3.258230662s including waiting)
  Normal   Created    20s (x3 over 62s)  kubelet            Created container live
  Normal   Started    20s (x3 over 62s)  kubelet            Started container live
  Normal   Pulled     20s                kubelet            Successfully pulled image "registry.k8s.io/liveness" in 1.967659206s (1.967683925s including waiting)
  Normal   Pulling    1s (x4 over 64s)   kubelet            Pulling image "registry.k8s.io/liveness"
  Warning  Unhealthy  1s (x9 over 49s)   kubelet            Liveness probe failed: HTTP probe failed with statuscode: 500
  Normal "  Killing   " 1s (x3 over 43s)   kubelet            Container live failed liveness probe, will be restarted
++++++++++++++++++++++++++++="
anji@anji:~/tempfol$ kubectl get pod -o wide 
NAME   READY   STATUS             RESTARTS       AGE   IP          NODE     NOMINATED NODE   READINESS GATES
live   0/1     CrashLoopBackOff   21 (63s ago)   48m   10.44.0.1   worker   <none>           <none>
anji@anji:~/tempfol$ 
############################################################33
https://roytuts.com/docker-compose-dockerizing-flask-mysql-app/

Docker Compose â€“ Dockerizing Flask MySQL App

ImportError: cannot import name 'MySql' from 'flaskext.mysql' (/home/anji/.local/lib/python3.8/site-packages/flaskext/mysql.py)


https://www.pythonanywhere.com/forums/topic/11739/

I tried with flask_mysqldb after install flask-mysql. However, it also doesn't work saying no module named flask_mysqldb. So, in my case flaskext.mysql, flask_mysql, flask_mysqldb don't work at all on both python2.7 and 3.4.
deleted-user-2750410 | 


    mysql = MySql()
NameError: name 'MySql' is not defined

answer==  
https://pypi.org/project/flask-mysql-connector/
pip install flask-mysql-connector

from flask import Flask
from flask_mysql_connector import MySQL

app = Flask(__name__)
app.config['MYSQL_USER'] = 'root'
app.config['MYSQL_DATABASE'] = 'sys'
mysql = MySQL(app)

EXAMPLE_SQL = 'select * from sys.user_summary'


# using the new_cursor() method
@app.route('/new_cursor')
def new_cursor():
    cur = mysql.new_cursor(dictionary=True)
    cur.execute(EXAMPLE_SQL)
    output = cur.fetchall()
    return str(output)


# using the connection property
@app.route('/connection')
def connection():
    conn = mysql.connection
    cur = conn.cursor()
    cur.execute(EXAMPLE_SQL)
    output = cur.fetchall()
    return str(output)


# using the execute_sql() method to easily
# select sql and optionally output to Pandas
@app.route('/easy_execute')
def easy_execute():
    df = mysql.execute_sql(EXAMPLE_SQL, to_pandas=True)
    return str(df.to_dict())


if __name__ == '__main__':
    app.run(debug=True)

++++++++++++++++++++=
MYSQL_DATABASE_HOST 	default is â€˜localhostâ€™
MYSQL_DATABASE_PORT 	default is 3306
MYSQL_DATABASE_USER 	default is None
MYSQL_DATABASE_PASSWORD 	default is None
MYSQL_DATABASE_DB 	default is None
MYSQL_DATABASE_CHARSET 	default is â€˜utf-8â€™

https://www.askpython.com/python-modules/flask/flask-mysql-database

3. Installing Flask- MySQL library in our system

Flask uses flask_mysqldb connector to use MySQL. Run the following command to install the package:
pip install flask_mysqldb

Perfect !!
Setting up a Flask MySQL Database Connection

Now we will connect and use MySQL to store data into our DB. If youâ€™re not sure how to create a flask application, check out the flask introduction tutorial.
1. Connecting Flask Application with MySQL

The procedure we follow to connect Flask-MySQL is as follows:
from flask import Flask,render_template, request
from flask_mysqldb import MySQL
 
app = Flask(__name__)
 
app.config['MYSQL_HOST'] = 'localhost'
app.config['MYSQL_USER'] = 'root'
app.config['MYSQL_PASSWORD'] = ''
app.config['MYSQL_DB'] = 'flask'
 
mysql = MySQL(app)
2. Setting up MySQL connection cursor

Just with the above set-up, we canâ€™t interact with DB tables. For that, we need something called a cursor.

So Cursor provides a way for Flask to interact with the DB tables. It can scan through the DB data, execute different SQL queries, and as well as Delete table records.

The cursor is used in the following way:
mysql = MySQL(app)
 
#Creating a connection cursor
cursor = mysql.connection.cursor()
 
#Executing SQL Statements
cursor.execute(''' CREATE TABLE table_name(field1, field2...) ''')
cursor.execute(''' INSERT INTO table_name VALUES(v1,v2...) ''')
cursor.execute(''' DELETE FROM table_name WHERE condition ''')
 
#Saving the Actions performed on the DB
mysql.connection.commit()
 
#Closing the cursor
cursor.close()

Since MySQL is not an auto-commit DB, we need to commit manually, ie, save the changes/actions performed by the cursor execute on the DB .
3. Coding a Flask application

Now we will build a small Flask application that will store data submitted by the user in the MySQL DB Table. Consider the following Application Code:
from flask import Flask,render_template, request
from flask_mysqldb import MySQL
 
app = Flask(__name__)
 
app.config['MYSQL_HOST'] = 'localhost'
app.config['MYSQL_USER'] = 'root'
app.config['MYSQL_PASSWORD'] = ''
app.config['MYSQL_DB'] = 'flask'
 
mysql = MySQL(app)
 
@app.route('/form')
def form():
    return render_template('form.html')
 
@app.route('/login', methods = ['POST', 'GET'])
def login():
    if request.method == 'GET':
        return "Login via the login Form"
     
    if request.method == 'POST':
        name = request.form['name']
        age = request.form['age']
        cursor = mysql.connection.cursor()
        cursor.execute(''' INSERT INTO info_table VALUES(%s,%s)''',(name,age))
        mysql.connection.commit()
        cursor.close()
        return f"Done!!"
 
app.run(host='localhost', port=5000)

When the user submits the data, it is added into the MySQL DB via the cursor.execute command. My table name is info_table.

The form.html will be:
<form action="/login" method = "POST">
   <p>name <input type = "text" name = "name" /></p>
   <p>age <input type = "integer" name = "age" /></p>
   <p><input type = "submit" value = "Submit" /></p>
</form>
4. Implementing the Code

Now fire up the server and go to â€œ/formâ€ (see Flask forms)
FormForm

Enter the details and hit Submit
SuccessSuccess

Now letâ€™s check it in the phpMyAdmin web interface

https://stackoverflow.com/questions/9845102/using-mysql-in-flask



Easy with Mysql,

    Create db with following command

    CREATE TABLE MyUsers ( firstname VARCHAR(30) NOT NULL,  lastname VARCHAR(30) NOT NULL);

    Copy paste below code in app.py file

    from flask import Flask, render_template, request 
    from flask_mysqldb import MySQL

    app = Flask(__name__)


    app.config['MYSQL_HOST'] = 'localhost'
    app.config['MYSQL_USER'] = 'root'
    app.config['MYSQL_PASSWORD'] = 'root'
    app.config['MYSQL_DB'] = 'MyDB'

        mysql = MySQL(app)


    @app.route('/', methods=['GET', 'POST'])
    def index():
        if request.method == "POST":
            details = request.form
            firstName = details['fname']
            lastName = details['lname']
            cur = mysql.connection.cursor()
            cur.execute("INSERT INTO MyUsers(firstName, lastName) VALUES (%s, %s)", (firstName, lastName))
            mysql.connection.commit()
            cur.close()
            return 'success'
        return render_template('index.html')


    if __name__ == '__main__':
           app.run()

'
++++++++++++++++
https://www.geeksforgeeks.org/profile-application-using-python-flask-and-mysql/

# Store this code in 'app.py' file
from flask import Flask, render_template, request, redirect, url_for, session
from flask_mysqldb import MySQL
import MySQLdb.cursors
import re


app = Flask(__name__)


app.secret_key = 'your secret key'


app.config['MYSQL_HOST'] = 'localhost'
app.config['MYSQL_USER'] = 'root'
app.config['MYSQL_PASSWORD'] = 'password'
app.config['MYSQL_DB'] = 'geekprofile'


mysql = MySQL(app)


@app.route('/')
@app.route('/login', methods =['GET', 'POST'])
def login():
	msg = ''
	if request.method == 'POST' and 'username' in request.form and 'password' in request.form:
		username = request.form['username']
		password = request.form['password']
		cursor = mysql.connection.cursor(MySQLdb.cursors.DictCursor)
		cursor.execute('SELECT * FROM accounts WHERE username = % s AND password = % s', (username, password, ))
		account = cursor.fetchone()
		if account:
			session['loggedin'] = True
			session['id'] = account['id']
			session['username'] = account['username']
			msg = 'Logged in successfully !'
			return render_template('index.html', msg = msg)
		else:
			msg = 'Incorrect username / password !'
	return render_template('login.html', msg = msg)

@app.route('/logout')
def logout():
session.pop('loggedin', None)
session.pop('id', None)
session.pop('username', None)
return redirect(url_for('login'))

@app.route('/register', methods =['GET', 'POST'])
def register():
	msg = ''
	if request.method == 'POST' and 'username' in request.form and 'password' in request.form and 'email' in request.form and 'address' in request.form and 'city' in request.form and 'country' in request.form and 'postalcode' in request.form and 'organisation' in request.form:
		username = request.form['username']
		password = request.form['password']
		email = request.form['email']
		organisation = request.form['organisation']
		address = request.form['address']
		city = request.form['city']
		state = request.form['state']
		country = request.form['country']
		postalcode = request.form['postalcode']
		cursor = mysql.connection.cursor(MySQLdb.cursors.DictCursor)
		cursor.execute('SELECT * FROM accounts WHERE username = % s', (username, ))
		account = cursor.fetchone()
		if account:
			msg = 'Account already exists !'
		elif not re.match(r'[^@]+@[^@]+\.[^@]+', email):
			msg = 'Invalid email address !'
		elif not re.match(r'[A-Za-z0-9]+', username):
			msg = 'name must contain only characters and numbers !'
		else:
			cursor.execute('INSERT INTO accounts VALUES (NULL, % s, % s, % s, % s, % s, % s, % s, % s, % s)', (username, password, email, organisation, address, city, state, country, postalcode, ))
			mysql.connection.commit()
			msg = 'You have successfully registered !'
	elif request.method == 'POST':
		msg = 'Please fill out the form !'
	return render_template('register.html', msg = msg)


@app.route("/index")
def index():
	if 'loggedin' in session:
		return render_template("index.html")
	return redirect(url_for('login'))


@app.route("/display")
def display():
	if 'loggedin' in session:
		cursor = mysql.connection.cursor(MySQLdb.cursors.DictCursor)
		cursor.execute('SELECT * FROM accounts WHERE id = % s', (session['id'], ))
		account = cursor.fetchone()
		return render_template("display.html", account = account)
	return redirect(url_for('login'))

@app.route("/update", methods =['GET', 'POST'])
def update():
	msg = ''
	if 'loggedin' in session:
		if request.method == 'POST' and 'username' in request.form and 'password' in request.form and 'email' in request.form and 'address' in request.form and 'city' in request.form and 'country' in request.form and 'postalcode' in request.form and 'organisation' in request.form:
			username = request.form['username']
			password = request.form['password']
			email = request.form['email']
			organisation = request.form['organisation']
			address = request.form['address']
			city = request.form['city']
			state = request.form['state']
			country = request.form['country']
			postalcode = request.form['postalcode']
			cursor = mysql.connection.cursor(MySQLdb.cursors.DictCursor)
			cursor.execute('SELECT * FROM accounts WHERE username = % s', (username, ))
			account = cursor.fetchone()
			if account:
				msg = 'Account already exists !'
			elif not re.match(r'[^@]+@[^@]+\.[^@]+', email):
				msg = 'Invalid email address !'
			elif not re.match(r'[A-Za-z0-9]+', username):
				msg = 'name must contain only characters and numbers !'
			else:
				cursor.execute('UPDATE accounts SET username =% s, password =% s, email =% s, organisation =% s, address =% s, city =% s, state =% s, country =% s, postalcode =% s WHERE id =% s', (username, password, email, organisation, address, city, state, country, postalcode, (session['id'], ), ))
				mysql.connection.commit()
				msg = 'You have successfully updated !'
		elif request.method == 'POST':
			msg = 'Please fill out the form !'
		return render_template("update.html", msg = msg)
	return redirect(url_for('login'))

if __name__ == "__main__":
	app.run(host ="localhost", port = int("5000"))
++++++++++++=
https://www.codementor.io/@adityamalviya/python-flask-mysql-connection-rxblpje73

from flask import Flask, render_template, request
from flask_mysqldb import MySQL
app = Flask(__name__)


app.config['MYSQL_HOST'] = 'localhost'
app.config['MYSQL_USER'] = 'root'
app.config['MYSQL_PASSWORD'] = 'root'
app.config['MYSQL_DB'] = 'MyDB'

mysql = MySQL(app)


@app.route('/', methods=['GET', 'POST'])
def index():
    if request.method == "POST":
        details = request.form
        firstName = details['fname']
        lastName = details['lname']
        cur = mysql.connection.cursor()
        cur.execute("INSERT INTO MyUsers(firstName, lastName) VALUES (%s, %s)", (firstName, lastName))
        mysql.connection.commit()
        cur.close()
        return 'success'
    return render_template('index.html')


if __name__ == '__main__':
    app.run()

+++++++++++++++++=
https://devopscube.com/setup-grafana-kubernetes/

Grafana HTTP Error Bad Gateway and Templating init failed errors

https://stackoverflow.com/questions/48338122/grafana-http-error-bad-gateway-and-templating-init-failed-errors

Add Data Source from grafana, got HTTP Error Bad Gateway error:

HTTP Error Bad Gateway when using prometheus

https://devopscube.com/setup-prometheus-monitoring-on-kubernetes/
https://hub.docker.com/r/prom/prometheus

http://192.168.122.226:32000/dashboards
http://192.168.122.226:31000/

Error reading Prometheus: Post "http://localhost:9090/api/v1/query": dial tcp [::1]:9090: connect: connection refused

https://substrate.stackexchange.com/questions/2738/error-reading-prometheus-post-https-localhost9090-api-v1-query-dial-tcp
Getting error "Get http://localhost:9443/metrics: dial tcp 127.0.0.1:9443: connect: connection refused"
https://substrate.stackexchange.com/questions/2738/error-reading-prometheus-post-https-localhost9090-api-v1-query-dial-tcp

https://github.com/substrate-developer-hub/substrate-docs/pull/1040

Error reading Prometheus: Post "https://localhost:9090/api/v1/query": dial tcp [::1]:9090: connect: connection refused [closed]

https://octopus.com/blog/kubernetes-pod-cpu-memory

dell@worker:~$ kubectl top pod
error: Metrics API not available
dell@worker:~$ kubectl top node
error: Metrics API not available
dell@worker:~$ kubectl exec -it podname -- sh
Error from server (NotFound): pods "podname" not found
dell@worker:~$ kubectl exec -it nginx  -- sh
# ls
bin   docker-entrypoint.d   home   media  proc	sbin  tmp
boot  docker-entrypoint.sh  lib    mnt	  root	srv   usr
dev   etc		    lib64  opt	  run	sys   var
# ^C
# exit
command terminated with exit code 130
dell@worker:~$ cat /sys/fs/cgroup/memory/memory.usage_in_bytes
3219169280
dell@worker:~$ cat /sys/fs/cgroup/cpu/cpuacct.usage
2838934185674
dell@worker:~$ cat /sys/fs/cgroup/memory.current
cat: /sys/fs/cgroup/memory.current: No such file or directory
dell@worker:~$ cat /sys/fs/cgroup/cpu.stat
cat: /sys/fs/cgroup/cpu.stat: No such file or directory
dell@worker:~$ 

how to find utilise ram cpu pod in kubernetes  commands
Checking Kubernetes pod CPU and memory
https://mail.google.com/mail/u/0/#search/real/KtbxLvHTCLhwjSzlKqMxBhlLCGBftTnmQV
Get CPU and Memory Usage of NODES and PODS - Kubectl | K8s
https://www.middlewareinventory.com/blog/cpu-memory-usage-nodes-k8s/
Checking kubernetes pod CPU and memory
https://stackoverflow.com/questions/54531646/checking-kubernetes-pod-cpu-and-memory

kubectl exec nginx -n default -- cat /sys/fs/cgroup/cpu/cpuacct.usage

anji@anji:~/tempfol$ kubectl get pod
NAME                          READY   STATUS    RESTARTS       AGE
grafana-6f459ddbcf-9tf8k      1/1     Running   1 (4h9m ago)   16h
nginx                         1/1     Running   0              8m32s
prometheus-54f6c6d9ff-n6d79   1/1     Running   1 (4h9m ago)   15h
anji@anji:~/tempfol$ kubectl top pod nginx 
error: Metrics API not available
anji@anji:~/tempfol$ kubectl exec nginx -n default -- cat /sys/fs/cgroup/cpu/cpuacct.usage
67683061
anji@anji:~/tempfol$ 

https://stackoverflow.com/questions/52694238/kubectl-top-node-error-metrics-not-available-yet-using-metrics-server-as-he
kubectl top node `error: metrics not available yet` . Using metrics-server as Heapster Depricated
error: Metrics API not available

meliwex@hetzner:~$ kubectl get pods
No resources found in default namespace.
meliwex@hetzner:~$ kubectl create deployment nginx-dpl --image=nginx:latest
deployment.apps/nginx-dpl created
meliwex@hetzner:~$ kubectl create deployment wordpress-dpl --image=wordpress:latest
deployment.apps/wordpress-dpl created
meliwex@hetzner:~$ kubectl create deployment httpd-dpl --image=httpd:latest
deployment.apps/httpd-dpl created
meliwex@hetzner:~$ kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
httpd-dpl-5b7f5d99bd-qjvls       1/1     Running   0          2m17s
nginx-dpl-f4bff5b4-4g88b         1/1     Running   0          3m56s
wordpress-dpl-869dd745bc-hl8wf   1/1     Running   0          3m30s
meliwex@hetzner:~$ kubectl top pods 
NAME                             CPU(cores)   MEMORY(bytes)   
httpd-dpl-5b7f5d99bd-qjvls       1m           7Mi             
wordpress-dpl-869dd745bc-hl8wf   1m           22Mi            
meliwex@hetzner:~$ 
+++++++++++++++++++++++++++++++++++++++++++++
https://stackoverflow.com/questions/52991038/how-to-create-a-servicemonitor-for-prometheus-operator
https://medium.com/kubernetes-tutorials/simple-management-of-prometheus-monitoring-pipeline-with-the-prometheus-operator-b445da0e0d1a

anji@anji:~/tempfol$ kubectl describe serviceaccount 
Name:                default
Namespace:           default
Labels:              <none>
Annotations:         <none>
Image pull secrets:  <none>
Mountable secrets:   <none>
Tokens:              <none>
Events:              <none>


Name:                prometheus-operator
Namespace:           default
Labels:              <none>
Annotations:         <none>
Image pull secrets:  <none>
Mountable secrets:   <none>
Tokens:              <none>
Events:              <none>
anji@anji:~/tempfol$ kubectl get ns
NAME              STATUS   AGE
argocd            Active   50d
default           Active   52d
hp                Active   36d
kick              Active   36d
kube-node-lease   Active   52d
kube-public       Active   52d
kube-system       Active   52d
anji@anji:~/tempfol$ kubectl get sa
NAME                  SECRETS   AGE
default               0         52d
prometheus-operator   0         89s
##########################
anji@anji:~/tempfol$ kubectl describe svc rpc-app-service
Name:              rpc-app-service
Namespace:         default
Labels:            app=rpc-app
Annotations:       <none>
Selector:          app=rpc-app
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.103.113.188
IPs:               10.103.113.188
Port:              web  8081/TCP
TargetPort:        8081/TCP
Endpoints:         10.44.0.5:8081
Session Affinity:  None
Events:            <none>
anji@anji:~/tempfol$ 

################################################333
https://blog.container-solutions.com/prometheus-operator-beginners-guide

kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/bundle.yaml

anji@anji:~/tempfol/prometheus-operator/contrib/kube-prometheus$ kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/bundle.yaml
customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com configured
customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com configured
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com configured
customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com configured
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com configured
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com configured
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com configured
clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator unchanged
clusterrole.rbac.authorization.k8s.io/prometheus-operator unchanged
serviceaccount/prometheus-operator unchanged
service/prometheus-operator unchanged

anji@anji:~/tempfol$ kubectl get crds
NAME                                        CREATED AT
alertmanagerconfigs.monitoring.coreos.com   2023-03-21T11:00:21Z
alertmanagers.monitoring.coreos.com         2023-03-21T11:00:21Z
podmonitors.monitoring.coreos.com           2023-03-21T11:00:21Z
probes.monitoring.coreos.com                2023-03-21T11:00:21Z
prometheusrules.monitoring.coreos.com       2023-03-21T11:00:21Z
servicemonitors.monitoring.coreos.com       2023-03-21T11:00:21Z
thanosrulers.monitoring.coreos.com          2023-03-21T11:00:21Z

anji@anji:~/tempfol$ kubectl get deploy 
NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
grafana               1/1     1            1           42h
prometheus            1/1     1            1           41h
prometheus-operator   0/1     1            0           39m
rpc-app-deployment    1/1     1            1           36m

anji@anji:~/tempfol$ kubectl get pods 
NAME                                   READY   STATUS             RESTARTS       AGE
grafana-6f459ddbcf-9tf8k               1/1     Running            3 (96m ago)    42h
nginx                                  1/1     Running            2 (96m ago)    26h
prometheus-54f6c6d9ff-n6d79            1/1     Running            3 (96m ago)    41h
prometheus-operator-799cfc9bc9-lp6jp   0/1     CrashLoopBackOff   12 (18s ago)   40m
rpc-app-deployment-69b9878f68-r9cqj    1/1     Running            0              37m

anji@anji:~/tempfol$ kubectl get service
NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
prometheus-operator   ClusterIP   None             <none>        8080/TCP         4m46s


RBAC Permissions

The Prometheus server needs access to the Kubernetes API to scrape targets and reach the Alertmanager clusters. Therefore, a ServiceAccount is required to provide access to those resources, which must be created and bound to a ClusterRole accordingly:

apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/metrics
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: default
++++++++++++++++++++++++++++++++++++++++++=
anji@anji:~/tempfol$ kubectl describe clusterrolebinding prometheus
Name:         prometheus-operator
Labels:       app.kubernetes.io/component=controller
              app.kubernetes.io/name=prometheus-operator
              app.kubernetes.io/version=0.63.0
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  prometheus-operator
Subjects:
  Kind            Name                 Namespace
  ----            ----                 ---------
  ServiceAccount  prometheus-operator  default
+++++++++++++++++++++++++++++++
Prometheus

After creating the Prometheus ServiceAccount and giving it access to the Kubernetes API, we can deploy the Prometheus instance.

Create a file prometheus.yaml with this content:

apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
spec:
  serviceAccountName: prometheus
  serviceMonitorNamespaceSelector: {}
  serviceMonitorSelector: {}
  podMonitorSelector: {}
  resources:
    requests:
      memory: 400Mi
+++++++++++++++++++++++++++++++++++++++++++
ServiceMonitor

The operator uses ServiceMonitors to define a set of targets to be monitored by Prometheus. It uses label selectors to define which Services to monitor, the namespaces to look for, and the port on which the metrics are exposed.

Create a file service-monitor.yaml with the following content to add a ServiceMonitor so that the Prometheus server scrapes only its own metrics endpoints:

apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prometheus
  labels:
    name: prometheus
spec:
  selector:
    matchLabels:
      operated-prometheus: "true"
  namespaceSelector:
    any: true
  endpoints:
    - port: web
+++++++++++++++++++++++++++==
PodMonitor

There could be use cases that require scraping Pods directly, without direct association with services (for instance scraping sidecars). The operator also includes a PodMonitor CR, which is used to declaratively specify groups of pods that should be monitored.

As an example, weâ€™re using the front-end app from the microservices-demo project, which, as we mentioned before, simulates the user-facing part of an e-commerce website that exposes a /metrics endpoint.

Define a PodMonitor in a manifest file podmonitor.yaml to select only this deployment pod from the sock-shop namespace. Even though it could be selected using a ServiceMonitor, weâ€™ve used a targetPort field instead. This is because the pod exposes metrics on port 8079 and doesnâ€™t include a port name:

apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: front-end
  labels:
    name: front-end
spec:
  namespaceSelector:
    matchNames:
      - sock-shop
  selector:
    matchLabels:
      name: front-end
  podMetricsEndpoints:
  - targetPort: 8079
###############################3
Alertmanager

The Prometheus Operator also introduces an Alertmanager resource, which allows users to declaratively describe an Alertmanager cluster. It also adds an AlertmanagerConfig CR, which allows users to declaratively describe Alertmanager configurations.

First, create an alertmanager-config.yaml file to define an AlertmanagerConfig resource that sends notifications to a non-existent wechat receiver and its corresponding Secret file:

apiVersion: monitoring.coreos.com/v1alpha1
kind: AlertmanagerConfig
metadata:
  name: config-alertmanager
  labels:
    alertmanagerConfig: socks-shop
spec:
  route:
    groupBy: ['job']
    groupWait: 30s
    groupInterval: 5m
    repeatInterval: 12h
    receiver: 'wechat-socks-shop'
  receivers:
  - name: 'wechat-socks-shop'
    wechatConfigs:
    - apiURL: 'http://wechatserver:8080/'
      corpID: 'wechat-corpid'
      apiSecret:
        name: 'wechat-config'
        key: 'apiSecret'
---
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: wechat-config
data:
  apiSecret: cGFzc3dvcmQK
---
apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  name: socks-shop
spec:
  replicas: 1
  alertmanagerConfigSelector:
    matchLabels:
      alertmanagerConfig: example
PrometheusRules

The PrometheusRule CR supports defining one or more RuleGroups. These groups consist of a set of rule objects that can represent either of the two types of rules supported by Prometheus, recording or alerting.

As an example, create the prometheus-rule.yaml file with the following PrometheusRule that will always trigger an alert:

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  creationTimestamp: null
  labels:
    prometheus: socks-shop
    role: alert-rules
  name: prometheus-example-rules
spec:
  groups:
  - name: ./example.rules
    rules:
    - alert: ExampleAlert
      expr: vector(1)
---
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
spec:
  serviceAccountName: prometheus
  serviceMonitorNamespaceSelector: {}
  serviceMonitorSelector: {}
  podMonitorSelector: {}
  additionalScrapeConfigs:
    name: additional-scrape-configs
    key: prometheus-additional-job.yaml
  resources:
    requests:
      memory: 400Mi
  enableAdminAPI: false
  alerting:
    alertmanagers:
    - namespace: default
      name: alertmanager-operated
      port: web
  ruleSelector:
    matchLabels:
      role: alert-rules
      prometheus: socks-shop
+++++++++++++++++++++++++++++++++++++++      
custom resource definition(CRD). 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus-operator
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    k8s-app: prometheus-operator
  name: prometheus-operator
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: prometheus-operator
  template:
    metadata:
      labels:
        k8s-app: prometheus-operator
    spec:
      containers:
      - args:
        - --kubelet-service=kube-system/kubelet
        - --config-reloader-image=quay.io/coreos/configmap-reload
        image: quay.io/coreos/prometheus-operator
        name: prometheus-operator
        ports:
        - containerPort: 8080
          name: http
        resources:
          limits:
            cpu: 30m
            memory: 20Mi
          requests:
            cpu: 20m
            memory: 10Mi
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: prometheus-operator  
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rpc-app-deployment
spec:
  selector:
    matchLabels:
      app: rpc-app
  replicas: 1
  template:
    metadata:
      labels:
        app: rpc-app
    spec:
      containers:
      - name: rpc-app-cont
        image: supergiantkir/prometheus-test-app
        ports:
        - name: web
          containerPort: 8081  
---
apiVersion: v1
kind: Service
metadata:
  name: rpc-app-service
  labels:
    app: rpc-app
spec:
  ports:
  - name: web
    port: 8081
    targetPort: 8081
    protocol: TCP
  selector:
    app: rpc-app

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: rpc-app
  labels:
    env: production
spec:
  selector:
    matchLabels:
      app: rpc-app
  endpoints:
  - port: web
---
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
spec:
  serviceAccountName: prometheus
  serviceMonitorNamespaceSelector: {}
  serviceMonitorSelector: {}
  podMonitorSelector: {}
  resources:
    requests:
      memory: 400Mi
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prometheus
  labels:
    name: prometheus
spec:
  selector:
    matchLabels:
      operated-prometheus: "true"
  namespaceSelector:
    any: true
  endpoints:
    - port: web
---
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: front-end
  labels:
    name: front-end
spec:
  namespaceSelector:
    matchNames:
      - sock-shop
  selector:
    matchLabels:
      name: front-end
  podMetricsEndpoints:
  - targetPort: 8079
---
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
spec:
  serviceAccountName: prometheus
  serviceMonitorNamespaceSelector: {}
  serviceMonitorSelector: {}
  podMonitorSelector: {}
  additionalScrapeConfigs:
    name: additional-scrape-configs
    key: prometheus-additional-job.yaml
  resources:
    requests:
      memory: 400Mi
  enableAdminAPI: false              
---
apiVersion: monitoring.coreos.com/v1alpha1
kind: AlertmanagerConfig
metadata:
  name: config-alertmanager
  labels:
    alertmanagerConfig: socks-shop
spec:
  route:
    groupBy: ['job']
    groupWait: 30s
    groupInterval: 5m
    repeatInterval: 12h
    receiver: 'wechat-socks-shop'
  receivers:
  - name: 'wechat-socks-shop'
    wechatConfigs:
    - apiURL: 'http://wechatserver:8080/'
      corpID: 'wechat-corpid'
      apiSecret:
        name: 'wechat-config'
        key: 'apiSecret'
---
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: wechat-config
data:
  apiSecret: cGFzc3dvcmQK
---
apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  name: socks-shop
spec:
  replicas: 1
  alertmanagerConfigSelector:
    matchLabels:
      alertmanagerConfig: example
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  creationTimestamp: null
  labels:
    prometheus: socks-shop
    role: alert-rules
  name: prometheus-example-rules
spec:
  groups:
  - name: ./example.rules
    rules:
    - alert: ExampleAlert
      expr: vector(1)
---
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
spec:
  serviceAccountName: prometheus
  serviceMonitorNamespaceSelector: {}
  serviceMonitorSelector: {}
  podMonitorSelector: {}
  additionalScrapeConfigs:
    name: additional-scrape-configs
    key: prometheus-additional-job.yaml
  resources:
    requests:
      memory: 400Mi
  enableAdminAPI: false
  alerting:
    alertmanagers:
    - namespace: default
      name: alertmanager-operated
      port: web
  ruleSelector:
    matchLabels:
      role: alert-rules
      prometheus: socks-shop
---
---
apiVersion: v1
kind: DaemonSet
metadata:
  labels:
    app: node-exporter
  name: node-exporter
  namespace: prometheus
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: node-exporter
    spec:
      containers:
      - args:
        - --web.listen-address=0.0.0.0:9100
        - --path.procfs=/host/proc
        - --path.sysfs=/host/sys
        image: quay.io/prometheus/node-exporter:v0.18.1
        imagePullPolicy: IfNotPresent
        name: node-exporter
        ports:
        - containerPort: 9100
          hostPort: 9100
          name: metrics
          protocol: TCP
        resources:
          limits:
            cpu: 200m
            memory: 50Mi
          requests:
            cpu: 100m
            memory: 30Mi
        volumeMounts:
        - mountPath: /host/proc
          name: proc
          readOnly: true
        - mountPath: /host/sys
          name: sys
          readOnly: true
      hostNetwork: true
      hostPID: true
      restartPolicy: Always
      tolerations:
      - effect: NoSchedule
        operator: Exists
      - effect: NoExecute
        operator: Exists
      volumes:
      - hostPath:
          path: /proc
          type: ""
        name: proc
      - hostPath:
          path: /sys
          type: ""
        name: sys
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: node-exporter
  name: node-exporter
  namespace: prometheus
spec:
  ports:
  - name: node-exporter
    port: 9100
    protocol: TCP
    targetPort: 9100
  selector:
    app: node-exporter
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app: node-exporter
    serviceMonitorSelector: prometheus
  name: node-exporter
  namespace: prometheus
spec:
  endpoints:
  - honorLabels: true
    interval: 30s
    path: /metrics
    targetPort: 9100
  jobLabel: node-exporter
  namespaceSelector:
    matchNames:
    - prometheus
  selector:
    matchLabels:
      app: node-exporter
+++++++++++++++++++++++++++++++++++++++++++++++=#################################
# GRAFANA   PROMETHEUS  SUCCESSFULL DEPLOYMENT  = OK 

apiVersion: apps/v1
kind: Deployment
metadata: 
   name: grafana
   labels:
      app: grafana
spec: 
  replicas: 1
  selector: 
     matchLabels:
        app: grafana
  template:
     metadata:
       labels:
          app: grafana
     spec:
       containers:
        - name: grafana
          image:  grafana/grafana:latest
          ports:
            - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata: 
   name: grafana
spec: 
   selector: 
     app: grafana
   ports:
     - port: 3000
       targetPort: 3000
       nodePort: 32000
   type: NodePort
         
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus

  labels:
    app: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
        - name: prometheus
          image: prom/prometheus
          ports:
             - containerPort: 9090
---
apiVersion: v1
kind: Service
metadata: 
   name:  prometheus
spec: 
   selector: 
     app: prometheus
   ports:
     - port: 9090
       targetPort: 9090
       nodePort: 31000
   type: NodePort
########################################################
https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-ansible-on-ubuntu-22-04
ANSIBLE  ANSIBLE ANSIBLE
https://www.cyberciti.biz/faq/how-to-install-and-configure-latest-version-of-ansible-on-ubuntu-linux/
How do I install Ansible on a Ubuntu Linux 16.04/18.04/20.04/22.04 LTS or 22.10 desktop control 


How To Install and Configure Ansible on Ubuntu 22.04

sudo apt update -y && apt upgrade -y 

sudo apt-add-repository  ppa:ansible/ansible
sudo apt update 
sudo apt install ansible
sudo nano /etc/ansible/hosts
  
[all]
  app1 ansible_host=192.168.122.68
  app2 ansible_host=192.168.122.229

[all:vars]
  ansible_python_interpreter=/usr/bin/python3
+++++++++++++
ansible-inventory --list -y

apt-add-repository ppa
https://itsfoss.com/ppa-guide/

What is PPA?

PPA stands for Personal Package Archive. The PPA allows application developers and Linux users to create their own repositories to distribute software. With PPA, you can easily get newer software version or software that are not available via the official Ubuntu repositories.

Why is PPA used?

As you can see, Ubuntu controls what software and more importantly which version of a software you get on your system. But imagine if a software developer releases a new version of the software.

Ubuntu wonâ€™t make it available immediately. There is a procedure to check if the new version of the software is compatible with the system or not. This ensures the stability of the system.

But this also means that it will be some weeks or in some cases, some months before it is made available by Ubuntu. Not everyone would want to wait that long to get their hands on the new version of their favorite software.

Similarly, suppose someone develops a software and wants Ubuntu to include that software in the official repositories. It again will take months before Ubuntu makes a decision and includes it in the official repositories.

Another case would be during  beta testing. Even if a stable version of the software is available in the official repositories, a software developer may want some end users to test their upcoming release. How do they enable the end user to beta test the upcoming release?

Enter PPA!
How to use PPA? How does PPA work?

PPA, as I already told you, means Personal Package Archive. Mind the word â€˜Personalâ€™ here. That gives the hint that this is something exclusive to a developer and is not officially endorsed by the distribution.

Ubuntu provides a platform called Launchpad that enables software developers to create their own repositories. An end user i.e. you can add the PPA repository to your sources.list and when you update your system, your system would know about the availability of this new software and you can install it using the standard sudo apt install command like this.

######################################################################################################33
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor

apiVersion: v1
kind: ServiceAccount
==========
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
====================
apiVersion: monitoring.coreos.com/v1
kind: Prometheus

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
===========================
apiVersion: monitoring.coreos.com/v1alpha1
kind: AlertmanagerConfig

apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
=====================
#PodMonitor : = 
         The PodMonitor custom resource definition (CRD) allows 
to declaratively define how a dynamic "set of pods "should be monitored. 
Which pods are selected to be monitored with the desired configuration is defined using "label selections."

#ServiceMonitor: =
       The ServiceMonitor custom resource definition (CRD) allows
 to declaratively define how a dynamic "set of services "should be monitored. 
 Which services are selected to be monitored with the desired configuration is defined using" label selections."

#######################################
node_exporter   Download Node Exporter

https://developer.couchbase.com/tutorial-node-exporter-setup

https://prometheus.io/download/  

wget  https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gz

Prometheus Node Exporter Setup

    This step is optional, but enables system metric ingestion using the Node Exporter agent
    Learn how to gather metrics from each server running Couchbase

The Node Exporter is an agent that gathers system metrics and exposes them in a format which can be ingested by Prometheus. The Node Exporter is a project that is maintained through the Prometheus project. This is a completely optional step and can be skipped if you do not wish to gather system metrics. The following will need to be performed on each server that you wish to monitor system metrics for.
Download Node Exporter

Download the Node Exporter binary to each Couchbase Server that you want to monitor. The Node Exporter will export system related stats.

wget \
  https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gz

Visit the Prometheus downloads page for the latest version.
Create User

Create a Node Exporter user, required directories, and make prometheus user as the owner of those directories.

sudo groupadd -f node_exporter
sudo useradd -g node_exporter --no-create-home --shell /bin/false node_exporter
sudo mkdir /etc/node_exporter
sudo chown node_exporter:node_exporter /etc/node_exporter

Unpack Node Exporter Binary

Untar and move the downloaded Node Exporter binary

tar -xvf node_exporter-1.0.1.linux-amd64.tar.gz
mv node_exporter-1.0.1.linux-amd64 node_exporter-files

Install Node Exporter

Copy node_exporter binary from node_exporter-files folder to /usr/bin and change the ownership to prometheus user.

sudo cp node_exporter-files/node_exporter /usr/bin/
sudo chown node_exporter:node_exporter /usr/bin/node_exporter

Setup Node Exporter Service

Create a node_exporter service file.

sudo vi /usr/lib/systemd/system/node_exporter.service

Add the following configuration

[Unit]
Description=Node Exporter
Documentation=https://prometheus.io/docs/guides/node-exporter/
Wants=network-online.target
After=network-online.target

[Service]
User=node_exporter
Group=node_exporter
Type=simple
Restart=on-failure
ExecStart=/usr/bin/node_exporter \
  --web.listen-address=:9200

[Install]
WantedBy=multi-user.target

sudo chmod 664 /usr/lib/systemd/system/node_exporter.service

** Note: The default port for the node_exporter is actually :9100 but that is the same port as the Couchbase Index Admin Port and cannot be used.
Reload systemd and Start Node Exporter

Reload the systemd service to register the prometheus service and start the prometheus service.

sudo systemctl daemon-reload
sudo systemctl start node_exporter

Check the node exporter service status using the following command.

sudo systemctl status node_exporter

Node Exporter Status

Configure node_exporter to start at boot

sudo systemctl enable node_exporter.service

If firewalld is enabled and running, add a rule for port 9200

sudo firewall-cmd --permanent --zone=public --add-port=9200/tcp
sudo firewall-cmd --reload

Verify Node Exporter is Running

Verify the exporter is running by visiting the /metrics endpoint on the node on port 9200

http://<node_exporter-ip>:9200/metrics

You should be able to see something similar to the following:

# HELP go_gc_duration_seconds A summary of the GC invocation durations.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 0
go_gc_duration_seconds{quantile="0.25"} 0
go_gc_duration_seconds{quantile="0.5"} 0
go_gc_duration_seconds{quantile="0.75"} 0
go_gc_duration_seconds{quantile="1"} 0
go_gc_duration_seconds_sum 0
go_gc_duration_seconds_count 0
# HELP go_goroutines Number of goroutines that currently exist.
# TYPE go_goroutines gauge
go_goroutines 7
# HELP go_info Information about the Go environment.
# TYPE go_info gauge
go_info{version="go1.12.5"} 1
# HELP go_memstats_alloc_bytes Number of bytes allocated and still in use.
# TYPE go_memstats_alloc_bytes gauge
go_memstats_alloc_bytes 919280
...

Clean Up

Remove the download and temporary files

rm -rf node_exporter-1.0.1.linux-amd64.tar.gz node_exporter-files
#################################################################################33
Ubuntu How to Install Prometheus on Ubuntu 20.04 LTS

https://linuxhint.com/install-prometheus-on-ubuntu/

$ chmod a+r file.pl

Delete execute permission for all everyone (a):
$ chmod a-x myscript.sh

Adds read and execute permissions for everyone (a):
$ chmod a+rx pager.pl

Next, sets read and write permission for user, sets read for group, and remove all access for others:
$ chmod u=rw,g=r,o= birthday.cgi

In this file example, sets read and write permissions for user and group:
$ chmod ug=rw /var/www/html/data.php

 chown root:root /usr/local/bin/"something"
$ chmod u+s /usr/local/bin/"something"

#########################################################################3333
ERROR: Ansible requires the locale encoding to be UTF-8; Detected ISO8859-1.

root ALL=(ALL:ALL) ALL The first field indicates the username that the rule will apply to (root).

root ALL=(ALL:ALL) ALL The first â€œALLâ€ indicates that this rule applies to all hosts.

root ALL=(ALL:ALL) ALL This â€œALLâ€ indicates that the root user can run commands as all users.

root ALL=(ALL:ALL) ALL This â€œALLâ€ indicates that the root user can run commands as all groups.

root ALL=(ALL:ALL) ALL The last â€œALLâ€ indicates these rules apply to all commands.

----------------------------------------------------------------------
ell@dell:~/playbooks$ ansible-playbook nginx.yaml 

PLAY [all] *********************************************************************

TASK [Gathering Facts] *********************************************************
fatal: [192.168.122.104]: FAILED! => {"msg": "Missing sudo password"}
fatal: [192.168.122.153]: FAILED! => {"msg": "Missing sudo password"}

PLAY RECAP *********************************************************************
192.168.122.104            : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
192.168.122.153            : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   

dell@dell:~/playbooks$ ansible-playbook nginx.yaml 

PLAY [all] *********************************************************************

TASK [Gathering Facts] *********************************************************
fatal: [192.168.122.104]: FAILED! => {"msg": "Missing sudo password"}
fatal: [192.168.122.153]: FAILED! => {"msg": "Missing sudo password"}

PLAY RECAP *********************************************************************
192.168.122.104            : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
192.168.122.153            : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   

+++++++++++++++++++++++++++++++++
---
- hosts: all
  gather_facts: False
  tasks: 
    - name: install nginx
      apt: name=nginx state=latest
++++++++++++++++++++++
dell@dell:~/playbooks$ ansible-playbook nginx.yaml 

PLAY [all] *********************************************************************

TASK [Gathering Facts] *********************************************************
ok: [192.168.122.153]
ok: [192.168.122.104]

TASK [install nginx] ***********************************************************
fatal: [192.168.122.104]: FAILED! => {"cache_update_time": 1679828579, "cache_updated": false, "changed": false, "msg": "'/usr/bin/apt-get -y -o 
\"Dpkg::Options::=--force-confdef\" -o \"Dpkg::Options::=--force-confold\"       install 'nginx'' failed: E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\nE: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n", "rc": 100, "stderr": 
"E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\nE: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n", "stderr_lines": ["E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)",
 "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?"], "stdout": "", "stdout_lines": []}
fatal: [192.168.122.153]: FAILED! => {"cache_update_time": 1679888479, "cache_updated": false, "changed": false, "msg": "'/usr/bin/apt-get -y -o \"Dpkg::Options::=--force-confdef\" -o \"Dpkg::Options::=--force-confold\"       install 'nginx'' failed: E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\nE: Unable to acquire the 
dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n", "rc": 100,
 "stderr": "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\nE: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n", "stderr_lines": ["E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)", "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?"], "stdout": "", "stdout_lines": []}

PLAY RECAP *********************************************************************
192.168.122.104            : ok=1    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
192.168.122.153            : ok=1    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   

dell@dell:~/playbooks$ 

+++++++++++++++++++++++++++++=
SOLUTIONS:  
https://bobcares.com/blog/ansible-error-missing-sudo-password/

# User alias specification

# Cmnd alias specification

# User privilege specification
root    ALL=(ALL:ALL) ALL
dell    ALL=(ALL) NOPASSWD:  ALL      
# Members of the admin group may gain root privileges
%admin ALL=(ALL) ALL

# Allow members of group sudo to execute any command

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://groups.google.com/g/ansible-project/c/6DNc1MafnZs
Now when I am trying to achieve same using ansible playbook, I am getting following error -

FAILED! => {"failed": true, "msg": "Missing sudo password"}

Following is my task definition

- name: install basic packages
  apt: name={{item}} update_cache=yes
  become: yes
  become_method: sudo
  with_items:
    - tree
    - nload
    - htop

I have tried removing become and become_method. Then I am getting standard dpkg lock error with -vvv flag - 
Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n

What could be potential issue here ?
++++++++++++++++++++++++++++++
---
- hosts: all
  become: true
  become_method: su
  become_user:  dell
  tasks: 
    - name: install nginx
      apt: name=nginx state=latest

dell@dell:~/playbooks$ ansible-playbook nginx.yaml 

PLAY [all] ***********************************************************************************************************

TASK [Gathering Facts] ***********************************************************************************************
fatal: [192.168.122.153]: FAILED! => {"msg": "Timeout (12s) waiting for privilege escalation prompt: "}
fatal: [192.168.122.104]: FAILED! => {"msg": "Timeout (12s) waiting for privilege escalation prompt: "}

PLAY RECAP ***********************************************************************************************************
192.168.122.104            : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
192.168.122.153            : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
++++++++++++++++++++++++++++++++++++=
---
- hosts: all
  remote_user: dell
  become: true

  tasks: 
    - name: install nginx
      apt: name=nginx state=latest
dell@dell:~/playbooks$ ansible-playbook nginx.yaml 

PLAY [all] ***********************************************************************************************************

TASK [Gathering Facts] ***********************************************************************************************
fatal: [192.168.122.153]: FAILED! => {"msg": "Timeout (12s) waiting for privilege escalation prompt: "}
fatal: [192.168.122.104]: FAILED! => {"msg": "Timeout (12s) waiting for privilege escalation prompt: "}

PLAY RECAP ***********************************************************************************************************
192.168.122.104            : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
192.168.122.153            : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   

dell@dell:~/playbooks$ nano nginx.yaml 
dell@dell:~/playbooks$ ansible-playbook nginx.yaml 

PLAY [all] ***********************************************************************************************************

TASK [Gathering Facts] ***********************************************************************************************
fatal: [192.168.122.153]: FAILED! => {"msg": "Missing sudo password"}
fatal: [192.168.122.104]: FAILED! => {"msg": "Missing sudo password"}

PLAY RECAP ***********************************************************************************************************
192.168.122.104            : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
192.168.122.153            : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
+++++++++++++++++++++++++++=
---
- hosts: all
  become: yes
  become_user: dell
  become_method: su
  become_exe: sudo su -

  tasks: 
    - name: install nginx
      apt: name=nginx state=latest
dell@dell:~/playbooks$ ansible-playbook nginx.yaml 

PLAY [all] ***********************************************************************************************************

TASK [Gathering Facts] ***********************************************************************************************
fatal: [192.168.122.104]: FAILED! => {"msg": "Timeout (12s) waiting for privilege escalation prompt: "}
fatal: [192.168.122.153]: FAILED! => {"msg": "Timeout (12s) waiting for privilege escalation prompt: "}

PLAY RECAP ***********************************************************************************************************
192.168.122.104            : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
192.168.122.153            : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
+++++++++++++++++++++
dell@dell:~/playbooks$ ansible all -b -m apt -a "name=nginx state=absent"
192.168.122.153 | FAILED! => {
    "msg": "Missing sudo password"
}
192.168.122.104 | FAILED! => {
    "msg": "Missing sudo password"
}
dell@dell:~/playbooks$ 
+++++++++++++++++++++++++++++=   success SUCCESS 
answer 
results  =  one user one .ssh same key 


ansible@dell:~/playbook$ ansible-playbook nginx 
PLAY [all] *******************************************************************************
TASK [Gathering Facts] *******************************************************************
ok: [192.168.122.104]
ok: [192.168.122.153]
TASK [install nginx] *********************************************************************
changed: [192.168.122.153]
changed: [192.168.122.104]
PLAY RECAP *******************************************************************************
192.168.122.104            : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.153            : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

---
- hosts: all
  become: yes
  tasks: 
    - name: install nginx
      apt: name=nginx state=latest

###################################################################3

https://www.middlewareinventory.com/blog/ansible-apt-examples/
How to install a list of packages in One Go with Ansible apt
https://opensource.com/article/20/9/install-packages-ansible
https://linuxhint.com/install_multiple_packages_centos_ansible/
Installing Multiple Packages Easily on CentOS Using Ansible
https://www.redhat.com/sysadmin/software-packages-ansible

https://stackoverflow.com/questions/54944080/installing-multiple-packages-in-ansible
https://tekneed.com/creating-managing-ansible-variables/

- name: Ansible apt module examples
  hosts: web
  become: yes
  tasks:
    - name: Ansible apt to install multiple packages - LAMP
      register: updatesys
      apt:
        update_cache: yes
        name:
          - firewalld
          - apache2
          - mariadb-server
          - php
          - php-mysql
        state: present

- name: Install mongodb
  yum:
    name:
    - "mongodb-org-{{ mongodb_version }}"
    - "mongodb-org-server-{{ mongodb_version }}"
    - "mongodb-org-mongos-{{ mongodb_version }}"
    - "mongodb-org-shell-{{ mongodb_version }}"
    - "mongodb-org-tools-{{ mongodb_version }}"
    state: present
  notify: Restart mongodb

Is there a way I can indicate the version without having to use a loop like this? What is a more "elegant" way of writing this?

- name: Install mongodb
  yum:
    name: "{{ item }}-{{ mongodb_version }}"
    state: present
    loop:
    - mongodb-org-server
    - mongodb-org-mongos
    - mongodb-org-shell
    - mongodb-org-tools
  notify: Restart mongodb
++++++++++++++++
---
- hosts: all
  become: yes
  tasks:
    - name: Ansible apt to install multiple packages - LAMP
      apt:
        name:
          - firewalld
          - apache2
          - docker
          - git
          - tmux
          - curl
          - docker
          - java
          - python3-pip
          - ssh
          - openssh-server
          - htop
          - terraform
          - awscli
          - dmucs

ansible@dell:~/playbook$ ansible-playbook nginx 
PLAY [all] *********************************************************************
TASK [Gathering Facts] *********************************************************
ok: [192.168.122.153]
ok: [192.168.122.104]
TASK [Ansible apt to install multiple packages - LAMP] *************************
changed: [192.168.122.153]
changed: [192.168.122.104]
PLAY RECAP *********************************************************************
192.168.122.104            : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.153            : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
###################################33
nsible@master:~$ ansible -m ping all
The authenticity of host '192.168.122.168 (192.168.122.168)' can't be established.
ECDSA key fingerprint is SHA256:1JhAF9exLx117iwpFRcvtDud7loGmGzVWUDfA3VnsAg.
The authenticity of host '192.168.122.188 (192.168.122.188)' can't be established.
ECDSA key fingerprint is SHA256:1JhAF9exLx117iwpFRcvtDud7loGmGzVWUDfA3VnsAg.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
192.168.122.168 | UNREACHABLE! => {
    "changed": false,
    "msg": "Failed to connect to the host via ssh: Warning: Permanently added '192.168.122.168' (ECDSA) to the list of known hosts.\r\nansible@192.168.122.168: Permission denied (publickey,password).",
    "unreachable": true
}

192.168.122.188 | UNREACHABLE! => {
    "changed": false,
    "msg": "Failed to connect to the host via ssh: Host key verification failed.",
    "unreachable": true
}
ansible@master:~$ 

solutions1: answer1

ansible@master:~$ ssh-keygen 
Generating public/private rsa key pair.
Enter file in which to save the key (/home/ansible/.ssh/id_rsa): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/ansible/.ssh/id_rsa
Your public key has been saved in /home/ansible/.ssh/id_rsa.pub
The key fingerprint is:
SHA256:/ianRna+EY6jLYk0AqG6tglOT8zsSUX1YSSIW5Od1Lo ansible@master
The key's randomart image is:
+---[RSA 3072]----+
|    . =+++       |
|.  . =.o+..      |
|..  o.. ..       |
|o  ..  .         |
|..   .  S .      |
|. = +  Eoo..     |
|.o O o ++oo      |
|=.* o oooo+.     |
|.+.+  .oo=o.     |
+----[SHA256]-----+
ansible@master:~$ ssh-copy-id ansible@192.168.122.168
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/ansible/.ssh/id_rsa.pub"
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
ansible@192.168.122.168's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'ansible@192.168.122.168'"
and check to make sure that only the key(s) you wanted were added.

ansible@master:~$ ssh-copy-id ansible@192.168.122.188
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/ansible/.ssh/id_rsa.pub"
The authenticity of host '192.168.122.188 (192.168.122.188)' can't be established.
ECDSA key fingerprint is SHA256:1JhAF9exLx117iwpFRcvtDud7loGmGzVWUDfA3VnsAg.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
ansible@192.168.122.188's password: 
Number of key(s) added: 1
Now try logging into the machine, with:   "ssh 'ansible@192.168.122.188'"
and check to make sure that only the key(s) you wanted were added.

ansible@master:~$ ansible -m ping all
192.168.122.168 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
192.168.122.188 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
###################################################################
ansible@master:~$ ansible all -m copy -a  "src=/home/ansible/adocfile  dest=/opt/ansible"
192.168.122.188 | FAILED! => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "checksum": "da39a3ee5e6b4b0d3255bfef95601890afd80709",
    "msg": "Destination /opt not writable"
}
192.168.122.168 | FAILED! => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "checksum": "da39a3ee5e6b4b0d3255bfef95601890afd80709",
    "msg": "Destination /opt/ansible not writable"
}
ansible@master:~$ ansible all -m copy -a  "src=/home/ansible/adocfile  dest=/opt/ansible"
192.168.122.188 | FAILED! => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "checksum": "da39a3ee5e6b4b0d3255bfef95601890afd80709",
    "msg": "Destination /opt not writable"
}
192.168.122.168 | CHANGED => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": true,
    "checksum": "da39a3ee5e6b4b0d3255bfef95601890afd80709",
    "dest": "/opt/ansible/adocfile",
    "gid": 1001,
    "group": "ansible",
    "md5sum": "d41d8cd98f00b204e9800998ecf8427e",
    "mode": "0664",
    "owner": "ansible",
    "size": 0,
    "src": "/home/ansible/.ansible/tmp/ansible-tmp-1679988835.7800913-4758-261095678042272/source",
    "state": "file",
    "uid": 1001
}
ANSWER= SOLUTIONS: = 
ansible@slave1:~$ cd /opt/
ansible@slave1:/opt$ ll
total 12
drwxr-xr-x  3 root root 4096 Mar 28 10:32 ./
drwxr-xr-x 20 root root 4096 Mar 28 10:08 ../
drwx--x--x  4 root root 4096 Mar 28 10:32 containerd/
ansible@slave1:/opt$ mkdir ansible
mkdir: cannot create directory â€˜ansibleâ€™: Permission denied
ansible@slave1:/opt$ sudo mkdir ansible
ansible@slave1:/opt$ cd ansible/
ansible@slave1:/opt/ansible$ ll
total 8
drwxr-xr-x 2 root root 4096 Mar 28 13:02 ./
drwxr-xr-x 4 root root 4096 Mar 28 13:02 ../
ansible@slave1:/opt/ansible$ ll
total 8
drwxr-xr-x 2 root root 4096 Mar 28 13:02 ./
drwxr-xr-x 4 root root 4096 Mar 28 13:02 ../
ansible@slave1:/opt/ansible$ cd ../
ansible@slave1:/opt$ ll
total 16
drwxr-xr-x  4 root root 4096 Mar 28 13:02 ./
drwxr-xr-x 20 root root 4096 Mar 28 10:08 ../
drwxr-xr-x  2 root root 4096 Mar 28 13:02 ansible/
drwx--x--x  4 root root 4096 Mar 28 10:32 containerd/
ansible@slave1:/opt$ chmod 777 *
chmod: changing permissions of 'ansible': Operation not permitted
chmod: changing permissions of 'containerd': Operation not permitted
ansible@slave1:/opt$ sudo chmod 777 *
ansible@slave1:/opt$ ll
total 16
drwxr-xr-x  4 root root 4096 Mar 28 13:02 ./
drwxr-xr-x 20 root root 4096 Mar 28 10:08 ../
drwxrwxrwx  2 root root 4096 Mar 28 13:02 ansible/
drwxrwxrwx  4 root root 4096 Mar 28 10:32 containerd/
ansible@slave1:/opt$ cd ansible/
ansible@slave1:/opt/ansible$ ls
adocfile
ansible@slave1:/opt/ansible$ ll
total 8
drwxrwxrwx 2 root    root    4096 Mar 28 13:03 ./
drwxr-xr-x 4 root    root    4096 Mar 28 13:02 ../
-rw-rw-r-- 1 ansible ansible    0 Mar 28 13:03 adocfile
ansible@slave1:/opt/ansible$ 
##############################################################################3

ansible@master:~$ ansible all -m file -a "path=/opt/test mode=755 state=directory" -b
192.168.122.188 | CHANGED => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": true,
    "gid": 0,
    "group": "root",
    "mode": "0755",
    "owner": "root",
    "path": "/opt/test",
    "size": 4096,
    "state": "directory",
    "uid": 0
}
192.168.122.168 | CHANGED => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": true,
    "gid": 0,
    "group": "root",
    "mode": "0755",
    "owner": "root",
    "path": "/opt/test",
    "size": 4096,
    "state": "directory",
    "uid": 0
}

https://stackoverflow.com/questions/65949211/create-a-directory-test-at-home-user-using-ansible-adhoc-command
ansible all -m file -a "path=/opt/anji  mode=755  state=directory" -b

- a  meaning= In general, for an Ansible ad-hoc command, any module arguments are supplied using the -a option, as one single string. Due to this, the way Ansible distinguishes the individual arguments, is by spaces.

ansible@master:~$ ansible all -m file -a "path=/opt/dell  "
192.168.122.168 | FAILED! => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "msg": "file (/opt/dell) is absent, cannot continue",
    "path": "/opt/dell",
    "state": "absent"
}
192.168.122.188 | FAILED! => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "msg": "file (/opt/dell) is absent, cannot continue",
    "path": "/opt/dell",
    "state": "absent"
}
ansible@master:~$ ansible all -m file -a "path=/opt/dell  " -b
192.168.122.168 | FAILED! => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "msg": "file (/opt/dell) is absent, cannot continue",
    "path": "/opt/dell",
    "state": "absent"
}
192.168.122.188 | FAILED! => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "msg": "file (/opt/dell) is absent, cannot continue",
    "path": "/opt/dell",
    "state": "absent"
}

++++++++++++++++++
ansible@master:~$ ansible all -m file -a "path=/opt/dell state=directory " 
192.168.122.168 | FAILED! => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "msg": "There was an issue creating /opt/dell as requested: [Errno 13] Permission denied: b'/opt/dell'",
    "path": "/opt/dell"
}
192.168.122.188 | FAILED! => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "msg": "There was an issue creating /opt/dell as requested: [Errno 13] Permission denied: b'/opt/dell'",
    "path": "/opt/dell"
}

ansible@master:~$ ansible all -m file -a "path=/opt/dell state=directory mode=777 " 
192.168.122.168 | FAILED! => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "msg": "There was an issue creating /opt/dell as requested: [Errno 13] Permission denied: b'/opt/dell'",
    "path": "/opt/dell"
}
192.168.122.188 | FAILED! => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "msg": "There was an issue creating /opt/dell as requested: [Errno 13] Permission denied: b'/opt/dell'",
    "path": "/opt/dell"
}
ansible@master:~$ ansible all -m file -a "path=/opt/dell mode=777  state=directory  " 
192.168.122.168 | FAILED! => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "msg": "There was an issue creating /opt/dell as requested: [Errno 13] Permission denied: b'/opt/dell'",
    "path": "/opt/dell"
}
192.168.122.188 | FAILED! => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "msg": "There was an issue creating /opt/dell as requested: [Errno 13] Permission denied: b'/opt/dell'",
    "path": "/opt/dell"
}
########################################################
Working of adhoc commands and Modules in Ansible 
How to Create a File in Ansible


https://www.tutorialspoint.com/ansible/ansible_ad_hoc_commands.htm
https://blog.knoldus.com/working-of-adhoc-commands-and-modules-in-ansible/
https://phoenixnap.com/kb/ansible-create-file
https://www.middlewareinventory.com/blog/ansible-ad-hoc-commands/

https://www.techtransit.org/install-java-ansible-playbook-centos-almalinux-redhat-debian-ubuntu-linux/


ansible@master:~$ ansible all -m command -a "git --version"
192.168.122.168 | CHANGED | rc=0 >>
git version 2.25.1
192.168.122.188 | CHANGED | rc=0 >>
git version 2.25.1
+++++++++++++++++++==

ansible@master:~$ ansible all -a "/sbin/reboot" -f 5
192.168.122.168 | FAILED | rc=1 >>
Failed to set wall message, ignoring: Interactive authentication required.
Failed to reboot system via logind: Interactive authentication required.
Failed to open initctl fifo: Permission denied
Failed to talk to init daemon.non-zero return code
192.168.122.188 | FAILED | rc=1 >>
Failed to set wall message, ignoring: Interactive authentication required.
Failed to reboot system via logind: Interactive authentication required.
Failed to open initctl fifo: Permission denied
Failed to talk to init daemon.non-zero return code
ansible@master:~$ ansible all -a "/sbin/reboot" -f 5 -u root
192.168.122.168 | UNREACHABLE! => {
    "changed": false,
    "msg": "Failed to connect to the host via ssh: root@192.168.122.168: Permission denied (publickey,password).",
    "unreachable": true
}
192.168.122.188 | UNREACHABLE! => {
    "changed": false,
    "msg": "Failed to connect to the host via ssh: root@192.168.122.188: Permission denied (publickey,password).",
    "unreachable": true
}
ansible@master:~$ ansible all -a "/sbin/reboot" -f 5 -u ansible
192.168.122.168 | FAILED | rc=1 >>
Failed to set wall message, ignoring: Interactive authentication required.
Failed to reboot system via logind: Interactive authentication required.
Failed to open initctl fifo: Permission denied
Failed to talk to init daemon.non-zero return code
192.168.122.188 | FAILED | rc=1 >>
Failed to set wall message, ignoring: Interactive authentication required.
Failed to reboot system via logind: Interactive authentication required.
Failed to open initctl fifo: Permission denied
Failed to talk to init daemon.non-zero return code

ansible@master:~$ ansible all -m  file -a "dest=/home/ansible/opt/anji  state=absent"
192.168.122.188 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "path": "/home/ansible/opt/anji",
    "state": "absent"
}
192.168.122.168 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "path": "/home/ansible/opt/anji",
    "state": "absent"
}

---
- hosts: web 
  user: root
  become: yes
  connection: ssh
  tasks:
    - name: Copy file with owner and permissions
      copy:
        src: ./filesend2.txt
        dest: /Downloads
        owner: root
        group: root
        mode: '0644'
---
- hosts: web 
  user: root
  become: yes
  connection: ssh
  tasks:
    - name: change file with owner and permissions
      file:
        path: /Downloads/filesend2.txt
        owner: root
        group: root
        mode: '0744'
---
- hosts: web 
  user: root
  become: yes
  connection: ssh
  tasks:
  - name: install ntpdate
    package:
      name: ntpdate
      state: present

#######################################
---
- hosts: all
  tasks:
  - name: Creating an empty file
    file:
      path: "/your path"
      state: touch
---
- hosts: all
  tasks:
  - name: Creating a file with content
    copy:
      dest: "/your path"
      content: |
        line 01
        line 02
---
- hosts: all
  tasks:
  - name: Create multiple files
    file:
      path: "{{ item }}"
      state: touch
    with_items:
    - test01.txt
    - test02.txt
    - test03.txt
    - test04.txt

---
- hosts: all
  tasks:
  - name: Creating a new directory
    file:
      path: "/your path"
      state: directory
---
- hosts: all
  tasks:
  - name: Removing a file
    file:
      path: "/your path"
      state: absent
---
- hosts: all
  tasks:
  - name: Create a new file with permissions
    file:
      path: "/your path"
      state: touch
      mode: 0755
      owner: test
---
- hosts: all
  tasks:
  - name: Create a new file with permissions
    file:
      path: "/your path"
      state: touch
      mode: u=rwx,g=rx,o=rx
      owner: test
ansible@master:~$ ansible all -m shell -a uptime
192.168.122.168 | CHANGED | rc=0 >>
 15:44:26 up  4:12,  2 users,  load average: 0.01, 0.02, 0.00
192.168.122.188 | CHANGED | rc=0 >>
 15:44:26 up  4:11,  2 users,  load average: 0.31, 0.08, 0.03
ansible@master:~$ ansible  all -m shell -a "free -m"
192.168.122.188 | CHANGED | rc=0 >>
              total        used        free      shared  buff/cache   available
Mem:           3923         784        1336           9        1802        2846
Swap:          2047           0        2047
192.168.122.168 | CHANGED | rc=0 >>
              total        used        free      shared  buff/cache   available
Mem:           3923         781        1338           9        1804        2849
Swap:          2047           0        2047
#####################################################################3333
ansible@master:~$ ansible all -m shell -a "free -m" -f 1
192.168.122.168 | CHANGED | rc=0 >>
              total        used        free      shared  buff/cache   available
Mem:           3923         780        1339           9        1804        2850
Swap:          2047           0        2047
192.168.122.188 | CHANGED | rc=0 >>
              total        used        free      shared  buff/cache   available
Mem:           3923         780        1340           9        1803        2850
Swap:          2047           0        2047
 
ansible@master:~$ ansible all -m shell -a "free -m" -f 0
usage: ansible [-h] [--version] [-v] [-b] [--become-method BECOME_METHOD] [--become-user BECOME_USER]
               [-K | --become-password-file BECOME_PASSWORD_FILE] [-i INVENTORY] [--list-hosts] [-l SUBSET]
               [-P POLL_INTERVAL] [-B SECONDS] [-o] [-t TREE] [--private-key PRIVATE_KEY_FILE] [-u REMOTE_USER]
               [-c CONNECTION] [-T TIMEOUT] [--ssh-common-args SSH_COMMON_ARGS]
               [--sftp-extra-args SFTP_EXTRA_ARGS] [--scp-extra-args SCP_EXTRA_ARGS]
               [--ssh-extra-args SSH_EXTRA_ARGS] [-k | --connection-password-file CONNECTION_PASSWORD_FILE] [-C]
               [--syntax-check] [-D] [-e EXTRA_VARS] [--vault-id VAULT_IDS]
               [--ask-vault-password | --vault-password-file VAULT_PASSWORD_FILES] [-f FORKS] [-M MODULE_PATH]
               [--playbook-dir BASEDIR] [--task-timeout TASK_TIMEOUT] [-a MODULE_ARGS] [-m MODULE_NAME]
               pattern
ansible: error: The number of processes (--forks) must be >= 1

ansible@master:~$ ansible all -m fetch -a "src=/opt/anji.txt  dest=~/"
192.168.122.168 | CHANGED => {
    "changed": true,
    "checksum": "da39a3ee5e6b4b0d3255bfef95601890afd80709",
    "dest": "/home/ansible/192.168.122.168/opt/anji.txt",
    "md5sum": "d41d8cd98f00b204e9800998ecf8427e",
    "remote_checksum": "da39a3ee5e6b4b0d3255bfef95601890afd80709",
    "remote_md5sum": null
}
192.168.122.188 | CHANGED => {
    "changed": true,
    "checksum": "da39a3ee5e6b4b0d3255bfef95601890afd80709",
    "dest": "/home/ansible/192.168.122.188/opt/anji.txt",
    "md5sum": "d41d8cd98f00b204e9800998ecf8427e",
    "remote_checksum": "da39a3ee5e6b4b0d3255bfef95601890afd80709",
    "remote_md5sum": null
}
++++++++++++++++++++++++
tree 192.168.122.168

ansible@master:~$ tree 192.168.122.168
192.168.122.168
â””â”€â”€ opt
    â””â”€â”€ anji.txt

1 directory, 1 file
ansible@master:~$ tree 192.168.122.188
192.168.122.188
â””â”€â”€ opt
    â””â”€â”€ anji.txt

1 directory, 1 file

ansible@master:~$ ansible  192.168.122.168 -m fetch -a "src=/opt/anji.txt  dest=~/ flat=yes  "
192.168.122.168 | CHANGED => {
    "changed": true,
    "checksum": "da39a3ee5e6b4b0d3255bfef95601890afd80709",
    "dest": "/home/ansible/anji.txt",
    "md5sum": "d41d8cd98f00b204e9800998ecf8427e",
    "remote_checksum": "da39a3ee5e6b4b0d3255bfef95601890afd80709",
    "remote_md5sum": null
}
+++++++++++++
ansible@master:~$ ansible 192.168.122.168 -m file -a "path=/opt/sex.txt  state=touch"
192.168.122.168 | FAILED! => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "msg": "Error, could not touch target: [Errno 13] Permission denied: b'/opt/sex.txt'",
    "path": "/opt/sex.txt"
}
ansible@master:~$ ansible 192.168.122.168 -m file -a "path=/opt/sex.txt  state=touch"   --become
192.168.122.168 | CHANGED => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": true,
    "dest": "/opt/sex.txt",
    "gid": 0,
    "group": "root",
    "mode": "0644",
    "owner": "root",
    "size": 0,
    "state": "file",
    "uid": 0
}
###########################3
We have got permission denied error as "fdisk -l" command can be executed only as root user. 
In such case we must use -b or --become to execute the command with sudo privilege.
 If you remember during ansible configuration stage we had given 
 full sudo privilege to ansible user.

ansible@master:~$ ansible all -m command -a "fdisk -l"  
192.168.122.188 | CHANGED | rc=0 >>
fdisk: cannot open /dev/loop0: Permission denied
fdisk: cannot open /dev/loop1: Permission denied
fdisk: cannot open /dev/loop2: Permission denied
fdisk: cannot open /dev/loop3: Permission denied
fdisk: cannot open /dev/loop4: Permission denied
fdisk: cannot open /dev/loop5: Permission denied
fdisk: cannot open /dev/vda: Permission denied
fdisk: cannot open /dev/sr0: Permission denied
192.168.122.168 | CHANGED | rc=0 >>
fdisk: cannot open /dev/loop0: Permission denied
fdisk: cannot open /dev/loop1: Permission denied
fdisk: cannot open /dev/loop2: Permission denied
fdisk: cannot open /dev/loop3: Permission denied
fdisk: cannot open /dev/loop4: Permission denied
fdisk: cannot open /dev/loop5: Permission denied
fdisk: cannot open /dev/vda: Permission denied
fdisk: cannot open /dev/sr0: Permission denied
ansible@master:~$ ansible all -m command -a "fdisk -l"  --become
192.168.122.168 | CHANGED | rc=0 >>
Disk /dev/loop0: 61.98 MiB, 64966656 bytes, 126888 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes


Disk /dev/loop1: 346.3 MiB, 363118592 bytes, 709216 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes


Disk /dev/loop2: 4 KiB, 4096 bytes, 8 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes


Disk /dev/loop3: 91.7 MiB, 96141312 bytes, 187776 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes


Disk /dev/loop4: 54.24 MiB, 56872960 bytes, 111080 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes


Disk /dev/loop5: 46.98 MiB, 49242112 bytes, 96176 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes


Disk /dev/vda: 55 GiB, 59055800320 bytes, 115343360 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xe38e94f9

Device     Boot   Start       End   Sectors  Size Id Type
/dev/vda1  *       2048   1050623   1048576  512M  b W95 FAT32
/dev/vda2       1052670 115341311 114288642 54.5G  5 Extended
/dev/vda5       1052672 115341311 114288640 54.5G 83 Linux
+++++++++++++++++++
ansible@master:~$ ansible all -m command -a "fdisk -l"  --become --ask-become-pass
BECOME password: 
192.168.122.168 | CHANGED | rc=0 >>
Disk /dev/loop0: 61.98 MiB, 64966656 bytes, 126888 sectors

Disk /dev/loop1: 346.3 MiB, 363118592 bytes, 709216 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes

Disk /dev/loop5: 46.98 MiB, 49242112 bytes, 96176 sectors
Disk /dev/vda: 55 GiB, 59055800320 bytes, 115343360 sectors
Device     Boot   Start       End   Sectors  Size Id Type
/dev/vda1  *       2048   1050623   1048576  512M  b W95 FAT32
/dev/vda2       1052670 115341311 114288642 54.5G  5 Extended
/dev/vda5       1052672 115341311 114288640 54.5G 83 Linux
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
ansible@master:~$ ansible all -m apt -a "name=openjdk-11-jdk state=latest  "  -b
192.168.122.188 | CHANGED => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "cache_update_time": 1679986286,
    "cache_updated": false,
    "changed": true,
    "stderr": "",
    "stderr_lines": [],
    "stdout": "Reading package lists...\nBuilding dependency tree...\nReading state information...\nThe following packages were automatically installed and are no longer required:\n  chromium-codecs-ffmpeg-extra gir1.2-goa-1.0 gstreamer1.0-vaapi\n  libgstreamer-plugins-bad1.0-
++++++++++++++++++++
ansible@master:~$ ansible all -m command -a "whoami" --ask-pass
SSH password: 
192.168.122.168 | CHANGED | rc=0 >>
ansible
192.168.122.188 | CHANGED | rc=0 >>
ansible
+++++++++++++++++
ansible@master:~$ ansible all -m command -a "whoami" --user rx100fuckst  --ask-pass
SSH password: 
192.168.122.168 | UNREACHABLE! => {
    "changed": false,
    "msg": "Invalid/incorrect password: Permission denied, please try again.",
    "unreachable": true
}
192.168.122.188 | UNREACHABLE! => {
    "changed": false,
    "msg": "Invalid/incorrect password: Permission denied, please try again.",
    "unreachable": true
}
ansible@master:~$ ansible all -m command -a "whoami" --user prod  --ask-pass
SSH password: 
192.168.122.168 | CHANGED | rc=0 >>
prod
192.168.122.188 | CHANGED | rc=0 >>
prod
++++++++++++++++++
ansible@master:~$ ansible all -m apt -a "name=docker  state=latest" -b
192.168.122.168 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "cache_update_time": 1679986242,
    "cache_updated": false,
    "changed": false
}
192.168.122.188 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "cache_update_time": 1679986286,
    "cache_updated": false,
    "changed": false
}
ansible@master:~$ ansible all -m command -a "docker -v"
192.168.122.188 | CHANGED | rc=0 >>
Docker version 23.0.1, build a5ee5b1
192.168.122.168 | CHANGED | rc=0 >>
Docker version 23.0.1, build a5ee5b1 "
##########################################    dauily USED ANSIBLE AD HOC  COMMANDS#######33
REAL TIME USED ANSIBLE ADHOC COMMANDS 
ansible all -m shell -a uptime
ansible all -m shell -a "free -m"
 ansible server2 -m shell -a "free -m"
ansible all -m shell -a "free -m" -f 1
ansible server2 -m copy -a "src=/tmp/ctrl_file dest=/home/ansible/"
 ansible server2 -m copy -a "content='Hello, My name is deepak' dest=~/ctrl_file"
ansible all -m fetch -a "src=/opt/anji.txt  dest=~/"
   ansible  192.168.122.168 -m fetch -a "src=/opt/anji.txt  dest=~/ flat=yes  "
 ansible 192.168.122.168 -m file -a "path=/opt/styx.txt  state=touch"

https://www.golinuxcloud.com/ansible-ad-hoc-commands/
https://www.golinuxcloud.com/ansible-inventory-files/    advanced     IMPORTENT  ANSIBLECOMMANDS  ANSIBLE COMMANDS

ansible@master:~$ ansible 192.168.122.168 -m file -a "path=/opt/sex.txt  state=touch"   --become

ansible all -m command -a "fdisk -l"  --become

ansible all -m command -a "fdisk -l"  --become --ask-become-pass
ansible all -m apt -a "name=openjdk-11-jdk state=latest  "  -b
  ansible all -m command -a "whoami" --ask-pass

 ansible all -m command -a "whoami" --user prod  --ask-pass

ansible@master:~$ ansible-playbook play.yaml --syntax-check

ansible@master:~$ ansible-playbook play.yaml  --check

ansible-playbook play.yaml -vvvv
  ansible-playbook play.yaml -vv
ansible-playbook play.yaml -vvvvvvvvvv




###########@@@@@@@@@@@######++++++++++++++++++++++++++++++###############################
[{{}{{}}{{{{}{}{{}{{}{}{}{}}{}{{{{}}}[[[[][{}{}{}{{[[][][][]}}}}{{}{}{}{{}{}}}]}}]]]]}}}}}}}]


PLAYBOOKS NEW ANOTHER METHOD PLAYBOOKS ANSIBLE PLAYBOOKS ANOTHER MODEL 

---
- hosts: all
  gather_facts: True
  become: True
  tasks:
    - name: apt update
      shell: apt update -y

    - name: install nginx
      shell: apt install nginx -y

    - name: git install
      shell: apt install git -y     

ansible@master:~$ ansible-playbook play.yaml 

PLAY [all] *********************************************************************

TASK [Gathering Facts] *********************************************************
ok: [192.168.122.188]
ok: [192.168.122.168]

TASK [apt update] **************************************************************
changed: [192.168.122.168]
changed: [192.168.122.188]

TASK [install nginx] ***********************************************************
changed: [192.168.122.168]
changed: [192.168.122.188]

TASK [git install] *************************************************************
changed: [192.168.122.168]
changed: [192.168.122.188]

PLAY RECAP *********************************************************************
192.168.122.168            : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.188            : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
######################################
ansible@master:~$ ansible-playbook play.yaml --syntax-check

playbook: play.yaml
#########################################
ansible@master:~$ ansible-playbook play.yaml  --check

PLAY [all] ********************************************************************************************************

TASK [Gathering Facts] ********************************************************************************************
ok: [192.168.122.188]
ok: [192.168.122.168]

TASK [apt update] *************************************************************************************************
skipping: [192.168.122.188]
skipping: [192.168.122.168]

TASK [install nginx] **********************************************************************************************
skipping: [192.168.122.168]
skipping: [192.168.122.188]

TASK [git install] ************************************************************************************************
skipping: [192.168.122.168]
skipping: [192.168.122.188]

PLAY RECAP ********************************************************************************************************
192.168.122.168            : ok=1    changed=0    unreachable=0    failed=0    skipped=3    rescued=0    ignored=0   
192.168.122.188            : ok=1    changed=0    unreachable=0    failed=0    skipped=3    rescued=0    ignored=0   

#########################################################################################################33333
ansible@master:~$ ansible-playbook play.yaml -vvvv
ansible-playbook [core 2.12.10]
  config file = /etc/ansible/ansible.cfg
  configured module search path = ['/home/ansible/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']
  ansible python module location = /usr/lib/python3/dist-packages/ansible
  ansible collection location = /home/ansible/.ansible/collections:/usr/share/ansible/collections
  executable location = /usr/bin/ansible-playbook
  python version = 3.8.10 (default, Mar 13 2023, 10:26:41) [GCC 9.4.0]
  jinja version = 2.10.1
  libyaml = True
Using /etc/ansible/ansible.cfg as config file
setting up inventory plugins
host_list declined parsing /etc/ansible/hosts as it did not pass its verify_file() method
script declined parsing /etc/ansible/hosts as it did not pass its verify_file() method
auto declined parsing /etc/ansible/hosts as it did not pass its verify_file() method
Parsed /etc/ansible/hosts inventory source with ini plugin
Loading callback plugin default of type stdout, v2.0 from /usr/lib/python3/dist-packages/ansible/plugins/callback/default.py
Skipping callback 'default', as we already have a stdout callback.
Skipping callback 'minimal', as we already have a stdout callback.
Skipping callback 'oneline', as we already have a stdout callback.

PLAYBOOK: play.yaml ******************************************************************************************************************************************************************************************************************************************************
Positional arguments: play.yaml
verbosity: 4
connection: smart
timeout: 10
become_method: sudo
tags: ('all',)
inventory: ('/etc/ansible/hosts',)
forks: 5
1 plays in play.yaml

PLAY [all] ***************************************************************************************************************************************************************************************************************************************************************

TASK [Gathering Facts] ***************************************************************************************************************************************************************************************************************************************************
task path: /home/ansible/play.yaml:2
<192.168.122.168> ESTABLISH SSH CONNECTION FOR USER: None
<192.168.122.168> SSH: EXEC ssh -vvv -C -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o 'ControlPath="/home/ansible/.ansible/cp/9973355b4e"' 192.168.122.168 '/bin/sh -c '"'"'echo ~ && sleep 0'"'"''
<192.168.122.188> ESTABLISH SSH CONNECTION FOR USER: None
<192.168.122.188> SSH: EXEC ssh -vvv -C -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o 'ControlPath="/home/ansible/.ansible/cp/be7a488b5e"' 192.168.122.188 '/bin/sh -c '"'"'echo ~ && sleep 0'"'"''
<192.168.122.188> (0, b'/home/ansible\n', b'OpenSSH_8.2p1 Ubuntu-4ubuntu0.5, OpenSSL 1.1.1f  31 Mar 2020\r\ndebug1: Reading confi

ok: [192.168.122.188]
ok: [192.168.122.168]
META: ran handlers

TASK [apt update] ********************************************************************************************************************************************************************************************************************************************************
task path: /home/ansible/play.yaml:6
<192.168.122.168> ESTABLISH SSH CONNECTION FOR USER: None
<192.168.122.168> SSH: EXEC ssh -vvv -C -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o 'ControlPath="/home/ansible/.ansible/cp/9973355b4e"' 192.168.122.168 '/bin/sh -c '"'"'echo ~ && sleep 0'"'"''
<192.168.122.168> (0, b'/home/ansible\n', b'OpenSSH_8.2p1 Ubuntu-4ubuntu0.5, OpenSSL 1.1.1f  31 Mar 2020\r\ndebug1: Reading configuration data /etc/ssh/ssh_config\r\ndebug1: /etc/ssh/ssh_config line 19: include /etc/ssh/ssh_config.d/*.conf matched no files\r\ndebug1: /etc/ssh/ssh_config line 21: Applying options for *\r\ndebug2: resolve_canonicalize: hostname 192.168.122.168 is address\r\ndebug1: auto-mux: Trying existing master\r\ndebug2: fd 3 setting O_NONBLOCK\r\ndebug2: mux_client_hello_exchange: master version 4\r\ndebug3: mux_client_forwards: request forwardings: 0 local, 0 remote\r\ndebug3: mux_client_request_session: entering\r\ndebug3: mux_client_request_alive: entering\r\ndebug3: mux_client_request_alive: done pid = 3205\r\ndebug3: mux_client_request_session: session request sent\r\ndebug3: mux_client_read_packet: read header failed: Broken pipe\r\ndebug2: Received exit status from master 0\r\n')
<192.168.122.168> ESTABLISH SSH CONNECTION FOR USER: None

changed: [192.168.122.168] => {
    "changed": true,
    "cmd": "apt update -y",
    "delta": "0:00:07.186338",
    "end": "2023-03-28 22:29:13.236558",
    "invocation": {
        "module_args": {
            "_raw_params": "apt update -y",
            "_uses_shell": true,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true,
            "warn": false
        }
    },
    "msg": "",
    "rc": 0,
    "start": "2023-03-28 22:29:06.050220",
    "stderr": "\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.",
    "stderr_lines": [
        "",
        "WARNING: apt does not have a stable CLI interface. Use with caution in scripts."
    ],
    "stdout": "Hit:1 https://download.docker.com/linux/ubuntu focal InRelease\nHit:2 http://in.archive.ubuntu.com/ubuntu focal InRelease\nGet:3 http://in.archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\nGet:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\nHit:5 http://ppa.launchpad.net/ansible/ansible/ubuntu focal InRelease\nGet:6 http://in.archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\nGet:7 http://in.archive.ubuntu.com/ubuntu focal-updates/main amd64 DEP-11 Metadata [275 kB]\nGet:8 http://in.archive.ubuntu.com/ubuntu focal-updates/universe amd64 DEP-11 Metadata [410 kB]\nGet:9 http://security.ubuntu.com/ubuntu focal-security/main amd64 DEP-11 Metadata [59.9 kB]\nGet:10 http://in.archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 DEP-11 Metadata [944 B]\nGet:11 http://in.archive.ubuntu.com/ubuntu focal-backports/main amd64 DEP-11 Metadata [7,980 B]\nGet:12 http://in.archive.ubuntu.com/ubuntu focal-backports/universe amd64 DEP-11 Metadata [30.5 kB]\nGet:13 http://security.ubuntu.com/ubuntu focal-security/universe amd64 DEP-11 Metadata [95.6 kB]\nGet:14 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 DEP-11 Metadata [940 B]\nFetched 1,216 kB in 2s (507 kB/s)\nReading package lists...\nBuilding dependency tree...\nReading state information...\n5 packages can be upgraded. Run 'apt list --upgradable' to see them.",
    "stdout_lines": [
        "Hit:1 https://download.docker.com/linux/ubuntu focal InRelease",
        "Hit:2 http://in.archive.ubuntu.com/ubuntu focal InRelease",
        "Get:3 http://in.archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]",

        "Fetched 1,216 kB in 2s (507 kB/s)",
        "Reading package lists...",
        "Building dependency tree...",
        "Reading state information...",
        "5 packages can be upgraded. Run 'apt list --upgradable' to see them."
    ]
}
<192.168.122.188> (0, b'\r\n{"changed": true, "stdout": "Hit:1 https://download.docker.com/linux/ubuntu focal InRelease\\nHit:2 http://in.archive.ubuntu.com/ubuntu focal InRelease\\nGet:3 http://in.archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\\nGet:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\\nHit:5 http://ppa.launchpad.net/ansible/ansible/ubuntu focal InRelease\\nGet:6 http://in.archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\\nGet:7 http://in.archive.ubuntu.com/ubuntu focal-updates/main amd64 DEP-11 Metadata [275 kB]\\nGet:8 http://in.archive.ubuntu.com/ubuntu focal-updates/universe amd64 DEP-11 Metadata [410 kB]\\nGet:9 http://in.archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 DEP-11 Metadata [944 B]\\nGet:10 http://in.archive.ubuntu.com/ubuntu focal-backports/main amd64 DEP-11 Metadata [7,980 B]\\nGet:11 http://in.archive.ubuntu.com/ubuntu focal-backports/universe amd64 DEP-11 Metadata [30.5 kB]\\nGet:12 http://security.ubuntu.com/ubuntu focal-security/main amd64 DEP-11 Metadata [59.9 kB]\\nGet:13 http://security.ubuntu.com/ubuntu focal-security/universe amd64 DEP-11 Metadata [95.6 kB]\\nGet:14 http://security.ubuntu.com/

TASK [install nginx] *****************************************************************************************************************************************************************************************************************************************************
task path: /home/ansible/play.yaml:9
<192.168.122.168> ESTABLISH SSH CONNECTION FOR USER: None
<192.168.122.168> SSH: EXEC ssh -vvv -C -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o 'ControlPath="/home/ansible/.ansible/cp/9973355b4e"' 192.168.122.168 '/bin/sh -c '"'"'echo ~ && sleep 0'"'"''
<192.168.122.168> (0, b'/home/ansible\n', b'OpenSSH_8.2p1 Ubuntu-4ubuntu0.5, OpenSSL 1.1.1f  
TASK [git install] *******************************************************************************************************************************************************************************************************************************************************
task path: /home/ansible/play.yaml:12
<192.168.122.168> ESTABLISH SSH CONNECTION FOR USER: None
<192.168.122.168> SSH: EXEC ssh -vvv -C -
changed: [192.168.122.188] => {
    "changed": true,
    "cmd": "apt install git -y",
    "delta": "0:00:00.583379",
    "end": "2023-03-28 22:29:14.998480",
    "invocation": {
        "module_args": {
            "_raw_params": "apt install git -y",
            "_uses_shell": true,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true,
            "warn": false
        }
    },
    "msg": "",
    "rc": 0,
    "start": "2023-03-28 22:29:14.415101",
    "stderr": "\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.",
    "stderr_lines": [
        "",
        "WARNING: apt does not have a stable CLI interface. Use with caution in scripts."
    ],
    "stdout": "Reading package lists...\nBuilding dependency tree...\nReading state information...\ngit is already the newest version (1:2.25.1-1ubuntu3.10).\nThe following packages were automatically installed and are no longer required:\n  chromium-codecs-ffmpeg-extra gir1.2-goa-1.0 gstreamer1.0-vaapi\n  libgstreamer-plugins-bad1.0-0 libva-wayland2\nUse 'sudo apt autoremove' to remove them.\n0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.",
    "stdout_lines": [
        "Reading package lists...",
        "Building dependency tree...",
        "Reading state information...",
        "git is already the newest version (1:2.25.1-1ubuntu3.10).",
        "The following packages were automatically installed and are no longer required:",
        "  chromium-codecs-ffmpeg-extra gir1.2-goa-1.0 gstreamer1.0-vaapi",
        "  libgstreamer-plugins-bad1.0-0 libva-wayland2",
        "Use 'sudo apt autoremove' to remove them.",
        "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded."
    ]
}
META: ran handlers
META: ran handlers

PLAY RECAP ***************************************************************************************************************************************************************************************************************************************************************
192.168.122.168            : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.188            : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
#############################################################################################################################333
---
- hosts: all
  gather_facts: True
  become: True
  tasks:
    - name: apt update
      shell: apt update -y

    - name: install nginx
      shell: apt install nginx -y

    - name: git install
      shell: apt install git -y     
      
ansible@master:~$ ansible-playbook play.yaml -vv
ansible-playbook [core 2.12.10]
  config file = /etc/ansible/ansible.cfg
  configured module search path = ['/home/ansible/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']
  ansible python module location = /usr/lib/python3/dist-packages/ansible
  ansible collection location = /home/ansible/.ansible/collections:/usr/share/ansible/collections
  executable location = /usr/bin/ansible-playbook
  python version = 3.8.10 (default, Mar 13 2023, 10:26:41) [GCC 9.4.0]
  jinja version = 2.10.1
  libyaml = True
Using /etc/ansible/ansible.cfg as config file
Skipping callback 'default', as we already have a stdout callback.
Skipping callback 'minimal', as we already have a stdout callback.
Skipping callback 'oneline', as we already have a stdout callback.

PLAYBOOK: play.yaml ******************************************************************************************************************************************************************************************************************************************************
1 plays in play.yaml

PLAY [all] ***************************************************************************************************************************************************************************************************************************************************************

TASK [Gathering Facts] ***************************************************************************************************************************************************************************************************************************************************
task path: /home/ansible/play.yaml:2
ok: [192.168.122.188]
ok: [192.168.122.168]
META: ran handlers

TASK [apt update] ********************************************************************************************************************************************************************************************************************************************************
task path: /home/ansible/play.yaml:6
changed: [192.168.122.168] => {"changed": true, "cmd": "apt update -y", "delta": "0:00:03.472722", "end": "2023-03-28 22:35:09.167536", "msg":
changed: [192.168.122.188] => {"changed": true, "cmd": "apt update -y", "delta": "0:00:03.476269", "end": "2023-03-28 22:35:09.133749", "msg": 

TASK [install nginx] *****************************************************************************************************************************************************************************************************************************************************
task path: /home/ansible/play.yaml:9
changed: [192.168.122.168] => {"changed": true, "cmd": "apt install nginx -y", "delta": "0:00:00.551136", "end": "2023-03-28 22:35:09.921474", 

changed: [192.168.122.188] => {"changed": true, "cmd": "apt install nginx -y", "delta": "0:00:00.565065", "end": "2023-03-28 22:35:09.914718", 

TASK [git install] *******************************************************************************************************************************************************************************************************************************************************
task path: /home/ansible/play.yaml:12
changed: [192.168.122.168] => {"changed": true, "cmd": "apt install git -y", "delta": "0:00:00.553810", "end": "2023-03-28 22:35:10.720998", "msg": 
changed: [192.168.122.188] => {"changed": true, "cmd": "apt install git -y", "delta": "0:00:00.561684", "end": "2023-03-28 22:35:10.708123", "msg": 

PLAY RECAP ***************************************************************************************************************************************************************************************************************************************************************
192.168.122.168            : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.188            : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
##########################################################################################################
How to Install Elastic Stack 8 on Ubuntu 20.04 LTS 

https://techviewleo.com/install-elastic-stack-elk-8-on-ubuntu/
https://www.fosstechnix.com/how-to-install-elastic-stack-8-on-ubuntu-20-04/



root@dell:~# filebeat setup --index-management -E output.logstash.enable=false -E 'output.elasticsearch.hosts=["192.168.122.21:9200"]'
Exiting: error unpacking config data: more than one namespace configured accessing 'output' (source:'/etc/filebeat/filebeat.yml')

root@dell:~# filebeat modules enable system
Error initializing beat: error unpacking config data: more than one namespace configured accessing 'output' (source:'/etc/filebeat/filebeat.yml')

filebeat -c /etc/filebeat/filebeat.yml

sudo filebeat modules list
sudo filebeat modules enable system

What is Filebeat?

Filebeat is a log shipper belonging to the Beats family â€” a group of lightweight shippers installed on hosts for shipping different kinds of data into the ELK Stack for analysis. Each beat is dedicated to shipping different types of information â€” Winlogbeat, for example, ships Windows event logs, Metricbeat ships host metrics, and so forth. Filebeat, as the name implies, ships log files.

https://techviewleo.com/install-elastic-stack-elk-8-on-ubuntu/#

https://phoenixnap.com/kb/how-to-install-elk-stack-on-ubuntu

curl: (7) Failed to connect to localhost port 9200: Connection refused

ANSWER==: 
https://stackoverflow.com/questions/31677563/elasticsearch-failed-to-connect-to-localhost-port-9200-connection-refused


root@prod:~# cat /etc/elasticsearch/jvm.options
################################################################
##
## JVM configuration
##
################################################################
##
## WARNING: DO NOT EDIT THIS FILE. If you want to override the
## JVM options in this file, or set any additional options, you
## should create one or more files in the jvm.options.d
## directory containing your adjustments.
##
## See https://www.elastic.co/guide/en/elasticsearch/reference/8.7/jvm-options.html
## for more information.
##
################################################################
######################################################
## IMPORTANT: JVM heap size
################################################################
##
## The heap size is automatically configured by Elasticsearch
## based on the available memory in your system and the roles
## each node is configured to fulfill. If specifying heap is
## required, it should be done through a file in jvm.options.d,
## which should be named with .options suffix, and the min and
## max should be set to the same value. For example, to set the
## heap to 4 GB, create a new file in the jvm.options.d
## directory containing these lines:
##
## -Xms4g
## -Xmx4g
##
## See https://www.elastic.co/guide/en/elasticsearch/reference/8.7/heap-size.html
## for more information
##

################################################################
#-Xms512m                                ---   ok   uncomment  not enough memory =====
#-Xms512m
========================================= +++++++++++++++++++++++++++++++++++"

root@prod:~# cat /etc/elasticsearch/elasticsearch.yml
# ======================== Elasticsearch Configuration =========================
#
# NOTE: Elasticsearch comes with reasonable defaults for most settings.
#       Before you set out to tweak and tune the configuration, make sure you
#       understand what are you trying to accomplish and the consequences.
#
# The primary way of configuring a node is via this file. This template lists
# the most important settings you may want to configure for a production cluster.
#
# Please consult the documentation for further information on configuration options:
# https://www.elastic.co/guide/en/elasticsearch/reference/index.html
#
# ---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster:
#
#cluster.name: my-application
#
#
# ------------------------------------ Node ------------------------------------
#
# Use a descriptive name for the node:
#
#node.name: node-1
#
# Add custom attributes to the node:
#
#node.attr.rack: r1
#
# ----------------------------------- Paths ------------------------------------
#
# Path to directory where to store the data (separate multiple locations by comma):
#
path.data: /var/lib/elasticsearch
#
# Path to log files:
#
path.logs: /var/log/elasticsearch
#
# ----------------------------------- Memory -----------------------------------
#
# Lock the memory on startup:
#
#bootstrap.memory_lock: true
#
# Make sure that the heap size is set to about half the memory available
# on the system and that the owner of the process is allowed to use this
# limit.
#
# Elasticsearch performs poorly when the system is swapping the memory.
#
# ---------------------------------- Network -----------------------------------
#
# By default Elasticsearch is only accessible on localhost. Set a different
# address here to expose this node on the network:
#
network.host: 0.0.0.0
#
# By default Elasticsearch listens for HTTP traffic on the first free port it
# finds starting at 9200. Set a specific HTTP port here:
#
http.port: 9200
#
# For more information, consult the network module documentation.
#
# --------------------------------- Discovery ----------------------------------
#
# Pass an initial list of hosts to perform discovery when this node is started:
# The default list of hosts is ["127.0.0.1", "[::1]"]
#
discovery.seed_hosts: []
#
# Bootstrap the cluster using an initial set of master-eligible nodes:
#
#cluster.initial_master_nodes: ["node-1", "node-2"]
#
# For more information, consult the discovery and cluster formation module documentation.
#
# ---------------------------------- Various -----------------------------------
#
# Allow wildcard deletion of indices:
#
#action.destructive_requires_name: false

#----------------------- BEGIN SECURITY AUTO CONFIGURATION -----------------------
#
# The following settings, TLS certificates, and keys have been automatically      
# generated to configure Elasticsearch security features on 07-04-2023 06:21:42
#
# --------------------------------------------------------------------------------

# Enable security features
xpack.security.enabled: false

xpack.security.enrollment.enabled: true

# Enable encryption for HTTP API client connections, such as Kibana, Logstash, and Agents
xpack.security.http.ssl:
  enabled: true
  keystore.path: certs/http.p12  "
###########################################################################################

root@prod:~# curl -X GET "localhost:9200"
{
  "name" : "prod",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "Z4m0QX5MT3GOGrKvFQu-Qw",
  "version" : {
    "number" : "8.7.0",
    "build_flavor" : "default",
    "build_type" : "deb",
    "build_hash" : "09520b59b6bc1057340b55750186466ea715e30e",
    "build_date" : "2023-03-27T16:31:09.816451435Z",
    "build_snapshot" : false,
    "lucene_version" : "9.5.0",
    "minimum_wire_compatibility_version" : "7.17.0",
    "minimum_index_compatibility_version" : "7.0.0"
  },
  "tagline" : "You Know, for Search"
}
root@prod:~# 

sudo filebeat setup --index-management -E output.logstash.enabled=false -E 'output.elasticsearch.hosts=["192.168.122.170:9200"]'

curl -XGET http://192.168.122.170:9200/_cat/indices?v
######################################################################################33

root@jira:~# aws cli
Traceback (most recent call last):
  File "/usr/bin/aws", line 19, in <module>
    import awscli.clidriver
  File "/usr/lib/python3/dist-packages/awscli/clidriver.py", line 36, in <module>
    from awscli.help import ProviderHelpCommand
  File "/usr/lib/python3/dist-packages/awscli/help.py", line 23, in <module>
    from botocore.docs.bcdoc import docevents
ImportError: cannot import name 'docevents' from 'botocore.docs.bcdoc' (/usr/local/lib/python3.8/dist-packages/botocore/docs/bcdoc/__init__.py)
root@jira:~# aws --version
Traceback (most recent call last):
  File "/usr/bin/aws", line 19, in <module>
    import awscli.clidriver
  File "/usr/lib/python3/dist-packages/awscli/clidriver.py", line 36, in <module>
    from awscli.help import ProviderHelpCommand
  File "/usr/lib/python3/dist-packages/awscli/help.py", line 23, in <module>
    from botocore.docs.bcdoc import docevents
ImportError: cannot import name 'docevents' from 'botocore.docs.bcdoc' (/usr/local/lib/python3.8/dist-packages/botocore/docs/bcdoc/__init__.py)

install:
    commands:
      - pip3 install awsebcli --upgrade
      - eb --version
      - pip3 install --upgrade awscli

    pre_build:
      commands:
      - AWS_REGION=${AWS_DEFAULT_REGION}
      - COMMIT_HASH=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)
      - IMAGE_VERSION=${COMMIT_HASH}
      ...
##################################################
#!/usr/bin/env python
import boto3
import json
from collections import defaultdict

ec2 = boto3.resource('ec2', region_name='us-west-1')
print ("Creating instance...")
ec2info = defaultdict()
vpc = ec2.Vpc('vpc-22222222')
instance = ec2.create_instances(
    VpcId='vpc-22222222'
    ImageId='ami-aaaaaaa',
    SubnetId='subnet-99999999',
    KeyName='skahmed-gss',
    SecurityGroupIds=["sg-5555555","sg-9999999"],
    MinCount=1,
    MaxCount=1,
    InstanceType='t2.micro',
    #BlockDeviceMappings=[{"DeviceName": "/dev/xvda","Ebs" : { "VolumeSize" : 350 }}]
   BlockDeviceMappings=[
    {
        'DeviceName': '/dev/sda1',
        'Ebs': {
            'VolumeSize': 20,
            'VolumeType': 'gp2'
        }
    }
]
)
print("Instance ID: " + instance[0].id)
ec2.create_tags(Resources = [instance[0].id], Tags = [{'Key': 'Name', 'Value': 'SWALK-CENTOS7'}, {'Key': 'Environment', 'Value': 'NON_PROD'},
 {'Key': 'scheduler:ec2-startstop', 'Value': 'default'},  {'Key': 'Server_Function', 'Value': 'Spacewalk'}, {'Key': 'System', 'Value': 'GSS/C
hef'}, {'Key': 'Fisma_Id', 'Value': 'CIS-0000-MMM-1111'}, {'Key': 'POC', 'Value': 'person@email.com'} ])

import boto3
ec2 = boto3.resource("ec2",region_name = "us-east-1")
res = ec2.create_instances(
    ImageId = "ami-0aa2b7722dc1b5612",
    MinCount = 1,
    MaxCount = 1,
    KeyName = "cpu",
    InstanceType = "t2.micro",
    SubnetId = "subnet-08b82a2496ed438fa"
)
print (res)
#################################################################
https://www.middlewareinventory.com/blog/ansible-copy-examples/

Overwrite and backup the original file

What if the copied file contains a few mistakes but the copy module had overwritten the previous version? No worries, we have the option in the ansible copy module to take a backup of the previous version of the destination file. So it's now easy to revert the copy.
# copy_file.yml
- name: copy files to destination
  hosts: localhost
  connection: local
  tasks:
    - name: copy src.txt to files/backup_test and create a backup of src.txt
      copy:
        src: files/src.txt
        dest: files/backup_test/
        backup: yes 
      tags:
        - backup

------------
How to disable Force Copy of Ansible Copy

If the force copy has to be disabled, i.e., ignore the copy task if the file is already present, then use the option force: no in the copy-module as shown below.
# copy_file.yml
- name: copy files to destination
  hosts: localhost
  connection: local
  tasks:
    - name: no force copy src.txt as dest.txt in the same dir 
      copy:
        src: files/src.txt
        dest: files/dest.txt
        force: no
      tags:
        - simple_copy_no_force<code class="language-yaml">
</code>
############################

Copying file to a non-existing directory

If the destination directory does not exist, the copy module takes care of creating it and copying the file to the new directory with the same name as the source file name.
# copy_file.yml
- name: copy files to destination
  hosts: localhost
  connection: local
  tasks:
    - name: copy src.txt to a non existing directory
      copy:
        src: files/src.txt
        dest: files/not_dir/
      tags:
        - dir_not_exist
+++++++++++++++++++++++++++"        
Copy entire directory
# copy_dir.yml
- name: copy module for directories
  hosts: localhost
  connection: local
  tasks:
    - name: copy dir1 to /tmp
      copy:
        src: dir1
        dest: /tmp/
        directory_mode:
      tags:
        - parentdir
++++++++++++
- name: copy module for directories
  hosts: localhost
  connection: local
  tasks:    
    - name: copy contents of dir1 to /tmp/dir1_contents
      copy:
        src: dir1/
        dest: /tmp/dir1_contents/
        directory_mode:
      tags:
        - dircontent
+++++++++++++++++
- name: copy files to destination
  hosts: localhost
  connection: local
  vars:
    somedict:
      key1: value1
      key2: value2
  tasks:
    - name: copy content to content_dest.txt
      copy:
        content: |
          Hello from ansible.
          This is a sample file.
          This is a sample dict,
          {{ somedict }}
        dest: files/content_dest.txt
      tags:
        - content        

++++++++++++++++++++++++++++++

How to Verify if the copy is successful

So far we have trusted the copy-module to have successfully copied our source file to the destination.

What if we would like to know if the copy was TRULY successful indeed. Normally calculating the checksum of a file is a good technique to verify if two files are identical and there is no loss or corruption of data during the copy.

So we will look into the ways to verify the result of the copy-module by comparing the checksums of the source and destination file.
- name: copy files to destination
  hosts: localhost
  connection: local
  tasks:
    - block:
      - name: get properties of src.txt
        stat:
          path: files/src.txt
          checksum_algorithm: sha1
        register: src_info
      - name: copy src.txt to dest.txt
        copy:
          src: files/src.txt
          dest: files/dest.txt
          force: yes
          checksum:
        register: copy_out
      - name: Fail if copy was a failure
        fail:
          msg: "Copy failed!"
        when: src_info.stat.checksum != copy_out.checksum
      - name: Print Copy successful
        debug:
          msg: "Copy Successful!"
      tags:
        - checksum
##############################################
Install elk  Kibana with Docker


https://www.elastic.co/guide/en/kibana/8.7/docker.html 
https://www.youtube.com/watch?v=U8dNNYeiD9o

docker network create elastic
docker pull docker.elastic.co/elasticsearch/elasticsearch:8.7.0
docker run --name es-node01 --net elastic -p 9200:9200 -p 9300:9300 -t docker.elastic.co/elasticsearch/elasticsearch:8.7.0

docker pull docker.elastic.co/kibana/kibana:8.7.0
docker run --name kib-01 --net elastic -p 5601:5601 docker.elastic.co/kibana/kibana:8.7.0

docker exec -it es-node01 /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic

docker exec -it es-node01 /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana

docker network rm elastic
docker rm es-node01
docker rm kib-01

###################################33

https://gist.github.com/anjilinux/e44b9e77301dc08ff6725aa0e18b874f
https://www.youtube.com/watch?v=I2ZS2Wlk1No
version: '3.7'

services:
  elasticsearch:
    image: elasticsearch:7.9.2
    ports:
      - '9200:9200'
    environment:
      - discovery.type=single-node
    ulimits:
      memlock:
        soft: -1
        hard: -1

  kibana:
    image: kibana:7.9.2
    ports:
      - '5601:5601'

  logstash:
    image: logstash:7.9.2
    ports:
      - '5000:5000'
    volumes:
      - type: bind
        source: ./logstash_pipeline/
        target: /usr/share/logstash/pipeline
        read_only: true
#########################################################33
https://www.youtube.com/watch?v=lDCiFe24kC8
https://www.docker.elastic.co/r/kibana
https://www.docker.elastic.co/

https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html

https://gist.github.com/anjilinux/1a06813d5ead845b5c50087604ab7c4d

version: '2.2'
services:
  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.13.4
    container_name: es01
    environment:
      - node.name=es01
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data01:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
    networks:
      - elastic

  es02:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.13.4
    container_name: es02
    environment:
      - node.name=es02
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es01,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data02:/usr/share/elasticsearch/data
    networks:
      - elastic

  es03:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.13.4
    container_name: es03
    environment:
      - node.name=es03
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es01,es02
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data03:/usr/share/elasticsearch/data
    networks:
      - elastic

  kib01:
    image: docker.elastic.co/kibana/kibana:7.13.4
    container_name: kib01
    ports:
      - 5601:5601
    environment:
      ELASTICSEARCH_URL: http://es01:9200
      ELASTICSEARCH_HOSTS: '["http://es01:9200","http://es02:9200","http://es03:9200"]'
    networks:
      - elastic

volumes:
  data01:
    driver: local
  data02:
    driver: local
  data03:
    driver: local

networks:
  elastic:
    driver: bridge

 docker-compose down -v    
#########################################

https://www.youtube.com/watch?v=-v1L3ym52I4

version: '3.7'

services:
  elasticsearch:
    image: elasticsearch:7.9.2
    ports:
      - '9200:9200'
    environment:
      - discovery.type=single-node
    ulimits:
      memlock:
        soft: -1
        hard: -1
  kibana:
    image: kibana:7.9.2
    ports:
      - '5601:5601'
#######################################################333
https://www.youtube.com/watch?v=MpzDWSEveO0
https://github.com/LianDuanTrain/Elastic/blob/main/1%20Introduction%20and%20Installation/1-2%20Install%20ELK%20(Elasticsearch%2C%20Kibana%2C%20and%20Filebeat)%20using%20Docker.md

te a Docker Network

    docker network create elastic

Install Elasticsearch

    docker run -d --name es-node-01 --net elastic --rm -p 127.0.0.1:9200:9200 -p 127.0.0.1:9300:9300 -e "discovery.type=single-node" docker.elastic.co/elasticsearch/elasticsearch:7.15.2
    curl localhost:9200

Install Kibana

    docker run -d --name kibana-01 --net elastic --rm -p 127.0.0.1:5601:5601 -e "ELASTICSEARCH_HOSTS=http://es-node-01:9200" docker.elastic.co/kibana/kibana:7.15.2

Install kscarlett/nginx-log-generator

    docker run --name nginxLogGenerator --rm --label type=nginxLog -d -e "RATE=10" kscarlett/nginx-log-generator:sha-5416ec2

Install Filebeat

  docker run -d \
    --name=filebeat \
    --net=elastic \
    --user=root \
    --rm \
    --volume="$(pwd)/filebeat.docker.yml:/usr/share/filebeat/filebeat.yml:ro" \
    --volume="/var/lib/docker/containers:/var/lib/docker/containers:ro" \
    --volume="/var/run/docker.sock:/var/run/docker.sock:ro" \
      docker.elastic.co/beats/filebeat:7.15.2  -e -strict.perms=false 
#######################################################################

https://github.com/anjilinux/project-elk-kibana-docker

https://www.youtube.com/watch?v=A9P7-V3ako4

#########################################################333
https://www.youtube.com/watch?v=U8Iq4vm2Ekk
https://github.com/anjilinux/project-elk-kibana-?organization=anjilinux&organization=anjilinux

$################################################3
https://github.com/anjilinux/project-elk-kibana-docker-

https://www.youtube.com/watch?v=4rju8N60T7Q

########################################  simple  

https://codingfundas.com/setting-up-elasticsearch-6-8-with-kibana-and-x-pack-security-enabled/index.html     simple 
https://github.com/anjilinux/project-elk-docker
https://www.youtube.com/watch?v=EClKhOE0p-o

version: '3'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.9.2-amd64
    env_file:
      - elasticsearch.env
    volumes:
      - ./data/elasticsearch:/usr/share/elasticsearch/data

  kibana:
    image: docker.elastic.co/kibana/kibana:7.9.2
    env_file:
      - kibana.env
    ports:
      - 5601:5601

#################################################################33

https://www.youtube.com/watch?v=31wJJPZgWrQ
https://github.com/anjilinux/project-eks-docker-compose?organization=anjilinux&organization=anjilinux

https://github.com/anjilinux/project-eks-docker-compose

##############################################3    simple 
https://gist.github.com/anjilinux/a04af2e0d6daa80946d4aa188db1ee30           simple
https://www.youtube.com/watch?v=H_1BQdvdlEg

#######################################33

Ansible Set Fact
---
- name: print greetings 
  hosts: "*"
  tasks: 
    - set_fact: 
         name: jenniborose
    - include_vars: user.yaml 
    - debug: 
        msg: "helo mydear  darling  {{ name }}"    

#######################3
ansible@s3:~/playbook$ ansible-playbook var.yaml 

PLAY [print greetings] *********************************************************

TASK [Gathering Facts] *********************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [set_fact] ****************************************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [include_vars] ************************************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [debug] *******************************************************************
ok: [192.168.122.95] => {
    "msg": "helo mydear  darling  jenniborose"
}
ok: [192.168.122.201] => {
    "msg": "helo mydear  darling  jenniborose"
}

PLAY RECAP *********************************************************************
192.168.122.201            : ok=4    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.95             : ok=4    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

#######################33  or  OR  OR #####################
---
- name: print greetings 
  hosts: "*"
  tasks: 
    - set_fact: 
         name: jenniborose
#    - include_vars: user.yaml 
    - debug: 
        msg: "helo mydear  darling  {{ name }}"    

ansible@s3:~/playbook$ ansible-playbook var.yaml 

PLAY [print greetings] *********************************************************

TASK [Gathering Facts] *********************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [set_fact] ****************************************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [debug] *******************************************************************
ok: [192.168.122.95] => {
    "msg": "helo mydear  darling  jenniborose"
}
ok: [192.168.122.201] => {
    "msg": "helo mydear  darling  jenniborose"
}

PLAY RECAP *********************************************************************
192.168.122.201            : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.95             : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
++++++++++++++++++++++++++++++++
Examples of Ansible Set Fact

---
- hosts: "*"
  tasks: 
    - name: i am using SET_FACT
      set_fact: 
       filename: sex.txt
    - name: now using sile 
      file: 
        path: /tmp/{{ filename }}
        state: touch
######################
PLAY [*] ***********************************************************************

TASK [Gathering Facts] *********************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [i am using SET_FACT] *****************************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [now using sile] **********************************************************
changed: [192.168.122.95]
changed: [192.168.122.201]

PLAY RECAP *********************************************************************
192.168.122.201            : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.95             : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
++++++++++++++++++++++++++++++++++++++++++++++++++++++
---
- hosts: "*"
  tasks: 
    - name: IA AM USING  SET_FACT  MODULE
      set_fact: 
        file1: book.txt 
        file2: pen.txt 
        file3: bag.txt
    - name:  file module using to create a  files with vraible values as a names 
      file: 
        path: /tmp/{{ item }} 
        state: touch 
      with_items: 
          -  "{{file1}}"
          - "{{file2}}"
          - "{{file3}}"
########################################3          
ansible@s3:~/playbook$ ansible-playbook var.yaml 

PLAY [*] ***********************************************************************

TASK [Gathering Facts] *********************************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [IA AM USING  SET_FACT  MODULE] *******************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [file module using to create a  files with vraible values as a names] *****
changed: [192.168.122.95] => (item=book.txt)
changed: [192.168.122.201] => (item=book.txt)
changed: [192.168.122.95] => (item=pen.txt)
changed: [192.168.122.201] => (item=pen.txt)
changed: [192.168.122.95] => (item=bag.txt)
changed: [192.168.122.201] => (item=bag.txt)

PLAY RECAP *********************************************************************
192.168.122.201            : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.95             : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
##########################################3
---
- hosts: all 
  tasks:
  - name: hellld
    set_fact:
       interfaces_list: "{{ ansible_interfaces | select('match', '^(eth|wlan)[0-9]+') | list}}"
  - name: helo 
    debug:
      msg: The interface's list on "{{ ansible_hostname }}" is "{{ interfaces_list }}"

ansible@s3:~/playbook$ ansible-playbook var.yaml 

PLAY [all] *********************************************************************

TASK [Gathering Facts] *********************************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [hellld] ******************************************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [helo] ********************************************************************
ok: [192.168.122.95] => {
    "msg": "The interface's list on \"s3\" is \"[]\""
}
ok: [192.168.122.201] => {
    "msg": "The interface's list on \"s3\" is \"[]\""
}

PLAY RECAP *********************************************************************
192.168.122.201            : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.95             : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
####################3
---
- hosts: all
  become: true
  tasks: 
   - name: create a user 
     set_fact: 
        username: sex
   - user: 
       name: "{{ username }}"
       group: root
       shell: /bin/bash

PLAYBOOK: var.yaml *************************************************************
1 plays in var.yaml

PLAY [all] *********************************************************************

TASK [Gathering Facts] *********************************************************
task path: /home/ansible/playbook/var.yaml:2
ok: [192.168.122.201]
ok: [192.168.122.95]
META: ran handlers

TASK [create a user] ***********************************************************
task path: /home/ansible/playbook/var.yaml:5
ok: [192.168.122.95] => {"ansible_facts": {"username": "sex"}, "changed": false}
ok: [192.168.122.201] => {"ansible_facts": {"username": "sex"}, "changed": false}

TASK [user] ********************************************************************
task path: /home/ansible/playbook/var.yaml:8
changed: [192.168.122.201] => {"changed": true, "comment": "", "create_home": true, "group": 0, "home": "/home/sex", "name": "sex", "shell": "/bin/bash", "state": "present", "system": false, "uid": 1002}
changed: [192.168.122.95] => {"changed": true, "comment": "", "create_home": true, "group": 0, "home": "/home/sex", "name": "sex", "shell": "/bin/bash", "state": "present", "system": false, "uid": 1002}
META: ran handlers
META: ran handlers

PLAY RECAP *********************************************************************
192.168.122.201            : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.95             : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
#################
Ansible - Only do action if on specific distribution (Debian, Ubuntu, CentOS or RHEL) or distribution version (ubuntu precise, ubuntu trusty)

Published: 09-11-2014 | Last update: 16-12-2018 | Author: Remy van Elst | Text only version of this article

â— This post is over four years old. It may no longer be up to date. Opinions may have changed.
Table of Contents

    Specific Distribution
    Specific Distribution Version
    Package module (2015 short update)

This Ansible playbook example helps you execute actions only if you are on a certain distribution. You might have a mixed environment with CentOS and Debian and when using Ansible to execute actions on nodes you don't need to run Yum on Debian, or Apt on CentOS. Some package names are different and such, so this helps you with an only if statement to select a specific distribution. As a bonus, you also get an only_if for specific distribution versions, like Ubuntu precise (12.04 LTS) or Ubuntu Trusty (14.04 LTS).

Recently I removed all Google Ads from this site due to their invasive tracking, as well as Google Analytics. Please, if you found this content useful, consider a small donation using any of the options below:

I'm developing an open source monitoring app called Leaf Node Monitoring, for windows, linux & android. Go check it out!

Consider sponsoring me on Github. It means the world to me if you show your appreciation and you'll help pay the server costs.

You can also sponsor me by getting a Digital Ocean VPS. With this referral link you'll get $100 credit for 60 days.

    2018-12-16: update ansible syntax to version 2.5, use become
    2015-09-24: Added package module, changed only_if to when
    2014-09-11: Initial release

Specific Distribution

On a specific action, add the following when statement:

when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat
Enterprise Linux'

This is for RHEL and Centos, the following is for Debian/Ubuntu:

when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'

This example playbook installs Apache2 on both Debian/Ubuntu and CentOS. This example used apache because the name package name is different on the two distributions.

---
- hosts: example
  become: true
  user: remy
  connection: ssh 

  tasks:
  - name: Install apache
    apt: 
      name: {{ item }} 
      state: latest
    with_items:
     - apache2
    when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'

  - name: Install httpd
    yum: 
      name: {{ item }} 
      state: latest
    with_items:
     - httpd
    when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'

  - name: restart apache
    service: 
      name: apache2 
      state: started 
      enabled: yes
    when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'

  - name: restart httpd
    service: 
      name: httpd 
      state: started 
      enabled: yes
    when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'

---
- hosts: all
  become: true
  tasks: 
  - name: apache install in linux
    set_fact: 
      pkg: "apache2"
    when: ansible_distribution == "Debian" or ansible_distribution == "Ubuntu" 

  - name: httpd
    set_fact: 
      pkg: "httpd"
    when: ansible_distribution == "CentOS"  or ansible_distribution == "Red Hat Enterprise Linux" 

---
---
- hosts: all
  become: true
  user: ansible
  connection: ssh 

  tasks:
  - name: Install apache
    apt: 
      name: {{ item }} 
      state: latest
    with_items:
     - apache2
    when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'

  - name: Install httpd
    yum: 
      name: {{ item }} 
      state: latest
    with_items:
     - httpd
    when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'

  - name: restart apache
    service: 
      name: apache2 
      state: started 
      enabled: yes
    when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'

  - name: restart httpd
    service: 
      name: httpd 
      state: started 
      enabled: yes
    when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'
---
- hosts: all
  tasks
- name: install the latest version of Apache on CentOS
  yum: name=httpd state=latest
  when: ansible_os_family == "RedHat"

- name: install the latest version of Apache on Debian
  apt: pkg=httpd state=latest 
  when: ansible_os_family == "Debian"
---
- hosts: all
  gather_facts: yes
  become: yes
  tasks:

  - name: install nginx 
    apt: name=nginx state=latest
  - name: starting httpd
    service: name=nginx state=started enabled=yes

  - name: httpd status
    command: service nginx status
    register: nginx_status  
  
  - name: httpd status output
    debug:
      var: nginx_status

ansible@s3:~/playbook$ ansible-playbook var.yaml 

PLAY [all] *********************************************************************

TASK [Gathering Facts] *********************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [install nginx] ***********************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [starting httpd] **********************************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [httpd status] ************************************************************
changed: [192.168.122.95]
changed: [192.168.122.201]

TASK [httpd status output] *****************************************************
ok: [192.168.122.95] => {
    "nginx_status": {
        "changed": true,
        "cmd": [
            "service",
            "nginx",
            "status"
        ],
        "delta": "0:00:00.020432",
        "end": "2023-04-26 09:21:43.387386",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2023-04-26 09:21:43.366954",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "â— nginx.service - A high performance web server and a reverse proxy server\n     Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled)\n     Active: active (running) since Wed 2023-04-26 08:53:38 IST; 28min ago\n       Docs: man:nginx(8)\n    Process: 712 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process on; (code=exited, status=0/SUCCESS)\n    Process: 743 ExecStart=/usr/sbin/nginx -g daemon on; master_process on; (code=exited, status=0/SUCCESS)\n   Main PID: 745 (nginx)\n      Tasks: 3 (limit: 2476)\n     Memory: 7.3M\n     CGroup: /system.slice/nginx.service\n             â”œâ”€745 nginx: master process /usr/sbin/nginx -g daemon on; master_process on;\n             â”œâ”€746 nginx: worker process\n             â””â”€748 nginx: worker process\n\nApr 26 08:53:38 s3 systemd[1]: Starting A high performance web server and a reverse proxy server...\nApr 26 08:53:38 s3 systemd[1]: Started A high performance web server and a reverse proxy server.",
        "stdout_lines": [
            "â— nginx.service - A high performance web server and a reverse proxy server",
            "     Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled)",
            "     Active: active (running) since Wed 2023-04-26 08:53:38 IST; 28min ago",
            "       Docs: man:nginx(8)",
            "    Process: 712 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process on; (code=exited, status=0/SUCCESS)",
            "    Process: 743 ExecStart=/usr/sbin/nginx -g daemon on; master_process on; (code=exited, status=0/SUCCESS)",
            "   Main PID: 745 (nginx)",
            "      Tasks: 3 (limit: 2476)",
            "     Memory: 7.3M",
            "     CGroup: /system.slice/nginx.service",
            "             â”œâ”€745 nginx: master process /usr/sbin/nginx -g daemon on; master_process on;",
            "             â”œâ”€746 nginx: worker process",
            "             â””â”€748 nginx: worker process",
            "",
            "Apr 26 08:53:38 s3 systemd[1]: Starting A high performance web server and a reverse proxy server...",
            "Apr 26 08:53:38 s3 systemd[1]: Started A high performance web server and a reverse proxy server."
        ]
    }
}

PLAY RECAP *********************************************************************
192.168.122.201            : ok=5    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.95             : ok=5    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


#####################################################33333
Ansible Register Variable-Ansible Register ModuleAnsible Register ModuleÂ  How to Use Ansible Register ModuleAnsible register variable or ansible register module is used to capture or store the output of the command or task. By using the register module, you can store that output into any variable.In the following example, I will show you how to find the status of httpd. I will start the httpd and I will see the status and I will store the status into a variable called httpd_status by using ansible register module. With below playbook, you can check whether httpd started with errors or without errors. The use of register module is you can verify your task is executed or not by checking task output in the log.Ansible register is a way to capture the output from task execution and store it in a variable. This is an important feature, as this output is different for each remote host, and the basis on that we can use conditions loops to do some other tasks. Also, each register value is valid throughout the playbook executionhttps://www.decodingdevops.com/ansible-register-module-with-examples/
---
- name: how REGISTER WORKS 
  hosts: all
  tasks: 
  - name: show loop register 
    shell: "echo {{ item }}"
    loop: 
      - "one"
      - "two"
    register: echo
  - name: show register results
    debug: 
      var: echo 
      
           
ansible@s3:~/playbook$ ansible-playbook  var.yaml 

PLAY [how REGISTER WORKS] ******************************************************

TASK [Gathering Facts] *********************************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [show loop register] ******************************************************
changed: [192.168.122.201] => (item=one)
changed: [192.168.122.95] => (item=one)
changed: [192.168.122.201] => (item=two)
changed: [192.168.122.95] => (item=two)

TASK [show register results] ***************************************************
ok: [192.168.122.95] => {
    "echo": {
        "changed": true,
        "msg": "All items completed",
        "results": [
            {
                "ansible_loop_var": "item",
                "changed": true,
                "cmd": "echo one",
                "delta": "0:00:00.003389",
                "end": "2023-04-26 08:57:38.634874",
                "failed": false,
                "invocation": {
                    "module_args": {
                        "_raw_params": "echo one",
                        "_uses_shell": true,
                        "argv": null,
                        "chdir": null,
                        "creates": null,
                        "executable": null,
                        "removes": null,
                        "stdin": null,
                        "stdin_add_newline": true,
                        "strip_empty_ends": true,
                        "warn": false
                    }
                },
                "item": "one",
                "msg": "",
                "rc": 0,
                "start": "2023-04-26 08:57:38.631485",
                "stderr": "",
                "stderr_lines": [],
                "stdout": "one",
                "stdout_lines": [
                    "one"
                ]
            },
            {
                "ansible_loop_var": "item",
                "changed": true,
                "cmd": "echo two",
                "delta": "0:00:00.002504",
                "end": "2023-04-26 08:57:38.871690",
                "failed": false,
                "invocation": {
                    "module_args": {
                        "_raw_params": "echo two",
                        "_uses_shell": true,
                        "argv": null,
                        "chdir": null,
                        "creates": null,
                        "executable": null,
                        "removes": null,
                        "stdin": null,
                        "stdin_add_newline": true,
                        "strip_empty_ends": true,
                        "warn": false
                    }
                },
                "item": "two",
                "msg": "",
                "rc": 0,
                "start": "2023-04-26 08:57:38.869186",
                "stderr": "",
                "stderr_lines": [],
                "stdout": "two",
                "stdout_lines": [
                    "two"
                ]
            }
        ],
        "skipped": false
    }
}
PLAY RECAP *********************************************************************
192.168.122.201            : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.95             : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
##############################################################
https://blog.learncodeonline.in/ansible-register-module
What is vsftpd or Very Secure FTP Daemon

The File Transfer Protocol or FTP is a protocol used to access files on servers from private computer networks or the Internet. FTP has been used since 1985 and is now widely used. Many FTP servers around the world allow you to connect to them anywhere on the Internet, and files placed on them are then transferred (uploaded or downloaded).
Very Secure FTP Daemon does not bring significant changes here; it only helps to make files more accessible with a more friendly interface than FTP applications. FTP is one of the oldest and most common methods of sending files over the Internet. This article shows you how to install and configure the Very Secure FTP Daemon (vsftpd), which is the FTP base server that ships with most Linux distributions.

---
- name: ansible register 
  hosts: all
  become: yes
  tasks: 
    - name: installing  vsftp
      apt: name=vsftpd state=latest

    - name: start status vsftpd  
      service: name=vsftpd  state=started   enabled=yes
    
    - name: vsftpd status  
      command: service vsftpd status 
      register: vsftpd_status

    - name:   statsus opuput
      debug: 
        var: vsftpd_status

ansible@s3:~/playbook$ ansible-playbook var.yaml 

PLAY [ansible register] ************************************************************************************************************

TASK [Gathering Facts] *************************************************************************************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [installing  vsftp] ***********************************************************************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [start status vsftpd] *********************************************************************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [vsftpd status] ***************************************************************************************************************
changed: [192.168.122.201]
changed: [192.168.122.95]

TASK [statsus opuput] **************************************************************************************************************
ok: [192.168.122.95] => {
    "vsftpd_status": {
        "changed": true,
        "cmd": [
            "service",
            "vsftpd",
            "status"
        ],
        "delta": "0:00:00.012989",
        "end": "2023-04-26 11:38:08.981146",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2023-04-26 11:38:08.968157",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "â— vsftpd.service - vsftpd FTP server\n     Loaded: loaded (/lib/systemd/system/vsftpd.service; enabled; vendor preset: enabled)\n     Active: active (running) since Wed 2023-04-26 11:29:06 IST; 9min ago\n   Main PID: 5108 (vsftpd)\n      Tasks: 1 (limit: 2476)\n     Memory: 520.0K\n     CGroup: /system.slice/vsftpd.service\n             â””â”€5108 /usr/sbin/vsftpd /etc/vsftpd.conf\n\nApr 26 11:29:06 s3 systemd[1]: Starting vsftpd FTP server...\nApr 26 11:29:06 s3 systemd[1]: Started vsftpd FTP server.",
        "stdout_lines": [
            "â— vsftpd.service - vsftpd FTP server",
            "     Loaded: loaded (/lib/systemd/system/vsftpd.service; enabled; vendor preset: enabled)",
            "     Active: active (running) since Wed 2023-04-26 11:29:06 IST; 9min ago",
            "   Main PID: 5108 (vsftpd)",
            "      Tasks: 1 (limit: 2476)",
            "     Memory: 520.0K",
            "     CGroup: /system.slice/vsftpd.service",
            "             â””â”€5108 /usr/sbin/vsftpd /etc/vsftpd.conf",
            "",
            "Apr 26 11:29:06 s3 systemd[1]: Starting vsftpd FTP server...",
            "Apr 26 11:29:06 s3 systemd[1]: Started vsftpd FTP server."
        ]
    }
}
##################################3+++++++++++++++++++
---
- name: ansible register 
  hosts: "192.168.122.95"
  become: yes
  tasks: 
    - name: installing  vsftp
      apt: name=vsftpd state=latest

    - name: start status vsftpd  
      service: name=vsftpd  state=started   enabled=yes
    
    - name: vsftpd status  
      command: service vsftpd status 
      register: anji

    - name:   " An array of stderrlines one per line. When stderr is returned we also always provide this field which is a list of strings, one item per line from the original"
      debug: 
        var: anji.stderr_lines

    - name:  " An array of stdout lines one per line. When stdout is returned, Ansible always provides a list of strings, each containing one item per line from the original output"
      debug: 
        var: anji.stdout_lines



    - name:  " The command execution begin time"
      debug: 
        var: anji.start

    - name:  "The command execution finished time"
      debug: 
        var: anji.end

    - name:  "A Boolean indicating if the task had to make changes"
      debug: 
        var: anji.changed


    - name:  "The total time taken to run the command. The value of this property is the difference between the end and the start time outputs"

      debug: 
        var: anji.delta

    - name: "The actual command which ran to generate the output"
      debug: 
        var: anji.cmd

    - name: "The output of the command"
      debug: 
        var: anji.stdout

    - name: " The error output of the command"
      debug: 
         var: anji.stderr

ansible@s3:~/playbook$ ansible-playbook var.yaml 

PLAY [ansible register] ***********************************************************************************************************************************************************************************************************************

TASK [Gathering Facts] ************************************************************************************************************************************************************************************************************************
ok: [192.168.122.95]

TASK [installing  vsftp] **********************************************************************************************************************************************************************************************************************
ok: [192.168.122.95]

TASK [start status vsftpd] ********************************************************************************************************************************************************************************************************************
ok: [192.168.122.95]

TASK [vsftpd status] **************************************************************************************************************************************************************************************************************************
changed: [192.168.122.95]

TASK [An array of stderrlines one per line. When stderr is returned we also always provide this field which is a list of strings, one item per line from the original] ************************************************************************
ok: [192.168.122.95] => {
    "anji.stderr_lines": []
}

TASK [An array of stdout lines one per line. When stdout is returned, Ansible always provides a list of strings, each containing one item per line from the original output] ******************************************************************
ok: [192.168.122.95] => {
    "anji.stdout_lines": [
        "â— vsftpd.service - vsftpd FTP server",
        "     Loaded: loaded (/lib/systemd/system/vsftpd.service; enabled; vendor preset: enabled)",
        "     Active: active (running) since Wed 2023-04-26 11:29:06 IST; 1h 45min ago",
        "   Main PID: 5108 (vsftpd)",
        "      Tasks: 1 (limit: 2476)",
        "     Memory: 520.0K",
        "     CGroup: /system.slice/vsftpd.service",
        "             â””â”€5108 /usr/sbin/vsftpd /etc/vsftpd.conf",
        "",
        "Apr 26 11:29:06 s3 systemd[1]: Starting vsftpd FTP server...",
        "Apr 26 11:29:06 s3 systemd[1]: Started vsftpd FTP server."
    ]
}

TASK [The command execution begin time] *******************************************************************************************************************************************************************************************************
ok: [192.168.122.95] => {
    "anji.start": "2023-04-26 13:14:58.574314"
}

TASK [The command execution finished time] ****************************************************************************************************************************************************************************************************
ok: [192.168.122.95] => {
    "anji.end": "2023-04-26 13:14:58.582838"
}

TASK [A Boolean indicating if the task had to make changes] ***********************************************************************************************************************************************************************************
ok: [192.168.122.95] => {
    "anji.changed": true
}

TASK [The total time taken to run the command. The value of this property is the difference between the end and the start time outputs] *******************************************************************************************************
ok: [192.168.122.95] => {
    "anji.delta": "0:00:00.008524"
}

TASK [The actual command which ran to generate the output] ************************************************************************************************************************************************************************************
ok: [192.168.122.95] => {
    "anji.cmd": [
        "service",
        "vsftpd",
        "status"
    ]
}

TASK [The output of the command] **************************************************************************************************************************************************************************************************************
ok: [192.168.122.95] => {
    "anji.stdout": "â— vsftpd.service - vsftpd FTP server\n     Loaded: loaded (/lib/systemd/system/vsftpd.service; enabled; vendor preset: enabled)\n     Active: active (running) since Wed 2023-04-26 11:29:06 IST; 1h 45min ago\n   Main PID: 5108 (vsftpd)\n      Tasks: 1 (limit: 2476)\n     Memory: 520.0K\n     CGroup: /system.slice/vsftpd.service\n             â””â”€5108 /usr/sbin/vsftpd /etc/vsftpd.conf\n\nApr 26 11:29:06 s3 systemd[1]: Starting vsftpd FTP server...\nApr 26 11:29:06 s3 systemd[1]: Started vsftpd FTP server."
}

TASK [The error output of the command] ********************************************************************************************************************************************************************************************************
ok: [192.168.122.95] => {
    "anji.stderr": ""
}

PLAY RECAP ************************************************************************************************************************************************************************************************************************************
192.168.122.95             : ok=13   changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

Before I decode the above output I want to mention one thing that whenever you use register module for storing values it outputs following values:

changed - A Boolean indicating if the task had to make changes.

cmd â€“ The actual command which ran to generate the output.

stdout â€“ The output of the command.

stderr â€“ The error output of the command.

start â€“ The command execution begin time

end â€“ The command execution finished time

delta â€“ The total time taken to run the command. The value of this property is the difference between the end and the start time outputs.

stdout_lines â€“ An array of stdout lines one per line. When stdout is returned, Ansible always provides a list of strings, each containing one item per line from the original output.

stderr_lines â€“ An array of stderrlines one per line. When stderr is returned we also always provide this field which is a list of strings, one item per line from the original.
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++
---
- hosts: "192.168.122.201"
  become: yes
  tasks: 
    - name: find txt file
      shell: "find *.txt"
      args: 
        chdir: "/home/ansible/ansi/"
      register: cpu

    - shell: "rm -rf {{ item }}"
      args: 
        chdir: "/home/ansible/ansi"
      with_items:
        - "{{ cpu.stdout_lines }}"      
ansible@s3:~/playbook$ ansible-playbook var.yaml  -vv
ansible-playbook [core 2.12.10]
  config file = /etc/ansible/ansible.cfg
  configured module search path = ['/home/ansible/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']
  ansible python module location = /usr/lib/python3/dist-packages/ansible
  ansible collection location = /home/ansible/.ansible/collections:/usr/share/ansible/collections
  executable location = /usr/bin/ansible-playbook
  python version = 3.8.10 (default, Mar 13 2023, 10:26:41) [GCC 9.4.0]
  jinja version = 2.10.1
  libyaml = True
Using /etc/ansible/ansible.cfg as config file
Skipping callback 'default', as we already have a stdout callback.
Skipping callback 'minimal', as we already have a stdout callback.
Skipping callback 'oneline', as we already have a stdout callback.

PLAYBOOK: var.yaml *********************************************************************************************************************************
1 plays in var.yaml

PLAY [192.168.122.201] *****************************************************************************************************************************

TASK [Gathering Facts] *****************************************************************************************************************************
task path: /home/ansible/playbook/var.yaml:2
ok: [192.168.122.201]
META: ran handlers

TASK [find txt file] *******************************************************************************************************************************
task path: /home/ansible/playbook/var.yaml:5
changed: [192.168.122.201] => {"changed": true, "cmd": "find *.txt", "delta": "0:00:00.002652", "end": "2023-04-26 14:11:25.006424", "msg": "", "rc": 0, "start": "2023-04-26 14:11:25.003772", "stderr": "", "stderr_lines": [], "stdout": "a.txt\nb.txt\nc.txt", "stdout_lines": ["a.txt", "b.txt", "c.txt"]}

TASK [shell] ***************************************************************************************************************************************
task path: /home/ansible/playbook/var.yaml:11
changed: [192.168.122.201] => (item=a.txt) => {"ansible_loop_var": "item", "changed": true, "cmd": "rm -rf a.txt", "delta": "0:00:00.002337", "end": "2023-04-26 14:11:25.307558", "item": "a.txt", "msg": "", "rc": 0, "start": "2023-04-26 14:11:25.305221", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}
changed: [192.168.122.201] => (item=b.txt) => {"ansible_loop_var": "item", "changed": true, "cmd": "rm -rf b.txt", "delta": "0:00:00.002278", "end": "2023-04-26 14:11:25.501582", "item": "b.txt", "msg": "", "rc": 0, "start": "2023-04-26 14:11:25.499304", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}
changed: [192.168.122.201] => (item=c.txt) => {"ansible_loop_var": "item", "changed": true, "cmd": "rm -rf c.txt", "delta": "0:00:00.002384", "end": "2023-04-26 14:11:25.670252", "item": "c.txt", "msg": "", "rc": 0, "start": "2023-04-26 14:11:25.667868", "stderr": "", "stderr_lines": [], "stdout": "", "stdout_lines": []}
META: ran handlers
META: ran handlers

PLAY RECAP *****************************************************************************************************************************************
192.168.122.201            : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

+++++++++++++++++++++++++++
---
- hosts: "192.168.122.95"
  become: yes
  tasks: 
   - name: Using loops
     shell:
       cmd: rm -f "{{item}}"
     register: removed_output
     loop:
       - test_file.txt
       - abc.txt

   - name: Print the removed output
     debug:
       msg:
         - Return code for {{removed_output.results.0.item}} is {{removed_output.results.0.rc}}
         - Return code for {{removed_output.results.1.item}} is {{removed_output.results.1.rc}} 
ansible@s3:~/playbook$ ansible-playbook var.yaml 

PLAY [192.168.122.95] *************************************************************************************************

TASK [Gathering Facts] ************************************************************************************************
ok: [192.168.122.95]

TASK [Using loops] ****************************************************************************************************
changed: [192.168.122.95] => (item=test_file.txt)
changed: [192.168.122.95] => (item=abc.txt)

TASK [Print the removed output] ***************************************************************************************
ok: [192.168.122.95] => {
    "msg": [
        "Return code for test_file.txt is 0",
        "Return code for abc.txt is 0"
    ]
}

PLAY RECAP ************************************************************************************************************
192.168.122.95             : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

---
- hosts: 
  become: yes
  tasks: 
    -  name: using loops with with_items
       debug: 
         var: item
       with_items:
         - cpu
         - memory
         - ram
         - ssd
         - led
         - keyboard
         - mouse
         - motherboard  
PLAYBOOK: var.yaml ****************************************************************************************************
1 plays in var.yaml

PLAY [localhost] ******************************************************************************************************

TASK [Gathering Facts] ************************************************************************************************
task path: /home/ansible/playbook/var.yaml:2
ok: [localhost]
META: ran handlers

TASK [using loops with with_items] ************************************************************************************
task path: /home/ansible/playbook/var.yaml:5
ok: [localhost] => (item=cpu) => {
    "ansible_loop_var": "item",
    "item": "cpu"
}
ok: [localhost] => (item=memory) => {
    "ansible_loop_var": "item",
    "item": "memory"
}
ok: [localhost] => (item=ram) => {
    "ansible_loop_var": "item",
    "item": "ram"
}
ok: [localhost] => (item=ssd) => {
    "ansible_loop_var": "item",
    "item": "ssd"
}
ok: [localhost] => (item=led) => {
    "ansible_loop_var": "item",
    "item": "led"
}
ok: [localhost] => (item=keyboard) => {
    "ansible_loop_var": "item",
    "item": "keyboard"
}
ok: [localhost] => (item=mouse) => {
    "ansible_loop_var": "item",
    "item": "mouse"
}
ok: [localhost] => (item=motherboard) => {
    "ansible_loop_var": "item",
    "item": "motherboard"
}
META: ran handlers
META: ran handlers

PLAY RECAP ************************************************************************************************************
localhost                  : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
++++++++++++++++++++++++++++++++++++++++++++++++++++++++
---
- hosts: all1
  become: yes
  vars:
    listvar1:
       - 'a'
       - 'b'
       - 'c'
  
  tasks: 
  - name: install ubuntu packages
    ansible.inbuiltin.apt:
      name: "{{ item }}"
      state: latest   
    with_items:
      - git
      - apache2
      - net-tools
      - firewalld

  - name: copy index.html
    ansible.builtin.copy:
      src: /opt/index.html
      dest: /var/www/html/index.html    
  
  - name: start the apache server
    ansible.builtin.service: 
        name: apache2
        state: started
  - command: echo {{ item }}
    loop: [0,2,4,8,7,9]
  - command: echo {{ item }}
    loop: [0,2,4,8,7,9] 
    when: item > 5

  - name: remove user  aaa  bbb ccc  
    user: 
      name: "{{ item }}"
      state: absent
      remove: yes
    with_items: 
      - aaa
      - bbb
      - ccc
  - shell: echo "nested  test  a={{ item[0] }}  b={{ item[1] }} c={{ item[2] }}"
    with_nested:
      - ['red','blue','green']
      - [1,2,3]
      - ['up','down','strange']
  shell: echo "nested  test a={{ item[0] }}  b={{ item[1] }}"
  with_nested: 
   - listvar1
   - [1,2,3]       

---
- hosts: all
  become: yes
  tasks: 
  - name: create the users 
    user:
      name: "{{ item }}"
      group: ansible
    with_items:
      - 'aaa'
      - 'bbb'
      - 'ccc'  

ansible@s3:~/playbook$ ansible-playbook var.yaml  
PLAY [all] ************************************************************************************************************
TASK [Gathering Facts] ************************************************************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [create the users] ***********************************************************************************************
changed: [192.168.122.201] => (item=aaa)
changed: [192.168.122.95] => (item=aaa)
changed: [192.168.122.201] => (item=bbb)
changed: [192.168.122.95] => (item=bbb)
changed: [192.168.122.201] => (item=ccc)
changed: [192.168.122.95] => (item=ccc)

PLAY RECAP ************************************************************************************************************
192.168.122.201            : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.95             : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
++++++++++++++++++++++++++
https://www.devopsschool.com/blog/how-to-loop-and-iteration-executations-in-ansible-playbook-with-example/

---
- hosts: "*"
  become: yes
  tasks: 
    - name: Remove users ï¿½Chuckï¿½ and ï¿½Craigï¿½ from the system.
      user:
        name: "{{ item }}"
        state: absent
        remove: yes
      with_items:
        - aaa
        - bbb
        - ccc
++++++++++++++++++++++++++++++++++++++++++++++++++++
"192.168.122.95"

ansible.utils.index_of filter â€“ Find the indices of items in a list matching some criteria
https://docs.ansible.com/ansible/devel/collections/ansible/utils/index_of_filter.html
https://docs.ansible.com/ansible/latest/collections/index_lookup.html

Synopsisïƒ

    This plugin returns the indices of items matching some criteria in a list.

    When working with a list of dictionaries, the key to evaluate can be specified.

    index_of is also available as a lookup plugin for convenience.

    Using the parameters below- data|ansible.utils.index_of(test, value, key, fail_on_missing, wantlist)

Ansible Regex_Search to Filter Data
Today, we are going to discover another filter of the Ansible tool which is the regex search filter. We shall discover how Ansibleâ€™s regex search functions. We will also explore how regex search might be useful when using Ansible to find a matching string or character in the Ansible playbook.

In Ansible, reg-ex means regular expression and the search stands for to find something. So, the regex_search filter in Ansible is a kind of complex scan that seeks particular sequences rather than particular words and phrases. Instead of creating many literal search queries, regex makes it possible to find a specific character string using regular expressions. To retrieve a particular sequence, regex works with a search query and metacharacters. Regular expressionâ€™s fundamental building elements are metacharacters. Finding some specific strings like security numbers, license numbers, webpages addresses, mailing addresses, registration numbers, etc that fit into specific sequences can be done using regular expressions.

---
- hosts: "192.168.122.201"
  become: yes
  
  vars:
    fruit: 
      - apple
      - banana
      - orange
      - grapes

  tasks: 
    - name: test my fruits details
      debug: 
        msg: "banana  index number = {{ lookup('ansible.utils.index_of', fruit, 'regex','banana' ) }}"    
ansible@s3:~/playbook$ ansible-playbook var.yaml 

PLAY [192.168.122.201] ******************************************************************

TASK [Gathering Facts] ******************************************************************
ok: [192.168.122.201]

TASK [test my fruits details] ***********************************************************
ok: [192.168.122.201] => {
    "msg": "banana  index number = 1"
}

PLAY RECAP ******************************************************************************
192.168.122.201            : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
#############################3+++++++++++++++++++++++
---
- hosts: "192.168.122.95"
  become: yes
  tasks: 
    - name: store the contents of  value.txt in the 'out' varaible
      shell: cat value.txt 

      register: out

    - name: display the contents of the 'out' varaibles match two 
      debug: 
        msg: "{{ out.stdout | regex_search('.*two.*', ignorecase=True) }}"
+++++++++++++++++
- hosts: localhost

  gather_facts: false
  vars:
    my_var: "Test first file\nTest second file\nTest third file"


  tasks:
    - name: display the content of the 'my_var' variable, matching 'file'
      debug:
        msg: "{{ my_var | regex_findall('.*file.*', multiline=True, ignorecase=True) }}"
######################################
---
- hosts: localhost
  become: yes
  tasks: 
  - name: contents in data in out  varaible
    #shell: cat /home/ansible/playbook/data.txt
    shell: cat data.txt
    register: anjiout

  - debug: 
      var: anjiout
ansible@s3:~/playbook$ ansible-playbook var.yaml 

PLAY [localhost] ************************************************************************

TASK [Gathering Facts] ******************************************************************
ok: [localhost]

TASK [contents in data in out  varaible] ************************************************
changed: [localhost]

TASK [debug] ****************************************************************************
ok: [localhost] => {
    "anjiout": {
        "changed": true,
        "cmd": "cat /home/ansible/playbook/data.txt",
        "delta": "0:00:00.009290",
        "end": "2023-04-27 11:42:19.284978",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2023-04-27 11:42:19.275688",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "Line One\nLine Two\nLine Three",
        "stdout_lines": [
            "Line One",
            "Line Two",
            "Line Three"
        ]
    }
}

PLAY RECAP ******************************************************************************
localhost                  : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

---
- hosts: localhost
  become: yes
  tasks: 
  - name: contents in data in out  varaible
    #shell: cat /home/ansible/playbook/data.txt
    shell: cat data.txt
    register: anjiout

  # - debug: 
  #     var: anjiout         or  OR   OR   
      
  - name: display the content of the varaible two 
    debug: 
      msg: "{{ anjiout |  regex_search('.*two.*',ignorecase=True) }}"
      
ansible@s3:~/playbook$ ansible-playbook var.yaml 

PLAY [localhost] ************************************************************************

TASK [Gathering Facts] ******************************************************************
ok: [localhost]

TASK [contents in data in out  varaible] ************************************************
changed: [localhost]

TASK [display the content of the varaible two] ******************************************
ok: [localhost] => {
    "msg": {
        "changed": true,
        "cmd": "cat data.txt",
        "delta": "0:00:00.002368",
        "end": "2023-04-27 13:20:43.597935",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2023-04-27 13:20:43.595567",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "Line One\nLine Two\nLine Three",
        "stdout_lines": [
            "Line One",
            "Line Two",
            "Line Three"
        ]
    }
}

PLAY RECAP ******************************************************************************
localhost                  : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
++++++++++
---
- hosts: localhost
  tasks:
  - name: contents stored the data.txt  
    shell: cat  data.txt
    register: fuckstar
  - name: using stdout get data in file
    debug:
      msg: "{{  fuckstar.stdout  | regex_search('.*two.*', ignorecase=True)   }}"
      
ansible@s3:~/playbook$ ansible-playbook var.yaml 
[WARNING]: Could not match supplied host pattern, ignoring: localhoat

PLAY [localhoat] ************************************************************************
skipping: no hosts matched

PLAY RECAP ******************************************************************************

ansible@s3:~/playbook$ nano var.yaml 
ansible@s3:~/playbook$ ansible-playbook var.yaml 

PLAY [localhost] ************************************************************************

TASK [Gathering Facts] ******************************************************************
ok: [localhost]

TASK [contents stored the data.txt] *****************************************************
changed: [localhost]

TASK [using stdout get data in file] ****************************************************
ok: [localhost] => {
    "msg": "Line Two"
}

PLAY RECAP ******************************************************************************
localhost                  : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

#################################################################
By default, regex_search will only return the first match. In this example, the first line that contains "Line" will be returned, meaning "Line Two" and "Line Three" are not returned.
http://www.freekb.net/Article?id=2563
---
- hosts: localhost
  vars:
    azur: "for  APPLE\n for ball\nfor CAT"
  tasks: 
    - name: display the content of the match line 
      debug: 
        msg: "{{ azur  | regex_search('.*for.*', multiline=True, ignorecase=True )  }}"
     
---
- hosts: localhost
  vars:
    foo: "Line One\nLine Two\nLine Three"
  tasks:
  - name: display the content of the 'foo' variable, matching 'Line'
    debug: 
      msg: "{{ foo | regex_search('.*Line.*') }}"

  - name: display the content of the 'foo' variable, matching 'Line'
    debug: 
       msg: "{{ foo | regex_search('.*Line.*', multiline=True, ignorecase=True) }}"

  - name: display the content of the 'out' variable, matching the 'foo' variable
    debug: 
      msg: "{{ foo | regex_search('(.*' + Line + '.*)') }}"


  - debug: 
      msg: The foo variable does contain 'Line'
        when: foo | regex_search('.*Line.*')

  - debug: 
      msg: The foo variable does NOT contain 'bar'
        when: not foo | regex_search('.*bar.*')


ansible@s3:~/playbook$ nano var.yaml 
ansible@s3:~/playbook$ ansible-playbook var.yaml 

PLAY [localhost] ************************************************************************

TASK [Gathering Facts] ******************************************************************
ok: [localhost]

TASK [display the content of the 'foo' variable, matching 'Line'] ***********************
ok: [localhost] => {
    "msg": "Line One"
}

TASK [display the content of the 'foo' variable, matching 'Line'] ***********************
ok: [localhost] => {
    "msg": "Line One"
}

+++++++++++++++++++++++++++
---
- hosts: localhost
  vars: 
    cpu: "Line One\nLine Two\nLine Three"
  tasks: 
    - name: display the content of the variable  match line
      debug: 
        msg: "{{ cpu | regex_search('.*Line.*') }}" 

  ansible@s3:~/playbook$ ansible-playbook var.yaml 

PLAY [localhost] ************************************************************************

TASK [Gathering Facts] ******************************************************************
ok: [localhost]

TASK [display the content of the variable  match line] **********************************
ok: [localhost] => {
    "msg": "Line One"
}

PLAY RECAP ******************************************************************************
localhost                  : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
########################################3
---
- hosts: localhost
  tasks: 
  -  name:  print the items 
     debug: 
       msg: this letter is a {{ item }}
     with_items:
     - "a"
     - "b"
     - "c"  
ansible@s3:~/playbook$ ansible-playbook var.yaml 

PLAY [localhost] ************************************************************************

TASK [Gathering Facts] ******************************************************************
ok: [localhost]

TASK [print the items] ******************************************************************
ok: [localhost] => (item=a) => {
    "msg": "this letter is a a"
}
ok: [localhost] => (item=b) => {
    "msg": "this letter is a b"
}
ok: [localhost] => (item=c) => {
    "msg": "this letter is a c"
}

PLAY RECAP ******************************************************************************
localhost                  : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
##################3
---
- hosts: 
  tasks: 
  -  debug: 
      msg: present running databases list {{ item }}
     with_items:
      - ["mysql", "postgresssql"]  
      - "sql"
      - ["mongodb","documentdb","firestore"]
      - "nosql"
ansible@s3:~/playbook$ ansible-playbook var.yaml 

PLAY [localhost] ************************************************************************

TASK [Gathering Facts] ******************************************************************
ok: [localhost]

TASK [debug] ****************************************************************************
ok: [localhost] => (item=mysql) => {
    "msg": "present running databases list mysql"
}
ok: [localhost] => (item=postgresssql) => {
    "msg": "present running databases list postgresssql"
}
ok: [localhost] => (item=sql) => {
    "msg": "present running databases list sql"
}
ok: [localhost] => (item=mongodb) => {
    "msg": "present running databases list mongodb"
}
ok: [localhost] => (item=documentdb) => {
    "msg": "present running databases list documentdb"
}
ok: [localhost] => (item=firestore) => {
    "msg": "present running databases list firestore"
}
ok: [localhost] => (item=nosql) => {
    "msg": "present running databases list nosql"
}

PLAY RECAP ******************************************************************************
localhost                  : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
###############################
---
- hosts: localhost
  become: yes
  tasks: 
    - name: create file 
      file:
        path: /opt/{{ item.name }}
        state: touch
        mode: "{{ item.perm }}"
      register: fuck  
      with_items: 
        - { name: banana , perm: "0777" }

        - {  name: apple , perm: "0644" }

    - debug: 
        var: fuck   
PLAY [localhost] ************************************************************************

TASK [Gathering Facts] ******************************************************************
ok: [localhost]

TASK [create file] **********************************************************************
changed: [localhost] => (item={'name': 'banana', 'perm': '0777'})
changed: [localhost] => (item={'name': 'apple', 'perm': '0644'})

TASK [debug] ****************************************************************************
ok: [localhost] => {
    "fuck": {
        "changed": true,
        "msg": "All items completed",
        "results": [
            {
                "ansible_loop_var": "item",
                "changed": true,
                "dest": "/opt/banana",
                "diff": {
                    "after": {
                        "atime": 1682590668.0565805,
                        "mtime": 1682590668.0565805,
                        "path": "/opt/banana",
                        "state": "touch"
                    },
                    "before": {
                        "atime": 1682590558.0719194,
                        "mtime": 1682590558.0719194,
                        "path": "/opt/banana",
                        "state": "file"
                    }
                },
                "failed": false,
                "gid": 0,
                "group": "root",
                "invocation": {
                    "module_args": {
                        "_diff_peek": null,
                        "_original_basename": null,
                        "access_time": null,
                        "access_time_format": "%Y%m%d%H%M.%S",
                        "attributes": null,
                        "follow": true,
                        "force": false,
                        "group": null,
                        "mode": "0777",
                        "modification_time": null,
                        "modification_time_format": "%Y%m%d%H%M.%S",
                        "owner": null,
                        "path": "/opt/banana",
                        "recurse": false,
                        "selevel": null,
                        "serole": null,
                        "setype": null,
                        "seuser": null,
                        "src": null,
                        "state": "touch",
                        "unsafe_writes": false
                    }
                },
                "item": {
                    "name": "banana",
                    "perm": "0777"
                },
                "mode": "0777",
                "owner": "root",
                "size": 0,
                "state": "file",
                "uid": 0
            },

#########################
Combine Them With Playbooks  developing an Ansible role in Linux    Ansible roles

Roles provide a framework for fully independent or interdependent collections of files, tasks, templates, variables, and modules.

The role is the primary mechanism for breaking a playbook into multiple files. This simplifies writing complex playbooks and makes them easier to reuse. 
The breaking of the playbook allows you to break the playbook into reusable components.

Each role is limited to a particular functionality or desired output, with all the necessary steps to provide that result either within the same role itself or in other roles listed as dependencies.

Roles are not playbooks. Roles are small functionality that can be used within the playbooks independently. Roles have no specific setting for which hosts the role will apply


defaults â€“  Includes default values for variables of the role. Here we define some sane default variables, but they have the lowest priority and are usually overridden by other methods to customize the role.
files  â€“ Contains static and custom files that the role uses to perform various tasks.
handlers â€“ A set of handlers that are triggered by tasks of the role. 
meta â€“ Includes metadata information for the role, its dependencies, the author, license, available platform, etc.
tasks â€“ A list of tasks to be executed by the role. This part could be considered similar to the task section of a playbook.
templates â€“ Contains Jinja2 template files used by tasks of the role. (Read more about how to create an Ansible template.)
tests â€“ Includes configuration files related to role testing.
vars â€“ Contains variables defined for the role. These have quite a high precedence in Ansible.

Ansible roles are consists of many playbooks, which is similar to modules in puppet and cook books in chef. We term the same in ansible as roles.
Roles are a way to group multiple tasks together into one container to do the automation in very effective manner with clean directory structures.
Roles are set of tasks and additional files for a certain role which allow you to break up the configurations.
It can be easily reuse the codes by anyone if the role is suitable to someone.
It can be easily modify and will reduce the syntax errors.

defaults: contains default variables for the role. Variables in default have the lowest priority so they are easy to override.
vars: contains variables for the role. Variables in vars have higher priority than variables in defaults directory.
tasks: contains the main list of steps to be executed by the role.
files: contains files which we want to be copied to the remote host. We donâ€™t need to specify a path of resources stored in this directory.
templates: contains file template which supports modifications from the role. We use the Jinja2 templating language for creating templates.
meta: contains metadata of role like an author, support platforms, dependencies.
handlers: contains handlers which can be invoked by â€œnotifyâ€ directives and are associated with service.

https://galaxy.ansible.com/geerlingguy/mysql

ansible-galaxy install geerlingguy.mysql

s3@s3:~$ cat /etc/ansible/ansible.cfg 
# Since Ansible 2.12 (core):
# To generate an example config file (a "disabled" one with all default settings, commented out):
               $ ansible-config init --disabled > ansible.cfg
#
# Also you can now have a more complete file by including existing plugins:
 ansible-config init --disabled -t all > ansible.cfg

# For previous versions of Ansible you can check for examples in the 'stable' branches of each version
# Note that this file was always incomplete  and lagging changes to configuration settings

# for example, for 2.9: https://github.com/ansible/ansible/blob/stable-2.9/examples/ansible.cfg

ANSWER== :   s3@s3:~$ ls
Desktop  Documents  Downloads  Music  Pictures  playbook  Public  Templates  Videos
s3@s3:~$ wget https://github.com/ansible/ansible/blob/stable-2.9/examples/ansible.cfg
--2023-04-28 12:05:16--  https://github.com/ansible/ansible/blob/stable-2.9/examples/ansible.cfg
Resolving github.com (github.com)... 20.207.73.82
Connecting to github.com (github.com)|20.207.73.82|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: unspecified [text/html]
Saving to: â€˜ansible.cfgâ€™

ansible.cfg                  [ <=>                            ] 298.39K  --.-KB/s    in 0.1s    

2023-04-28 12:05:17 (2.03 MB/s) - â€˜ansible.cfgâ€™ saved [305549]

s3@s3:~$ ls
ansible.cfg  Desktop  Documents  Downloads  Music  Pictures  playbook  Public  Templates  Videos
ORÂ  ORÂ  ORÂ  
ansible@s3:~$ cd /etc/ansible/
ansible@s3:/etc/ansible$ ll
total 28
drwxr-xr-x Â  3 root root Â 4096 Apr 28 11:52 ./
drwxr-xr-x 137 root root 12288 Apr 27 09:03 ../
-rw-r--r-- Â  1 root root Â  612 Apr 28 11:52 ansible.cfg
-rw-r--r-- Â  1 root root Â 1065 Apr 27 14:47 hosts
drwxr-xr-x Â  2 root root Â 4096 Oct 25 Â 2022 roles/
ansible@s3:/etc/ansible$ cat ansible.cfg 
# Since Ansible 2.12 (core):
# To generate an example config file (a "disabled" one with all default settings, commented out):
Â  Â  Â  Â  Â  Â  Â  Â $ ansible-config init --disabled > ansible.cfg
#
# Also you can now have a more complete file by including existing plugins:
Â ansible-config init --disabled -t all > ansible.cfg

# For previous versions of Ansible you can check for examples in the 'stable' branches of each version
# Note that this file was always incomplete Â and lagging changes to configuration settings

# for example, for 2.9: https://github.com/ansible/ansible/blob/stable-2.9/examples/ansible.cfg
ansible@s3:/etc/ansible$ ansible-config init Â > ansible.cfg 
-bash: ansible.cfg: Permission denied
ansible@s3:/etc/ansible$ sudo Â ansible-config init Â > ansible.cfg 
-bash: ansible.cfg: Permission denied
ansible@s3:/etc/ansible$ sudo chmod 777 *
ansible@s3:/etc/ansible$ sudo Â ansible-config init Â > ansible.cfg 
ansible@s3:/etc/ansible$ ls
ansible.cfg Â hosts Â roles
ansible@s3:/etc/ansible$ cat ansible.cfg 
[defaults]
# (boolean) By default Ansible will issue a warning when received from a task action (module or action plugin)
# These warnings can be silenced by adjusting this setting to False.
action_warnings=True

# (list) Accept list of cowsay templates that are 'safe' to use, set to empty list if you want to enable all installed templates.
cowsay_enabled_stencils=bud-frogs, bunny, cheese, daemon, default, dragon, elephant-in-snake, elephant, eyes, hellokitty, kitty, luke-koala, meow, milk, moofasa, moose, ren, sheep, small, stegosaurus, stimpy, supermilker, three-eyes, turkey, turtle, tux, udder, vader-koala, vader, www

# (string) Specify a custom cowsay path or swap in your cowsay implementation of choice
cowpath=

# (string) This allows you to chose a specific cowsay stencil for the banners or use 'random' to cycle through them.
cow_selection=default
##########################################################
https://galaxy.ansible.com/geerlingguy/mysql
ansible-galaxy install geerlingguy.mysql


ansible@s3:~/.ansible/roles/geerlingguy.mysql$ tree

â”œâ”€â”€ defaults
â”‚Â Â  â””â”€â”€ main.yml
â”œâ”€â”€ handlers
â”‚Â Â  â””â”€â”€ main.yml
â”œâ”€â”€ LICENSE
â”œâ”€â”€ meta
â”‚Â Â  â””â”€â”€ main.yml
â”œâ”€â”€ molecule
â”‚Â Â  â””â”€â”€ default
â”‚Â Â      â”œâ”€â”€ converge.yml
â”‚Â Â      â””â”€â”€ molecule.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ tasks
â”‚Â Â  â”œâ”€â”€ configure.yml
â”‚Â Â  â”œâ”€â”€ databases.yml
â”‚Â Â  â”œâ”€â”€ main.yml
â”‚Â Â  â”œâ”€â”€ replication.yml
â”‚Â Â  â”œâ”€â”€ secure-installation.yml
â”‚Â Â  â”œâ”€â”€ setup-Archlinux.yml
â”‚Â Â  â”œâ”€â”€ setup-Debian.yml
â”‚Â Â  â”œâ”€â”€ setup-RedHat.yml
â”‚Â Â  â”œâ”€â”€ users.yml
â”‚Â Â  â””â”€â”€ variables.yml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ my.cnf.j2
â”‚Â Â  â”œâ”€â”€ root-my.cnf.j2
â”‚Â Â  â””â”€â”€ user-my.cnf.j2
â””â”€â”€ vars
    â”œâ”€â”€ Archlinux.yml
    â”œâ”€â”€ Debian-10.yml
    â”œâ”€â”€ Debian-11.yml
    â”œâ”€â”€ Debian.yml
    â”œâ”€â”€ RedHat-7.yml
    â”œâ”€â”€ RedHat-8.yml
    â””â”€â”€ RedHat-9.yml
8 directories, 27 files
############################
ansible@s3:~/.ansible/roles$ ll
total 12
drwxrwxr-x  3 ansible ansible 4096 Apr 28 16:11 ./
drwxrwxr-x  7 ansible ansible 4096 Apr 28 16:15 ../
drwxrwxr-x 10 ansible ansible 4096 Apr 28 16:11 geerlingguy.mysql/
ansible@s3:~/.ansible/roles$ ansible-galaxy role init mylamprole 
- Role mylamprole was created successfully
ansible@s3:~/.ansible/roles$ ll
total 16
drwxrwxr-x  4 ansible ansible 4096 Apr 28 17:32 ./
drwxrwxr-x  7 ansible ansible 4096 Apr 28 16:15 ../
drwxrwxr-x 10 ansible ansible 4096 Apr 28 16:11 geerlingguy.mysql/
drwxrwxr-x 10 ansible ansible 4096 Apr 28 17:32 mylamprole/
ansible@s3:~/.ansible/roles$ tree mylamprole/
mylamprole/
â”œâ”€â”€ defaults
â”‚Â Â  â””â”€â”€ main.yml
â”œâ”€â”€ files
â”œâ”€â”€ handlers
â”‚Â Â  â””â”€â”€ main.yml
â”œâ”€â”€ meta
â”‚Â Â  â””â”€â”€ main.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ tasks
â”‚Â Â  â””â”€â”€ main.yml
â”œâ”€â”€ templates
â”œâ”€â”€ tests
â”‚Â Â  â”œâ”€â”€ inventory
â”‚Â Â  â””â”€â”€ test.yml
â””â”€â”€ vars
    â””â”€â”€ main.yml

8 directories, 8 files
ansible@s3:~/.ansible/roles$ 

ansible@s3:~$ ansible -m setup -a 'filter=*_os_*'  all 
192.168.122.95 | SUCCESS => {
    "ansible_facts": {
        "ansible_os_family": "Debian",
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false
}

ansible@s3:~$ ansible -m setup -a 'filter=ansible_fqdn'  all 
192.168.122.95 | SUCCESS => {
    "ansible_facts": {
        "ansible_fqdn": "s3",
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false
}

+++++++++++++++++++++++++++++++++++++++++++++++
enp1s0
enp0s10:
| | |
v | |
en| |   --> ethernet
  v |
  p0|   --> bus number (0)
    v
    s10 --> slot number (10)
myrole/
â”œâ”€â”€ defaults
â”‚Â Â  â””â”€â”€ main.yml
â”œâ”€â”€ files
â”‚Â Â  â””â”€â”€ index.html
â”œâ”€â”€ handlers
â”‚Â Â  â””â”€â”€ main.yml
â”œâ”€â”€ meta
â”‚Â Â  â””â”€â”€ main.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ tasks
â”‚Â Â  â””â”€â”€ main.yml
â”œâ”€â”€ templates
â”‚Â Â  â””â”€â”€ apache2.j2
â”œâ”€â”€ tests
â”‚Â Â  â”œâ”€â”€ inventory
â”‚Â Â  â””â”€â”€ test.yml
â””â”€â”€ vars
    â””â”€â”€ main.yml

8 directories, 10 files
jira@jira:~/.ansible$ 
##################################
â”œâ”€â”€ defaults
â”‚Â Â  â””â”€â”€ main.yml
=============================
â”œâ”€â”€ files
â”‚Â Â  â””â”€â”€ index.html

WELCOME TO  INDIA
##################################3
â”œâ”€â”€ handlers
â”‚Â Â  â””â”€â”€ main.yml

---
# handlers file for myrole
name: test_service 
service: 
   name: "{{ test_service }}"
   state: restarted
######################33
â”œâ”€â”€ meta
â”‚Â Â  â””â”€â”€ main.yml
galaxy_info:
  author: anjireddy
  description: this is role apache2 
  company: visa.com
######################3
â”œâ”€â”€ tasks
â”‚Â Â  â””â”€â”€ main.yml
---
# tasks file for myrole

- name: INATALLING THE APACHE2  
  apt: 
    name: "{{ task_apache2 }}"
    state: present
    update_cache: yes

- name: handlers daemon using  apache service retser 
  service: 
    name: "{{ test_service }}"
    state: started

- name: copy index.html  var/www/html/
  copy: 
    src: index.html
    dest: "{{ index_dest_root }}"

- name:  add apache configuration setting 
  template:              #/etc/apache2/conf-enabled/apache.j2  => UBUNTU
    src: apache.j2       # /etc/httpd/conf.d/apache.j2  => centos/redhat 
    dest: "{{ j2_config }}"    
  notify: restart_apache 
#####################################33
â”œâ”€â”€ templates
â”‚Â Â  â””â”€â”€ apache2.j2
---
# tasks file for myrole

- name: INATALLING THE APACHE2  
  apt: 
    name: "{{ task_apache2 }}"
    state: present
    update_cache: yes

- name: handlers daemon using  apache service retser 
  service: 
    name: "{{ test_service }}"
    state: started

- name: copy index.html  var/www/html/
  copy: 
    src: index.html
    dest: "{{ index_dest_root }}"

- name:  add apache configuration setting 
  template:              #/etc/apache2/conf-enabled/apache.j2  => UBUNTU
    src: apache.j2       # /etc/httpd/conf.d/apache.j2  => centos/redhat 
    dest: "{{ j2_config }}"    
  notify: restart_apache 
#################################3
â”œâ”€â”€ tests
â”‚Â Â  â”œâ”€â”€ inventory
â”‚Â Â  â””â”€â”€ test.yml
 inventry
 localhost
++++++++
---
- hosts: localhost
  remote_user: ansible
  roles:
    - role:  myrole
  become: 
###########################3
â””â”€â”€ vars
    â””â”€â”€ main.yml
---
# vars file for myrole
task_apache2: apache2
test_service: apache2
index_dest_root: /var/www/html/
j2_config: /etc/apache2/conf-enabled/
#################################################################################
Ansible Stat Module Usage
https://linuxhint.com/ansible-stat-module/

Check if File Exists

The stat module will fetch information about a specified file or directory and save it using the register parameter.

In the following example playbook, we check if the file /var/log/alternatives.log exists.
---
- name: ansible stat module
  hosts: all
  become: yes
  tasks:
  - name: check alternatives.log
    stat:
      path: /var/log/alternatives.log
    register: info
  - name: tell if the file is there
    debug:
      msg: file exists
    when: info.stat.exists
  - name: tell if file missing
    debug:
      msg: the file missing
    when: not info.stat.exists

In the example above, we call the stat module to gather info about the file /var/log/alternatives.log from the remote host.

Check if a Directory Exists

The playbook to check if a directory exists using the stat module is similar to the one shown above. However, we provide a path to a target directory as shown below:
---
- name: ansible stat module
  hosts: all
  become: yes
  tasks:
  - name: check log directory
    stat:
      path: /var/log/
    register: dir_info
  - name: tell if directory exists
    debug:
      msg:  target directory exists
    when: dir_info.stat.exists
  - name: tell if dir is missing
    debug:
      msg: directory is missing
    when: not dir_info.stat.exists

Once we run the playbook, we should see an output similar to the one shown below:
Check if a user owns a file

The ansible stat module returns a collection of values for the specified file or directory. One such return variable is pw_name; this variable returns the username of the target file or directory owner.

---
- name: check file ownership
  hosts: all
  gather_facts: no
  become: yes
  tasks:
  - name: get file info
    stat:
      path: /var/log/kern.log
    register: file_info
  - name: owned by ubuntu usert?
    debug:
      msg:  file is owned by the ubuntu user
    when: file_info.stat.pw_name != 'ubuntu'
  - name: not owned by the ubuntu user?
    debug:
      msg: file is not owned by the ubuntu user
    when: not file_info.stat.pw_name != 'ubuntu'

Check file type

Another return value of the stat module allows us to check the file type. Using return values such as isreg and isdir, we can check if a file is a directory:
---
- name: check file type
  hosts: all
  become: ye
  tasks:
  - name: get file info
    stat:
      path: /var/log/kern.log
    register: file_info
  - name: regular file?
    debug:
      msg:  specified path is a regular file
    when: file_info.stat.isreg
  - name: is a directory?
    debug:
      msg: specified path is a directory
    when: file_info.stat.isdir
Ansible stat return values

The following are the values returned by the ansible stat module:

    attributes â€“ Returns the attributes of the specified file.
    executable â€“ Returns true if the invoking user has executed permissions on the target path.
    exists â€“ Returns true if the specified path exists.
    gr_name â€“ Returns the name of the group of the file owner.
    islbk â€“ Returns true if the specified file is a block device
    ischr â€“ Returns true if the specified file is a character file.
    isreg â€“ Returns true if the specified file is a regular file
    isdir â€“ Returns true if the specified file is a directory.
    islnk â€“ Returns true if the target file is a link
    mode â€“ Returns the file permission in octal notation
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://github.com/anjilinux/project-ansible-role-tomcat-.git

ira@jira:~/git/project-ansible-role-tomcat-$ tree
.
â”œâ”€â”€ defaults
â”‚Â Â  â””â”€â”€ main.yml
â”œâ”€â”€ handlers
â”‚Â Â  â””â”€â”€ main.yml
â”œâ”€â”€ LICENSE
â”œâ”€â”€ meta
â”‚Â Â  â””â”€â”€ main.yml
â”œâ”€â”€ molecule
â”‚Â Â  â”œâ”€â”€ default
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ create.yml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ destroy.yml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Dockerfile.j2
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ INSTALL.rst
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ molecule.yml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ playbook.yml
â”‚Â Â  â”‚Â Â  â””â”€â”€ prepare.yml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test_default.py
â”œâ”€â”€ README.md
â”œâ”€â”€ tasks
â”‚Â Â  â”œâ”€â”€ configure.yml
â”‚Â Â  â”œâ”€â”€ install.yml
â”‚Â Â  â”œâ”€â”€ main.yml
â”‚Â Â  â”œâ”€â”€ set-non-production-permissions.yml
â”‚Â Â  â”œâ”€â”€ set-production-permissions.yml
â”‚Â Â  â””â”€â”€ uninstall.yml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ host-manager-context.xml.j2
â”‚Â Â  â”œâ”€â”€ manager-context.xml.j2
â”‚Â Â  â”œâ”€â”€ tomcat-server-7.0.xml.j2
â”‚Â Â  â”œâ”€â”€ tomcat-server-8.0.xml.j2
â”‚Â Â  â”œâ”€â”€ tomcat-server-8.5.xml.j2
â”‚Â Â  â”œâ”€â”€ tomcat-server-9.0.xml.j2
â”‚Â Â  â”œâ”€â”€ tomcat.service.j2
â”‚Â Â  â”œâ”€â”€ tomcat.setenv.sh.j2
â”‚Â Â  â”œâ”€â”€ tomcat-users-7.0.xml.j2
â”‚Â Â  â”œâ”€â”€ tomcat-users-8.0.xml.j2
â”‚Â Â  â”œâ”€â”€ tomcat-users-8.5.xml.j2
â”‚Â Â  â””â”€â”€ tomcat-users-9.0.xml.j2
â”œâ”€â”€ tests
â”‚Â Â  â”œâ”€â”€ inventory
â”‚Â Â  â””â”€â”€ test.yml
â””â”€â”€ vars
    â””â”€â”€ main.yml
##########################################3
What are Jinja2 templates?

    {% ... %} is used for statements.
    {{ ... }} is used for variables.
    {# ... #} is used for comments.
    # ... ## is used for line statements.

{% if ... %}
{% elif ... %}
{% else %}
{% endif %}
{% for elements in array %}
    ...
{% endfor %}
{% if true %}
{% endif %}

{% extends "base.html" %}
{% block title %}Members{% endblock %}
{% block content %}
  <ul>
  {% for user in users %}
    <li><a href="{{ user.url }}">{{ user.username }}</a></li>
  {% endfor %}
  </ul>
{% endblock %}

Python Jinja module

Jinja is a template engine for Python. It is similar to the Django template engine.

A template engine or template processor is a library designed to combine templates with a data model to produce documents. Template engines are often used to generate large amounts of emails, in source code preprocessing, or producing dynamic HTML pages.

We create a template engine, where we define static parts and dynamic parts. The dynamic parts are later replaced with data. The rendering function later combines the templates with data.
Jinja installation

$ sudo pip3 install jinja2

Jinja delimiters

Jinja uses various delimiters in the template strings.

    {% %} - statements
    {{ }} - expressions to print to the template output
    {# #} - comments which are not included in the template output
    # ## - line statements

Jinja simple example

In the first example, we create a very simple template.
simple.py

#!/usr/bin/python

from jinja2 import Template

name = input("Enter your name: ")

tm = Template("Hello {{ name }}")
msg = tm.render(name=name)

print(msg)
######################################################################################################3
# ansible playbook for apache2 ufw firewall ansible playbook for ufw register debug
---
- hosts: all
  tasks: 
    - name: install apache2 on ubuntu 
      apt: 
        name: apache2
        state: latest
        update_cache: yes
    - name: allow http trafiic on ufw firewall 
      ufw: 
        rule: allow
        port: http
        proto: tcp
      notify: 
        - restart apache2
        - reload ufw firewall 
      register: httpd

    -  debug: 
        msg: "httpd.apache2_status" 


  handlers:        
     - name: restrt apache2 
       service: 
          name: apache2
          state: restarted 
     - name:  reload ufw firewall 
       ufw: 
         state: enabled 

PLAY [all] ***************************************************************************************

TASK [Gathering Facts] ***************************************************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [install apache2 on ubuntu] *****************************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [allow http trafiic on ufw firewall] ********************************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [debug] *************************************************************************************
ok: [192.168.122.95] => {
    "msg": "httpd.apache2_status"
}
ok: [192.168.122.201] => {
    "msg": "httpd.apache2_status"
}

PLAY RECAP ***************************************************************************************
192.168.122.201            : ok=4    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.95             : ok=4    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  
##############################################3
ANSIBLE RUNNING IN CONTAINER   ANSIBLE RUNNING ION DOCKER  CONTAINER  ANSIBLE MASTER RUNNING IN CONTAINER ANSIBLE CONTROLLER RUNNING ON CONTAINER  

$ mkdir docker_server && cd $_
$ mkdir vars && cd $_ && touch default.yml
$ cd .. && touch main.yml

The directory layout should look like:
docker_server/
|-- main.yml
`-- vars
   `-- default.yml
 
1 directory, 2 files

Letâ€™s see what each of these files are:

    docker_server: This is the project root directory containing all variable files and main playbook.
    vars/default.yml: Variable file resides in vars directory through which you are going to customize the playbook settings.
    main.yml: Here, you are going to define the task that is going to execute on the remote server.

vars/default.yml

Now first begin with the playbookâ€™s variable file. Here you are going to customize your Docker setup. Open vars/default.yml in your editor of choice:
$ cd docke_server && nano vars/default.yml

Copy the below lines and paste it in vars/default.yml:
---
containers: 2
container_name: docker_ubuntu
container_image: ubuntu:18.04
container_command: sleep 1d

A brief explanation of each of these variables:

    containers: You can define n number of containers you want to launch. Just make sure that your remote system has enough juice to run it smoothly.
    container_name: This variable is used to name the running containers.
    container_image: Image that you use when creating containers.
    container_command: Command that is going to run inside the new containers.

main.yml

In this file, you are going to define all tasks, where you are going to define the group of servers that should be targeted with privilege sudo. Here you are also going to load the vars/default.yml variable file you created previously. Again paste the following lines, make sure that file is in a format that follows the YAML standards.
---
- hosts: all
  become: true
  vars_files:
   - vars/default.yml
 
 tasks:
   - name: Install aptitude using apt
     apt: name=aptitude state=latest update_cache=yes force_apt_get=yes
 
   - name: Install required system packages
     apt: name={{ item }} state=latest update_cache=yes
     loop: [ 'apt-transport-https', 'ca-certificates', 'curl', 'software-properties-common', 'python3-pip', 'virtualenv', 'python3-setuptools']
 
   - name: Add Docker GPG apt Key
     apt_key:
       url: https://download.docker.com/linux/ubuntu/gpg
       state: present
 
   - name: Add Docker Repository
     apt_repository:
       repo: deb https://download.docker.com/linux/ubuntu bionic stable
       state: present
 
   - name: Update apt and install docker-ce
     apt: update_cache=yes name=docker-ce state=latest
 
   - name: Install Docker Module for Python
     pip:
       name: docker
 
   - name: Pull default Docker image
     docker_image:
       name: "{{ container_image }}"
       source: pull
 
   - name: Create default containers
     docker_container:
       name: "{{ container_name }}{{ item }}"
       image: "{{ container_image }}"
       command: "{{ container_command }}"
       state: present
     with_sequence: count={{ containers }}
https://gcore.com/blog/install-and-setup-docker-using-ansible-on-ubuntu-18-04-part-2/
###################################################33
ERROR! vars file vars/onecontainer.yml was not found
Could not find file on the Ansible Controller.
If you are using a module and expect the file to exist on the remote, see the remote_src option
https://gcore.com/blog/install-and-setup-docker-using-ansible-on-ubuntu-18-04-part-2/

ANSWER==- hosts: all 
  vars_files: 
    - ./onecontainer.yml 

---
containers: 2
container_name: docker_ubuntu
container_image: ubuntu:18.04
container_command: sleep 1d

###################33

fatal: [192.168.122.201]: FAILED! => {"changed": false, "msg": "Error connecting: Error while fetching server API version: request() got an unexpected keyword argument 'chunked'"}

---
- hosts: aws
  remote_user: ec2-user
  tasks:
  - name: get info on the nginx container
    docker_container_info:
      name: nginx
...

---
- hosts: aws
  remote_user: ec2-user
  become: yes
  become_method: sudo
  tasks:
  - name: get info on the nginx container
    docker_container_info:
      name: nginx
...

fatal: [192.168.122.201]: FAILED! => {"changed": false, "msg": "value of state must be one of: absent, present, got: build"}

---
- hosts: webservers
  remote_user: linx
  become: yes
  become_method: sudo
  tasks:

    - name: install docker-py
      pip: name=docker-py

    - name: Build Docker image from Dockerfile
      docker_image:
        name: web
        path: docker
        state: build

    - name: Running the container
      docker_container:
        image: web:latest
        path: docker
        state: running

    - name: Check if container is running
      shell: docker ps

---
- hosts: webservers
  remote_user: ec2-user
  become: yes
  become_method: sudo
  tasks:
    - name: install docker
      yum: name=docker

    **- name: Ensure service is enabled
      command: service docker restart***

    - name: copying file to remote
      copy:
        src: ./docker
        dest: /home/ec2-user/docker
    - name: Build Docker image from Dockerfile
      docker_image:
        name: web
        path: /home/ec2-user/docker
        state: build
    - name: Running the container
      docker_container:
        image: web:latest
        name: web
    - name: Check if container is running
      shell: docker ps
============
- name: docker login
  hosts: my_server
  become: yes
  become_user: docker_user
  tasks:
    - docker_login:
        registry: myregistry.com
        username: myusername
        password: mysecret
=====================
---
- hosts: all
  tasks:

  - name: Create a data container
    docker_container:
      name: test
      image: abiosoft/caddy:0.9.3
      tls_hostname: tcp://[xxxx:xxxx:xxxx:xxxx::xxxx]:2376
      tls_verify: true
      key_path: /Users/jan/.docker/custom/server/key.pem
      cert_path: /Users/jan/.docker/custom/server/cert.pem
      cacert_path: /Users/jan/.docker/custom/server/ca.pem
 export DOCKER_CERT_PATH=/Users/jan/.docker/custom/server; 
$ export DOCKER_HOST=tcp://[xxxx:xxxx:xxxx:xxxx::xxxx]:2376; 
$ export DOCKER_TLS_VERIFY=1
$ docker info

$ pip install urllib3
$ pip install pyopenssl
$ pip install ndg-httpsclient
$ pip install pyasn1

==========
---
- hosts: all 
  vars_files: 
    - ./onecontainer.yml 
   
  tasks: 
    - name: install aptitude using apt 
      apt: name=aptitude  state=latest update_cache=yes 
      #remote_src: yes
    
    - name: install requred packages using loop 
      apt: name={{ item  }} state=latest  update_cache=yes 
      
      loop: 
        - apt-transport-https
        - ca-certificates
        - curl
        - software-properties-common
        - python3-pip
        - virtualenv
        - python3-setuptools
        
    - name: add docker gpg apt key 
      apt_key: 
        url:  https://download.docker.com/linux/ubuntu/gpg
        state: present
        

    - name: add docker repository 
      apt_repository: 
         repo:  deb https://download.docker.com/linux/ubuntu bionic stable
         state: present

    - name: update apt and install docker-ce 
      apt: update_cache=yes name=docker-ce  state=latest 

    - name: install docker module for python 
      pip: 
        name: docker 

    - name: pull default image 
      docker_image: 
        name: "{{  container_image }}"
        source: pull

    - name: creare default containers
      docker_container:
        name: "{{ container_name }}{{ item }}"
        image: "{{ container_image }}"
        command: "{{  container_command  }}"
        state: present 
      with_sequence: count={{ containers }}  
#####
root@ff82f26369fe:~/playbooks# ansible-playbook docker.yaml 

PLAY [all] **************************************************************************************

TASK [Gathering Facts] **************************************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [install aptitude using apt] ***************************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [install requred packages using loop] ******************************************************
ok: [192.168.122.95] => (item=apt-transport-https)
ok: [192.168.122.201] => (item=apt-transport-https)
ok: [192.168.122.95] => (item=ca-certificates)
ok: [192.168.122.201] => (item=ca-certificates)
ok: [192.168.122.95] => (item=curl)
ok: [192.168.122.201] => (item=curl)
ok: [192.168.122.201] => (item=software-properties-common)
ok: [192.168.122.95] => (item=software-properties-common)
ok: [192.168.122.95] => (item=python3-pip)
ok: [192.168.122.201] => (item=python3-pip)
ok: [192.168.122.95] => (item=virtualenv)
ok: [192.168.122.201] => (item=virtualenv)
ok: [192.168.122.201] => (item=python3-setuptools)
ok: [192.168.122.95] => (item=python3-setuptools)

TASK [add docker gpg apt key] *******************************************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [add docker repository] ********************************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [update apt and install docker-ce] *********************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [install docker module for python] *********************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [pull default image] ***********************************************************************
fatal: [192.168.122.201]: FAILED! => {"changed": false, "msg": "Error connecting: Error while fetching server API version: request() got an unexpected keyword argument 'chunked'"}
fatal: [192.168.122.95]: FAILED! => {"changed": false, "msg": "Error connecting: Error while fetching server API version: request() got an unexpected keyword argument 'chunked'"}

PLAY RECAP **************************************************************************************
192.168.122.201            : ok=7    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
192.168.122.95             : ok=7    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   

##################################
---
- hosts: all 
  become: true 
  tasks: 
   - name: ensure repository key is installed 
     apt_key: 
       url:  https://download.docker.com/linux/ubuntu/gpg
       state: present
   - name: ensure docker registry is  available 
     apt_repository:      
         repo: 'deb https://download.docker.com/linux/ubuntu bionic stable'
         state: present
   - name: ensure docker and dependencies are installed 
     apt: 
       name: docker-ce 
       update_cache: yes 

   - service: 
      name: docker 
      state: restarted 
SUCCESS: = 
PLAY [all] **************************************************************************************

TASK [Gathering Facts] **************************************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [ensure repository key is installed] *******************************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [ensure docker registry is  available] *****************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [ensure docker and dependencies are installed] *********************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [service] **********************************************************************************
changed: [192.168.122.95]
changed: [192.168.122.201]

PLAY RECAP **************************************************************************************
192.168.122.201            : ok=5    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.95             : ok=5    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0         

---------------
---
- hosts: all
  tasks:
  - name: Install prerequisites for Docker repository
    apt:
      name: ['apt-transport-https', 'ca-certificates', 'curl', 'gnupg2', 'software-properties-common']
      update_cache: yes

  - name: Add Docker GPG key
    apt_key:
      url: https://download.docker.com/linux/ubuntu/gpg

  - name: Add Docker APT repository
    apt_repository:
      repo: deb [arch=amd64] https://download.docker.com/{{ ansible_system | lower }}/{{ ansible_distribution | lower }} {{ ansible_distribution_release }} stable

  - name: Install Docker CE
    apt:
      name: ['docker-ce', 'docker-ce-cli', 'containerd.io']
      update_cache: yes

  - name: Install prerequisites for docker-compose
    apt:
      name: ['python3-pip', 'python3-setuptools', 'virtualenv']

  - name: Install docker-compose
    pip:
      name: docker-compose
============================
- hosts: all
  tasks:
  - name: Add Docker GPG key
    apt_key: url=https://download.docker.com/linux/ubuntu/gpg

  - name: Add Docker APT repository
    apt_repository:
      repo: deb [arch=amd64] https://download.docker.com/linux/ubuntu {{ansible_distribution_release}} stable

  - name: Install list of packages
    apt:
      name: "{{ item }}"
      state: installed
      update_cache: yes
    with_items:
      - apt-transport-https
      - ca-certificates
      - curl
      - software-properties-common
      - docker-ce
==============
---
- hosts: all
  become: true
  vars:
    container_count: 4
    default_container_name: docker
    default_container_image: ubuntu
    default_container_command: sleep 1d

  tasks:
    - name: Install aptitude
      apt:
        name: aptitude
        state: latest
        update_cache: true

    - name: Install required system packages
      apt:
        pkg:
          - apt-transport-https
          - ca-certificates
          - curl
          - software-properties-common
          - python3-pip
          - virtualenv
          - python3-setuptools
        state: latest
        update_cache: true

    - name: Add Docker GPG apt Key
      apt_key:
        url: https://download.docker.com/linux/ubuntu/gpg
        state: present

    - name: Add Docker Repository
      apt_repository:
        repo: deb https://download.docker.com/linux/ubuntu focal stable
        state: present

    - name: Update apt and install docker-ce
      apt:
        name: docker-ce
        state: latest
        update_cache: true

    - name: Install Docker Module for Python
      pip:
        name: docker

    - name: Pull default Docker image
      community.docker.docker_image:
        name: "{{ default_container_image }}"
        source: pull

    - name: Create default containers
      community.docker.docker_container:
        name: "{{ default_container_name }}{{ item }}"
        image: "{{ default_container_image }}"
        command: "{{ default_container_command }}"
        state: present
      with_sequence: count={{ container_count }}

 ANSWER=: 
 root@ff82f26369fe:~/playbooks# ansible-playbook docker.yaml 

PLAY [all] **************************************************************************************

TASK [Gathering Facts] **************************************************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [Install aptitude] *************************************************************************
fatal: [192.168.122.201]: FAILED! => {"changed": false, "msg": "E:Conflicting values set for option Signed-By regarding source https://download.docker.com/linux/ubuntu/ focal: /etc/apt/keyrings/docker.gpg != , E:The list of sources could not be read."}
ok: [192.168.122.95]

TASK [Install required system packages] *********************************************************
ok: [192.168.122.95]

TASK [Add Docker GPG apt Key] *******************************************************************
ok: [192.168.122.95]

TASK [Add Docker Repository] ********************************************************************
ok: [192.168.122.95]

TASK [Update apt and install docker-ce] *********************************************************
ok: [192.168.122.95]

TASK [Install Docker Module for Python] *********************************************************
ok: [192.168.122.95]

TASK [Pull default Docker image] ****************************************************************
fatal: [192.168.122.95]: FAILED! => {"changed": false, "msg": "Error connecting: Error while fetching server API version: request() got an unexpected keyword argument 'chunked'"}

PLAY RECAP **************************************************************************************
192.168.122.201            : ok=1    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
192.168.122.95             : ok=7    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   

https://www.digitalocean.com/community/tutorials/how-to-use-ansible-to-install-and-set-up-docker-on-ubuntu-20-04
#################################################################
Conflicting values set for option Signed-By regarding source https://download.docker.com/linux/ubuntu/ focal: /etc/apt/keyrings/docker.gpg != 

https://unix.stackexchange.com/questions/732030/conflicting-values-set-for-option-signed-by-regarding-source

root@s3:~# apt update -y 
E: Conflicting values set for option Signed-By regarding source https://download.docker.com/linux/ubuntu/ focal: /etc/apt/keyrings/docker.gpg != 
E: The list of sources could not be read.
root@s3:~# sudo chmod a+r /etc/apt/keyrings/docker.gpg
root@s3:~# cd /etc/apt/keyrings
root@s3:/etc/apt/keyrings# ls
docker.gpg
root@s3:/etc/apt/keyrings# rm -rf * 
root@s3:/etc/apt/keyrings# sudo rm /etc/apt/keyrings/docker.gpg
rm: cannot remove '/etc/apt/keyrings/docker.gpg': No such file or directory
root@s3:/etc/apt/keyrings# sudo rm /etc/apt/sources.list.d/docker.list
root@s3:/etc/apt/keyrings# cd /etc/apt/sources.list.d
root@s3:/etc/apt/sources.list.d# ll
total 20
drwxr-xr-x 2 root root 4096 May  6 17:53 ./
drwxr-xr-x 8 root root 4096 May  6 15:47 ../
-rw-r--r-- 1 root root  132 May  6 15:47 ansible-ubuntu-ansible-focal.list
-rw-r--r-- 1 root root  110 Apr 24 12:51 docker.list.save
-rw-r--r-- 1 root root  117 May  6 15:47 download_docker_com_linux_ubuntu.list
root@s3:/etc/apt/sources.list.d# sudo rm download_docker_com_linux_ubuntu.list download_docker_com_linux_ubuntu.list.save
rm: cannot remove 'download_docker_com_linux_ubuntu.list.save': No such file or directory
root@s3:/etc/apt/sources.list.d# rm -rf docker.list.save 
root@s3:/etc/apt/sources.list.d# rm -rf ansible-ubuntu-ansible-focal.list ansible-ubuntu-ansible-focal.list ansible-ubuntu-ansible-focal.list ansible-ubuntu-ansible-focal.list ^C
root@s3:/etc/apt/sources.list.d# 
root@s3:/etc/apt/sources.list.d# 
root@s3:/etc/apt/sources.list.d# ll

$ sudo chmod a+r /etc/apt/keyrings/docker.gpg
$ cd /etc/apt/keyrings
$ ls -la

sudo rm /etc/apt/keyrings/docker.gpg
sudo rm /etc/apt/sources.list.d/docker.list

And if someone keeps getting the same error then it should be removed the following files:

cd /etc/apt/sources.list.d
sudo rm download_docker_com_linux_ubuntu.list download_docker_com_linux_ubuntu.list.save
##########################################################  ANSIBLE PLAYBOOK FOR  DOCKER INSTALLATION SUCCESS  DOCKER INSTALLATION USING ANSIBLE PLAYBOOK SUCCESS 

https://medium.com/@pierangelo1982/install-docker-with-ansible-d078ad7b0a54

---
- hosts: all
  become: true
  tasks:
  - name: installa dipendenze
    apt:
        name: "{{item}}"
        state: present
        update_cache: yes
    loop:
        - apt-transport-https
        - ca-certificates
        - curl
        - gnupg-agent
        - software-properties-common
  - name: aggiungi chiave GPG
    apt_key:
        url: https://download.docker.com/linux/ubuntu/gpg
        state: present
  - name: aggiungi repository docker
    apt_repository:
        repo: deb https://download.docker.com/linux/ubuntu bionic stable
        state: present
  - name: installa docker
    apt:
        name: "{{item}}"
        state: latest
        update_cache: yes
    loop:
        - docker-ce
        - docker-ce-cli
        - containerd.io
  - name: assicurati che docker sia attivo
    service:
        name: docker
        state: started
        enabled: yes
  handlers:
    - name: restart docker
      service: 
        name: docker 
        state: restarted
SUCCESS :== 

PLAY [all] **************************************************************************************

TASK [Gathering Facts] **************************************************************************
ok: [192.168.122.201]
ok: [192.168.122.95]

TASK [installa dipendenze] **********************************************************************
ok: [192.168.122.95] => (item=apt-transport-https)
ok: [192.168.122.201] => (item=apt-transport-https)
ok: [192.168.122.201] => (item=ca-certificates)
ok: [192.168.122.95] => (item=ca-certificates)
ok: [192.168.122.201] => (item=curl)
ok: [192.168.122.95] => (item=curl)
ok: [192.168.122.95] => (item=gnupg-agent)
changed: [192.168.122.201] => (item=gnupg-agent)
ok: [192.168.122.95] => (item=software-properties-common)
ok: [192.168.122.201] => (item=software-properties-common)

TASK [aggiungi chiave GPG] **********************************************************************
ok: [192.168.122.95]
ok: [192.168.122.201]

TASK [aggiungi repository docker] ***************************************************************
ok: [192.168.122.95]
changed: [192.168.122.201]

TASK [installa docker] **************************************************************************
ok: [192.168.122.95] => (item=docker-ce)
ok: [192.168.122.201] => (item=docker-ce)
ok: [192.168.122.95] => (item=docker-ce-cli)
ok: [192.168.122.201] => (item=docker-ce-cli)
ok: [192.168.122.95] => (item=containerd.io)
changed: [192.168.122.201] => (item=containerd.io)

TASK [assicurati che docker sia attivo] *********************************************************
ok: [192.168.122.95]
changed: [192.168.122.201]

PLAY RECAP **************************************************************************************
192.168.122.201            : ok=6    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
192.168.122.95             : ok=6    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
#####################################################################################
https://www.techcrumble.net/2019/12/how-install-docker-and-docker-compose-using-ansible-playbooks/

- hosts: all
  become: yes
  gather_facts: false
  tasks:
  - name: Install docker packages
    remote_user: ubuntu
    apt:
      name: "{{ item }}"
      state: present
      update_cache: yes
    with_items:
      - apt-transport-https
      - ca-certificates
      - curl
      - software-properties-common
    tags:
      - docker
  - name: Add Docker s official GPG key
    remote_user: ubuntu
    apt_key:
      url: https://download.docker.com/linux/ubuntu/gpg
      state: present
    tags:
      - docker
  - name: Verify that we have the key with the fingerprint
    remote_user: ubuntu
    apt_key:
      id: 0EBFCD88
      state: present
    tags:
      - docker
  - name: Set up the stable repository
    remote_user: ubuntu
    apt_repository:
      repo: deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable
      state: present
      update_cache: yes
    tags:
      - docker
  - name: Update apt packages
    remote_user: ubuntu
    apt:
      update_cache: yes
    tags:
      - docker
  - name: Install docker
    remote_user: ubuntu
    apt:
      name: docker-ce
      state: present
      update_cache: yes
    #notify: Start docker on boot
    tags:
      - docker
  - name: Add remote "ubuntu" user to "docker" group
    remote_user: ubuntu
    user:
      name: "ubuntu"
      group: "docker"
      append: yes
    tags:
      - docker
  - name: Install docker-compose
    remote_user: ubuntu
    get_url: 
      url : https://github.com/docker/compose/releases/download/1.25.1-rc1/docker-compose-Linux-x86_64
      dest: /usr/local/bin/docker-compose
      mode: 'u+x,g+x'
#######################################################
https://alexhernandez.info/articles/infrastructure/how-to-install-docker-using-ansible/
---
- hosts: all
  remote_user: ubuntu
  become: true
  tasks:
    - name: install dependencies
      apt:
        name: "{{item}}"
        state: present
        update_cache: yes
      loop:
        - apt-transport-https
        - ca-certificates
        - curl
        - gnupg-agent
        - software-properties-common
    - name: add GPG key
      apt_key:
        url: https://download.docker.com/linux/ubuntu/gpg
        state: present
    - name: add docker repository to apt
      apt_repository:
        repo: deb https://download.docker.com/linux/ubuntu bionic stable
        state: present
    - name: install docker
      apt:
        name: "{{item}}"
        state: latest
        update_cache: yes
      loop:
        - docker-ce
        - docker-ce-cli
        - containerd.io
    - name: check docker is active
      service:
        name: docker
        state: started
        enabled: yes
    - name: Ensure group "docker" exists
      ansible.builtin.group:
        name: docker
        state: present
    - name: adding ubuntu to docker group
      user:
        name: ubuntu
        groups: docker
        append: yes
    - name: Install docker-compose
      get_url:
        url: https://github.com/docker/compose/releases/download/1.29.2/docker-compose-Linux-x86_64
        dest: /usr/local/bin/docker-compose
        mode: 'u+x,g+x'
    - name: Change file ownership, group and permissions
      ansible.builtin.file:
        path: /usr/local/bin/docker-compose
        owner: ubuntu
        group: ubuntu
#######################################
https://dzone.com/articles/install-and-setup-docker-using-ansible-on-ubuntu-1
https://www.digitalocean.com/community/tutorials/how-to-create-a-kubernetes-cluster-using-kubeadm-on-ubuntu-20-04
https://www.linuxsysadmins.com/install-kubernetes-cluster-with-ansible/
https://digitalvarys.com/install-kubernetes-cluster-with-kubeadm-and-ansible-ubuntu/

https://www.arubacloud.com/tutorial/how-to-create-kubernetes-cluster-with-kubeadm-and-ansible-ubuntu-20-04.aspx





https://man7.org/linux/man-pages/man8/visudo.8.html
       line: 'k8  ALL=(ALL:ALL)  NOPASSWD: ALL'
        validate: 'visudo -cf %s'

     The options are as follows:

     -c, --check
                 Enable check-only mode.  The existing sudoers file (and
                 any other files it includes) will be checked for syntax
                 errors.  If the path to the sudoers file was not
                 specified, visudo will also check the file ownership
                 and permissions (see the -O and -P options).  A message
                 will be printed to the standard output describing the
                 status of sudoers unless the -q option was specified.
                 If the check completes successfully, visudo will exit
                 with a value of 0.  If an error is encountered, visudo
                 will exit with a value of 1.

     -f sudoers, --file=sudoers
                 Specify an alternate sudoers file location, see below.
                 As of version 1.8.27, the sudoers path can be specified
                 without using the -f option.

     -h, --help  Display a short help message to the standard output and
                 exit.

     -I, --no-includes
                 Disable the editing of include files unless there is a
                 pre-existing syntax error.  By default, visudo will
                 edit the main sudoers file and any files included via
                 @include or #include directives.  Files included via
                 @includedir or #includedir are never edited unless they
                 contain a syntax error.

     -O, --owner
                 Enforce the default ownership (user and group) of the
                 sudoers file.  In edit mode, the owner of the edited
                 file will be set to the default.  In check mode (-c),
                 an error will be reported if the owner is incorrect.
                 This option is enabled by default if the sudoers file
                 was not specified.

     -P, --perms
                 Enforce the default permissions (mode) of the sudoers
                 file.  In edit mode, the permissions of the edited file
                 will be set to the default.  In check mode (-c), an
                 error will be reported if the file permissions are
                 incorrect.  This option is enabled by default if the
                 sudoers file was not specified.

     -q, --quiet
                 Enable quiet mode.  In this mode details about syntax
                 errors are not printed.  This option is only useful
                 when combined with the -c option.

     -s, --strict
                 Enable strict checking of the sudoers file.  If an
                 alias is referenced but not actually defined or if
                 there is a cycle in an alias, visudo will consider this
                 a syntax error.  It is not possible to differentiate
                 between an alias and a host name or user name that
                 consists solely of uppercase letters, digits, and the
                 underscore (â€˜_â€™) character.

     -V, --version
                 Print the visudo and sudoers grammar versions and exit.

     A sudoers file may be specified instead of the default,
     /etc/sudoers.  The temporary file used is the specified sudoers
     file with â€œ.tmpâ€ appended to it.  In check-only mode only, â€˜-â€™ may
     be used to indicate that sudoers will be read from the standard
     input.  Because the policy is evaluated in its entirety, it is not
     sufficient to check an individual sudoers include file for syntax
     errors.

FILES         top

     /etc/sudo.conf            Sudo front-end configuration

     /etc/sudoers              List of who can run what

     /etc/sudoers.tmp          Default temporary file used by visudo
---
- hosts: all 
  become: yes 
  tasks: 
    - name: create k8 user 
      user: 
        name: k8
        group: root
        append: yes
        state: present 
        createhome: yes
        shell: /bin/bash

    - name: allow k8 to have password less sudo 
      lineinfile: 
        line: 'k8  ALL=(ALL:ALL)  NOPASSWD: ALL'
        validate: 'visudo -cf %s'
        
    - name: set up authorized keys for the k8 user 
      authorized_key: 
        user: k8 
        key: "{{ item }}"
      with_file: 
        - ~/.ssh/id_rsa.pub

fatal: [192.168.122.95]: FAILED! => {"cache_update_time": 1683443290, "cache_updated": true, "changed": false, "msg": "'/usr/bin/apt-get -y -o \"Dpkg::Options::=--force-confdef\" -o \"Dpkg::Options::=--force-confold\"       install 'docker.io=20.10.21-0ubuntu1~20.04.2'' failed: E: Unable to correct problems, you have held broken packages.\n", "rc": 100, "stderr": "E: Unable to correct problems, you have held broken packages.\n", "stderr_lines": ["E: Unable to correct problems, you have held broken packages."], "stdout": "Reading package lists...\nBuilding dependency tree...\nReading state information...\nSome packages could not be installed. This may mean that you have\nrequested an impossible situation or if you are using the unstable\ndistribution that some required packages have not yet been created\nor been moved out of Incoming.\nThe following information may help to resolve the situation:\n\nThe following packages have unmet dependencies:\n docker.io : Depends: containerd (>= 1.2.6-0ubuntu1~)\n", "stdout_lines": ["Reading package lists...", "Building dependency tree...", "Reading state information...", "Some packages could not be installed. This may mean that you have", "requested an impossible situation or if you are using the unstable", "distribution that some required packages have not yet been created", "or been moved out of Incoming.", "The following information may help to resolve the situation:", "", "The following packages have unmet dependencies:", " docker.io : Depends: containerd (>= 1.2.6-0ubuntu1~)"]}

N: Skipping acquire of configured file 'stable/binary-i386/Packages' as repository 'https://download.docker.com/linux/ubuntu focal InRelease' doesn't support architecture 'i386'
root@s3:~# cd /etc/apt/sources.list.d/
root@s3:/etc/apt/sources.list.d# ll
total 20
drwxr-xr-x 2 root root 4096 May  6 15:47 ./
drwxr-xr-x 8 root root 4096 May  6 15:47 ../
-rw-r--r-- 1 root root  132 May  6 15:47 ansible-ubuntu-ansible-focal.list
-rw-r--r-- 1 root root  132 Apr 28 14:20 ansible-ubuntu-ansible-focal.list.save
-rw-r--r-- 1 root root  117 May  6 15:47 download_docker_com_linux_ubuntu.list
root@s3:/etc/apt/sources.list.d# rm -rf download_docker_com_linux_ubuntu.list 
############################################################################################
How To Create a Kubernetes Cluster Using Kubeadm on Ubuntu 20.04


---
- hosts: all 
  become: yes 
  tasks: 
    - name: create docker config directory 
      file: 
        path:  /etc/docker
        state: directory

    - name: changing the docker to systemd driver 
      copy: 
        dest: "/etc/docker/daemon.json"    
        content: |
          {
          "exec-opts": ["native.cgroupdriver=systemd"]
           }
    - name: install docker 
      apt: 
        name: docker.io 
        state: present 
        update_cache: true

    - name: install apt transport https 
      apt:   
       name: apt-transport-https 
       state: present 

    - name: add kubernetes apt-key 
      apt_key: 
        url:  https://packages.cloud.google.com/apt/doc/apt-key.gpg
        state: present

    - name: add kubernetes apt repository 
      apt_repository: 
        repo: deb http://apt.kubernetes.io/ kubernetes-xenial main 
        state: present 
        filename:  'kubernetes'

    - name: install kubelet 
      apt: 
        name: kubelet=1.25.4-00
        state: present
        update_cache: true 

    - name: install kubeadm 
      apt: 
        name: kubeadm=1.25.4-00
        state: present
    - name: install kubectl 
      apt: 
        name: kubectl=1.25.4-00
        state: present
        force: yes

root@e5c6fe74a4c3:~/playbooks# ansible-playbook k8.yaml 

PLAY [all] ***********************************************************************************************************

TASK [Gathering Facts] ***********************************************************************************************
ok: [192.168.122.95]

TASK [create docker config directory] ********************************************************************************
ok: [192.168.122.95]

TASK [changing the docker to systemd driver] *************************************************************************
ok: [192.168.122.95]

TASK [install docker] ************************************************************************************************
changed: [192.168.122.95]

TASK [install apt transport https] ***********************************************************************************
ok: [192.168.122.95]

TASK [add kubernetes apt-key] ****************************************************************************************
changed: [192.168.122.95]

TASK [add kubernetes apt repository] *********************************************************************************
changed: [192.168.122.95]

TASK [install kubelet] ***********************************************************************************************
changed: [192.168.122.95]

TASK [install kubeadm] ***********************************************************************************************
changed: [192.168.122.95]

TASK [install kubectl] ***********************************************************************************************
changed: [192.168.122.95]

PLAY RECAP ***********************************************************************************************************
192.168.122.95             : ok=10   changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
#######################################################








        


     









































       



  



















  









 



































































                      




















      
              

































































  







 




























